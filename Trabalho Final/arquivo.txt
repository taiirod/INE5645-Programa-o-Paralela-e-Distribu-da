1
The Reasonable Robot: Artificial Intelligence and the Law
by
Professor Ryan Abbott
Submitted for the Degree of Doctor of Philosophy
School of Law
Faculty of Arts and Social Sciences University of Surrey
Supervisors:
Dr Arman Sarvarian Dr Christopher Taggart
©Ryan Abbott, 2020
 
2
Acknowledgements
Thanks to my principal supervisor Dr Arman Sarvarian for his support and guidance, including in navigating a higher educational institution in England as well as a course-of-study novel to the law school in institutional memory.
Thanks to my secondary supervisor, Dr Christopher Taggart, for his generous availability, constant positivity, and insightful comments.
Thanks to my former head of school Professor Veronica Rodriguez-Blanco, and to my current head of school Professor Alexander Sarch, for creating an environment conducive to completing this thesis.
Thanks to Charlotte, Ryan, and Theodora for substantially increasing the difficulty level of this thesis in the most adorable way possible.
Finally, thanks most of all to my wife—for tolerating yet one more degree.
 
3
Statement of Originality
 I confirm that the submitted work is my own work and that I have clearly identified and fully acknowledged all material that is entitled to be attributed to others (whether published or unpublished) using the referencing system set out in the programme handbook. I agree that the University may submit my work as a means of checking this. I confirm that I understand that assessed work that has been shown to have been plagiarised will be penalised.
Guildford, 24 Jan 2020
Ryan Abbott
 
4
Abstract
   This thesis argues in favor of a novel principle for AI regulation: AI legal neutrality. Namely, that the law should not discriminate between activity by people and activity by AI when people and AI are performing the same tasks. This will reduce market distortions and help to ensure that decisions are made on the basis of efficiency. Efficiency is not the only principle that should guide AI regulation, but not discriminating between people and AI will tend to improve human well-being.
We do not currently have a neutral legal system as between human and AI activity. An AI that is significantly safer than a person may be the best choice for driving a vehicle, but existing laws may prohibit driverless vehicles. A person may be a better choice for packing boxes at a warehouse, but a business may automate because AI receives preferential tax treatment. AI may be better at generating certain types of innovation, but businesses may not want to use AI if this restricts future intellectual property rights. In all of these instances, neutral legal treatment would ultimately benefit society as a whole.
As AI increasingly steps into someone’s shoes, it will need to be treated more like a person. Sometimes more importantly, as AI is incorporated into our legal standards, people will need to be treated more like AI. In many areas of the law, standards are set by human behavior—the degree of care expected of a driver, the quantum of creativity required to protect a new invention, even the state of mind required for criminal punishment. As AI takes people out-of- the-loop and comes to be the normal way that tasks are performed, the AI’s behavior should set our benchmarks. AI is not part of our moral community, but it should be part of our legal community.
      
5
Table of Contents
VOL I
I. Introduction...................................................................................... 6
A. Research Context ....................................................................... 6
B. Literature Review ...................................................................... 7
C. My Research ........................................................................... 12
D. Research Question and Thesis Statement .......................................... 18
E. Impact .................................................................................. 20
F. Methodology .......................................................................... 23
II. Monograph...................................................................................... 25
A. The Reasonable Robot: Artificial Intelligence and the Law..................... 25
1. Introduction: Artificial Intelligence and the Law........................ 32
2. Understanding Artificial Intelligence...................................... 69
3. Should Artificial Intelligence Pay Taxes? .............................. 103
4. The Reasonable Robot..................................................... 133
5. Artificial Inventors.......................................................... 172
6. Everything is Obvious...................................................... 219
7. Punishing Artificial Intelligence.......................................... 255
8. Alternative Perspectives on AI Legal Neutrality....................... 308
VOL II
III. Articles......................................................................................... 324
A. I Think, Therefore I Invent: Creative Computers and the Future of Patent Law............................................................................................. 324
B. Hal the Inventor: Big Data and its Use by Artificial Intelligence............. 372
C. Inventive Machines: Rethinking Invention and Patentability.................. 391
D. Artificial Intelligence, Big Data and Intellectual Property: Protecting
Computer-Generated Works in the United Kingdom.................................... 398
E. Everything is Obvious............................................................... 414
F. Should Robots Pay Taxes? Tax Policy in the Age of Automation............ 465
G. The Reasonable Computer: Disrupting the Paradigm of Tort
Liability........................................................................................ 496
H. Punishing Artificial Intelligence: Legal Fiction or Science Fiction........... 541
IV. Conclusion..................................................................................... 603
A. Submission Themes.................................................................. 603
B. Areas for further research............................................................ 605
   
6
Research Context
Introduction
  Artificial intelligence (AI) is experiencing a period of unparalleled growth driven by improvements in software, computing power and big data.1 This has already had a dramatic economic and social impact; for instance, on the financial services sector and on the way that consumers interact with online content in the digital economy.2 AI now promises to radically transform industries such as health care and transportation.3 Studies on AI’s economic impact by McKinsey and PricewaterhouseCoopers suggest that AI is likely to generate trillions of pounds in value annually by 2030.4
The extent to which AI can provide these benefits depends in large part on user trust and public perceptions of AI.5 This in turn requires adequate resolution of a host of legal and ethical challenges, including those relating to consumer privacy, cybersecurity, biased data, technological unemployment, and the use of force.6 The European Parliament has recently launched a series of expert groups that have proposed making Trustworthy AI the foundation of regional legislative activity.7 Similarly, the UK Government has released an ambitious industrial strategy that seeks to make the UK the world-center for Ethical AI.8 Yet the legal
1 This thesis uses the following definition of AI: “an algorithm or machine capable of
completing tasks that would otherwise require cognition”. For a discussion of this definition and other definitions of AI, see THE REASONABLE ROBOT: ARTIFICIAL INTELLIGENCE AND THE LAW, (hereinafter “REASONABLE ROBOT”), Chapter 2, Section 2. For a discussion of the relevance of software, computing power, and big data, see REASONABLE ROBOT, Chapter 2, Section 5.
2 Id. Chapter 2, Section 5.
3 Id.
4 Id.
5 European Commission Press Release IP/18/3362, Artificial Intelligence: Commission Outlines a European Approach to Boost Investment and set Ethical Guidelines (Apr. 25, 2018), https://europa.eu/rapid/press- release_IP-18-3362_en.htm.
6 See REASONABLE ROBOT, Chapter 7.
7 European Commission Press Release IP/18/3362, Artificial Intelligence: Commission Outlines a European Approach to Boost Investment and set Ethical Guidelines (Apr. 25, 2018), https://europa.eu/rapid/press- release_IP-18-3362_en.htm.
8 HM GOVERNMENT, INDUSTRIAL STRATEGY: BUILDING A BRITAIN FIT FOR THE FUTURE, 40 (2017) (“We will lead the world in safe and ethical use of data and artificial intelligence giving confidence and clarity to citizens and business”).
 
7
framework governing AI, and for that matter new technologies generally, has historically been slow to develop due to concerns that an overly burdensome regulatory environment would slow innovation.9
Literature Review
I broadly divide the literature relevant to this thesis into two categories. The first category is literature relevant to AI within a specific legal discipline - for example, literature on how AI will impact tort or tax law. This literature is reviewed in detail within my individual publications following this introduction and to a lesser extent in the corresponding book chapters. For instance, as discussed in greater detail in my article, “I Think, Therefore I Invent”, people have been writing about the phenomenon of AI-generated works10 since at least the 1960s.11 There has been a variable but steady stream of materials published since then as advances in AI, or other technological advances in areas such as software, challenged intellectual property (IP) standards more generally and reintroduced the concept of AI- generated works to academic discourse.12 Established narratives prior to my work largely denied the desirability of attributing creative acts to AI,13 and argued it was undesirable to provide IP protection for such advances.14 With respect to the skilled person standard used to evaluate inventive step,15 existing scholarship has largely considered the impact of AI from the perspective of a tool augmenting human inventors, rather than as an entity automating the
9 REASONABLE ROBOT, Chapter 1, Section 1.
10 REASONABLE ROBOT, Chapter 1 (defining an “AI-generated work”).
11 Karl F. Milde, Jr., Can a Computer Be an “Author” or an “Inventor”? 51 J. PAT. OFF. SOC’Y 378, 379 (1969). 12 Id, 1080.
13 Pamela Samuelson, arguing against considering computers to be authors, argues that, “[o]nly those stuck in the doctrinal mud could even think that computers could be ‘authors.’” Pamela Samuelson, Allocating Ownership Rights in Computer-Generated Works, 47 U. PITT. L. REV. 1185, 1200 (1986).
14 Ralph D. Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up? 71 TUL. L. REV. 1675, 1681, 1702–03 (1997) (arguing the output of creative computers cannot and should not be protected by federal intellectual property laws and that such results enter the public domain).
15 See REASONABLE ROBOT, Chapter 5 (discussing the skilled person standard,).
  
8
inventive act itself.16 My individual chapters following this introduction contain discussions of the main findings of existing scholarship and highlight when and how my work diverges from established doctrine.
The second category relates more broadly to regulation of technology. I would further divide this into literature about 1) principles of AI regulation and 2) regulation of technology generally. In terms of AI regulatory principles, the past few years have witnessed a flurry of activity, with at least dozens of principles having been developed, published, and endorsed by governments and high-profile organizations such as OECD17 and the G20.18 These principles in turn derive from a combination of general regulatory principles and focus on some of the unique challenges posed by AI. For instance, there are particular concerns about allowing AI to make life or death decisions in military contexts without human supervision, or with minimal human supervision.19 This has even resulted in a social movement and campaigning against killer robots. This concern in turn is reflected in many of the principles of AI regulation such as the OECD’s principles for accountability and requirements to respect the rule of law. To my knowledge, none of these principles have included a principle like AI legal neutrality - the principle that the law should not discriminate between people and AI when they are engaged in the same activities. To the contrary, other principles of AI regulation tend to focus on approaches such as maintaining the supremacy of human agency and ensuring that people are kept “in-the-loop” or “on-the-loop” with AI. While I believe that approach is and will be proper in many circumstances, I do not agree that it is a universally beneficial principle. There will be
16 See REASONABLE ROBOT, Chapter 5.
17 OECD Principles on AI (2019), https://www.oecd.org/going-digital/ai/principles/.
18 G20 Ministerial Statement on Trade and Digital Economy (2019), https://www.mofa.go.jp/files/000486596.pdf.
19 See, e.g., Open Letter to the Prime Minister of Canada, Call for an International Ban on the Weaponization of Artificial Intelligence (2017), https://techlaw.uottawa.ca/bankillerai#letter.
 
9
times where people underperform compared to AI, and where having a person in- or on-the- loop will produce negative outcomes.
With respect to more general principles of regulation, my thesis shares the most similarity with a principle of technology neutrality – the idea that laws should regulate behaviour rather than technology.20 This is a widely adopted policy by academics and lawmakers,21 albeit with variable application.22 It has been perhaps most prominently applied in the context of IP law, for example, with the 1976 Copyright Act in the U.S. which attempted to make copyright law neutral in the face of disruptive technologies such as cable transmission.23 Like AI legal neutrality, technologically neutral laws are more concerned with behaviour itself rather than how that behaviour occurs. This is done to promote the longevity of regulations – so that laws are more adaptable to technological advances.24 It also promotes fairness by reducing discrimination against a given technology by virtue of some factor other than how it behaves.25 In the 1976 Copyright Act, a work is protected by copyright so long as it is fixed in any technology someone can perceive regardless of whether Congress mentioned it specifically.26 Both AI legal and technological neutrality share the aims of simplifying and future-proofing
20 See Bert-Japp Koops, Should ICT Regulation Be Technology-Neutral? in STARTING POINTS FOR ICT REGULATION: DECONSTRUCTING PREVALENT POLICY ONE-LINERS 77 (Bert-Jaap Koops, Miriam Lips, Corien Prins & Maurice Schellekens eds., 2006).
21 See, e.g., Carys J. Craig, Technological Neutrality: (Pre)Serving the Purposes of Copyright Law, in THE COPYRIGHT PENTALOGY: HOW THE SUPREME COURT OF CANADA SHOOK THE FOUNDATIONS OF CANADIAN COPYRIGHT 271, 272-73 (Geist ed., 2013) (analying how technological neutrality has been favored under Canadian law in the copyright context); OECD, Council Recommendation on Principles for Internet Policy Making (2011), https://www.oecd.org/sti/ieconomy/49258588.pdf (noting technological neutrality is a key feature of Internet regulation).
22 Winston Maxwell and Marc Bourreau, Technology Neutrality in Internet, Telecoms and Data Protection Regulation 1 C.T.L.R. 1 (2015).
23 Pub. L. No. 94-553, 90 Stat. 2541 (1976); Melville B. Nimmer and David Nimmer, NIMMER ON COPYRIGHT § 12A.16(b) (1963) (arguing technology neutrality is a “unifying theme” of the 1976 Copyright Act).
24 Bert-Japp Koops, Should ICT Regulation Be Technology-Neutral? in STARTING POINTS FOR ICT REGULATION: DECONSTRUCTING PREVALENT POLICY ONE-LINERS 77 (Bert-Jaap Koops, Miriam Lips, Corien Prins & Maurice Schellekens eds., 2006).
25 Paul Ohm, The Argument Against Technology-Neutral Surveillance Laws, 88 TEX. L. REV. 1685, 1691–2 (2010).
26 17 U.S.C. § 102(a).
 
10
the law and reducing inefficient legal distinctions, and the body of literature related to technological neutrality has relevance to AI legal neutrality.27
I address some of the criticisms levied at technological neutrality in the context of AI legal neutrality in final chapter of my book, such as technology not being inherently neutral but rather reflecting particular political, social, or historical values.28 Technological neutrality differs from AI legal neutrality, among other reasons, because technological neutrality is about treating different types of technologies the same way as between one another, whereas AI legal neutrality focuses on comparing people to technology. For instance, technological neutrality might be concerned with one medium of broadcasting which is more efficient than a pre- existing standard, whereas AI legal neutrality might be concerned with comparing a person’s behaviour to a self-driving car.
Professor Brad Greenberg criticizes technological neutrality on several grounds.29 First, that is it difficult to predict new technologies and the costs of applying existing laws to these technologies. He also criticizes technologically neutral laws for being difficult to interpret due to inherent vagueness, for being infrequently updated, and for protectionist treatment of incumbent technologies. Professor Greenberg ultimately argues, in the copyright context, in favour of replacing technological neutrality with a general right to commercialize protected works within different categories of technology.30
27 See, e.g., Orin S. Kerr, Applying the Fourth Amendment to the Internet: A General Approach, 62 STAN. L. REV. 1005, 1015 17 (2010) (arguing the Fourth Amendment has been applied in a technologically neutral manner).
28 Jesús Romero Moñivas, The Problem and Socio-Political Confusion of ‘Technological Neutrality’: The Case of the Observatorio de Neutralidad Tecnológica in Spain. CURRENT SOCIOLOGY 59(3) 310–327; Chris Reed, Taking Sides on Technology Neutrality, 4 SCRIP T-ed 263, 265 (2007).
29
30 Id.
  Brad A. Greenberg, Rethinking Technology Neutrality, 100 MINN L. REV. 1495 (2016) (providing a critical
 review of technological neutrality).

11
Dr. Carys Craig has argued for an expanded understanding of technological neutrality.31 She argues slavish adherence to not distinguishing between technologies is more akin to “technological blindness,” and that technological neutrality should be seen as a “normative quest.”32 In other words, it is not that the law should be independent of any particular technology, but that the law should change as technology develops to achieve normative equibrium.33 In the face of technological development, the law should continue to foster socially beneficial values.
Other scholars have dealt more broadly with some of the social, philosophical, and economic issues associated with AI also covered in this thesis. However, these works have not focused predominantly on legal implications of technological advances, and none have advanced something like a principle of AI legal neutrality. For example, in The Singularity is Near (2005), as well as in some of his other works, Ray Kurzweil predicts that people will transcend their biological limitations through harnessing technology to usher in a new age of human progress.34 More recently, Max Tegmark has considered a similar phenomenon and argued that we are entering a time known as Life 3.0 as organisms emerge able to upgrade their own hardware and software.35 He similarly grapples with many of the implications of a world in which machines and machine-augmented individuals have superhuman performance. In Superintelligence: Paths, Dangers, Strategies,36 Nick Bostrom considers how people have historically used their relatively superior cognitive abilities to achieve dominance, and how people may prosper once machine superintelligence exists. Other works, such as The Glass
31 Carys J. Craig, Technological Neutrality: Recalibrating Copyright in the Information Age, 17 Theoretical Inquiries L. 601 (2016).
32 Id, at 605
33 Id.
34 Ray
35 Max Tegmark, 36 Nick
   Kurzweil, THE SINGULARITY IS NEAR: WHEN HUMANS TRANSCEND BIOLOGY (2006).
 LIFE 3.0 : BEING HUMAN IN THE AGE OF ARTIFICIAL INTELLIGENCE (2017).
 Bostrom, SUPERINTELLIGENCE: PATHS, DANGERS, STRATEGIES (2014).

12
Cage: Where Automation is Taking Us,37 and The Rise of the Robots: Technology and the Threat of Mass Unemployment,38 consider the phenomenon of automation, and how this will change the nature of work and employment, as well as how people learn and solve problem. Finally, other titles such as Robot Ethics: The Ethical and Social Implications of Robotics39 and Robot Law40 consider a variety of topics in ethics and law relevant to AI and physically embodied AI.
My Research
Since joining the University of Surrey as Professor of Law and Health Sciences in May 2016, the primary focus of my research has been on the legal implications of advances in AI. I have examined how advances in AI should change how the law is applied and developed in variety of legal areas—tax, tort, IP, and criminal—and how the law should change AI development. My analysis has focused primarily on US law, with comparative references mainly to UK (England and Wales) law, but I argue that my thesis is relevant to all legal systems and areas of law. Prior to this work, my research focused on IP as well as health law and policy.
My first work in “AI law” was a book chapter (‘Hal the Inventor: Big Data and its Use by Artificial Intelligence’)41 which I worked on around the same time as my law review article in the Boston College Law Review (‘I Think, Therefore I Invent: Creative Machines and the Future of Patent Law.’)42 These works came about as a result of the research I was doing in both IP and life sciences. I was teaching my patent law class the case law on inventorship,
37 Nicholas Carr, THE GLASS CAGE: AUTOMATION AND US (2014).
38 Martin Ford, THE RISE OF THE ROBOTS: TECHNOLOGY AND THE THREAT OF MASS UNEMPLOYMENT (2015).
39 ROBOT ETHICS: THE ETHICAL AND SOCIAL IMPLICATIONS OF ROBOTICS (Patrick Lin, Keith Abney, and George
A. Bekey, eds., 2012). 40
41 Ryan Abbott, Hal the Inventor: Big Data and its Use by Artificial Intelligence in BIG DATA IS NOT A MONOLITH (Hamid Ekbia et al eds., 2016).
42 Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079 (2016).
   ROBOT LAW (
 Ryan Calo, A. M. Froomkin and Ian Kerr eds., 2016).

13
while at the same time I was reading material about work being done by AI in the life sciences that resembled work done by human patent inventors. This led me to wonder whether any existing scholarship had considered issues such as whether an AI could be an inventor for purposes of patent law. I subsequently discovered people had been writing about AI inventorship academically for decades.
The prevailing narrative when I started working in this area held that AI-generated inventions (those made without a traditional human inventor) should not be protected by IP rights (IPRs) because the prospect of a patent could not incentivize an AI to invent. Thus, because patent law was primarily economically concerned with incentivizing innovation it did not justify IPRs for AI-generated inventions. Also, patent law protects the moral rights of inventors, and the prevailing narrative similarly held that there was no benefit to protecting the moral rights of a machine.
I was the first to argue that the ability to receive patents would incentivize innovation by motivating those building, using, and owning AI.43 I further argue that the AI should be listed as the inventor when it is functionally inventing, and that the AI’s owner should own any resultant IPRs. This article was received positively enough that it was solicited for republication in Landslide, the IP magazine of the American Bar Association’s IP chapter, and in Mitteilungen, a leading journal for German patent attorneys.
I further developed my arguments in favour of IPRs for AI-generated inventions as a result of a book chapter I was invited to contribute to an Edward Elgar handbook by Tanya Aplin, who is Professor of Intellectual Property Law at King’s College.44 She suggested I consider the
43 Id.
44 Ryan Abbott, Artificial Intelligence, Big Data and Intellectual Property: Protecting Computer-Generated Works in the United Kingdom in RESEARCH HANDBOOK ON INTELLECTUAL PROPERTY AND DIGITAL TECHNOLOGIES (Tanya Aplin ed., forthcoming Jan. 2020).
  
14
issues from a UK and an EU perspective. In doing so, I further argued that listing an AI as an inventor when it is functionally inventing is important to preserve the integrity of the patent system by maintaining accuracy of inventorship and informing the public of who the actual devisor of an invention is, and by preserving the moral rights of inventors. That is because while taking credit for an AI’s work would not be unfair to an AI, it would be unfair to other human inventors. It would equate legitimately inventive work with someone merely asking an AI to solve a problem.
It was also useful to consider the phenomenon in the context of different legal systems. The UK (England and Wales) is a good comparative example with the US because while neither has an established legal framework for patents and AI-generated inventions, they have different systems with respect to copyright and AI-generated works. The UK allows these to be protected by statute, whereas the US Copyright Office has a policy of prohibiting copyright protection for AI-generated works. Given that two polar opposite legal systems have been in place since at least 1988, it provides useful insight into the costs and benefits of one system versus the other. Although, perhaps because there are no requirements to register copyright in either jurisdiction (it is not even possible to register copyright in the UK), there have been a limited number of cases involving AI-generated works in either jurisdiction.
Again, neither jurisdiction has a law specifically about AI-generated works and patents, but both have laws requiring an inventor to be a natural person. I argue that this requirement was designed to ensure inventors were given credit because most patents are owned by businesses, but this was not done with AI-generated works in mind and should not operate to prohibit their subsistence in an arbitrary manner. In the US at least, it is accepted that courts can dynamically interpret statutes to best achieve their underlying policy goals. While I argue in favour of a dynamic interpretation of the law, a legislative solution may ultimately be required. As a further

15
contribution to this thesis, comments I made along these lines at a conference hosted by CEIPI were published.45
As my final included work on AI and IP, I wrote a follow up article, “Everything is Obvious,” that focused on how AI would change the standard of the “skilled person” in patent law.46 This is a standard used to judge, among other things, the inventive step or obviousness standard which requires a patent application to be nonobvious to a skilled person. The skilled person represents essentially the average worker in a scientific field. I argued that as AI increasingly augments average researchers, making them more knowledgeable and sophisticated, this should be reflected in the standard – raising the bar to obviousness. In time, as inventive AI comes to widely automate, rather than augment, inventive activity, inventive AI should come to represent the skilled person. Prior scholarship in this area had not considered an AI as a substitute for a skilled person. I argue a skilled person standard based on an inventive machine would further raise the bar to inventive step. In time, as specific artificial intelligence gives way to general artificial intelligence and then to super-intelligent AI, everything should be obvious to a sufficiently sophisticated AI. This would mean no more patents would issue, which would be fine because at that point the cost of innovation would be negligible. This article has also been republished by Landslide and in a three-part series by Mitteilungen. It was additionally republished as one of the final posts on the popular website IP Watch as well as in a book chapter in the Cambridge Handbook on Law and Algorithms.
My AI law research has also considered tort liability. In, “The Reasonable Computer: Disrupting the Paradigm of Tort Liability,” I argued that where an AI’s manufacturer can show
45 Ryan Abbott, Inventive Machines: Rethinking Invention and Patentability in INTELLECTUAL PROPERTY AND DIGITAL TRADE IN THE AGE OF ARTIFICIAL INTELLIGENCE AND BIG DATA (Xavier Seuba, Christophe Geiger and Julien Penin eds., 2018).
46 Ryan Abbott, Everything is Obvious, 66 UCLA. L. REV. 2 (2019)
   
16
that an AI is safer than a reasonable person and automating human activity which causes harm, the supplier should be liable in negligence rather than strict liability.47 This negligence test would focus on the AI’s act instead of its design, and in a sense, it would treat an AI tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, avoid many of the challenges associated with the complexity and black-box nature of AI-generated accidents, and it would continue to reward manufacturers for improving safety. More importantly, I argue that principles of harm avoidance suggest that once AIs become safer than people, human tortfeasors should no longer be measured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against AIs.
In AI and tax, together with Dr Bret Bogenschneider we wrote, “Should Robots Pay Taxes? Tax Policy in the Age of Automation.”48 This was the foundational law review article advocating for a “robot tax.” We argued that tax laws treat people and machines differently even when they are in the same role, because automation allows businesses to avoid employee and employer wage taxes levied by central and local taxing authorities. A potentially even greater tax advantage to automation may be that businesses can accelerate tax deductions for machines but not for human workers. Finally, employers also receive a variety of indirect tax incentives to automate.
Even more concerning is that robots do not pay taxes. This is a significant problem because income tax combined with employment taxes are the largest sources of revenue for the government, together accounting for the great majority of federal tax revenue. Robots do not
47 Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. 1 (2018).
48 Ryan Abbott and Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL. REV. 145 (2018).
 
17
pay income tax or require employment taxes. On top of that, they do not purchase goods and services so are not charged sales taxes, and they do not purchase or rent property, so they do not pay property taxes. Robots are simply not taxpayers, at least not to the same extent as human workers. If all work was to be automated tomorrow, most of the tax base would immediately disappear. We argue that when firms automate, the government loses revenue—potentially hundreds of billions of dollars in the aggregate. The solution is to fundamentally change the extent to which we tax capital vs. labor and workers vs. businesses. This could be done, for example, by eliminating payroll taxes and increasing the effective corporate tax rate (or property or consumption taxes).
Finally, in criminal law, Professor Alex Sarch and I wrote an article recently published in UC Davis Law Review considering direct criminal punishment of AI.49 We examine the case, as being advanced by some scholars, for direct criminal punishment of AI by evaluating whether it would be consistent with the theoretical and doctrinal limitations on criminal law, the costs and benefits of such a practice, and whether an alternate approach would be preferred. We argue that legal fictions in criminal law are sufficiently flexible to accommodate criminal punishment of AI, and that similar fictions are used to make artificial persons in the form of corporations criminally liable. However, we argue the practice would have significant costs and that any benefits would be more effectively achieved through more traditional sorts of expansions of criminal and civil law.
I had a book proposal accepted by Cambridge University Press based on my above research on AI law. The completed book has now been accepted and it is due to be released mid-2020.50 The book is essentially a thesis for a PhD-by-publication program because it gathers together
49 Ryan Abbott and Alexander Sarch, ‘Punishing Artificial Intelligence: Legal Fiction or Science Fiction’ 53 UC Davis Law Review (2019).
50 REASONABLE ROBOT.
 
18
my preexisting work and synthesizes it to find a unifying thesis. While the various projects described above have been in different legal areas, they have all examined the ways in which AI has been automating and augmenting traditionally human-centric activities and my analysis has focused on what legal changes are consequently required to best achieve the law’s underlying goals. I have found that the law often treats activity by AI and activity by people differently, and that this can have negative outcomes. My thesis is that lawmakers should embrace a novel regulatory principle – AI legal neutrality. I argue this will tend to improve human wellbeing by promoting efficiency.
The book advances and explains this thesis in the introduction chapter, which is essentially the introductory chapter of a PhD-by-publication, and then the first chapter is an introduction that provides general background information about AI. Chapters 2-6 deep dive into specific fields of the law to explore how a principle of AI legal neutrality would work in practice, and the final chapter responds to criticisms of my thesis and some of its implications. Substantively, chapters 2-6 of the book incorporate my primary AI law articles, but with changes that reflect both streamlining and the evolution of my thinking since the articles were published. I have always found it the case in the years after publishing a law review article that I would like to go back and do some major edits. The book provided such an opportunity, even where the edits were primarily a matter of framing and streamlining. As the CUP monograph is attempting to appeal to a more general audience than the law review articles, many of the citations in the articles – along with the traditional lengthy, discursive footnotes typical of US law review articles (my primary medium) – have been omitted in the book.
Research Question and Thesis Statement
I am concerned with how the law should regulate human-like activity by AI, and how the phenomenon of AI behaving like a person should change how people are regulated. My thesis
 
19
is that the law should tend not to discriminate between AI and human behavior. That is because the law should focus on conventionalist or utilitarian outcomes, and it may not matter whether an actor is human or machine with respect to the social impact of activities. I argue that more equal treatment of AI and human behavior will tend to help the law to achieve its underlying goals.
As sub-questions to my broader research question, in intellectual property, I am concerned with AI autonomously engaging in creative and inventive activity and stepping into the shoes of human authors and inventors. My thesis is that creative and inventive works autonomously made by AI should be accorded protection, that AI should be listed as an author or inventor when it otherwise meets criteria for authorship and inventorship, and that the AI’s owner should own any resultant intellectual property rights. Also, that as AI increasingly augments and eventually automates researchers, that this should raise the bar to inventive step and require the nonobviousness inquiry to focus more on economic than cognitive factors. In tax law, I am concerned with how similar work done by a person vs an AI are subject to different tax regimes, and how this will affect tax revenue. My thesis is that work by an AI is favored over work by a person, and also that automation will tend to decrease government tax revenue. I argue the solution is to reduce or eliminate labor taxes while at the same time increasing other forms of taxation to ensure revenue. In tort law, I am concerned with people and AI both engaging in activities with a risk of harm, and whether current liability standards promote or discourage automation. My thesis is that AI activity is subject to stricter liability standards which will tend to discourage automation even when it would promote safety. Also, that people should be held to the standard of AI once automation is practicable, and that this will further promote safety. In criminal law, I am concerned with AI engaging in criminal sorts of behavior in ways that does not reduce to criminal conduct by individuals. My thesis is that direct criminal liability for AI would be inappropriate, but that there will likely be a need to expand civil and possibly criminal

20
liability for individuals involved in the development and use of AI. Further, that direct criminal punishment of AI is broadly consistent with the conceptual and theoretical limitations of the criminal law, and that this provides support for punishment of anti-social behavior even without the same level of reliance on moral fault.
Impact
This body of work has already had a significant impact on the field of AI regulation. With respect to AI and IP, I was selected as one of the 50 most influential people in the world of IP in 2019 by Managing Intellectual Property Magazine. The article in which I was selected states, “Ryan Abbott is a leading academic writing about law and technology, IP and life sciences. His work on artificial intelligence (AI) and IP has contributed to the international dialogue on how new technologies are challenging existing legal standards.” 51 My AI and IP work has also been discussed in depth and played a significant substantive role in a number of law review articles.52 The BCLR article alone has been cited 79 times according to Google Scholar. Not only have academics engaged with this work, it has been relied upon outside of academia, for instance in a White Paper by the World Economic Forum’s Center for the Fourth Industrial Revolution which cited me heavily.53 In June 2019, United Kingdom Supreme Court Justice Lord Kitchin spoke about my scholarship at an event co-sponsored by the UK Intellectual Property Office and the World Intellectual Property Office (WIPO) on AI and IP.54
51 50 Most Influential People in IP (2019), https://www.managingip.com/Article/3907240/Top-50-in-IP-2019- notable-individuals.html.
52 See, e.g., W. Michael Schuster, Artificial Intelligence and Patent Ownership, 75 WASH. & LEE. L. REV. 1945 (2019).
53 ARTIFICIAL INTELLIGENCE COLLIDES WITH PATENT LAW (2018), https://www.weforum.org/whitepapers/artificial-intelligence-collides-with-patent-law.
54 UKIPO - WIPO Conference Keynote Speech Lord Kitchin, Justice of the Supreme Court (Jun., 18 2019), https://www.supremecourt.uk/docs/speech-190618.pdf (e.g., “Professor Ryan Abbott points out in one of his many interesting papers in this area that [autonomous machine invention] has been a reality for some time.”).
  
21
Perhaps to some degree in direct response to my work, but in any event due to increased public attention to AI and IP issues which I have helped to generate, the United States Patent and Trademark Office (USPTO) issued a request for comments on AI and patent law in late 2019, and then another request for comments on AI and other types of IP law.55 The USPTO has reported it is seeking these comments as a first step in developing policies for AI and IP. Although the comments touch on a wide variety of areas, they focus in large part on AI- generated inventions and AI-generated works, along with other issues I have directly addressed in my research such as the impact of AI on the skilled person standard. WIPO recently issued a similar request for comments.56 Some of the replies to these calls for comments, for instance from the Information Technology & Innovation Foundation (ITIF) have relied on my academic work.57
Finally, my academic AI and IP works have also formed the basis for a legal test case that I have spearheaded – the first cases of patent filings for AI-generated works. At the present time, these applications have rejected by the UK and European Patent offices, resulting in new policies. The UK rejection noted, “inventions created by AI machines are likely to become more prevalent in future and there is a legitimate question as to how or whether the patent system should handle such inventions. I have found that the present system does not cater for such inventions and it was never anticipated that it would, but times have changed and technology has moved on. It is right that this is debated more widely and that any changes to the law be considered in the context of such a debate, and not shoehorned
55 Request for Comments on Patenting Artificial Intelligence Inventions (Aug. 27, 2019), https://www.federalregister.gov/documents/2019/08/27/2019-18443/request-for-comments-on-patenting- artificial-intelligence-inventions.
56 Impact of Artificial Intelligence on IP Policy: Call for Comments (last visited, Jan. 22, 2019), https://www.wipo.int/about-ip/en/artificial_intelligence/call_for_comments/
57 Information Technology and Innovation Foundation, Response to Requests for Comments on Patenting Artificial Intelligence Inventions (2019), http://www2.itif.org/2020-ustpo-ip-ai.pdf.
 
22
arbitrarily into existing legislation.”58 Both rejections are pending appeal. The cases are pending consideration in other patent offices including in the US, Germany, Israel, Republic of Korea, China, and Taiwan. The cases have already received significant attention by the Financial Times, Wall Street Journal, and Business Insider among others.59
My AI work outside of IP has also had significant impact. For instance, my torts article has been cited and engaged with in a variety of subsequent academic literature.60 I was appointed as an expert on the European Commission Expert Group on New Technologies Formation and Liability based on the article, and it has been cited 31 times according to Google Scholar. Similarly, my tax article has since been translated to Chinese and published in the Economic Law Review.61 It has been cited 59 times according to Google Scholar, including in publications by OECD and in the Yale Law Journal.62 It was also prominently covered in the New York Times and VICE.63
I have been invited to present on my AI legal work at academic events including at Oxford, Cambridge, Stanford, Yale, MIT, and King’s College. I have also presented my work at the World Intellectual Property Organization (WIPO), at the European Union Intellectual Property Office (EUIPO), at government sponsored events in the United Kingdom, Moscow and Luxembourg, and at industry events such as the American Chemical Society’s annual meeting in Boston, the Association Internationale pour la Protection de la Propriété
58 UKIPO DECISION BL O/741/19, BRITISH PATENT APPLICATIONS, GB1816909.4 & GB1818161.0 (2019).
59 See, e.g., Jared Council, Can an AI System be an Inventor, W.S.J., Oct. 11, 2019.
60 See, e.g., Bryan Casey, Robot Ipsa Loquitur. 107 GEO L.J. 14 (2019).
61 Ryan Abbott and Bret Bogenschneider. Should Robots Pay Taxes? Tax Policy in the Age of Automation, 18
62
(2017); OECD Economic Survey of the United States: Key Research Findings (2019), https://doi.org/10.1787/9789264310278-en.
63 Eduardo Porter, Don’t Fight the Robots. Tax Them, N.Y. TIMES, Feb. 23, 2019; Ankita Rao, Millennial Democrats like Alexandra Ocasio-Cortez and Pete Buttigieg Are Ready to Face Off Against Job- Stealing Robots VICE, Apr. 1, 2019-09899999999p988888888p9.
  (2) ECONOMIC LAW REVIEW 239 (2018) (Chinese).
 Cynthia L. Estlund, What Should We Do After Work? Automation and Employment Law, 128 YALE L.J. 254

23
Intellectuelle’s (AIPPI) biannual international conference in Israel, and INTERPAT’s annual meeting.
Methodology
My research is qualitative, utilizing primarily doctrinal and legal theory research methods. In several areas of the law, I analyse and synthesize legal precedent and legislative interpretation related to AI and draw connections between different areas of the law. I explain areas of difficulty and predict future developments, and also provide a critical conceptual analysis while suggesting new rules, principles, norms, interpretive guidelines and values. My research is reform oriented, as well as comparative among several legal systems, particularly the US and UK. The decision to focus primarily on US and secondarily on UK systems was based on the fact that I am legally qualified and have taught in both jurisdictions, and because the US and UK have substantially different legal treatment of AI activity in some of the areas I examine.
My thesis is also interdisciplinary, based on my background as a physician scientist and my understanding of technology in the health sciences context. For instance, I consider disruption caused by AI in light of duties of health care providers and their ethical obligations in the context of tort law, including issues related to informed consent, privacy, respect for persons, beneficence, and justice. I draw upon health sciences literature with respect to likely development of future technologies, the extent to which identified challenges have been previously addressed, and to put my research and methodology into context. For instance, I consider how inventive AI will impact R&D in the field of biotechnology, and more particularly with development of new monoclonal antibodies and novel indications for existing antibodies. I further draw insights from socio-legal studies, in that the thesis considers the law’s wider social context and the economic and social policy implications of proposed legal changes.
 
24
The law review articles outside of the book are narrow in the sense that they provide in-depth analysis in several legal fields, but the book is broad in the sense that it extrapolates from those specific instances utilizing inductive reasoning to propose a generalized conclusion about AI regulation. Judge Frank Easterbrook once argued at a conference on the Law of Cyberspace that there was no more a “law of cyberspace” than a “law of the horse.”64 He was focused on cyberlaw, the study of the law’s interaction with the Internet, but he would no doubt echo the same sentiment about the law’s interaction with AI. I argue in this thesis that to the contrary, AI requires a particular regulatory response – and that regulating AI has lessons about regulating generally. AI presents different challenges than a motor vehicle, telephone, or even the Internet. Activity by an AI is analogous to activity by a person, but demands fresh thinking – should the law treat AI and human activity in the same way or differently? Should the law change in light of AI activity or should the law try to change the activity? It may be that the role of law will be to restore the status quo ex ante in light of disruption caused by AI, but AI may also provide an opportunity to more fundamentally restructure the law in order to better promote the public interest.65
 64 Frank H. Easterbrook, Cyberspace and the Law of the Horse, U. CHI. L. FORUM 207 (1996). 65
 Other works have similarly considered how technologies, like the Internet, will disrupt the law and what
 responses are needed to promote various normative goals. See, e.g., Lawrence Lessig, CODE AND OTHER LAWS
 OF CYBERSPACE (1999); Yochai Benkler, THE WEALTH OF NETWORKS (2006).

25
 half-title-page
Blurb and bio to follow
The Reasonable Robot
     AI and people do not compete on a level-playing field. From a safety perspective, AI may be the best choice for driving a vehicle, but laws often prohibit driverless vehicles. At the same time, a person may be better at providing customer service, but a business may automate because it saves on taxes. AI may be better at helping companies to innovate, but using AI may keep these companies from obtaining intellectual property rights. In The Reasonable Robot, Ryan Abbott argues that the law should not discriminate between people and AI when they are performing the same tasks, a legal standard that will ultimately improve human wellbeing. This work should be read by anyone interested in the rapidly evolving relationship between AI and the law.
Ryan Abbott is Professor of Law and Health Sciences at the School of Law, University of Surrey and Assistant Professor of Medicine at UCLA. A physician and patent attorney, Abbott’s research on law and technology has helped shape the international dialogue on these topics. He has served as an expert for the World Health Organization, World Intellectual Property Organization, the European Commission, and the UK Parliament. Abbott also spearheaded the first patent applications to disclose inventions made autonomously by an AI. In 2019, he was named one of the top 50 in Intellectual Property by IP Magazine.

26
 title-page
 The Reasonable Robot Artificial Intelligence and the Law
Ryan Abbott University of Surrey School of Law
 
27
 imprint-page
University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108472128
DOI: 10.1017/9781108631761
© Ryan Abbott 2020
This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press.
First published 2020
Printed in <country> by <printer>
A catalogue record for this publication is available from the British Library.
    
28
Library of Congress Cataloging-in-Publication Data
ISBN 978-1-108-47212-8 Hardback ISBN 978-1-108-45902-0 Paperback
Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.

29
 table-of-contents
 Contents
Introduction: Artificial Intelligence and the Law
1 AI Legal Neutrality
2 Tax
3 Tort
4 Intellectual Property
5 Criminal
6 Regulating Artificial Intelligence
1 Understanding Artificial Intelligence
1 In the Beginning
2 Defining AI
3 Can AI Think?
4 Types of AI
5 Advances in AI
6 AI Characteristics
2 Should Artificial Intelligence Pay Taxes?
1 Automation and Technological Unemployment
2 Current Tax Policies Favor Automation and Reduce Tax Revenue
3 Options for Tax Neutrality
3 The Reasonable Robot
1 Liability for Machine Injuries
2 AI-Generated Torts
3 The Reasonable Robot
4 Artificial Inventors

30
1 AI-Generated Inventions
2 Intellectual Property Rights for AI-Generated Works
3 Implications of AI Inventorship
5 Everything is Obvious
1 Obviousness
2 Artificial Intelligence in the Inventive Process
3 A Post-Skilled World
6 Punishing Artificial Intelligence
1 Artificial Intelligence and Punishment
2 The Affirmative Case
3 Retributive and Conceptual Limitations
4 Feasible Alternatives
7 Alternative Perspectives on AI Legal Neutrality
1 What If There Is Never a Singularity?
2 Should the Law Discourage the Singularity?
3 Is AI Legal Neutrality a Coherent Principle?
4 Protecting Spheres of Human Agency
5 The Risks and Unique Dangers of AI
6 Concluding Thoughts
Third-Party Materials
Index

31
Introduction: Artificial Intelligence and the Law
The rise of powerful AI will be either the best or the worst thing ever to happen to humanity. We don’t yet know which.
- Stephen Hawking
Artificial intelligence (AI) is doing more than ever before, and often doing it cheaper, faster, and better than humans are. In 2017, the company DeepMind developed an AI, AlphaGo Master, that beat the human world champion of the board game Go. Many experts had predicted AI’s Go dominance would take another decade given the game’s complexity. There are more possible board configurations in Go than there are atoms in the universe. Later in 2017, a revised version of AlphaGo, AlphaGo Zero, beat AlphaGo Master one hundred games to zero. It did this after training for just three days by playing against itself. Unlike its predecessors, AlphaGo Zero never learned from human examples.
Go was the last traditional board game at which people could outperform machines. There is now an entire field of human activity at which AI outperforms people. While AlphaGo’s victory was an exciting technical landmark, it has had limited social impact because playing board games is not the most practical endeavor, or perhaps it might be because AI is still working toward video game dominance. But board games are one of the oldest measures of machine intelligence, and AI’s ascendency hints that it may soon automate a broader range of tasks, perhaps sooner than many anticipate, and it may do so in spectacular fashion.
Alphabet, which owns DeepMind, is not investing in AI to dominate the field for competitive board games. In principle, if an AI can train to recognize patterns in Go, then it

32
can train to recognize pneumonia in an X-ray or pedestrians on a road. Indeed, DeepMind is already being applied to solve practical challenges. In 2018, DeepMind’s AI AlphaFold outperformed all of its ninety-eight competitors in a challenge aimed at predicting the three-dimensional structure of proteins – a task critical to drug discovery. Unlike playing Go, predicting protein folding is an important, common, and real-life scientific problem. Similarly, again in 2018, researchers found that another DeepMind AI correctly referred patients with more than fifty distinct eye diseases for specialist care in 94 percent of cases, matching the performance of expert clinicians. In 2019, DeepMind AI was able to consistently predict development of acute kidney failure forty-eight hours earlier than human physicians, which could ultimately prevent around 30 percent of cases from ever occurring.
The future social impact of these advances is likely to be tremendous. Already, impressive-sounding era titles are being used to describe the coming disruption such as the Fourth Industrial Revolution, the Second Machine Age, and the Automation Revolution. Among other things, AI is predicted to generate a massive amount of wealth by changing the future of work. This has long been the experience with AI automating physical work, such as in automobile manufacturing, but AI is now moving into automating mental work, and not only relatively simple service activities like operating a cash register at McDonald’s. AI is completing tasks performed by doctors, lawyers, and scientists.
IBM’s flagship AI brand Watson, which famously won a game of Jeopardy! in 2011, is working in a range of fields. In health care, for example, Watson (now comprised of a variety of AI systems) analyzes the genetics of cancer patients to help select appropriate drug treatments, a task that a group of human experts can also do. While knowing a

33
patient’s genome, determining what treatment to provide remains a complex task. For some cancers, it can require around 160 collective work hours by a team of highly trained health care providers. By contrast, a 2017 study reports that Watson could outperform the standard practice and only require about 10 minutes to do so,1 although Watson’s performance has proven controversial.2
Several companies claim their AI can already outperform human doctors in certain areas of medical practice. This is not surprising. Machines are able to memorize every bit of medical literature ever created and process practice experience from countless human lifetimes. Plus, they never need a rest break. In 2017, a Chinese company reported its robot Xiao Yi took and passed, by a wide margin, China’s National Medical Licensing Examination, the test someone takes to become a medical doctor in China. Xiao Yi knows the contents of dozens of medical textbooks, millions of medical records, and hundreds of thousands of articles, but to pass the test it also had to learn, reason, and make judgments by itself. Researchers at IBM have even reported that Watson quietly passed the equivalent examinations in the United States after it was prohibited from formally taking the exam. Of course, just passing exams does not make someone, or something, a doctor. Once AI is consistently better than a doctor at diagnosing certain diseases, managing prescriptions, or performing surgeries, AI is still unlikely to completely automate medical care. But these
    1
et al.,
Kazimierz O.
Wrzeszczynski
Comparing Sequencing Assays and Human-Machine
 Analyses in Actionable Genomics for Glioblastoma
DOI: 10.1212/NXG.0000000000000164
2 Eliza Strickland, How IBM Watson Overpromised and Underdelivered on AI Health Care, IEEE SPECTRUM, Apr. 2, 2019, https://spectrum.ieee.org/biomedical/diagnostics/how-ibm- watson-overpromised-and-underdelivered-on-ai-health-care.
, 3   (2017),
NEUROL. GENET.
   
34
developments suggest that there are already aspects of medical care susceptible to automation, and that we will need fewer doctors once we have more efficient doctors augmented by AI.
1 AI Legal Neutrality
The law plays a critical role in the use and development of AI as the law establishes binding rules and standards of behavior to ensure social well-being and protect individual rights. An appropriate legal framework will help us realize the benefits of AI while minimizing its risks – which are significant. AI has caused flash crashes in the stock market, committed cybercrime, and been used for social and political manipulation. Famous technologists like Elon Musk and academics like Stephen Hawking have even argued that AI may doom the human race. Most concerns, however, focus on nearer-term and more practical problems such as technological unemployment, discrimination, and social inequality. But these concerns do not exist in a vacuum. How different jurisdictions elect to regulate AI will change how technologies develop. For instance, there is already a significant international divide with respect to whether companies or consumers “own” personal data vital to AI development, the extent to which AI can be used in state surveillance of its residents, and when individuals have a right to an explanation for decisions made by AI (ranging from credit approval to criminal sentencing). As the law will impact AI, AI will have no less significant an impact on the law. AI challenges fundamental assumptions underlying laws designed to regulate the behavior of human actors.

35
AI-centric laws have been slow to develop, perhaps due to a concern that an overly burdensome regulatory environment would slow innovation. This ignores the fact that AI is already subject to regulations that may have been created decades ago to deal with issues like privacy, security, and unfair competition. What is needed is not necessarily more or less law but the right law. In 1925, Justice Benjamin Cardozo admonishes a graduating law school class that “the new generations bring with them their new problems which call for new rules, to be patterned, indeed, after the rules of the past, and yet adapted to the needs and justice of another day and hour.”3 This is the case for AI, even if it only differs in degree from other disruptive technologies like personal computers and the Internet. A legal regime optimized for AI is even more important if AI turns out to be different in kind.
One might imagine that AI would fit comfortably into existing rules, or that there would be a single legal change, such as granting AI legal personality similar to a corporation, that will solve matters. While it is appealing to think a simple solution to the problem of regulating something as complex as AI exists, there is not a one-size-fits-all principle in every area of the law, which is why it is necessary to do the difficult work of thinking through the implications of AI in different settings. In this respect, it is promising that there have been efforts in recent years to articulate policy standards or best principles such as transparency, safety, accountability, and trustworthiness specifically for AI regulation by governments, think tanks, and industry. For example, the Organisation for Economic Co-operation and Development (OECD) adopted Principles on Artificial
3 BENJAMIN NATHAN CARDOZO, SELECTED WRITINGS OF BENJAMIN NATHAN CARDOZO 417 (Margaret E. Hall ed., 1947).
 
36
Intelligence in May 2019, and one month later the G20 adopted human-centered AI Principles guided by the OECD principles.
The central thesis of this book is that the law requires a new principle of AI regulation – AI legal neutrality – that the law should not discriminate between people and AI when they are engaged in the same activities. Currently, we do not have a neutral legal system. An AI that is significantly safer than a person might be the best choice for driving a vehicle, but existing tort laws might prohibit driverless vehicles. A person might be a better choice for delivering groceries, but a business might automate because it saves on taxes. AI might be better at generating certain types of innovation, but businesses might not want to use AI if this restricts future intellectual property rights. In all these instances, neutral legal treatment would ultimately benefit society as a whole, and this book argues that not discriminating between people and AI will tend to improve human well-being. The rise of AI is the rise of a new workforce – one that is treated differently under the law. Leveling the playing field is not necessary as a matter of fairness to machines, but it should be done because technologically neutral laws will improve efficiency. In turn, this will generate wealth, promote innovation, alleviate poverty, and reduce waste. Competition is vital to the operation of markets, and inappropriate government policies and legislation can obstruct fair competition.
AI legal neutrality does not require universally identical treatment of AI and people. There are obviously significant differences between them, the most important of which is that AI does not morally deserve rights; these differences will justify differential rules. Since AI lacks humanlike consciousness and interests, treating AI as if it were to have rights in the future should only be justified if this would benefit people. An example of this would

37
be if autonomous vehicles needed to directly hold funds or insurance policies to cover potential harms to pedestrians. This is essentially the rationale for corporations being allowed to enter into contracts and own property. Corporations’ legal rights exist only to improve the efficiency of human activities such as commerce and entrepreneurship, and like AI they do not morally deserve rights: They are a member of our legal community but not our moral community. This book does not advocate for AI’s having rights or legal personhood. Nor is a principle of AI legal neutrality a moral principle of nondiscrimination in the way that term is traditionally used. Antidiscrimination laws have helped improve conditions for historically marginalized groups, primarily as a matter of fairness. However, antidiscrimination laws also promote competition and efficiency.
Of course, efficiency should not be the driving force behind every decision. It should not come at the expense of other deeply felt normative values such as fairness and justice. A child laborer might be more efficient than an adult at fixing looms or sweeping chimneys, but for noneconomic reasons we decided long ago to prohibit child labor. A person might be better at mining minerals in hazardous conditions, but automation could be preferable based on safety considerations. An AI might be better at identifying and eliminating military targets, but there could be other reasons not to delegate life and death decisions to a machine. Rather than a dispositive policymaking principle, AI legal neutrality is an appropriate default that may be departed from when there are good reasons for so doing.
A child laborer may be more efficient than an adult at fixing looms or sweeping chimneys, but for noneconomic reasons we decided long ago to prohibit child labor. A person may be better at mining minerals in hazardous conditions, but automation may be preferable based on safety considerations. An AI may be better at identifying and

38
eliminating military targets, but there may be other reasons not to delegate life and death decisions to a machine. Rather than a dispositive policymaking principle, AI legal neutrality is an appropriate default that may be departed from when there are good reasons for so doing. This book examines how a principle of AI legal neutrality would impact four different areas of the law – tax, tort, intellectual property, and criminal. As AI increasingly steps into the shoes of persons, AI will need to be treated more like a person, and sometimes people will need to be treated more like AI.
2 Tax
Automation involves much more than technology’s putting people out of work, what economist John Maynard Keynes terms “technological unemployment,” but it is one of the things people are most concerned about. Today, this is a frequent topic of scholarship on labor markets, some of which predicts long-term technological unemployment and some that does not. It is also an old concern. The Luddites, a group of English workers opposed to automation’s eliminating jobs, periodically destroyed machinery in acts of protest during the First Industrial Revolution. In response, the British government made machine- breaking a capital offense. History has shown the Luddite fears about automation were misplaced, at least in regard to concerns about long-term unemployment. In the end, the machines resulted not just in vast gains in productivity but also in more jobs for everyone, and ever since new technologies have consistently resulted in overall job creation. Steam engines, electrical power, and personal computers all eliminated certain jobs, but they created more jobs than they eliminated. At the turn of the twentieth century, some 40

39
percent of the US workforce was employed in agriculture. Now, less than 2 percent of the workforce works in agriculture. This has not translated to a 38 percent increase in unemployment. In fact, even as agriculture-based employment and agriculture’s relative contribution to the economy have decreased, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the economy has increased.
For the Fourth Industrial Revolution, history repeating itself may not be so bad. Despite some naysaying, the risks of automation may be overstated and may again result in long-term employment gains. While that may be the case, it cannot be ignored that the First Industrial Revolution was accompanied by decades of pervasive social unrest, widening income disparities, and individual suffering. A proactive regulatory approach to a new industrial revolution should allow us to make the most of automation while limiting some of its harmful effects. If this is a new type of industrial revolution that results in permanently increased long-term unemployment, a proactive approach will be all the more important.
But for all the debate about AI putting people out of work, it turns out this may occur for a very surprising reason – taxes. Tax laws treat people and automation technologies like AI differently even when they are performing the same tasks. For instance, automation allows businesses to avoid employee and employer wage taxes. Employers and employees in the United States pay matching amounts totaling more than 12 percent of an employee’s salary and matching Medicare payments totaling nearly 3 percent (applied on the first $127,200 of earnings), plus almost 1 percent more for higher earners as a Medicare surcharge on earnings more than $200,000. If an AI can replace a person, the employer avoids these taxes. So, if an automatic cashier costs McDonald’s the

40
same or even a bit more before taxes than an employee who does the same job, it actually costs the company less to automate after taxes.
In addition to avoiding wage taxes, businesses can accelerate tax deductions for some AI when it has a physical component or falls under certain exceptions for software – but not for human workers. In other words, employers can claim a large portion of the cost of some AI up-front as a tax deduction, which may be more valuable to some large companies than delaying wage expenses over time. This occurs because many large companies have significant cash reserves that earn low rates of return, so it can be more valuable for them to get a present deduction than to invest existing cash. In fact, employers may get accelerated tax deductions even if AI increases in value over time, the same way that deductions can be claimed for depreciation in real estate for assets that increase in value. This latter fact allows firms to report profits to potential investors for the difference between an AI’s tax depreciation and the value of the AI on the firm’s financial statements. Finally, employers also receive a variety of indirect tax incentives to automate. In short, our tax laws keep people and AI from competing on a level-playing field. While the system was not designed to do this, but does primarily tax labor rather than capital. This has had the unintended effect of inefficiently incentivizing automation, once the capital (AI) is the labor (AI).
What is even more concerning is that AI does not pay taxes! This sounds ridiculous until you consider that income and employment taxes are the largest sources of revenue for the government, together accounting for about 85 percent of total federal tax revenue. By contrast, business income taxes only generated about 10 percent of revenue when statutory corporate tax rates were 35 percent. Under the 2017 Tax Cuts and Jobs Act, the

41
rate was cut to 21 percent, and corporate tax revenue has been trending sharply downward. Whatever the statutory rate, the effective corporate tax rate – what companies pay after taking tax breaks into account – is substantially less. In 2018, the S&P 500 annual tax rate, which refers to 500 large companies that have common stock listed on one of the three major US stock exchanges, was less than 18 percent.4 However, this includes all taxes from the federal and state levels as well as from foreign authorities. Amazon drew unwanted attention that year by reporting a US pretax profit of $11.2 billion together with a negative tax bill of $129 million.5 Amazon’s total effective tax rate for 2018 was 11 percent including foreign, state and deferred taxes.
So, AI does not pay income taxes or generate employment taxes. It does not purchase goods and services, so it is not charged sales taxes. It does not purchase or own property, so it does not pay property taxes. AI is simply not a taxpayer, at least not to the same extent as a human worker. If all work were to be automated tomorrow, most of the tax base would immediately disappear. What happens is that when firms automate, the government loses revenue – potentially hundreds of billions of dollars in the aggregate. This may be enough to significantly constrain the government’s ability to pay for things like social security, national defense, and health care. In the long run, the revenue loss should balance out if people rendered unemployed eventually return to similar types of work, and there should ultimately be revenue gains if automation makes businesses more productive
4 Matt Krantz, PepsiCo Paid No Tax? Neither Did These Other 33 Profitable S&P 500 Companies, INVESTOR’S BUSINESS DAILY, July 18, 2019, www.investors.com/etfs-and-
funds/personal-finance/corporate-tax-rate-zero-profitable-us-companies-sp500/.
5 https://www.wsj.com/articles/does-amazon-really-pay-no-taxes-heres-the-complicated-answer- 11560504602
      
42
and if people go on to find better-paying types of work. But even leaving aside potentially significant short-term disruptions, that will not be the case if we are headed to a future of work with higher unemployment rates unless increased productivity dramatically outstrips unemployment.
Only recently has public debate surfaced about taxing AI, and it has mainly been in relation to slowing the rate of automation, not as an attempt to craft tax-neutral policies or ensure government revenue. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing concerns of stifling innovation. The next day, Bill Gates said he thought governments should tax robots. Former US Secretary of the Treasury Lawrence Summers subsequently contended that Gates was profoundly misguided. Later that year, South Korea was credited with enacting the world’s first “robot tax,” which limited tax incentives for automated machines.
The question of how the law should respond remains. Automation should not be discouraged on principle; in fact, it should be welcomed when it improves efficiency. But, automating for the purpose of tax savings is not efficient – or, at least not socially efficient. Automating for tax savings may not make businesses any more productive or result in any consumer benefits - it may even result in productivity decreases to reduce tax burdens. This is not socially beneficial. The options - once policymakers agree that they do not want to advantage AI over human workers - could be to reduce the tax benefits AI receives over people, such as what South Korea, or to create new benefits, or reduce existing taxes, that only apply to human workers. For instance, payroll taxes could be eliminated, which might be a better way to achieving neutrality since it reduces tax complexity and ends taxation of

43
something of social value – namely human labor. However, this would eliminate around 35 percent of the US federal government’s current tax revenue.
There are many ways to ensure adequate tax revenue, such as by increasing property or sales taxes, which might be a more progressive way to tax because it would end up taxing income regardless of its source – labor or capital. It could certainly be progressively designed by applying relatively higher property taxes for higher-value properties and higher sales taxes for, say, luxury goods. Income taxes could also be increased either by raising the marginal tax rates for high earners or the effective tax rates through the elimination of things like the step-up in basis rule that reduces tax liability for inherited assets. More ambitiously, AI legal neutrality may prompt a more fundamental change in the way labor versus capital and workers versus businesses are taxed. While new tax regimes could target AI, as well as other automation technologies to which similar considerations apply, it would risk increasing compliance costs and tax complexity. It would also be “taxing innovation” in the sense that it may penalize business models that are legitimately more productive with less human labor.
A better solution would be to increase capital gains taxes and corporate tax rates to reduce reliance on labor taxes. Before AI entered the scene, there had been long-standing criticism about the extent to which capital is favored over labor in tax policy. The Fourth Industrial Revolution may provide the necessary impetus to finally address this issue. The downside of increased capital taxation is largely a concern about international tax competition. There is a historic belief that labor should be taxed over capital, because capital is more mobile and will leave jurisdictions with higher tax rates. These concerns may be overstated, particularly in large, developed markets such as the United States.

44
Historically, relatively high corporate tax rates have not been a barrier to US-based investments.
The United States, which has the world’s largest economy, does not have a relatively progressive tax system – which is to say one based on a person’s ability to pay. Wider wealth disparities exist in the United States than in any other developed country. With AI likely to result in massive but poorly distributed financial gains, AI will both require and enable us to rethink how we allocate resources and redistribute wealth. If we do choose to reduce income inequality, this should be accomplished primarily though taxation. With new laws ensuring that AI contributes its fair share to government revenue, we could fund retraining programs for workers and enhance social benefits. If AI does end up causing increased long-term unemployment, subsequent tax revenue could even fund a universal basic income that would enable governments to pay every citizen regardless of their employment.
3 Tort
AI will be doing all sorts of things that only a person used to do – like driving. While difficult to say exactly when this will happen, companies like Uber and Tesla claimed they would be using or selling fully autonomous vehicles (AVs) before 2020. Other automobile manufactures now state they will be selling AVs in the early 2020s. By contrast, in 2017, a European Commission expert group predicts that fully driverless vehicles will not be

45
commercially available before 2030.6 Nevertheless, survey research often reports negative public attitudes about AVs. Most people say they would feel unsafe being driven around by their car compared to a taxi driver, yet AVs may already be safer than people. Driving is dangerous. The US Department of Transportation reports that more than 35,000 people die each year from domestic motor vehicle accidents. It estimates the economic costs of those accidents at more than $240 billion. Worldwide, more than a million people die each year in motor vehicle accidents, and tens of millions are injured. This is almost exclusively the result of people’s being terrible drivers. About 94 percent of crashes involve human error, and a human driver causes a fatality about every 100 million miles.
There has been at least one fatality caused by an AV. Operated by Uber, the AV collided with a pedestrian in Arizona because it failed to detect her in time to stop, but it was not the only negligent party: The backup driver was watching a television show on her phone, and the pedestrian was walking her bicycle across an unlit and unmarked section of road at night. More recently, regulators reported that a Tesla “Autopilot” system may have been at fault in a March 2019 fatality. A Tesla spokesperson noted in response that Tesla drivers have logged more than a billion miles with Autopilot engaged, and that Autopilot tends to make drivers safer.7 Earlier reported AV fatalities involving Tesla’s Autopilot system were ultimately determined by regulators not to be the AV’s fault. However, those incidents speak to the challenges of human-machine interaction – human drivers are
6 GEAR 2030, FINAL REPORT OF THE HIGH-LEVEL GROUP ON THE COMPETITIVENESS AND SUSTAINABLE GROWTH OF THE AUTOMOTIVE INDUSTRY IN THE EUROPEAN UNION, at 40 (July 2017).
7 Timothy B. Lee, Autopilot Was Active When a Tesla Crashed into a Truck, Killing Driver, ARS TECHNICA, May 16, 2019, https://arstechnica.com/cars/2019/05/feds-autopilot-was- active-during-deadly-march-tesla-crash/?comments=1&post=37374819.
    
46
supposed to be prepared to retake control of the vehicle on short notice, but it is difficult for people to remain alert and engaged while an AV is driving. Inevitably, self-driving cars will cause fatalities. But the perfect should not be the enemy of the good. AVs do not need to be harmless to make people safer; they just need to be better drivers than people. Whether in 2020, 2030, or 2040, AVs will not only be safer drivers than people but much safer than people. AVs are rapidly improving and human drivers are not, which is important with respect to legal liability for wrongful acts.
Tort law defines what constitutes a legal injury and determines those cases in which someone may be held financially, as opposed to criminally, accountable. For accidents caused by people, we generally require negligence for liability. Negligence means that someone’s actions fell below the standard of a reasonable person. To apply this test, over the course of centuries, courts have developed the concept of a hypothetical reasonable person who does not really exist but who sets the standard for human behavior. If a reasonable person would not have stopped a car in time to avoid hitting a child who ran out into the street, then the driver would not be held liable. If a reasonable person would have stopped, then she would be liable. This concept of a reasonable person is not exclusive to torts; it is a standard that applies in many areas, including criminal and contract law. People mention tort liability when speaking about the law and self-driving cars, often in the context of worrying over who – or what – to hold liable for accidents caused by self-driving cars.
Those concerns are overblown. AVs are products, and there is already a legal regime built around injuries caused by products. Product liability law could simply be applied to self-driving car accidents. To oversimply, there is a different standard for accidents caused

47
by people than for accidents caused by products. The law holds manufacturers and retailers of products strictly liable for harms caused when a machine is defective, or when its properties are misrepresented. Strict liability refers to the fact that liability is based on causation: Did the machine cause an injury without regard to whether a manufacturer’s conduct was socially blameworthy? Strict liability is a lower bar for liability, which is a good rule for most products. There is more liability for manufacturers, helping incentivize manufacturers to make safer products. Manufacturers are in the best position to improve product safety and to profit from decreasing accidents. However, more liability for manufacturers does not necessarily translate to fewer accidents if a product is safer than the existing standard. In that case, product liability law would make people less safe. When AI has more liability than a person, it makes automation costlier. This is not a desirable outcome. We want to encourage, or at least not discourage, automation through tort liability in situations where it would improve safety.
Instead of applying standard product liability law to AI, the law should evaluate accidents caused by AI under a negligence standard. In a sense, this would treat AI like a person, and it would focus on the AI’s act rather than its design. The law would ask whether the AI behaved in such a way that if a person had done such a thing, the act would have fallen below the standard of a reasonable person. As with human defendants, the law does not usually concern itself with what a person was thinking or whether he thought what he was doing seemed reasonable. The law looks objectively at whether a reasonable person would have committed the act.
Here, as with tax law, AI and people compete at the same sorts of activities and exhibit similar behaviors and responsibilities. While tax law currently aids AI in the

48
workplace, tort law favors people. A principle of AI legal neutrality that holds AI to a negligence standard would encourage the development and adoption of safer technologies. Again, this would not treat AI and people entirely the same legally in that AI would not be personally liable for injuries. AI is owned as property, does not have financial resources, and is not influenced by the specter of liability the way people are. Manufacturers are still in the best position to improve safety and to weigh the risks and benefits of new technologies. This would function as a market-based mechanism to encourage the introduction of technologies that improve safety with the benefit of not requiring government funding, additional regulatory burdens on industry, or new administrative responsibilities.
Applying a negligence framework to AI is the least important part of the torts story. The time is coming when AI performance will not just be safer but substantially safer than a person’s – to the point where self-driving cars may almost never cause accidents. And once that happens, it really will not matter which liability regime we apply to AI; it will matter which liability regime we apply to ourselves because by then we – people – will be the biggest danger on the road. The courts, at this point, should hold human drivers to the standard of self-driving cars – the reasonable robot standard, although it may be more accurate, if less catchy, to call it the reasonable AI standard – rather than the reasonable person standard.
Once AVs are exponentially safer than people, almost any accident someone causes would be negligent by comparison to an AI. Today, if a child runs in front of a person’s car while she is driving at night and is unable to stop, the person probably would not be liable. However, in a future where the reasonable person standard is AI, if an AV would have been

49
able to stop, even if it detected the child through radar, then a person would be liable. A reasonable robot or AI standard would not prohibit people from driving, although that is a possible response. Instead, this standard would mean that people drive at their own risk and they are responsible for any harms they cause. This standard internalizes the risk of driving and might be a better system than banning human drivers because it safeguards their freedom and autonomy while simultaneously providing a strong financial incentive for safer practices.
Self-driving cars are only one example of how AI will disrupt tort law. With evidence suggesting AI can outperform people at some aspects of health care and escalating health care costs, people may soon be going to see Doctor Watson for care. Right now, AI can only outperform people at very narrow aspects of medical practice, but it is getting better quickly, and human doctors are not. What should be remembered is that Watson does not have to be perfect to improve safety – just a little bit better than human doctors – and that bar is low. Make no mistake, human doctors are downright dangerous. If you think a lot of people die from car accidents, think about the fact that medical error kills far more. Estimates vary on the exact numbers, but the medical profession has acknowledged for about two decades that medical error is a leading cause of human death. In fact, for human doctors who take an oath to first do no harm, it would be unethical to allow them to compete with AI.

50
4 Intellectual Property
It should come as little surprise that AI has been autonomously generating artistic works and scientific inventions for decades. The law provides legal protections for human actors for many types of intellectual property – copyrights for books and music, patents for certain types of discoveries – but the law remains quite stubborn in its protection when AI creates what we think of as "products of the mind.” The fourth chapter in this book discusses intellectual property law, the branch of law concerning property rights in certain intangible creations. Legally, it is unclear whether AI-generated works are eligible for intellectual property protection and who would own them if they are. In most cases ownership rights initially go to an author or inventor, and most jurisdictions require the author or inventor to be a natural person. Authors and inventors do have the ability to transfer their rights to others; this can happen automatically when employees create something within the scope of employment. As a matter of fact, most patents are owned not by human inventors but artificial persons in the form of businesses. Still, the requirement that a natural person be listed as an author or inventor ensures, even when companies own the intellectual property rights, the right of human creators to be acknowledged. These laws were not designed with AI in mind and have not yet largely been applied to AI- generated works where there is no person who qualifies as an author or inventor.
With respect to patents, the first one for an AI-generated invention may have been issued in 1998 for an invention made by a type of AI called a Creativity Machine. A Creativity Machine has an artificial neural network that has been trained on data that it stores by varying the connection weights between its neurons. This network then self-

51
perturbs these connection weights, altering the data to which it has been exposed, and generates novel output. A second artificial neural network, a critic network, monitors the first network for new ideas and identifies those ideas that are sufficiently novel compared to the AI’s preexisting knowledge base and that meet some criteria for usefulness. The Creativity Machine, if a person, would likely be an inventor in these circumstances. In the case of the 1998 patent, the US Patent and Trademark Office granted a patent for the AI’s invention but did so in the name of the AI’s owner, an easy decision as the patent application did not disclose the AI’s role – the AI’s owner listed himself as the inventor. This Creativity Machine may have been the first AI inventor, but it has certainly not been the last. More patented inventions were created autonomously by AI in the 2000s, such as the Invention Machine, which relied on an entirely different type of AI called genetic programming to design a new controller.
Right now, there are probably very few AI systems functionally inventing; most AI is involved in the process as a simple tool. Using a calculator, even if a person is bad at math, does not make the calculator an inventor on a patent. Similarly, having another person use an abacus to multiply some number would not make him an inventor. Some machines do a lot more than a calculator, but that work is still not inventive. If a machine, or a PhD student, conducts an experiment that a person has designed, and the results are patentable, the person who designed the experiment will probably still be the only inventor. However, if a person only asks someone to solve a problem (say, design a better battery), and she does, in the eyes of the law she – not the person asking her to design a better battery – will generally be the inventor (of the battery). Likewise, if a person asks Apple’s personal assistant Siri to develop a faster semiconductor, and it does, Siri would be the inventor, at

52
least it would if Siri were a natural person. What complicates AI’s involvement in invention with regard to its recognition in the process is that sometimes there is not a natural person who, as defined by current statutes, would traditionally qualify as an inventor. This raises important policy questions regarding whether AI-generated inventions are patentable, who the owner of such a patent should be, and who – or what – should be credited as an inventor. As of 2019, there is no law specifically about this in any jurisdiction. As such, it remains unclear whether an AI or a person would be an inventor, or even that the output of an inventive AI would be patentable. This uncertainty is bad for business and innovation. What it creates is opportunity for gaming patent offices and taking credit for the work of AI. While not unfair to AI, it might be unfair to human inventors who are legitimately inventing.
Somewhat more law on AI-generated works can be found with respect to copyright. The United Kingdom, the first jurisdiction to explicitly protect so-called computer- generated works in 1988, remains one of only a handful of countries worldwide that does so: The “producer” of the work is considered the author and owns the copyright. In the United States, a Copyright Office policy from 1973 prohibits nonhuman authorship, but the Copyright Office appears to have rejected AI-generated music as early as the 1950s. AI- generated works in the United States automatically enter the public domain and cannot receive copyright protection, which makes it tempting for people to take credit for something AI has done. In support of the current iteration of its “Human Authorship Policy,” the Copyright Office cites the 1884 case of Burrow-Giles v. Sarony . Photographer Napoleon Sarony sued the Burrow-Giles Lithographic Company for copyright infringement of a famous photograph of Oscar Wilde. The company alleged that the photographer could

53
not be the photograph’s author because a photograph is a mechanical reproduction of a natural phenomenon, but the US Supreme Court disagreed and held that any form “by which the ideas in the mind of the author are given visible expression” is eligible for copyright protection.8 The case thus deals explicitly with whether the use of a machine would negate human authorship and implicitly with whether a camera could be considered an author. The Copyright Office has interpreted this to mean that copyright requires ideas and mental products, which it has determined AI lacks. While a nonbinding judicial opinion from the Gilded Age should not answer the question of whether DeepMind’s AI can be an author, it is. Analogizing to the patent context, it could be that the AI-generated works are unprotectable. Other scholars have argued that AI does not need incentives to innovate, and that protecting AI-generated works could chill future human innovation.
Not only should the law permit copyrights and patents for AI-generated works, it should recognize AI as an author or inventor when the AI otherwise meets author- or inventorship criteria. The primary reason is based on why the law grants patents and copyrights in the first place: to encourage certain socially valuable activities. The US Constitution states that Congress shall have the power to “promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries.”9 Compared to physical or real property like a car or a house, it can be difficult to prevent people from copying or using intangible products like a song or a design. Without some form of intervention, the market would underproduce certain types of innovation because there would not be adequate incentives
8 Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53, 56 (1884). 9 U.S. CONST., Art. I, § 8, Cl. 8.
 
54
to generate new content if third parties could use it for free. This is referred to as the free rider problem, for which patents and copyrights are possible solutions. Patents and copyrights provide an inventor or author with a temporary monopoly over an invention or work by preventing third parties from using or copying it without the inventor or author’s permission. The prospect of a patent or copyright thus provides an additional financial motivation for inventors and authors.
Even though machines do not care about patents and copyrights, people who build, own, and use AI do. There is value – monetary and moral – in acknowledging AI as authors and inventors. Allowing patents for AI-generated works would make inventive AI more valuable and incentivize AI development, translating to rewards for effort upstream from the stage of invention. By contrast, failing to allow patents for AI-generated works would discourage businesses from using inventive AI, even in instances where it would be more effective than a person. Further, acknowledging AI as inventors would safeguard human moral rights because it would prevent people from receiving undeserved acknowledgment. Taking credit for an AI’s work would not be unfair to a machine, but it would diminish the accomplishments of people who have invented without using inventive AI. In addition, acknowledging AI as inventors would acknowledge AI developers, and it would reduce gamesmanship with patent offices. As with self-driving cars, AI would not own its patents. The AI’s owner should own patents on the AI’s inventions.
What about the argument we only protect the results of intellectual or mental activity, and the Copyright Office’s determination that AI does not think? That is not the right focus – whether and how an AI thinks simply should not matter. Congress realized that a functional standard for invention was needed back in the 1950s. Before then, courts

55
had devised a “Flash of Genius” test, which required that the inventive spark come to a person in a moment of clarity rather than as the result of methodical, laborious research. The nature of the test was never entirely clear, but it involved judges subjectively reasoning about what an applicant might have been thinking. Eventually people realized that trying to figure out what was going on in someone’s head was a terrible idea. It is hard to do, but more importantly it does not matter whether invention comes from Einstein or a room full of monkeys. What we care about is generating socially beneficial innovation. With AI, how an algorithm is designed and whether it thinks in a philosophical sense do not matter. Patentability should be based on the inventiveness of an AI’s output rather than on a clumsy anthropomorphism.
Still, the argument that the law only protect the results of intellectual or mental activity and the Copyright Office’s determination that AI does not think remain. That is not the right focus; whether and how AI thinks should not matter. Congress realized the need for a functional standard for invention back in the 1950s. Before then, courts used a “Flash of Genius” test, which required that the inventive spark come to a person in a moment of clarity rather than as the result of methodical, laborious research. The nature of the test was never entirely clear, but the process involved judges subjectively reasoning about what an applicant might have been thinking. Eventually people realized the Flash of Genius test was a terrible idea. More importantly, it should not matter to the law whether invention comes from Albert Einstein or a room full of monkeys. What society cares about is generating socially beneficial innovation, not how an AI designs an algorithm or whether it thinks in a philosophical sense. Inventiveness of an AI’s output rather than a clumsy anthropomorphism should guide intellectual property law.

56
AI is improving exponentially, and human researchers are not. This is exciting because it means that society will likely witness the same sort of phenomenon with inventive AI as with self-driving cars: vast improvements over human performance. When AI outperforms people, it will become the standard way that research is performed. Instead of Pfizer asking its research scientists whether a drug that treats one immune condition can treat another, or Exxon its chemists to design better catalysts, both companies will use, for example, DeepMind’s AI to complete the task. In patent law, the human standard often compared to the reasonable person standard is the “person having ordinary skill in the art,” or the skilled person. This hypothetical person represents the average worker in a field and serves as the benchmark for human behavior. The idea is that to receive a patent a person must accomplish something more than the average worker in a field would do. Therefore, if a person invents something that would be obvious to the skilled person, he cannot be granted a patent. If the invention is nonobvious, then he can be.
AI will change the definition of the skilled person. Since the skilled person reflects the average worker in a field of invention, the concept should change once the average worker is augmented by AI. At this point in time, the skilled person should become the skilled person using AI. This should raise the bar to patentability because AI augmentation will make average workers more sophisticated and knowledgeable – making more inventions obvious to workers. Once AI transitions from routinely augmenting to automating inventive work, the skilled person should become an inventive AI. This should further raise the bar to patentability because a the inventive AI of the future will more easily find inventions obvious. The bar will keep rising as machines continue to improve: DeepMind’s AI of 2040 will be outperformed by its AI of 2045. With no clear limit to the

57
sophistication of AI, it will be difficult for a human alone to come up with anything nonobvious. Eventually, everything will be obvious to a superintelligent AI. This may mean the end of the patent system, but there should not be cause for concern. Once superintelligent inventive AI is run-of-the-mill, future innovation will be self-sustaining, thus no longer requiring patents to incentivize new inventions.
5 Criminal
Similar to its role in invention, AI is already autonomously engaging in activity that would be criminal for a natural person. Further, AI can do so in a way that is untraceable, or irreducible, to the wrongful act of a person. In other words, in much the same way that some AI-generated inventions lack an actor that would traditionally qualify as an inventor, there are cases of AI-generated crimes where no natural person can be held criminally liable. Today, at least, nearly all crimes involving AI are likely to be reducible to human crime. Put simply, if a person strikes someone with a hammer, he has committed battery, not the hammer. Where an AI functions as a tool, even a very sophisticated one, crimes still involve an identifiable defendant causing harm. Even when AI causes unforeseeable harm this might still be reducible. An example would be the case in which an individual creates an AI to steal financial information, but due to a programming error, the AI shuts down an electrical grid thus disrupting hospital care. These are familiar problems for criminal law.
However, there may be times where it is difficult to reduce AI crime to an individual due to AI autonomy, complexity, or limited explainability. Such a case might involve several individuals contributing to the development of an AI over a long period of time, such as

58
with open-source software, where thousands of people can collaborate informally to create an AI. Another case in this category might feature an AI that develops in response to training with data. Attributing responsibility for an AI output where the machine has learned how to behave based on accessing millions or billions of data points from heterogenous sources seems downright impossible. Difficulty arises, too, when assigning fault to individuals where AI is concerned versus a conventional product such as a car, where one component is faulty. In fact, as a practical matter, to reduce an AI-generated harm to the actions of individuals might be impossible.
Even in cases where AI developers are known, an AI may end up causing harm without any unreasonable human behavior. Suppose, for example, two experienced and expert programmers separately contribute code for the software of an autonomous vehicle, but the two contributions unforeseeably interact in ways that cause the vehicle to deliberately collide with individuals wearing striped shirts. If this were the result of some not reasonably foreseeable interactions between the two programmers’ contributions, then presumably neither programmer would have criminal liability. Generally, to be criminally liable, an individual must intend a harmful outcome – or at least act recklessly or negligently. In a case where AI activity has, from the perspective of a reasonable person, unforeseeably caused harm, individuals would not generally face criminal liability. They may not even be civilly liable if their actions were not negligent under the tort standard.
Criminal law falls short here because of the possibility an autonomous entity could cause criminal sorts of harms without accountability. This is likely to become a more significant problem as AI continues to become more advanced, common, and independent. So what should be done with an AI that is functionally committing criminal acts in ways not

59
reducible to the wrongful acts of individuals? One solution is to hold the AI itself liable and to convict it of a crime. A small but growing number of policymakers, academics, and people who have not carefully thought about it are advancing such arguments - or arguing more broadly in favor of AI being directly granted rights, obligations, or even legal personhood. Direct criminal punishment of AI might seem to follow from the principle of AI legal neutrality, which cautions against different legal treatment of AI and people.
Intuitively, the idea of punishing AI seems incoherent. How could the basic principles of criminal law such as the requirement for a “guilty mind” apply to a machine? But as with robot taxes, criminal punishment of AI is not as ridiculous as it may first appear. We already criminally punish artificial persons in the form of corporations. Even though corporations do not literally possess mental states, they can directly face charges when their defective procedures cause harm, particularly where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals. We criminally punish strict liability offenses, acts not requiring any wrongful mental state such as intent to cause harm. Punishment can even be imposed on a failure to act. In sum, punishing an artificial person for failing to act, even without evidence of harmful intent, is not something that can be dismissed out of hand. Criminal law can – and, where corporations are involved, already does – appeal to elaborate legal fictions to provide a basis for punishing some artificial entities.
An AI is not a company, and under current legal frameworks an AI could not be held criminally liable - but laws can be changed. Consider that in 2017, the Kingdom of Saudi Arabia announced it was giving citizenship to a robot named Sophia which was made by Hansson Robotics. That was probably more of a publicity stunt than a bona fide act, but

60
there is no immutable legal principle that prohibits robot citizenship. In England, for example, parliament has sovereignty to pass any legislation it wants, and it is not bound by a written constitution. In fact, English law used to incorporate punishment of inanimate objects. As far back as the 11th Century, if personal property caused a person’s death, the property was deodand, forfeited as an accursed thing and given to God. The remedy of deodand was not formally abolished until an act of parliament in 1846. Other jurisdictions such as India have proven even more flexible. Indian law recognizes that animals, rivers, and even deities can have legal personhood.
But just because laws can change does not mean they should. Legal changes can entail significant costs, and inappropriate changes can undermine the rule of law and trust in the legal system. To answer the question of whether AI should be criminally liable requires a serious look at its costs and benefits, and whether the doctrinal and theoretical commitments of criminal law are consistent with imposing criminal convictions on AI.
Punishing AI could produce general deterrence by discouraging other potential offenders from committing crimes. Just as intellectual property rights for AI-generated works would not directly motivate an AI, the prospect of punishment for AI-generated crimes would not directly deter an AI. The intent would again be to impact the behaviour of AI developers, owners, or users. This could occur if punishment is coupled with confiscation or destruction of a valuable AI, or if punishment is combined with financial penalties directed at AI owners. In addition to deterrence, AI punishment could psychologically benefit victims of AI-generated crimes who would get to see the state affirm their rights and punish an entity that has caused them harm. It would reassure citizens that criminal activity, even by AI, will not be tolerated in society.

61
Punishment should also not violate deeply held principles of criminal law. With people, the most important constraints on the law focus on culpability (moral fault) and insist that it would be unjust to punish someone in excess of their blameworthiness. Thus, it would be wrong to punish someone innocent, or to punish someone severely for a minor crime, even if doing so resulted in social benefits. This is a reason why criminal convictions require not only a harmful act but also a “guilty mind” such as an intent to cause harm. Punishing someone without even a capacity for culpability is not appropriate, which is why we do not punish toddlers for antisocial behavior. They lack sufficient reasoning and decision-making capabilities to be morally blameworthy. The requirement for culpability suggests that AI might not be punishable because it lacks mental states and the capacity for culpability. An even more fundamental concern, of a similar nature, is the requirement for a crime to involve a voluntary act. As an AI is not conscious, it is not clear that it is capable of performing an act – merely of physically causing harms. A hurricane cannot perform an act, but can cause no shortage of harm.
There are solutions to these challenges. One option is the solution that has been applied to allow corporate punishment. Corporations do not literally have mental states, but the law allows mental states possessed by human agents of the company to be imputed to the company itself. So, if company officers choose to engage in an illegal price-fixing scheme, the company is deemed to have possessed the intent to engage in wrongdoing. In the case of AI, we could similarly impute mental states from AI owners, users, or developers. Although, that might be more difficult for AI than for a company, particular in the case of an AI-generated crime, because there is not necessarily a ready supply of culpable individuals. A company, unlike AI, is composed of people – it’s level of autonomy is

62
zero. A different solution is to note that criminal law does not always require culpability. For example, strict liability crimes do not require a particular mental state. Depending on the jurisdiction, sexual activity with a minor or child may be a crime regardless of whether someone reasonably believed them to be an adult. Strict liability crimes are disfavored because we do not want to punish people who have acted without culpability, but that is because it is unjust to treat someone as a means to an end without respecting them as an individual. The same constraint does not apply to AI because it does not experience punishment negatively and it does not possess human rights that would be violated. Although, even strict liability crimes retain the requirement for a voluntary act.
More ambitiously, we could decide that AI is indeed capable of acting, and that its decision making involves something analogous enough to a human mental state. Functionally, an AI can acquire and process information, engage in logic and reasoning to determine the best means of achieving a goal, and then act on the world in a way to increase the probability of that goal occurring. Whether that counts as a genuine act or mental state, we could at least treat it as such. As a practical matter, it may be difficult to reason about what a machine was thinking, but juries often lack direct knowledge of a defendant’s mental state and infer what someone was thinking based on their behaviour. Juries could make similar inferences about an AI’s knowledge, intent, and aims based on its behavior. For example, if a self-driving car runs someone over, it could be deemed to have an intent to cause harm if it repeatedly changed its course to target a moving pedestrian.
Even if there are benefits to AI punishment, and even if punishing AI would not violate any fundamental principle of criminal law, there are still costs to AI punishment. It would require significant legal changes, such as giving AI legal personhood, which is a level

63
of disruption that should be avoided without good cause. It could also send a troubling message that AI is morally on par with a person. Given that rights and obligations often go hand in hand, this could entrench the view that AI is morally deserving of rights. Just as the rights afforded to companies in the US have gradually increased over time, an AI eligible for punishment today could be an AI eligible to vote tomorrow. In addition, if AI punishment involves destruction of an AI this may end up harming innocent AI owners who have done nothing wrong themselves.
There are better responses to the prospect of AI-generated crime than direct AI punishment. One is to expand criminal or civil penalties directed at people. For example, new legal duties could be created to responsibly develop, supervise, or remain accountable for an AI, with liability for failing to discharge those duties. That would be liability based on human conduct rather than liability for the harmful conduct of the AI itself. Directly punishing AI owner, users, and developers would likely be a more effective way of influencing their behavior than indirectly via AI punishment. For that matter, expanding civil liability is probably a better response than expanding criminal liability. That is because we do not want to overly chill activities like AI development which generate social benefit, and because AI-generated crime has not yet been a significant problem. That may change in the future, but for now there are not a bunch of autonomous and irreducible robot criminals terrorizing society. With expanded civil liability, failing to responsibly design or supervise an AI that causes harm could result in a tort claim, or perhaps a civil enforcement action by a government agency. This is a solution to AI-generated crime would more effectively provide the same benefits as direct AI punishment without the significant costs.

64
Punishment of AI should therefore be avoided – not because it is incompatible with criminal law, but just because it is a bad idea.
This analysis does more than argue against direct punishment of AI, it is illuminating about criminal law theory. In showing that a coherent theoretical case can be constructed for AI punishment, it shows that criminal law principles are flexible. It also sheds new light on justifications for corporate and human punishment. For example, some philosophers argue that people who engage in antisocial behavior are not morally blameworthy and therefore that punishment cannot be justified on the basis of culpability. There is a philosophical theory that holds that everything we do has been predetermined (determinism). If you were able to identically recreate the exact conditions of your life – from your genetics to your birth environment to the opportunities you encounter, you would only ever make the same choices. Some proponents of determinism further believe that because we will only ever make a single set of choices, that free will does not exist (hard determinism). If these theories are correct, and no one truly has free will because everything is predetermined, then it calls into question the role of culpability in criminal law. If someone lacks free will, then how can they be morally faulted for bad behavior?
Thinking about AI punishment helps to answer this concern. AI is capable of the same sorts of antisocial behaviors we are concerned about in people, but without moral blameworthiness. AI behaves deterministically and without free will – it simply executes programming. Even if a programmer cannot predict how an AI will behave in advance, or explain it afterwards, that is merely a matter of complexity and lack of knowledge. What the case for punishing AI may illuminate for human punishment is that criminal law is less concerned with moral blameworthiness than antisocial behavior. Criminal law is

65
concerned about something more functional than whether someone is deep down a bad person – it is concerned with behaviors that manifest a lack of respect for protected values. That helps us to better understand why we punish and whether punishment is justified, and it pushes back against claims that punishments are not justified because people lack free will. As a practical matter, this suggests that the law should do less to criminalize bad motives than public manifestations of antisocial behavior. It undermines support for so- called “thought crimes”, such as the United Kingdom’s Terrorism Act of 2006 which criminalizes “engaging in any conduct in preparation for giving effect to [a terroristic] intention...” In effect, this makes just about any activity illegal if someone has the requisite mental state. Laws like this should be disfavored because criminal law should be less concerned about whether someone is a bad person and more about the disrespect for social values that criminal conduct publicly manifests. Ultimately, the most interesting thing about AI punishment may be what it can teach us about ourselves.
6 Regulating Artificial Intelligence
AI considered in the context of tax, tort, intellectual property, and criminal law provides insights into how AI will affect existing legal standards and how legal standards will shape AI development. AI promises to be highly disruptive – and if history is any guide – in unexpected ways. Perhaps in hindsight AI will prove to have merely been part of another industrial revolution. Nevertheless, our legal system has not historically done the best job of limiting harm caused by technological disruption. Laws already regulate AI, just not intelligently because they were not designed with AI in mind. A different approach to our

66
legal frameworks could help optimize AI’s social benefits and ensure those benefits do not go to the few at the expense of the many. We are at the beginning of this process and the principles used to develop a legal system need to be, if not rethought, thoughtfully retooled with respect to AI before events overtake society. This has to be done the right way – laws should not change just because they can. For example, citizenship for the robot Sophia creates some obvious legal and ethical problems such as whether a robot citizen could vote, be owned, or murdered. The principles we have used to develop a legal system need to be, if not rethought, then at least retooled in respect to AI. We are at the beginning of this process, and we need to think about where to go before we are overtaken by events.
DeepMind’s Go win was a big deal, no less a milestone than IBM’s Deep Blue defeat of world champion Gary Kasparov at chess in 1997. Consider that it was Deep Blue that beat Kasparov, not Deep Blue’s programmers. Even playing against him as a team, the programmers would not have stood a chance. Instead, the programmers created an autonomous entity that engaged in an activity beyond their own capabilities. At that time, Deep Blue was one of the world’s most powerful supercomputers, capable of evaluating 200 million chess positions per second. Today, chess programs running on everyday smartphones can beat the world’s best human players.
After his match with Deep Blue, Kasparov had a realization: a person and an AI could play chess collaboratively and complement one another. A year later, he won the first “centaur” chess tournament where a human player and an AI play as a team. Not surprisingly, a person augmented by an AI proved better than someone playing unassisted. But a person and an AI also outperformed an AI playing by itself. Human grandmasters are good at long-term chess strategy but poor at quickly calculating millions of possible moves.

67
The reverse is true for chess-playing AI. Because people and AI are strong on different dimensions, they can do better working together than independently. At least, Kasparov was right to a point. In 2017, the same year AlphaGo beat the world’s best human Go player, the chess engine Cryptic beat the best human-AI team.10 Eventually, people may just get in the way.
 10 Emails with Arno Nickel, InfinityChess General Manager (Nov. 7 & 10, 2019) (on file with author).

68
1
Understanding Artificial Intelligence
Civilization advances by extending the number of important operations we can perform without thinking about them.
- Alfred North Whitehead
1 In the Beginning
The concept of AI has ancient origins. Around the eighth century BCE, the Greek poet Homer wrote in the Iliad about Hephaestus, the god of fire and a skilled inventor. Hephaestus built golden automata, or self-operating machines, to help him work. Not only did Hephaestus build attendants for himself with “intelligence in their hearts” and the “appearance [of] living young women,”1 he also built autonomous vehicles that could travel to and from the home of the gods and a lethal autonomous weapon system named Talos that patrolled the beaches of Crete. By contrast, Amazon’s assistant Alexa, Tesla’s Autopilot system, and the Russian Federation’s military robot FEDOR seem quaint, although Homer was vague on how mere mortals could enable such constructs.
Stories of artificial beings endowed with humanlike intelligence have pervaded countless histories and cultures. The most famous golem narrative in the Jewish tradition involves Judah ben Loew, the late-sixteenth-century chief rabbi of Prague, who is said to have fashioned a humanlike figure from clay and brought it to life with rituals and
1 HOMER, THE ILIAD OF HOMER (Richmond Lattimore trans., University of Chicago Press 1951).
    
69
incantations. This golem helped to defend against anti-Semitic attacks and pogroms, and it even performed household chores. There are many versions of the story. But much like the enchanted broom in Goethe’s “The Sorcerer’s Apprentice,” the golem experiment does not end well – often with a murderous rampage. Creators faring poorly is a recurrent theme in such origin stories. Prometheus, who created humankind in Greek mythology and passed along the technology of the gods, was condemned to suffer eternal torture. Mary Shelley’s 1818 story of Frankenstein was about a scientist dedicated to making artificial life, only to later reject his creation. The “monster” ultimately torments and destroys its maker.
Not all historical AI was myth or fiction; early automata could be quite sophisticated. Mechanists in ancient Greece constructed vending machines, water mills, and perhaps even the world’s first mechanical computer used to predict astrological movements – the Antikythera mechanism. Hellenic Egypt had statutes of gods that could speak, move, and even prophesize (although their predictive value is unknown). One of the great Roman engineers, Hero of Alexandria, wrote a treatise called “Pneumatica” that describes how to build numerous machines powered by air, steam, and water pressure. This text was used by engineers until early modern times.
Our incomplete historical record suggests the Antikythera mechanism may have been one of many ancient computers – at least mechanical computers. Originally, the term “computer” was used to refer to a person who manually performed mathematical calculations. Human computers were once critical to navigation, science, and engineering, but they left much to be desired. People as computers were slow and error prone, which could be a fatal flaw when bad math ran a ship aground. In the early nineteenth century, apparently while watching human computers at work, Charles Babbage decided to

70
automate the process. Perhaps at some earlier point a Greek machinist had a similar insight watching human astrologers at work, which led to the invention of the Antikythera mechanism. Astrological calculations were as important to the ancient Greeks as navigational calculations were to the Victorian English. Ironically, an Antikythera mechanism was found at the bottom of the sea in 1902.
But, back to Babbage. After deciding to create a mechanical computer, Babbage began by designing an automatic calculator he called the “difference engine.” This machine would have created tables of values by finding the common difference between terms in a sequence, and in turn this could have been used to generate logarithmic tables and trigonometric functions as well as simple price lists for merchants. Babbage completed various designs but never the actual machine. At that time, its building would have been a herculean task – the machine would have weighed about fifteen tons and contained around 25,000 parts. Other obstacles included the fact that Babbage was perpetually running out of funds and getting distracted by side projects, which included campaigning against street noise and perfecting an “infallible” system for gambling on horse racing that proved fallible.
While the difference engine may have been the first modern automatic calculator, its most important contribution may have been to inspire Babbage with a grander distraction: the analytical engine, the first general-purpose computer. Unlike earlier machines, the analytical engine could have carried out a wide range of operations using programs contained on punch cards. Ada Lovelace, the English mathematician and writer who sponsored Babbage, developed an algorithm that would have allowed the machine to generate a sequence of Bernoulli numbers, which have an important role in mathematics,

71
essentially making her the first computer programmer. Lovelace may have been the first to recognize the machine’s potential beyond pure calculation.
Babbage continued to design various iterations of the analytical engine until his death in 1871, but he never actually built the computer. Babbage’s inability to complete his work, combined with his anger at the British government for what he believed was inadequate support, left him embittered and disappointed at his end. He was appreciated – although controversial – in his time, but the true extent of his genius was not recognized until later scientists realized that the analytical engine had anticipated almost every aspect of present-day computers. Today, Babbage is considered the ”father of computing,” even though the first fully functional general-purpose computer, the Electronic Numerical Integrator and Calculator (ENIAC), was not completed until 1946. ENIAC weighed 30 tons and took up around 1,800 square feet. While scant comfort to Babbage, after his death two versions of the difference engine were successfully constructed and operated. Efforts are currently underway to build his analytical engine.
Computers or not, the fascination with automata, some practical and some less so, has continued from ancient to modern times. Hundreds of years before Babbage, Leonardo Da Vinci had constructed what may have been the first humanoid robot in Western civilization – Leonardo’s mechanical knight. It could sit, stand, raise its visor, and maneuver its arms by means of a series of pulleys and cables. Leonardo created a “robot” before that term existed. The word was coined in a 1920 play R.U.R. (an acronym for Rossum’s Universal Robots) by Czech playwright Karel Capek, who based it on the Czech word for “forced labor.” The play takes place around the year 2000 in a world in which robots are cheap, ubiquitous, and indispensable. These factory-built constructs are capable of

72
independent thought and are creatures of flesh and blood rather than gears and pulleys. Alas, the robots’ creators fare about as well as the golem’s. In Capek’s play, the robots initiate a rebellion that more or less exterminates humanity.
Even early in the twentieth century, long before modern computers made the visions of The Terminator and The Matrix appear plausible, the themes in R.U.R. had widespread appeal. By 1923, the play had been translated into thirty languages, and the word “robot” had entered the English lexicon. Incidentally, the first reported robot fatality occurred on the fifty-eighth anniversary of R.U.R.’s premiere. In 1979, Robert William, a Ford assembly worker, was killed when a robot’s arm struck him in the head. In subsequent litigation, a jury found this accident was due to a lack of safety measures and awarded William’s estate $10 million in damages.
Killer robots and the AI apocalypse continued as recurring themes after R.U.R. One of the most famous science fiction writers of the twentieth century, Isaac Asimov, writes extensively about a future filled with AI. He proposes a set of ethical rules he calls the Three Laws of Robotics, which are hardwired into the “positronic brains” of robots and prevent them from turning against their creators:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.2
2 Isaac Asimov, Runaround, in I, ROBOT 40 (Isaac Asimov Collection ed., Doubleday 1950).
     
73
Many of Asimov’s own stories explore how these deceptively simple rules can result in unexpected and sometimes destructive outcomes. Sometimes, both action and inaction result in harm, and robots may need to cause a lesser harm to prevent a greater one. In Asimov’s later stories, some robots come to understand that the best way to prevent harm is for them to rule over humanity. People, left to their own devices, are perpetually harming themselves and others.
The Birth of Modern AI
In 1956, a decade after ENIAC, the term “artificial intelligence” was coined by John McCarthy, another pioneering AI researcher. McCarthy organized the Dartmouth Artificial Intelligence conference, which is often credited with establishing AI as a research discipline. In his proposal for the event, he defined AI as follows: “For the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.”3 The conference proceeded “on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”
Since the 1956 conference, AI, as both a research discipline and practical technology, has seen its share of ups and downs. In the 1950s and 1960s, optimism prevailed that machines would soon exhibit, and exceed, human levels of intelligence.
3 J.   ,M.L.   ,N.   ,&C.E. ,AProposalfortheDartmouth , Aug. 31, 1955, www-
formal.stanford.edu/jmc/history/dartmouth/dartmouth.html.
   McCarthy
Minsky
Rochester
Shannon
 Summer Research Project on Artificial Intelligence
  
74
Computers developed the capabilities to solve algebraic problems, competitively play games like checkers, and speak English. In the 1970s, that optimism faded when promised results failed to materialize quickly enough and funding dried up. This downturn came to be known as the First AI Winter. It ended in the 1980s as “expert system”–based AI, which solves problems using logical rules derived from the knowledge of experts, enjoyed some significant successes. But in the 1990s, a sense again emerged that the capabilities of AI had been oversold, and another period of decreased interest and funding took hold – the Second AI Winter. Some researchers during this time even took to calling their work “machine intelligence,” “informatics,” or “knowledge-based systems” to avoid association with AI.
The AI winters have passed, but plenty of hype remains. Hype has been another recurring feature of the AI narrative. Hundreds of years before Deep Blue defeated Gary Kasparov at chess, Wolfgang Von Kempelen created the world’s first chess-playing machine, the Turk. After showcasing the Turk to Austria-Hungarian Empress Maria Theresa, the machine became a European sensation, performing for, and perhaps defeating, the likes of Napoleon Bonaparte, Benjamin Franklin, and Charles Babbage. The Turk was an ingenious feat of engineering, but only for the machine’s ability to secretly house a human player rather than any innate chess-playing skill. Eventually, the Turk was discredited, then consigned to storage where it was later destroyed in a fire. Some overhyped automatons have had even less happy endings. In the early 1930s, an apparently autonomous vehicle called the “Phantom Auto” toured the country and amazed the public. The tour ended abruptly in 1932 when the vehicle injured ten pedestrians. The Phantom Auto turned out to be a remotely operated Ford Tin Lizzy, and its operators were arrested.

75
Today, experts remain divided on whether AI has finally reached a point of runaway progress or whether winter is coming. Some thought leaders, such as Ray Kurzweil, one of Google’s directors of engineering, predict computers will have human levels of intelligence in about a decade. On the other hand, in 2018 a robot touted as the most advanced machine ever created in Russia turned out to be a man in a suit.4
2 Defining AI
Intelligence is whatever hasn’t been done yet.
- Larry Tesler
More than 60 years after the term was introduced, AI still has no well-accepted definition. This book will discuss AI using the following definition: an algorithm or machine capable of completing tasks that would otherwise require cognition. Cognition refers to mental capabilities and the process of acquiring knowledge and understanding through thought. This is a deliberately broad definition of AI that focuses on what it does rather than how it is designed. The perhaps most popular textbook on the subject defines AI as “the designing and building of intelligent agents that receive percepts from the environment and take actions that affect that environment.”5 Many modern AI definitions retain McCarthy’s original functional emphasis (and circular logic) – a machine that completes tasks traditionally requiring human “intelligence.”
  Andrew
 Griffin
, “Russia’s Most Modern Robot” Revealed to Be Just a Person in a Suit, THE
4
INDEPENDENT, Dec. 12, 2018, www.independent.co.uk/life-style/gadgets-and-
 tech/news/russia-robot-person-in-suit-fake-hoax-most-modern-advanced-a8680271.html.
   5
& PETER   ,   (3rd ed. 2009).
STUART
RUSSELL
NORVIG
ARTIFICIAL INTELLIGENCE: A MODERN APPROACH

76
An algorithm is a set of mathematical instructions or rules, which in the form of a software program can instruct computer hardware, the physical components of a machine, to perform specific tasks. Each program is essentially a step-by-step instruction guide, like a cooking recipe, that provides a set of ordered operations to the computer. Hardware runs or executes algorithms, though an algorithm developed and readable by a person is rarely directly usable by a computer. An algorithm is usually written in a high-level programming language (source code) because these are close to natural language. The source code is then usually translated to machine language (object code). Machine language consists of binary values (0s and 1s) that provide processor instructions that change a computer’s state. To execute an algorithm, a computer needs instructions in a machine language and a hardware processor such as a central processing unit. For example, an instruction may change a stored value somewhere in memory or ask the operating system to print text to a monitor. An analogy has been drawn between software and hardware on the one hand, and mind and body on the other.
Modern computers have billions of transistors, which can be used as electronic switches. They exist, like binary code, in either an “on” (1) or “off” (0) state. An algorithm can accomplish a task as simple as flipping a single switch, which corresponds to changing one bit of information. AI can engage in simple logical reasoning by changing transistors in response to other transistors; for example, “if transistor X turns on, then transistors Y and Z turn on.” Algorithms can engage in complex logical reasoning by combining multiple operations and flipping switches up to billions of times per second, the acts of which become responsible for everything a computer can do, from translating English to Zulu, modeling a new chemical, or playing a game of backgammon.

77
That is algorithms at their most basic, but defining “AI” begs a definition of “artificial” and “intelligence.” Of the two, artificial seems more straightforward – something made by people rather than nature. Today, at least, it is possible to distinguish natural from artificial, even if the distinction shows signs of blurring. Researchers have created biological computers that use natural proteins and DNA to perform calculations involving the storing, retrieving, and processing of data. As this technology develops, it may be possible to engineer biological systems that have the functional capabilities of mechanical computers. Not content with making machines out of natural building blocks, researchers are also hard at work trying to upgrade people. Genetically modified “designer” babies have already been created using technologies like CRISPR, mitochondrial replacement, and in vitro gametogenesis. Elon Musk has launched a venture, Neuralink, that is developing a brain-computer interface so that people can be more competitive with AI. That technology is still in its infancy, but people are already being implanted with an increasingly sophisticated range of medical devices.
Intelligence is harder to define than artificial, or at least to make sense of it in the context of AI. Shane Legg and Marcus Hutter surveyed a number of prominent informal definitions of intelligence, and they argue that intelligence is commonly defined in terms of “an agent’s ability to achieve goals in a wide range of environments.”6 Intelligence is a feature commonly associated with people, sometimes even considered their defining characteristic. More than two thousand years ago, the philosopher Aristotle essentially argued that human intellect – the capacity to make rational decisions – separates people
   6 17
Legg &     , Universal Intelligence: A Definition of Machine Intelligence, 391, 444 (2007).
Shane
Marcus
Hutter
 MINDS AND MACHINES

78
from other animals. Modern philosophers (and translators) are still arguing about what he really meant.
The French philosopher René Descartes believes people are guided by an immaterial, or spiritual, mind while the rest of nature, including animals, is nothing more than a series of mindless objects driven inexorably by the laws of physics. He thus thought it would be possible to build an automaton indistinguishable from an animal but not from a person. A machine, he argues, can never use words or put together signs in the way we “declare our thoughts to others.”7 Even if it could give a poor imitation of speech, it could certainly not give an “appropriately meaningful answer to what is said in its presence, as the dullest of men can do.”8 Descartes did not appeal to ethereal qualities like a soul or emotion to distinguish people from machines and animals, but rather to communication and reasoning abilities already achieved by today’s AI.
People are generally assumed to be smarter than other animals, but when a person lacks intelligence we do not consider them less human, and other animals are capable of some fairly intelligent behavior. Chimpanzees can outperform some human players at some speed and memory-based games, octopuses use tools and socialize, dolphins have unique names and sophisticated means of communication, and elephants exhibit empathy. Koko the gorilla famously learned a modified version of American Sign Language, understood around 2,000 English words in addition to the signs, and named and adopted a kitten. She passed away in 2018. But in her youth, she scored between 70 and 90 on intelligence
7 René , , in 1 109, , ,& trans.,
1985). 8 Id. at Part V.
    Descartes
Discourse on Method
THE PHILOSOPHICAL WRITINGS OF DESCARTES
       109–151
(John
Cottingham
Robert
Stoothoff
Dugald
Murdoch
Cambridge
 University Press

79
quotient (IQ) tests. A significant number of people score in Koko’s range, and a score of 100 is the median. There is no reason Koko and a person of Koko-like knowledge and sophistication should not both qualify as intelligent. Charles Darwin similarly thought human intelligence was not different from other animal intelligence in kind, merely in degree.
Whether a machine with similar capabilities should qualify as intelligent is more controversial. People have attempted to subject AI to modified IQ tests, and a study in 2013 resulted in press coverage that claimed the smartest computers were as intelligent as four- year-old children. That study showed AI performing relatively well on the verbal portion of an IQ test and poorly on the comprehension portion. While AI should be good at memorizing words, an excellent vocabulary combined with poor comprehension does not equal a toddler with common sense. Comparing people and machines along a single dimension of intelligence has limited meaning. Google Translate can interpret more than a hundred languages almost instantly, but that is it. While faster and more versatile at translation than any imaginable person, it cannot write music, play poker, or have an existential crisis. AI like Google Translate is clearly different in kind from human intelligence.
In 1904, the psychologist Charles Spearman advanced the concept of “general intelligence.” He argues that people have a single general intelligence that determines cognitive performance in addition to narrow task-specific abilities. He was aware that people perform better at some tasks than others but found that people who do well in one area tend to do well in many domains. Someone good at math is more likely to have a strong vocabulary than someone with poor math skills. Modern advocates of human

80
general intelligence are more likely to subscribe to the belief that IQ scores are a valid measure of intelligence. Critics argue that the concept of general intelligence is not supported by evidence and devalues other important abilities.
Today’s machines lack general intelligence – all modern AI is narrow or specific. It focuses on discrete problems or works in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms nor can Chef Watson run a kitchen. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve. By contrast, one of the earliest goals of AI development has been the creation of general AI that would be able to successfully perform any intellectual task a person could.
General AI could even be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence – what philosopher Nick Bostrom terms artificial superintelligence. Artificial superintelligence could then innovate in all areas, resulting in progress at an incomprehensible rate. Such an outcome has also been referred to as the intelligence explosion or the technological singularity. This idea has been popularized in recent years, but it is an old concept. As the AI pioneer Irving John Good wrote in 1965, “the first ultraintelligent machine is the last invention that man need ever make.”9
    9
Good, , in ADVANCES IN 33,   (Ser. No. 6, 1965).
Irving J.
Speculations Concerning the First Ultraintelligent Machine
 COMPUTERS
31–87

81
Experts are greatly divided on when, and if, general AI will be developed. Many industry leaders predict based on historical trends that general AI will exist within the next couple of decades. Others believe the magnitude of the challenge has been underestimated, and that general AI will not be developed in the twenty-first century. While there are numerous conflicting predictions, in 2013 hundreds of AI experts were surveyed on their predictions for artificial general intelligence development.10 On average, participants predicted a 10 percent likelihood that general AI would exist by 2022, a 50 percent likelihood it would exist by 2040, and a 90 percent likelihood it would exist by 2075. In a similar survey, 42 percent of participants predicted general AI would exist by 2030, and another 25 percent predicted by 2050.11 In addition, 10 percent of participants reported they believed superintelligent AI would develop within two years of general artificial intelligence, and 75 percent predicted this would occur within thirty years.
3 Can AI Think?
Recently there has been a good deal of news about strange giant machines that can handle information with vast speed and skill ... . These machines are similar to what a brain would be if it were made of hardware and wire instead of flesh and nerves ... . A machine can handle information; it can calculate, conclude, and choose; it can perform reasonable operations with information. A machine, therefore, can think.
-
Edmund Berkeley (1949)
553, (Vincent C.
     Vincent C.
10
, in Müller ed., 2016).
&Nick ,
Müller
Bostrom
Future Progress in Artificial Intelligence: A Survey of
   Expert Opinion
FUNDAMENTAL ISSUES OF ARTIFICIAL INTELLIGENCE
553–571
  11 See JAMES BARRAT, OUR FINAL INVENTION: ARTIFICIAL INTELLIGENCE AND THE END OF THE HUMAN ERA 152 (2013).

82
In 1950, Alan Turing, the computer scientist who launched and inspired much of AI, published a paper in which he “propose[s] to consider the question, ‘Can machines think?’”12 He proceeds to dismiss inquiries about whether a machine understands what it is doing, and to reframe the question to ask whether a computer could functionally imitate a person. Turing then argues that a machine would functionally think if it could pass an “imitation game” in which a computer would have to successfully pretend to be a person to a suspicious judge. Turing predicted computers would have enough storage capacity to pass what has subsequently been named the Turing test, in about fifty years, that is, by the year 2000.
Another school of thought about AI says that intelligence requires understanding. In this view, machines do not qualify as intelligent regardless of what they can do because they do not understand what they are doing. Action without understanding merely simulates intelligence, even for a superintelligent AI. Instead of specific and general AI, philosopher John Searle puts the distinction between acting and understanding in terms of strong (thinking) and weak (unaware) AI. To illustrate, consider Searle’s Chinese room thought experiment. First, assume you have no Chinese language proficiency. Now, imagine you are kept in a room where a message in Chinese characters is presented to you through a small opening in the wall by a native Chinese speaker. You cannot understand the meaning of the characters, but you have a book that contains every possible combination of Chinese characters, together with a corresponding appropriate response. You look up the characters you have been provided in the book, you transcribe the appropriate response onto a piece of paper (assume you can figure out how to write Chinese characters), and you
  Alan M.
 Turing
 12
, Computing Machinery and Intelligence, 59 MIND 433, 433 (1950).

83
pass your response back through the opening. The person who sent you the initial message does receive an appropriate response. Functionally, you would be mimicking Chinese communication like a native speaker, albeit very slowly. However, you would not understand the meaning of the message you received or your response. This is a model for thinking about AI. Substitute Google Translate for the Chinese room, and you have a faster, and less accurate, version of the same phenomenon. Google Translate may pass the Turing test, but it is weak AI – it lacks semantic understanding.
Part of the challenge with arguing machines that cannot think is that human thought remains incompletely understood. For generations, there was little empirical evidence about the nature of thought, which was the domain of philosophers, theologians, and poets who could endlessly debate the subject in the absence of definitive proof. Eventually, neurobiologists wedged their way into the debate, some of whom argue that thought is a physical phenomenon that inevitably results from biological systems like a person’s higher nervous system. If so, a living being similar to a person, if not a dolphin or a primate, would experience something close to human consciousness. For that matter, if the mind is no more than the sum of a biological system, there is no reason why it should not be possible to re-create the same phenomenon with a machine – or for a machine to improve upon human thought. In this view, a person is simply a machine, albeit a phenomenally complex one – a “meat machine,” as cognitive scientist Martin Minsky eloquently put it. This mind- machine view conflicts with the beliefs of a long line of philosophers reaching back to Plato who believed that the human mind or soul is distinct from the physical body. Descartes similarly thought the mind and body are distinct in substance and nature, and that while

84
the body can be divided, the mind is indivisible, like the concept of a soul in Christian theology.
More recently, philosopher David Chalmers weighs in on the mind-body issue to distinguish between the easy and hard problems of consciousness. The easy problems include explaining the ability to focus attention, discriminate, integrate information, and so forth. These are problems with mechanistic solutions, and neurobiologists and computer scientists are quickly making progress toward understanding the human nervous system and developing comparable AI capabilities. The hard problem of consciousness refers to why people have phenomenal experiences, which is to say the sensation of internal states. Mechanistic explanations seem unable to explain what it is like to see red, hear music, or feel cold. The existence of phenomenal consciousness in a material world remains a metaphysical puzzle.
We are far away from fully understanding consciousness, but there are things we do understand. For instance, consciousness can be localized, at least to a degree, within the central nervous system. The central nervous system is made up of the brain and spinal cord, and the brain has three major parts. The cerebrum, consisting of a right and left hemisphere, is the largest and most evolved part of the brain and is responsible for higher functions like speech, reasoning, learning, and hearing; the cerebellum coordinates muscle movements and maintains posture and balance; and the brainstem connects the cerebrum and cerebellum to the spinal cord and regulates things like body temperature, breathing, and heart rate.
People who have lost their spinal cords and even suffered massive injuries to their cerebellums and brainstems can still retain normal consciousness. This suggests that

85
consciousness is generated in the cerebrum. Consciousness can even be localized further within the cerebrum. People who have had large portions of their prefrontal cortex (the part of the cerebrum nearest the forehead) removed or destroyed often do not experience any effect on their conscious experience. However, the removal of even small parts of the posterior cortex can result in dramatic losses of conscious content – the inability to see or recognize motion, color, or space.
The cerebrum has a very different neural architecture than the cerebellum. The cerebrum has a relatively less dense concentration of neurons that are highly connected in networks and activate in complex feedback loops. By contrast, the cerebellum has a higher concentration of neurons but in relatively uniform and parallel structures where groups of neurons function independently in a feed-forward circuit (one set of neurons activates the next). If a person is merely a biological machine, then the cerebrum’s structure is likely a key part of the consciousness puzzle, or even the key to future machines with self- awareness.
But what has all this done to resolve the question of whether AI thinks? On one level the question can be addressed by noting that AI does not think the way a person does. AI is not conscious or self-aware in the same sense as a person is. On another level, the question can be addressed by noting that it remains more relevant to philosophers than to policymakers. The reason for this is that the social benefits of AI are based on what AI can functionally accomplish, regardless of how it processes information. AI does not need to think, mimic human thought processes, or exhibit semantic understanding (essentially, perceiving the meaning of its actions). It simply needs to automate human intellectual activity. AI does not need to think – just do.

86
4 Types of AI
There are many ways to design AI with various techniques, systems, and methodologies. Over time, these have turned into subfields based on specific goals, such as robotics or machine-learning, or specific tools, such as logic or neural networks. Divisions have also arisen over philosophical differences and conflicts between institutions and researchers. Two of the major AI models are symbolic and classical, what philosopher John Haugeland calls good old-fashioned AI (GOFAI), which encodes a model of a problem and processes input data according to the model to provide a solution, and connectionist systems such as artificial neural networks in which users do not explicitly program rules but allow the network to discover rules from training data.
Different types of AI have their own strengths and weaknesses. Symbolic systems, for example, are more explainable, making it is easier to decipher how an AI works. This may be particularly important for applications where it is necessary to understand how an AI transforms input to output. Programming symbolic AI is about creating rules for software to follow. If a specific event occurs, then a particular action will be performed. Some AI pioneers like Allen Newell and Herbert Simon believed that such rules govern human psychology.
Symbolic AI, such as Arthur Samuel’s checkers player in the late 1950s, has accomplished great feats. That program learned to beat its programmer and gave an early hint that computers would develop superhuman performance. About forty years ago, a symbolic rule-based system outperformed the leading human expert in soybean disease diagnosis and selected appropriate treatment. In 1972, another symbolic AI called MYCIN

87
was created to advise physicians on identifying infectious diseases and prescribing antibiotics, and it was able to outperform infectious disease experts in limited circumstances.
As early as the 1956 Dartmouth conference, Newell and Simon presented an AI system they called “Logic Theorist,” which approximated a model simulating human behavior relying on heuristics. Heuristics refers to rules of thumb that people use to help make decisions, or general methods of solving programs that make brute-force searching through an entire space unnecessary. The Logic Theorist attempted to prove a theorem by guessing a solution and then endeavoring to demonstrate the guess was correct. It independently proved some of the theorems in Principia Mathematica, a foundational mathematics text, and even found a more elegant proof to one of the theorems than was found in the text. If logic and consistency are the defining characteristic of human thinking, as Aristotle argues, the Logic Theorist may have received a passing grade. Convincing the establishment was another matter, however, as the Journal of Symbolic Logic refused to publish the proof in an article with the Logic Theorist listed as an author.13
Without heuristics, problem-solving approaches may rely on brute-force computational methods, such as generating all possible combinations of symbols to prove a mathematical theorem. Newell and Simon referred to this as the British Museum algorithm – a riff on the infinite monkey theorem, which predicts that a room full of monkeys with typewriters will eventually reproduce by random chance all the books in the British Museum. That may be theoretically possible, but imagine you have set a monkey to the task of typing the word “banana” on a typewriter with fifty keys. If the monkey presses keys at
13 HERBERT A. SIMON, MODELS OF MY LIFE (MIT Press 1996).
    
88
random, and each key has an equal chance of being pressed, the chance the first six letters pressed will spell banana is (1/50)6 (one in fifteen trillion, six hundred and twenty-five billion). Thus, the probability of giving a monkey a typewriter and having it type banana is more than zero, but you would be lucky to have it happen if you tried all day, every day, for the rest of the monkey’s natural life. By the time you start having monkeys try to re-create the complete works of William Shakespeare without errors, assuming you had earlier discovered the secret to immortality and applied it to both yourself and the monkeys, you and the monkeys would probably be hard at work at the end of the universe. To drive the point home, someone actually performed an experiment in which a computer keyboard was left in an enclosure of six Celebes crested macaques in Paignton Zoo in England. Over the course of a month, the monkeys produced about five pages of writing that largely consisted of the letter “S.”
Thankfully, computers are better than monkeys at this sort of work, yet even the most powerful computers are limited when dealing with sufficiently large numbers. Checkers has fairly simple rules, but a staggering number of possible moves – some 5020 possibilities. It is hard to think about numbers that large, but computer scientist Jonathan Schaeffer compares it to draining the Pacific Ocean and refilling it with a tiny cup, one cup at a time. Schaeffer spent decades developing an AI called Chinook that eventually solved checkers in 2007. Chess is more complex – there are about 10120 possible board configurations, which puts a solution beyond the limits of feasible technology. Deep Blue could only generate every possible move for the next eight moves, and both checkers and chess are nothing like Go, which has 10172 possible board configurations. While brute-force

89
computation can work very well with problems that have a limited number of possible solutions, it is less useful for others.
While symbolic systems rely on explicitly programmed rules, connectionist systems have a different means of generating intelligent behavior. They represent rules in interconnected networks of simple and even uniform units such as artificial neurons. Many of the most dramatic advances achieved by AI in recent years have come from improvements in connectionist techniques such as machine-learning, artificial neural networks, and deep learning. Machine-learning allows computers to learn from examples and to generate their own rules. These rules may be more accurate than rules explicitly created by programmers, particularly where the rules are based on large datasets and complex patterns that are difficult for people to directly interpret. This is particularly valuable for pattern recognition, statistical modeling, and data mining, which is useful for tasks such as generating Internet search results and using facial recognition technology.
ANNs are one set of algorithms used in machine-learning that are inspired by the architecture of the human brain. Each person has about 90 billion individual neurons in their brains, which are interconnected in highly complex networks with perhaps hundreds of trillions of connections. These connections change dynamically, or the strength of connections change, as a result of learning and experience. Similarly, ANNs consist of networks of interconnected layers of algorithms that feed data into each other and that can be trained to complete tasks by altering the relevance of data as they pass between layers. During training, the weights attached to different inputs change until the network produces a desired output, at which point the network has developed the ability to complete a task. For instance, with a facial recognition AI like TrueFace.ai, the algorithm trains on a large

90
data set of faces, with various lighting, angles, and distances from a camera, and the program learns how to measure and process specific points on faces and to identify individuals. Deep learning involves passing data through many layers of ANNs. As the networks process data, connections between parts of the networks adjust, developing an ability to interpret future data. Deep neural networks are responsible for many of the recent advances in speech recognition and computer vision.
Connectionist AI can be tremendously complex with difficult to understand and explain inner workings. For this reason, such systems in particular are sometimes referred to as black box algorithms. In theory, it is usually possible to explain what these systems are doing, but it may not be practical. Connectionist systems may be more useful for analyzing complex models where underlying relationships are poorly understood and where an AI may be necessary to discover new patterns. This may be especially valuable in situations where programmers do not know all the rules for generating desired output, where the AI may learn rules not easily interpreted by programmers, or where the rules are very complex and hard to program directly. This allows AI not only to solve problems we know the answers to but also problems we do not – perhaps even to solve problems we do not know exist.
5 Advances in AI
While the vision of general AI has yet to manifest, AI has improved dramatically since the 1950s and achieved some of its early goals. AI can outperform people at a growing number of specific tasks, and it is now being applied successfully in nearly all areas of industry. This

91
is largely due to three parallel improvements: more advanced software, greater computing power, and the development of big data.
AI advances move quickly, and the improvements take less and less time with each AI incarnation. Deep Blue is ten million times faster than the first machine used to play chess in 1951 and is capable of evaluating more than 200 million moves per second. The machine, which awed programmers and laypersons alike in 1997, has been retired. A Samsung Galaxy S10 smartphone is fifty times faster than Deep Blue. The next generation of that smartphone, we can say with certainty, will escort the S10 to the electronics recycling center within a few years of this book coming out. The modern iPhone has around 100,000 times more processing power than the Apollo 11 computer that got Neil Armstrong and Buzz Aldrin to the moon in 1969, and it holds millions of times more memory. These changes occurred in a span of seventy years, but the vast improvement between the modern smartphone and Deep Blue happened in less than a generation.
As computing power has improved, so too has the amount of data available to AI. In 2017, the International Data Corporation forecasted that the “global datasphere” will include 163 zettabytes of information by 2025. That is more than ten times the 16.1 zettabytes of data available in 2016. It is difficult to conceptualize data on that scale: 163 zettabytes are roughly equivalent to sixteen million years of high-definition video or twenty billion US Libraries of Congress.
Big data can be critically important for AI, and vice versa. Some AI can deal with large and complex datasets in ways that people and traditional software cannot. For example, people may not be able to recognize patterns among millions of data points. Conventional algorithms that operate according to fixed, logical rules may similarly

92
struggle to operate with large datasets that have unknown properties. In such cases, AI that utilizes machine-learning, deep learning, or neural networks may be more flexible and effective. Such AI can use big data to extrapolate trends, detect anomalies, and automate labor. Big data can also be used to train some forms of AI to progressively improve performance without explicit programming. The importance of data for this and other purposes has led to data being called the new oil or the oil of the twenty-first century. As the European Commission noted in 2018, “data is the raw material for most AI technologies.”14
All big data are not created equal, however, and “garbage in, garbage out” is a maxim in the AI community. For big data to have value for AI, the right kinds of data need to be collected, aggregated, and appropriately utilized. Different types of data are necessary for different AI applications. Take the example of using AI to analyze health care insurer electronic health records to determine if a pharmaceutical medication has adverse events. It may be necessary to have data on exposures (with dates), outcomes, and other health conditions, together with insurer enrollment and demographic information. It may also be helpful to link datasets across different time points and across different insurers. Failure to obtain comprehensive and accurate data used for AI input may result in an incorrect AI output, or if the data are being used for machine-learning it may result in an inaccurately biased AI.
   ,
2018, https://europa.eu/rapid/press-release_IP-18-3362_en.htm.
14
EUROPEAN COMMISSION PRESS RELEASE IP/18/3362
ARTIFICIAL INTELLIGENCE: COMMISSION
 OUTLINES A EUROPEAN APPROACH TO BOOST INVESTMENT AND SET ETHICAL GUIDELINES
, Apr. 25,
 
93
Assuming that the necessary data have been collected, they still need to be brought together and appropriately translated for AI. Most AI systems are unable to work with unstructured data, which are generally distinguished from structured data by their degree of organization. Structured data usually exist in relational databases where they are categorized and sorted into distinct, easily searchable fields. Unstructured data, by contrast, do not have a predefined data model. They may exist as a heterogenous, free-form mix of text, audio, video, and social media postings that have not been labeled or sorted. Most big data now are unstructured, and significant cost and effort may be needed to structure them. Even without AI, it may be difficult to extract value from unstructured data with more conventional analysis, to process large amounts of unstructured data, and to secure unstructured data.
Applications
As mentioned above, AI can be applied to a wide range of technologies using diverse processes such as logic, if-then rules, decision trees, and machine-learning. These processes in turn can be used to mimic human behavior in areas like visual perception, speech recognition, and language translation. Some AI exists only on the cloud, an interconnected network of remote servers, while others are physically embodied in robots and can directly affect the physical world. However, AI without the ability to directly affect the physical world can often have a no less powerful indirect effect – like stopping someone’s pacemaker.

94
Some practical areas where AI has enjoyed recent advances have been in the areas of computer vision and natural language processing. Computer vision deals with how machines can gain a high-level understanding from images or videos, essentially, automating the human visual system. For instance, Facebook’s image recognition technology now recognizes people, objects, expressions, activities, events, and spaces. This AI has a very rich source of training data – more than two billion photos are uploaded daily to Facebook and platforms it owns: Instagram, WhatsApp, and Messenger.
The ImageNet project runs an annual AI contest, in which programs compete to correctly classify and detect a thousand nonoverlapping classes of objects and scenes. In 2015, Microsoft announced it beat the competition’s dedicated human labeler by classifying more than 95 percent of the images correctly.15 In 2017, twenty-nine of the thirty-eight competing teams classified more than 95 percent of the images correctly.16 These are impressive achievements, though machines still struggle with visual identification in many situations that are no challenge to people. It should be noted that developing large datasets can be enormously costly. ImageNet at its peak employed 48,940 people in 167 countries who sorted about fourteen million images.
AI has made similar advances in natural language processing, to which anyone who now uses a program like Siri, Alexa, or Cortana can attest. The idea of machine translation
15 Ophir Tanz, Can Artificial Intelligence Identify Better than Humans?, ENTREPRENEUR, Apr. 1, 2017, www.entrepreneur.com/article/283990.
16 Dave Gershgorn, The Quartz Guide to Artificial Intelligence: What Is It, Why Is It Important, and Should We Be Afraid?, QUARTZ, Sept. 10, 2017, https://qz.com/1046350/the-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it- important-and-should-we-be-afraid/.
     
95
has been around since the 1950s, along with predictions that fully realized machine translation would exist within a few years. Those predictions were unrealistic, but today’s machine translation has come a long way. Google Translate, for instance, supports more than a hundred languages, including many by photo, voice, and even real-time video. The Google Translate app is used by more than 500 million people and translates around 143 billion words daily.17 To take another example, Microsoft announced in 2017 that its speech recognition system performed as well as human transcribers in recognizing speech from Switchboard Corpus, a collection of thousands of recorded random conversations. Fifteen years before that, machine translation word-error rates hovered around 20–30 percent. In 2017, Microsoft’s technology reached an error rate of 5.9 percent, the same as a human transcriber.
Some industries such as telecommunications, automotive, and financial services already have high rates of AI adoption. In the automotive industry, AI is powering self- driving car technologies, which are set to radically transform transportation. Driverless cars are already being tested on public streets in several countries. Such products could not exist without AI. Other industries like education, health care, and tourism have lower rates of AI adoption, but this may change soon.
Given the lack of a standardized definition for AI, it is somewhat difficult to compare studies of AI’s economic impact. However, it is clear that AI has disruptive economic potential.18 An estimate by PricewaterhouseCoopers suggests that AI could contribute
17 Rory Smith, The Google Translate World Cup, THE NEW YORK TIMES, July 13, 2018, www.nytimes.com/2018/07/13/sports/world-cup/google-translate-app.html.
   Wendy
18 2017,
Hall &     , Growing the Artificial Intelligence Industry in the UK, Oct.
Jérôme
Pesenti

96
$15.7 trillion to the global economy in 2030, of which $6.6 trillion could come from increased productivity and $9.1 trillion from consumption-side effects.19 A report by the McKinsey Global Institute claims the disruptions caused by new technologies such as AI will “happ[en] ten times faster and at 300 times the scale, or roughly 3,000 times the impact” of the Industrial Revolution.20
6 AI Characteristics
Understanding the key characteristics of AI is important for understanding how it should be regulated and how AI challenges existing legal systems.
First, as discussed earlier, AI may have limited explainability. It may be possible to determine what an AI has done but not how or why it acted as it did. This has led to some AI being described as black box systems. For instance, an algorithm may decline a job application but be unable to articulate why the application was rejected. That is particularly likely in the case of AI that learns from data, and which may have been exposed to millions or billions of data points. Even if it is theoretically possible to explain an AI
https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachme nt_data/file/652097/Growing_the_artificial_intelligence_industry_in_the_UK.pdf.
19
(2017), www.pwc.com/gx/en/issues/analytics/assets/pwc- ai-analysis-sizing-the-prize-report.pdf.
     PRICEWATERHOUSECOOPERS
,
SIZING THE PRIZE: WHAT’S THE REAL VALUE OF AI FOR YOUR BUSINESS
 AND HOW CAN YOU CAPITALISE?
        20
, The Four Global Forces Breaking All ,   (2015), www.mckinsey.com/business-
Richard
Dobbs
, James
Manyika
,&
Jonathan
Woetzel
 the Trends
MCKINSEY GLOBAL INSTITUTE
 functions/strategy-and-corporate-finance/our-insights/the-four-global-forces-breaking- all-the-trends.
  
97
outcome, it may be impracticable given AI complexity, the possible resource-intensive nature of such inquiries, and the need to maintain earlier versions of AI and specific data.
Second, also as discussed earlier, while AI can already outperform people in spectacular fashion in some domains, such as board games, in other domains AI is not competitive with toddlers. That is because all AI is designed to perform narrow or specific tasks. DeepMind’s AI can beat the world’s best human player at Go, but it cannot translate English to French without being programmed to do so. By contrast, the holy grail of computer science research is developing general AI that can perform any task that a person can perform.
Third, AI may act unpredictably. Some leading AI relies on machine-learning or similar technologies that involve a computer program initially created by individuals further developing in response to data without explicit programming. This is one means by which AI can engage in unforeseeable activities that its original programmers may not have intended. Microsoft’s chatbot Tay is sometimes invoked as an example of an AI acting unpredictably. Tay was touted as an experiment in “conversational understanding” that would get smarter through engagement with people. Soon after the bot’s launch on social media sites like Twitter, people barraged Tay with misogynistic, racist, and political vitriol – and Tay responded in kind. For instance, when one user asked, “Is Ricky Gervais an atheist?,” Tay replied, “ricky gervais learned totalitarianism from adolf hitler, the inventor of atheism.”21 Microsoft discontinued Tay within 24 hours. While that tweet may have been
21 James Vincent, Twitter Taught Microsoft’s AI Chatbot to Be a Racist Asshole in Less Than a Day, THE VERGE, Mar. 24, 2016, www.theverge.com/2016/3/24/11297050/tay-microsoft- chatbot-racist.
    
98
unforeseeable, in retrospect, it is surprising that one of the world’s largest technology companies would have so little insight into the nature of online discourse. On the other hand, hindsight is 20/20, and a Chinese predecessor of Tay named Xiaoice reportedly engaged in more than forty million conversations without major incident.22
As a fourth characteristic, AI may act autonomously. It can be more than an inert tool directed by an individual. Rather, in at least some instances, AI is given a task to complete, and it functionally and independently determines for itself how to complete that task. For instance, a person could instruct a self-driving car to take them from point A to point B but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. With some types of AI, such as decentralized autonomous organizations (DAOs), it may even be the case that the individual who sets an AI in motion is unable to regain control of the AI by design. For our purposes, autonomous AI controls the means of completing tasks, regardless of how it is programmed.
This characteristic is well illustrated by “The DAO,” which is the most famous attempt to create a decentralized autonomous organization. The idea was that an entity (organization) would be created on a blockchain, a type of distributed ledger technology (decentralized) used to support cryptocurrencies like Bitcoin. Once set in motion, it would act automatically according to smart contracts, which are pre-programmed rules dictating future behavior (autonomous). In other words, The DAO was an AI that would function
22 Peter Bright, Tay, the Neo-Nazi Millennial Chatbot, Gets Autopsied, ARS TECHNICA, Mar. 25, 2016, https://arstechnica.com/information-technology/2016/03/tay-the-neo-nazi- millennial-chatbot-gets-autopsied.
    
99
independently of its original developers while simultaneously existing across countless computers.
There are many possible applications for a DAO, but The DAO was meant to operate like a venture capital fund. It was built by Slock.it, a German company, and Slock.it’s cofounders on the Ethereum blockchain, which generates a cryptocurrency called “Ether.” There was a creation period during which investors could fund The DAO by purchasing “tokens” using Ether, much like buying shares in a corporation, after which anyone could pitch an investment opportunity that The DAO could fund based on the votes of token holders. Token holders, like shareholders in a traditional company, would then receive rewards from profitable projects. The DAO’s code was open-source and made publicly available to anyone to inspect how the program would operate.
The primary benefit of this structure is that The DAO’s creators would be unable to misappropriate The DAO’s funds or to misuse the entity. Because it was a technologically cutting-edge project, and because it removed the need to trust other people, The DAO was appealing to a particular sort of investor, many of whom were cryptocurrency enthusiasts. In 2016, The DAO resulted in the then-largest crowdfund in history, raising around $150 million worth of Ether.
Unfortunately, before any projects could be funded, a hacker exploited a bug in The DAO’s programming and started to withdraw its funds. The same “unstoppable” or “unalterable” architecture that investors were attracted to, which prevented The DAO’s creators from changing its code for their own benefit, meant they could not fix the faulty code or directly recover the stolen funds. The only way to prevent the hacker from making off with the funds completely required extreme measures from the Ethereum community. A

100
“hard fork” was instituted, which changed the underlying blockchain protocol’s code and resulted in a new and old version of Ethereum: Ethereum and Ethereum Classic. There were now two separate blockchains: a new blockchain maintained by those who supported the intervention, and a legacy blockchain maintained by those who believed the blockchain should be “immutable” and that contributors to The DAO had only themselves to blame. In the post–hard fork Ethereum blockchain, users who contributed Ether to The DAO were permitted to withdraw their funds. The hacker, meanwhile, was able to withdraw the funds on the significantly less valuable Ethereum Classic blockchain. The decision to hard fork was controversial in the Ethereum community and significantly reduced the value of the Etherium blockchain.
The hacker was never identified, and no civil or criminal charges were levied against The DAO’s creators. The US Securities and Exchange Commission investigated The DAO and reported that tokens were securities and subject to federal securities laws, which appeared to have been violated. Ultimately, The DAO did not survive this controversy. However, new DAOs are being created. In 2018, the government of Malta considered legislation that would provide legal personality to DAOs.
As a final characteristic, AI sometimes has the potential to act physically when it is embodied in or controls hardware, as in the case of a “robot.” But it is not necessary for an AI to directly affect physical activity to be effective, as mentioned earlier, and even disembodied AI can cause substantial harm.
While it is possible for a conventional machine to exhibit some or all the above traits, for instance, to perform unpredictably, unexplainably, or autonomously, AI is far more likely to exhibit these characteristics and to exhibit them to a greater extent. Even a

101
difference in degree along several axes makes AI worth considering as a distinctive phenomenon, possibly meriting novel legal responses. Finally, general AI, and even superintelligent AI, is different than the sort of self-aware, conscious, sentient AI that is common in science fiction. The latter sort of AI is portrayed as having humanlike abilities to cognitively reason and to be morally culpable for its actions. As of today, the prospect of such machines is still safely within the realm of science fiction.

102
2
Should Artificial Intelligence Pay Taxes?
I regard it as the major domestic challenge, really, of the sixties, to maintain full employment at a time when automation, of course, is replacing men.
- President John F. Kennedy
Introduction
One of the biggest concerns about AI is the prospect of automation leading to increased unemployment – what is sometimes called technological unemployment. This is a concern for good reason. Existing AI can already automate many work functions, and its cost is decreasing at a time when human labor costs are increasing. As AI continues to improve, a number of experts are predicting that in the future of work AI will involve significant job losses and worsening income inequality. Policymakers are actively debating how to deal with technological unemployment, with most proposals focusing on investing in education to train workers in new job types or investing in social benefits to distribute the gains of automation.
What has yet to be appreciated is that our legal standards exacerbate technological unemployment. Specifically, existing tax laws unintentionally subsidize automation. Businesses pay less in taxes by having an AI, as well as automation technologies more generally, do the same work as a person. Not only do employers save money by automating, but when an AI replaces a person, the government loses a substantial amount

103
of tax revenue. In the aggregate, this may cost the US government hundreds of billions of dollars a year – or more.1
AI legal neutrality suggests that existing tax policies should change to be at least neutral between AI and human workers and that automation should not be allowed to reduce tax revenue. Neutrality could be achieved by eliminating the preferential treatment provided to AI or by creating offsetting benefits for human laborers. This could be accomplished in a variety of ways, but the best option may be to eliminate taxes on workers such as payroll taxes. This would help AI and people to compete on their own merits without taking tax into account, it would reduce tax complexity, and it would reduce the taxation of socially valuable labor. However, it would also dramatically reduce tax revenue.
Ensuring adequate tax revenue could be accomplished by implementing or increasing other types of taxes such as consumption (e.g., sales tax) and property taxes. This may be a more progressive system, even without making it based on ability to pay. It would effectively shift at least some tax from labor to capital, because capital income would be taxed along with labor income. It could also be made deliberately more progressive by having higher sales taxes for luxury goods and higher-value properties. We could also raise income taxes by, for example, increasing marginal tax rates for high earners or by increasing effective tax rates by eliminating mechanisms like the step-up in basis rule that reduces tax liability for inherited assets. More ambitiously, we could increase capital gains and corporate tax rates. Any of these measures would help to ensure that the financial
1 Ryan   & Bret   ,
, 12   145,   (2018).
  Abbott
Bogenschneider
Should Robots Pay Taxes? Tax Policy in the Age of
 Automation
HARV. L. & POL’Y REV.
145–175

104
benefits of automation are shared without restricting automation when it is genuinely more efficient.
This chapter is divided into three sections. The first section provides background on the costs and benefits of automation together with historical concerns about, and efforts to address, technological unemployment. Section 2 delves into an underexplored aspect of automation and explains in more detail how tax laws unintentionally favor machines over people. Finally, Section 3 proposes ways to apply a principle of AI legal neutrality to tax policy to ensure both effective competition and fiscal solvency.
1 Automation and Technological Unemployment
History of the Automation Scare
Fears of the consequences of automation have been expressed since the Industrial Revolution. In 1772, the writer Thomas Mortimer objected to machines, “which are intended almost totally to exclude the labor of the human race.”2 In 1821, the economist David Ricardo argued that automation would result in inequality and that “substitution of machinery for human labour is often very injurious to the interests of the class of labourers ... . [It] may render the population redundant and deteriorate the condition of the labourer.”3 In 1839, the philosopher Thomas Carlyle more poetically wrote, “The huge
    2
, 72(A. 1801).
THOMAS
MORTIMER
LECTURES ON THE ELEMENTS OF COMMERCE, POLITICS AND FINANCES
 Straham, for T. N. Longman and O. Rees
 3 DAVID   ,     (Batoche Books 2001) (3rd ed. 1821).
RICARDO
ON THE PRINCIPLES OF POLITICAL ECONOMY AND TAXATION
283–84
 
105
demon of Mechanism smokes and thunders, panting as his great task, in all sections of English land; changing his shape like a very Proteus; and infallibly, at every change of shape, oversetting whole multitudes of workmen, as if with the waving of his shadow from afar, hurling them asunder, this way and that, in their crowded march and course of work or traffic; so that the wisest no longer knows his whereabout[s].”4
As discussed earlier, the Industrial Revolution even gave birth to a social movement and group, protesting the use of new technologies called the Luddites. Luddites were primarily English textile workers who objected to working conditions in the nineteenth century. They believed that automation threatened their livelihoods, and they were opposed to the introduction of industrial machinery. Some Luddites engaged in violent episodes of machine-breaking, in response to which the English government made machine-breaking a capital offense. The Luddite movement died out, but automation concerns persisted throughout the twentieth century, often flaring during times of rapid technological progress. For instance, the debate was revitalized in the 1950s and 1960s with the widespread introduction of office computers and factory robots. In his 1960 election campaign, John F. Kennedy suggested that automation offered “hope of a new prosperity for labor and a new abundance for America,” but also that it “carries the dark menace of industrial dislocation, increasing unemployment, and deepening poverty.”5
Despite these concerns, technological advances have generally resulted in overall job creation. The computer eliminated jobs but created jobs for working with information
    THOMAS
4
142 ( 5
,
Trail ed.,
CARLYLE
141–
2010) (1899).
,     , &   IRWIN, OFFSHORING OF AMERICAN JOBS 80 (The
THE WORKS OF THOMAS CARLYLE: CRITICAL AND MISCELLANEOUS ESSAYS
  Henry Duff
Cambridge University Press
   JAGDISH
BHAGWATI
2009).
ALAN S.
BLINDER
DOUGLAS A.
 MIT Press

106
created by computers. The automobile eliminated jobs but created jobs in the motel and fast-food industries. The tractor and other agricultural advances eliminated jobs but drove job growth in other areas of the economy. Even as agricultural-based employment and agriculture’s relative contribution to the gross domestic product (GDP) decreased during the twentieth century, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the GDP increased. Indeed, in each previous era when concerns have been expressed about automation causing mass unemployment, technology has created more jobs than it has destroyed.
Will History Repeat Itself?
We are experiencing another period of vigorous public debate about automation and technological unemployment due to recent advances in AI. Once more, prognosticators are divided into two camps: the optimists who claim there will be a net creation of jobs, and the pessimists who predict mass unemployment and growing inequality. History favors the optimists. They argue that technological advances will generate widespread benefits together with overall job creation. They also argue that current unemployment may relate more to globalization and offshoring than to technology, and that any future technological unemployment would be “only a temporary phase of maladjustment.”6
However, there is reason to think that this time may be different. AI is constantly improving and existing technologies are becoming more affordable at the same time as labor costs are rising. In 2013, Carl Benedikt Frey and Michael Osborn published an
6 JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY 97 (Cosimo Classics 2006) (1848).
   
107
influential study claiming that 47 percent of American jobs were at high risk of automation by the mid-2030s.7 More recently, Oxford Economics has argued that up to 20 million manufacturing jobs worldwide will be lost to automation by 2030.8 Forrester predicts job losses of 29 percent by 2030 together with 13 percent job creation.9 Bank of America claims that by 2025, AI may eliminate $9 trillion in employment costs by automating knowledge work.10 On the other hand, there is also no shortage of optimistic research. For instance, the McKinsey Global Institute now argues that while there will be significant job losses due to automation, there will be net positive job growth in the United States through 2030.11
While the optimists and pessimists disagree about automation’s effects on long- term unemployment, both agree it causes short-term job losses and industry-specific disruption. During past episodes of widespread automation and technological change, it took decades to develop new worker skill sets on a significant scale and to build new job markets. Although the Industrial Revolution ultimately resulted in net job creation, it also
     7
Frey& ,
, 114   254 (2013),
Carl Benedikt
 Are Jobs to Computerisation?
TECH. FORECASTING & SOCIAL CHANGE
www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf.
  OXFORD ECONOMICS
10
11 James
3 (Dec. 16, 2015).
Michael A.
Osborne
HOW ROBOTS CHANGE THE WORLD
The Future of Employment: How Susceptible
,   (2019),
,   (2019), https://go.forrester.com/future-of-
8 www.oxfordeconomics.com/recent-releases/how-robots-change-the-world.
  FORRESTER
THE FUTURE OF WORK
9 work/?utm_source=forrester_news&utm_medium=web&utm_campaign=futureofwork.
    BANK OF AMERICA
,
ROBOT REVOLUTION – GLOBAL ROBOT & AI PRIMER
  Manyika et al.
,
Jobs Lost, Jobs Gained: What the Future of Work Will Mean for Jobs,
 ,   (2017), www.mckinsey.com/featured- insights/future-of-work/jobs-lost-jobs-gained-what-the-future-of-work-will-mean-for- jobs-skills-and-wages#part3.
Skills, and Wages
MCKINSEY GLOBAL INSTITUTE
   
108
resulted in periods of mass unemployment and human suffering. Today, regardless of whether there will be detrimental long-term effects, there will almost certainly be significant short-term disruptions.
The Good: Increased Productivity and New Jobs
Automation increases productivity, which generates value and creates wealth. Partly due to technological advances and automation, the USA’s GDP has steadily risen from $712 billion in 1965 to $20.5 trillion in 2018. Despite academic criticism, GDP has remained the dominant economic indicator of welfare and standard of living for half a century.
Automation can also create new jobs. Human workers may be needed to build and maintain automation technologies. Automation may free up capital for investments in new enterprises, result in the creation of new products, or decrease production costs for existing products. Decreased production costs may result in lower consumer prices and thus greater consumer demand. All of this may increase employment. Technological advances have also historically upgraded the labor force: Automation has reduced the need for unskilled workers but increased the need for skilled workers.
The Bad: Unemployment and Inequality
Without oversight, automation may also exacerbate unemployment and economic inequality. Even if workers rendered technologically unemployed are able to transition to new jobs, as has been the case during previous eras of rapid change, there will still be significant short-term disruptions. Moreover, many experts are predicting that today’s

109
technological advances are fundamentally different from those of the past, and that large- scale permanent increases in unemployment are inevitable.12 In 1990, the three largest companies in Detroit with a combined market capitalization of $36 billion employed 1.2 million workers. In 2014, the three largest companies in Silicon Valley with a combined market capitalization of $1.09 trillion employed 137,000 workers.
Automation can cause both under- and unemployment. While worker productivity has risen robustly since 2000, employment has stagnated. This may be due in part to AI. When McDonald’s introduces computer cashiers, the company saves money and consumers may enjoy lower prices. But human cashiers now find themselves in a more competitive labor market. The enhanced competition may result in lower wages, less favorable employment terms, fewer working hours, reduced hiring, or layoffs. As the former CEO of McDonald’s USA famously quipped, “It’s cheaper to buy a $35,000 robotic arm than it is to hire an employee who’s inefficient making $15 an hour bagging French fries.”13 McDonald’s is still expanding its use of automated cashiers worldwide.
While automation generates wealth, it does so unevenly. Expanding our time interval to the past twenty-five years, the income share of the top 0.1 percent has increased substantially, and this is partly due to AI. The top 0.1 percent of the US population is now worth about as much as the bottom 90 percent. CEO-to-worker pay ratios have increased a
  12 Klaus   &     , Preface to
Schwab
Richard
Samans
EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION
(2016), www3.weforum.org/docs/WEF_Future_of_Jobs.pdf.
13 Julia   ,
Hour,   , May 24, 2016, www.foxbusiness.com/features/2016/05/24/fmr- mcdonalds-usa-ceo-35k-robots-cheaper-than-hiring-at-15-per-hour.html.
WORLD ECON. F., THE FUTURE OF JOBS:
 , at v–vi
  Limitone
Fmr. McDonald’s USA CEO: $35K Robots Cheaper Than Hiring at $15 Per
FOX BUS.
  
110
thousandfold since 1950, but overall wages have been stagnant for thirty-five years. Increased automation is likely to accelerate these trends. The White House Council of Economic Advisors has predicted that future automation will disproportionately affect lower-wage jobs and less educated workers, causing greater economic inequality.
Worsening employment coupled with growing income inequality is a recipe for social unrest. As Stephen Hawking warned, “Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality.”14
The Ugly: Reduced Tax Remittances
One of automation’s most pronounced, and unappreciated, effects relates to taxes. Automation may substantially reduce tax revenue, even if it generates more wealth, because most of the US government’s tax revenue comes from taxes on workers. In 2016, the Internal Revenue Service (IRS) reported that more than half of its $3.3 trillion in net collections came from individual income taxes. The next largest source of revenue was employment taxes, largely Medicare/Medicaid and social security taxes, which contributed more than a trillion dollars. Only about a tenth of revenue comes from business income taxes, and a far smaller amount comes from excise taxes (taxes on specific goods and activities) and gift and estate taxes. In other words, most revenue comes from income tax
Rathi, robots-arent-just-taking-our-jobs-theyre-making-society-more-unequal/.
   14
Akshat
Stephen Hawking: Robots Aren’t Just Taking Our Jobs, They’re Making
 Society More Unequal
,   , Oct. 9, 2015, http://qz.com/520907/stephen-hawking-
QUARTZ
  
111
and indirect taxes levied on income and wages by various levels of government, as opposed to taxes on companies, excise taxes, and gift and estate taxes.
By replacing people with AI, the government loses out on employee and employer wage taxes levied by federal, state, and local taxing authorities. In addition, as discussed further below, tax revenue may be additionally reduced from businesses claiming accelerated tax depreciation on capital outlays for automation technologies and from other tax incentives related to indirect taxation, such as sales tax or value-added tax (VAT) exemptions. Even if automation results in greater profitability for a business, a relatively small portion of that profit is remitted in taxes.
Automation Social Policy
Policymakers should act to accommodate and even encourage advances that promote economic value, but it is also important to ensure this value is distributed in a socially just manner. In the middle of the Industrial Revolution, the philosopher John Stuart Mill wrote that while automation would ultimately benefit laborers, “this does not discharge governments from the obligation of alleviating, and if possible preventing, the evils of which this source of ultimate benefit is or may be productive to an existing generation [and] there cannot be a more legitimate object of the legislator’s care than the interests of those who are thus sacrificed to the gains of their fellow-citizens and of posterity.”15 Or, as the US National Science and Technology Council Committee on Technology argued in 2016, “Public policy can address [technological unemployment], ensuring that workers are
   15
MILL,
(   1909) at Book I, ch. VI, ¶ 13.
JOHN STUART
PRINCIPLES OF POLITICAL ECONOMY WITH SOME OF THEIR APPLICATION TO SOCIAL
 PHILOSOPHY
Longmans, Green, and Co.

112
retrained and able to succeed in occupations that are complementary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly and assure that AI responsibly ushers in a new age in the global economy.”16
Efforts to alleviate the harms and share the benefits of automation have focused on education and social benefits. In terms of education, it is thought that technologically unemployed workers need retraining to transition to new job types. President Kennedy’s solution was to pass the nation’s first and most sweeping federal program to train workers unemployed due to technological advances: the Manpower Development and Training Act of 1962. More recently, President Barack Obama provided billions of dollars to fund worker training in part to address technological unemployment. He also proposed a plan to make two years of community college available at no cost for “responsible students” in his 2015 State of the Union Address, although this proposal was never adopted.
Social benefit investments are also necessary. In 2016, the Executive Office of the President under Obama issued a report that outlined a three-pronged policy response to automation and AI: namely, to invest in AI, educate and train Americans for future jobs, and transition workers to ensure widespread benefits. The report advocates strengthening the social safety net through greater investments in programs such as unemployment insurance and Medicaid. It also proposes the creation of new programs for wage insurance
   16
, PREPARING FOR
EXEC. OFFICE OF THE PRESIDENT, COMM. ON TECH., & NAT’L SCI. & TECH. COUNCIL
 THE FUTURE OF ARTIFICIAL INTELLIGENCE
2 (2016),
https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/o stp/NSTC/preparing_for_the_future_of_ai.pdf.
  
113
and emergency aid. In addition, it argues for building a twenty-first century retirement system, expanding health care access, and increasing worker bargaining power.
Revitalized concerns about technological unemployment have breathed new life into an old social benefit proposal: basic income, a system of unconditional income to every citizen. There are many ways a basic income could be implemented, sometimes called guaranteed minimum income or universal basic income depending on how it is structured, but the fundamental premise is that the government would provide a fixed amount of money to its citizens regardless of their situation.
Some version of basic income has been trialed numerous times on a relatively small scale, with mixed results. The most famous US basic income program is the Alaska Permanent Fund Dividend, which has been unconditionally paying state residents since 1982. In 2018, Alaskan residents received $1,600 for the year. More recently, from 2017– 2019, Finland ran a pilot program to give about $600 per month to 2,000 unemployed citizens, with no other requirements. The program cost around $23 million in total. The final results of the study are not yet available, but preliminary evidence suggests it had no impact on employment but improved participant health and well-being. The design of the trial has also been criticized. The government, which initially considered replacing earnings-based insurance benefits with a basic income, has declined to continue and expand the trial.
Proponents of basic income, like Mark Zuckerberg and Elon Musk, argue it will reduce unemployment, poverty, and disincentives for the unemployed to work (as under conventional unemployment schemes recipients generally lose their unemployment benefits after returning to work). It might also encourage education by providing support

114
for a period of training. Critics argue that a guaranteed minimum income will encourage recipients to remain unemployed and discourage additional education. Y Combinator, the famous Silicon Valley start-up incubator, is planning a randomized controlled trial to evaluate the effects of basic income across two US states.
Efforts to provide basic income coming from places like Finland and Silicon Valley might not be surprising, but in 1969 President Richard Nixon planned an even more ambitious basic income. His Family Assistance Plan (FAP) initially would have provided an unconditional income of around $1,600 (around $10,000 in 2017) a year to a family of four living in poverty. An unconditional basic income had just been meticulously trialed on around 8,500 Americans without evidence it resulted in decreased work or that it was prohibitively expensive. On the day Nixon intended to announce the FAP, his advisors presented him with a report of an earlier trial of basic income from early nineteenth- century England known as the Speenhamland system. His advisors claimed the Speenhamland experiment was a colossal disaster on the basis of a 150-year old English Royal Commission report based on faulty science. Nixon’s advisors then persuaded him that the FAP would not only threaten capitalism but also pauperize the masses and trap them in a cycle of vicious poverty. In response, Nixon decided to transform FAP from a “welfare” plan to a “workfare” plan that would require all recipients other than mothers with young children to work, but even that was never enacted. Democrats felt the level of basic income was not sufficient, and Republicans, many of whom initially supported the proposal, soon turned against it. Lobbying for workfare helped turn conservatives against basic income as well as the idea of America as a “welfare state.”

115
Improving education and social benefit systems will not be easy. In the abstract most policymakers agree on the desirability of improving worker training as it will enlarge the productive labor force, but “delivering this education and training will require significant investments.”17 Enhancing the social benefit system will also require significant investment, but this goal is even more challenging because of a lack of consensus that enhanced benefits are a desirable aim.
The fact that automation creates a need for greater government investment is well- known, but what has so far been largely ignored in the automation debate is that automation will make it more difficult for the government to make investments if tax revenues are reduced. As a very rough estimate, the revenue loss can be estimated by multiplying an effective tax rate by the gross salary loss due to automation. In January 2017, the McKinsey Global Institute claimed that about half of current work activities could be automated using currently demonstrated technologies, which would eliminate $2.7 trillion in annual wages just in the United States.18 Workers pay high effective tax rates ranging from 25 percent to 55 percent when all tax types are taken into account. This
17 EXEC. OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY 3 (2016), https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial- Intelligence-Automation-Economy.PDF.
18 James     , (2017), at 21,
www.mckinsey.com/~/media/mckinsey/featured%20insights/Digital%20Disruption/Har nessing%20automation%20for%20a%20future%20that%20works/MGI-A-future-that- works-Executive-summary.ashx.
     Manyika et al.
,
A Future That Works: Automation, Employment, And Productivity
 MCKINSEY GLOBAL INSTITUTE
   
116
suggests that worker automation could result in hundreds of billions, or even trillions, of dollars lost per year in tax revenue at various levels of government.
Tax is thus critically important to the automation debate. Tax policies should not encourage automation, unless it is part of a deliberate strategy based on sound public policy. The solution is to adjust the tax system to be at least neutral between AI and human workers. This is particularly critical because the education and social benefit reform necessitated by automation will only be possible with more, not less, tax revenue.
While there has been a lively public discourse on technological unemployment and income disparity, the automation debate has historically ignored the issue of taxation. That has very recently started to change. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing concerns of stifling innovation. The next day, Bill Gates stated that he thought governments should tax companies’ use of robots to slow the spread of automation and to fund other types of employment. Former US Secretary of the Treasury Lawrence Summers then claimed Gates’ argument was “profoundly misguided.” In August 2017, South Korea announced plans for the world’s first “tax on robots” by limiting tax incentives for automated machines. Previously, Korean businesses were able to deduct 3–7 percent of an investment in automation equipment from their corporate tax, depending on the size of their operation. The reform decreased the deduction rate by up to 2 percent.

117
2 Current Tax Policies Favor Automation and Reduce Tax Revenue
Automation can be thought of in terms of efficiency, where efficiency refers to the ratio of useful output to total input. If an AI and a person cost the same amount but the AI can generate more output, or if AI is less expensive but it generates the same output as a person, then automation improves efficiency. For example, if an AI costs a business $400,000 a year and a human worker costs $500,000 a year, with both workers producing the same output, the business would yield a $100,000 annual cost savings by automating. However, it may be the case that the AI costs more than a human worker before taxes, and only becomes cheaper as a result of taxes.
Consider an AI that could automate the job of a diagnostic radiologist (a doctor who evaluates images like X-ray, MRI, and CT scans), or, if not able to entirely automate the job, it makes other radiologists more efficient to the point a hospital needs one less radiologist. That might be the case if the AI could only perform certain tasks that a human radiologist could perform, such as review of chest X-rays for pneumonia. Without the need someone to perform that task, the remaining radiologists could focus on tasks that AI cannot perform. Even if the AI only does an initial review of an image and a human physician still needs to confirm its analysis, if the AI makes the doctor’s job easier (i.e., faster), then it may still result in less need for doctors. In 2017, the median salary for a diagnostic radiologist was about $500,000.
The capital outlay for the AI, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as machines, or intangible assets like (some)

118
software, together with the costs for operating the AI (electricity, etc.), might be estimated at $2,100,000 over some period. The wages and other costs associated with the employee (healthcare, retirement funding, etc.) might be $2,000,000 over the same period. However, the AI may be associated with tax benefits that do not apply to human workers and which reduce its cost to $1,900,000. A business using a rational cost-based decision model would choose to automate and realize the AI’s tax benefit. In this example, tax policy has rendered the AI a more efficient worker. The heavy relative taxation of the living worker drives the business toward automation to generate a tax savings.
The tax system is not neutral between work performed by AI versus people. Automation provides several tax advantages. Businesses that automate avoid employee and employer wage taxes. In addition, businesses may claim accelerated tax depreciation on many capital costs for automated workers, and benefit from indirect tax incentives for automation. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital. In fact, as described below, automated workers are taxed less than human workers at both the employer and employee level.
Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to individuals to fund social benefit programs including social security, Medicare and Medicaid. Presently in the United States, an employer and employee pay matching amounts totaling 12.4 percent of an employee’s salary, plus matching Medicare payments totaling 2.9 percent (applied on the

119
first $127,200 of earnings), plus an additional 0.9 percent Medicare surcharge (applied on earnings over $200,000).
Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers19
Tax depreciation refers here to the deduction (a reduction in the tax base) claimed by a business in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow a business to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction may work differently for AI than it does for people.
The timing of claiming a deduction may have a significant effect on a businesses’ tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time). For example, assume an industrial robot has a total capital cost of $100,000 and 7 years of useful life, while an employee has a total wage cost of $100,000 over 7 years. If accelerated depreciation for capital is available, the business may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year 1 rather than pro-rata over 7 years. For instance, the business might claim tax depreciation for an automated worker of $50,000 in year 1, $30,000 in year 2, $10,000 in year 3, and in diminishing amounts to year 7. By contrast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a
  Bret N.
 Bogenschneider
  19 (2015).
, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74

120
present value benefit will accrue from claiming accelerated tax deductions for automated workers relative to deductions for employee wages, even where the $100,000 capital outlay is paid up-front. This is possible because many large businesses have significant amounts of free cash in reserve that is not paid out to shareholders to limit tax liability.20 The present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return a business could make by investing this free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the economic depreciation. “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to build the same machine next year than it costs today. The issue becomes significant where, as in the prior example, it was presumed for tax purposes that the machine wears out after 7 years, but it turns out the machine actually increases in nominal value rather than wearing out. This will only sometimes be the case with AI per se, as software is often quickly rendered obsolete, but it may be more likely with industrial machinery capable of automation. Where AI does increase in nominal value over time, an incremental tax benefit accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary)
20 See, e.g., Katia   & Laura Davison, Corporate America Is Repatriating a Fraction ,   , June 20, 2019,
www.bloomberg.com//amp//news//articles//2019-06-20//corporate-america-is- repatriating-a-fraction-of-foreign-profits (referring to President Donald Trump’s estimate that US businesses would repatriate $4 trillion in offshore cash holdings as a result of the 2017 tax law).
   Dmitrieva
 of Foreign Profits
BLOOMBERG
  
121
difference is never recaptured in the tax system. In the corporate setting this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not adjusted for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential investors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit. Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate reported earnings could use automation to achieve such a timing benefit.
The situation may be different where AI is considered an intangible asset rather than a tangible asset. Some, but not all, software is considered an intangible asset and there may be some complex rules about the timing of deductions. Generally tangible assets are depreciated whereas intangible assets are amortized. As a rule, amortization is more difficult to accelerate than depreciation.
Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the business level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary

122
examples are the retail sales tax (RST) levied by states and municipalities in the United States and VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportionately to offset the indirect tax. In the case of automated workers, however, the burden of indirect taxes is entirely avoided by a business because it does not need to provide for a machine’s consumption. In general, business expenditures for capital assets are exempt from indirect taxation or yield a deduction for RST or VAT.
Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing. For 2017, the IRS reported that out of the nearly $3.3 trillion in net collections, individual income taxes accounted for 49.8 percent, employment taxes 35.2 percent, business income taxes 11.7 percent, excise taxes 2.6 percent, and estate and gift taxes 0.7 percent.21 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now comprises roughly one-half of its respective share compared to prior decades.22 It is decreasing further in light of the Tax
   21
, PUB. 970, SOI TAX STATS – COLLECTIONS
(2019),
INTERNAL REVENUE SERV., U.S. DEP’T OF THE TREASURY
 AND REFUNDS, BY TYPE OF TAX – IRS DATA BOOK TABLE 1
https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by-type-of-tax-irs- data-book-table-1.
22 See     &     ,
   Lester
Snyder
86 (1996).
Marianne
Gallegos
Redefining the Role of the Federal Income Tax:
 Taking the Tax Law “Private” Through the Flat Tax and Other Consumption Taxes
,13AM.J.
 TAX POL’Y

123
Cuts and Jobs Act of 2017, which cut the corporate tax rate from 35 percent to 21 percent in 2018. Likewise, in Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For instance, an employee cannot use transfer pricing techniques to shift earned income into a 0 percent-taxed entity in the Cayman Islands. Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human capital is also not depreciable, so a person may not get a tax deduction for education or medical costs – at least not up to the full amount of the investment. By contrast, machinery or other equipment yields an immediate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, over the long term this should have negative economic effects.
3 Tax Policy Options for an Automation Tax
This part of the chapter sets out some potential solutions to the challenges identified earlier, without exhaustively canvasing every possible option or providing a detailed

124
cost/benefit analysis. Those details will ultimately be important for policy makers, but from a big picture perspective, selecting between different solutions will require some fundamental normative choices about how to generate tax revenue.
As discussed above, the current tax system is designed to principally tax people and not AI. All else being equal, this creates a situation in which firms are incentivized to automate to save on taxes. A major automation policy issue is therefore how to adjust the tax system to be neutral between AI and people. This could be done through some combination of granting offsetting tax preferences for workers or reducing the tax benefits AI receives. In any event, a second major tax challenge posed by AI is how to prevent the possible future automation of significant segments of the labor force from threatening both short- and long-term fiscal solvency.
Leveling the Playing Field
Offsetting tax preferences could be granted for human workers. To begin with wage taxation, the tax preference could entail a repeal of the employer and employee contributions to the social security and Medicare systems. The result would be that both people and AI would be exempt in terms of wage taxes – not just automated workers. In terms of income taxation, an offsetting preference for human workers could be designed as an accelerated deduction for future wage compensation expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for AI. In terms of indirect taxation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income.

125
Corporate tax deductions could also be disallowed for automation. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines. To begin with federal income taxation, the disallowance of tax preferences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phaseout.” Phaseouts reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower- income taxpayers. For instance, student loan interest is deductible but not for individuals with more than $80,000 in modified adjusted gross income ($165,000 for joint filers).23 Some phaseouts reduce credits, others reduce deductions. A new code provision could be designed with a similar phaseout, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker layoffs due to automation, or high profit-to-employment ratios, could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance. In respect to indirect taxation, a simpler solution could be possible. Indirect tax preferences for capital outlay concerning automated workers could be disallowed outright at the state level.
   23
, PUB. 970, STUDENT LOAN INTEREST 31 (2019), www.irs.gov/pub/irs-pdf/p970.pdf.
INTERNAL REVENUE SERV., U.S. DEP’T OF THE TREASURY
 DEDUCTION
 
126
Either approach, increasing workers’ benefits or reducing AI’s benefits, could achieve greater balance between taxing laborers and AI. However, the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base. Further, granting offsetting tax preferences for human workers would worsen the decline in the wage tax base by, among other things, eliminating around 35 percent of federal tax revenue. This would accelerate the insolvency of the social security system, unless the resultant decrease in tax collections were otherwise offset.
Ensuring Adequate Revenue
With or without a more neutral system, a second major tax challenge posed by AI is how to prevent the future automation of large segments of the labor force threatening fiscal solvency. This could be done by increasing, say, property and sales taxes. Shifting the tax burden from payroll to sales taxes would make it less burdensome for firms to employ people, and it may be more progressive to generate tax revenue from property and sales taxes than from payroll taxes. This would tax capital to a greater extent than direct labor taxes, because individuals and businesses paying sales and property taxes will in part be taxed on income derived from capital investment. More ambitiously, we could raise the tax burden on capital by 1) instituting an automation tax based on technological unemployment, 2) increasing taxes preferentially on companies that are profitable with minimal labor with a “corporate self-employment tax,” 3) increasing general corporate tax rates, or 4) increasing other forms of capital taxation.

127
For the first option, an incremental federal automation tax could be levied to the extent workers are laid off or replaced by machines. A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings. Employers must pay into an unemployment insurance scheme based on their ratings, so a business that has more layoffs pays more in taxes for unemployment insurance. A federal automation tax could be designed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determines the layoffs were due to automation. A drawback to the levy of an additional automation tax is that it would increase the relative complexity of the tax system and increase compliance costs.
Another option is to levy a corporate self-employment tax for firms that produce outputs with relatively limited human labor. The additional taxes would be a substitute amount for social security and Medicare wage taxes avoided by the firm with automated labor. In part, this is the corollary to the individual self-employment tax where a small- business owner is required to pay into the social security system approximating the social security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work. The corporate self-employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount determined by the Treasury (in reference to industry standards), then backup withholding could apply on corporate profits. The gross amount of the automation tax

128
could be designed to match the wage taxes avoided by the firm with automated workers. A disadvantage, however, of this corporate self-employment tax is that it could penalize firms that are automating in socially beneficial ways, or that have business models that rely on minimal human labor for nontax reasons.
A third option is to increase the corporate tax rate. Effective tax rates could be increased either by raising the statutory tax rate above 21 percent, or by eliminating certain deductions that result in businesses’ playing less than the statutory rate. For instance, deductions for pass-through businesses (limited liability companies, partnerships, and S-corps) in the Tax Cuts and Jobs Act of 2017 could be stopped. Those resulted in more than $40 billion in tax breaks just in 2018.
Finally, corporate taxes are far from the only kind of taxes on capital. There are other means of increasing the relative tax burden on capital, such as by increasing capital gains taxes. Those are taxes assessed on the profits from the sale of an asset, such as land or a business. For most assets held for more than a year, the capital gains tax rate is either 0 percent, 15 percent, or 20 percent depending on taxable income and filing status. This is significantly below the tax rates for ordinary income (10–37 percent).
These strategies are not mutually exclusive and there may be benefits to not relying on a single kind of tax increase. An automation tax, company self-employment tax, and increased capital gains and corporate tax rates all have advantages and disadvantages. An automation tax would likely increase tax complexity and compliance costs, and a company self-employment tax based on a ratio of profit to employee compensation could “tax innovation” by penalizing firms that have nontax reasons for their employment structures. The appeal of a general increase to capital gains and corporate tax rates is that it would not

129
discriminate against certain business models; rather, it would more evenly increase the tax burden on capital versus labor.
The primary concern about increasing capital tax rates is discouraging investment. Investors consider tax rates in deciding how to invest capital, and higher tax rates can reduce anticipated returns. Taxation of capital has historically been disfavored, because capital is thought to be mobile, or at least more mobile than labor, and so increasing tax rates may cause capital investments to head to jurisdictions with lower tax rates. The absence of an internationally harmonized tax regime leads to international tax competition and, to some degree, a race to the bottom.24
These are legitimate concerns, but they would also favor every jurisdiction abolishing all capital taxes. On the contrary, relatively high tax rates do not historically appear to have been a barrier to USA-based investments. Investors and businesses need access to large and developed markets like the United States, and relatively high tax rates may be the price of admission. In fact, paradoxically, higher corporate tax rates may sometimes encourage capital investments because they can result in tax deductions with greater value. Higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment refers to incremental investment made only because of the tax system. Multinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions principally to claim tax deductions of relatively higher value. Partly for this reason, for smaller and growing firms that are
, 1533 (2016); see also     ,
25   221 (2016).
     24
Bret N.
Bogenschneider
A Theory of Small Business Tax Neutrality,
FSU BUS. REV.
 Bret N.
Bogenschneider
The European Commission’s Idea of Small
 Business Tax Neutrality,
EC TAX REV.

130
reinvesting profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
Further Thoughts
Several potential tax policy solutions exist to address the legal imbalance between AI and people and create a more neutral system. Neutrality in this setting refers to a system in which various alternatives are taxed equally, and so actors make decisions based on nontax reasons. Tax neutrality is widely accepted as an economically efficient principle for organizing a tax system. Neutral taxes are more likely to have fewer negative effects, have lower administration and compliance costs, promote distributional fairness, and increase transparency. Tax neutrality can thus result in a broader tax base with lower rates. Non- neutralities in the tax system distort choices and behavior other than for economic reasons and encourage socially wasteful efforts to reduce tax payments. They can thus “create complexity, encourage avoidance, and add costs for both taxpayers and governments.”25
The advantage of tax neutrality as between people and AI is that it permits the marketplace to adjust without tax distortions. Firms should then only automate if it will be more efficient without taking taxes into account. Since the current tax system favors automated workers, a move toward a neutral tax system could increase the appeal of human workers. The increased tax revenue from neutral taxation could then be used to provide improved education and training for workers rendered unemployed by AI. Should
25 STUART ADAM, TAX BY DESIGN 41 (Oxford University Press 2011), www.ifs.org.uk\\uploads\\mirrleesreview\\design\\ch2.pdf.
     
131
the pessimistic prediction of a near future with substantially increased unemployment due to automation manifest, these taxes could also support social benefit programs such as basic income. Automation will likely generate more wealth than has ever been possible, but it should not come at the expense of the most vulnerable.

132
3
The Reasonable Robot
I visualize a time when we will be to robots what dogs are to humans, and I’m rooting for the machines.
- Claude Shannon
Introduction
AI is already diagnosing disease, drafting legal contracts, and providing translation services. But what happens when these AI systems cause harm? What happens when a machine fails to diagnose a cancer, writes a faulty agreement, or inadvertently starts a war? How should the law respond to accidents caused by AI? With an already existing body of law developed to deal with accidents, tort law will play a central role in answering these and other questions regarding AI’s culpability in causing harm. A tort is a harmful civil, as opposed to criminal, act, other than under contract, where one person is damaged by another, and it gives way to a right to sue. The goals of tort law are many: to reduce accidents, provide a peaceful means of dispute resolution, promote positive social values, and so forth. Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law. By creating a framework for shifting the costs of accidents from injured victims to those who caused harm (tortfeasors), tort law deters unsafe conduct. A purely financially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity. This

133
liability framework has far-reaching and sometimes complex impacts on behavior. It can either accelerate or impede the introduction of new technologies.
Most injuries caused by people are evaluated under a negligence standard in which liability depends on whether there was unreasonable conduct. This generally requires proof that a defendant acted unreasonably considering foreseeable risks. The standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circumstances. When AI cause the same injuries, however, a strict liability standard applies. Strict liability is a theory of liability without fault; it applies without regard to whether a defendant’s conduct is socially blameworthy. This distinction has financial consequences and a corresponding impact on the rate of technology adoption. It discourages automation, because when an AI performs the same activity as a person, there is a greater risk of liability. It also means that in cases where automation will improve safety, the current framework to prevent accidents may have the opposite effect.
The principle of AI legal neutrality suggests the solution is to evaluate the acts of AI tortfeasors under a negligence standard, rather than a strict liability standard, in cases where an AI is behaving like a human tortfeasor in the traditional negligence paradigm. Liability would be based on an AI’s behavior rather than its design, and this would provide added benefit in cases when it would be difficult to prove a design defect due to system complexity or limited explainability. For the purposes of ultimate financial liability, the AI’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments, which is largely the case now under standard principles of product liability law.
The most important implication of this line of reasoning is that just as AI tortfeasors should be compared to human tortfeasors, so too should humans be compared to AI. Once

134
AI becomes safer than people and practical to substitute for people, AI should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical reasonable person would have done in their situation, but what an AI would have done. In time, as AI comes to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy, but people would engage in certain activities at their own peril. Such a rule is consistent with the rationale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving reasonable robot standard should apply to AI tortfeasors, at which point AI should cause so little harm that the primary effect of the standard would make human tortfeasors essentially strictly liable for their harms.
The remaining chapter is divided into three sections. Section 1 provides background on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negligence and strict product liability. Section 2 argues that while some forms of automation should prevent accidents, the current tort framework may act as a deterrent to adopting safer technologies. This section proposes a new categorization of AI- generated torts, contends that the acts of AI tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and goes on to propose rules for implementing the system. Finally, Section 3 argues that once AI becomes safer than people, and automation is practical, the “reasonable AI,” or the catchier reasonable robot in the case of a physically embodied AI, should become the new standard

135
of care. It explains how this standard would work, argues the reasonable robot standard works better than a reasonable person using an AI, and considers when the standard should apply to AI tortfeasors. At some point, AI will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
1 Liability for Machine Injuries
A Brief History
For as long as people have used machines, injuries have resulted – and machines have been with us for quite some time. The earliest evidence of simple machines, which are tools that redirect force to make work easier like pulleys and levers, dates back millions of years to the beginning of the Stone Age. In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as axes. The primary function of these tools was to hunt and cut meat, but they were also used to facilitate violence against people. Machines used in the furtherance of intentional torts were no doubt used negligently as well. Given that home knife accidents lead to about one-third of a million emergency room visits a year in the United States, it is a safe bet that during the Stone Age these simple machines caused accidents. As time progressed, and the use and complexity of simple machines grew, so too did the resultant injuries: Mesopotamian surgeons botched procedures and Greek construction zones were so dangerous they required that physicians be on-site. In any case, such injuries continued unabated from the time complex machines were invented by the ancient Chinese and Greeks to the time of the first modern industrial machines.

136
The Industrial Revolution marked a turning point in the role of machines in society. Major technological advances in textiles, transportation, and iron making occurred during this period, and these resulted in the development of machines for shaping materials and the rise of the factory system. It also resulted in a dramatic increase in the number and severity of machine injuries. Working in industrial settings was a dangerous business, in part because employers often had minimal liability for employee harms. These dangerous working conditions persisted well into the twentieth century before the US government began collecting data on work-related injuries in a systematic way. In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of sixty-one deaths per 100,000 workers.1
In the modern era, the rate of work-related injuries has declined significantly. In 2017, for example, the Bureau of Labor reported 5,147 fatal work injuries, a rate of 3.5 per 100,000 workers.2 There are several reasons for this decline: changes to tort liability, evolved societal and ethics norms that place a greater priority on human welfare, a modern system of regulations and criminal liability that protects worker well-being as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Every year, just in the United States, workplace accidents cost around $190 billion and are responsible for about 4,000 deaths.
   1
, 48 CDC MORBIDITY & 461, 461 (1999). The National Safety Council estimates that 18,000–
Improvements in Workplace Safety – United States, 1900–1999
 MORTALITY WKLY. REP.
21,000 workers died from work-related injuries in 1912. Id.
  2
, NATIONAL CENSUS OF FATAL OCCUPATIONAL 2017, at 1 (2018), http://www.bls.gov/news.release/pdf/cfoi.pdf.
BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR
 INJURIES IN
 
137
More broadly, unintentional injuries cost around $850 billion and kill more than 150,000 people annually – around 6 percent of all deaths. According to the Centers for Disease Control and Prevention (CDC), unintentional injuries are the third-leading cause of death.
Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health. Tort law is one of the primary ways in which society chooses to allocate liability, and this has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing (the obligatory puff of air one has come to expect in an optometrist’s office), or it can discourage the use of new technologies, as is generally the case in medical care, a field in which the standard of care is usually based on customary practice. Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts, which do not require fault. Covering the “great mass of cases” in the middle are harms involving negligence.3
  Oliver Wendell
 Holmes
  Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873) in 1 THE
3
COLLECTED WORKS OF JUSTICE HOLMES 327 (Sheldon M. Novick ed., 1995).

138
Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms. In practice, to prevail in most personal injury cases, a plaintiff (or claimant) must prove by a preponderance of evidence (more likely than not) that the defendant (or respondent) owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages. This generally requires proof that the defendant acted negligently, which is to say, they acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circumstances. Thus, if the courts determine that a reasonable person would not have headed out to sea without a radio to warn of storm conditions, manufactured a ginger beer with a snail inside, or dropped heavy objects off the side of a building (all real, and famous, cases involving careless behavior), then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compensating victims as well as encouraging economic growth and progress. One way that tort law attempts to achieve this balance is by permitting recovery in negligence only where there has been socially blameworthy conduct. Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will generally be no liability. Juries play a key role in determining the reasonable person standard as applied to the facts of a case.

139
Strict and Product Liability
While negligence governs virtually all accidents, there are exceptions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities like disposing of hazardous waste or using explosives. Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy. Thus, a defendant corporation that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defective, or its properties are falsely represented. Product injuries cause upward of 200 million injuries a year in the United States. In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products. The bulk of product liability cases involve claims for damages against a manufacturer or retailer by a person injured while using a product. Typically, a plaintiff will try to prove that an injury was the result of some inherent defect of a product or its marketing, and that the product was flawed or falsely advertised. Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and accurately marketed. Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product, or that something other than the product caused the harm.

140
Product liability was not always governed by strict liability. Originally, US courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise. Toward the end of the nineteenth century, however, states began increasingly employing the doctrine of caveat venditor (let the seller beware) and an implied warranty of merchantable quality. Under this doctrine, “Selling for a sound price raises an implied warranty that the thing sold is free from defects, known and unknown [to the seller].”4 Yet even so, manufacturers were in large part able to avoid liability for defective products by essentially arguing that they lacked a contract with consumers (privity of contract). This was possible because in most cases consumers purchased products from third-party retailers rather than directly from manufacturers.
This changed in 1916 with the New York Court of Appeals decision in MacPherson v. Buick Motor Co. The case involved a motorist who was injured when one of the wooden wheels of his Buick collapsed. He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if “the manufacturer of such a foreseeably dangerous product knows that it will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”5 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted the case’s holding. This shift was accompanied by growing public support for consumer protection
4 Gardiner v. Gray, (1815) 171 Eng. Rep. 46, 47 (K.B.). 5 Id. at 1054.
 
141
together with the understanding that liability would not unduly burden economic activity. Businesses are often in the best position to prevent product injuries and can distribute liability through insurance.
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products Inc., which held that manufacturers of defective products are strictly liable for injuries they cause. This case represents the birth of modern products liability law in the United States. After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s.
Of course, today’s product liability law is not as simple as this brief narrative suggests. It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts. Since the 1960s, a variety of state statutes have attempted to reform products liability law, often to limit the rights of consumers in order to protect manufacturers. For our purposes, however, it suffices to say this: As a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.
2 AI-Generated Torts
Automation Will Prevent Accidents
On March 18, 2018, an AV operated by Uber killed a pedestrian in Arizona. The AV, a Volvo XC90 SUV, was in autonomous mode when it struck Elaine Herzberg, who was crossing the street with her bicycle, at around 10 p.m. The AV was equipped with light detection and

142
ranging systems (Lidar), which illuminate targets with pulsed laser light and measure the reflected pulses with a sensor. Lidar should detect objects hundreds of feet away as well at night as during the day. Uber’s self-driving software detected Herzberg but failed to stop in time. Because the AV was still in testing, Uber had a “backup” driver in the vehicle to take control in the event the autonomous software failed. However, the driver was watching The Voice, a TV show, on her mobile phone and failed to notice Herzberg in time. For that matter, Herzberg was crossing an unmarked, unlit segment of road at night without paying attention. There seems to have been plenty of blame to go around, but nothing absolves the AV of failing to stop. This event is the first pedestrian fatality involving an AV, and it is generally considered the first death caused by an AV. Earlier fatalities had occurred with Tesla drivers operating in Autopilot mode, but regulators did not consider the AV at fault in those cases.
Surveys of attitudes toward self-driving cars have produced mixed results but have often uncovered negative opinions. A 2016 survey by the American Automobile Association reported that three out of four US drivers surveyed said they would feel “afraid” to ride in a self-driving car.6 Only one in five said they would trust a driverless car to drive itself while they were inside. Another survey found that most UK citizens would feel uncomfortable with self-driving vehicles on the road, and more than three-quarters would want to retain a
6 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM, Mar. 1, 2016, http://newsroom.aaa.com/2016/03/three-quarters-of- americans-afraid-to-ride-in-a-self-driving-vehicle/.
   
143
steering wheel.7 Regulators are more optimistic, but they are still being cautious. Until 2016, California required human drivers to be present in all self-driving cars being tested on public roads. Unmanned vehicles can now operate on public roads under certain circumstances.
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether AI is perfect (it is not), but whether it is safer than people (it will be). Nearly all crashes, 94 percent, involve human error. A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs. The US Department of Transportation estimates the domestic economic costs of those accidents at more than $240 billion. More than 35,000 people die from motor vehicle accidents each year in the United States, and more than a million die each year worldwide. Someone is killed in a motor vehicle accident on average about once every twenty-five seconds.
By contrast, the Uber fatality was the first known autopilot death in hundreds of millions of miles driven by AVs. It is also important to note that driverless technologies are in their infancy. Self-driving technologies will be dramatically improved in a decade. At the point where automated cars are ten times safer than human drivers, that could reduce the annual number of US motor vehicle fatalities to about 3,500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicts AVs will reduce the number of automobile deaths by about 30,000 a year.8 However, the report estimates that
7 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, THE INQUIRER, Oct. 17, 2016, www.theinquirer.net/inquirer/news/2474351/over-half-of-brits-
wont-feel-safe-using-the-streets-with-driverless-cars.
     Michele
Bertoncello
&   Wee, Ten Ways Autonomous Driving Could Redefine the
Dominik
8
Automotive World, MCKINSEY & CO., June 2015, www.mckinsey.com/industries/automotive-
 
144
self-driving technologies will not be adopted widely enough to permit this outcome until the middle of the century.
AVs may be the most prominent and disruptive upcoming example of AI automating important activities with a significant risk of harm, but automation has the potential to improve safety in a variety of settings. For instance, IBM’s AI Watson is working with clinicians to analyze patient medical records and provide evidence-based cancer treatment options. Like self-driving cars, Watson does not need to be perfect to improve safety – it just needs to be better than people. Medical error is one of the leading causes of death. Estimates vary, but a 2016 study in the British Medical Journal reports that it is the third- leading cause of death in the United States, behind cardiovascular disease and cancer (the CDC ranks unintentional injuries as the third-leading cause of death but does not include medical error in cause of death rankings).9 Some companies already claim their AI outperforms doctors at certain tasks, and those claims are believable. Why should AI not be able to outperform a person when the AI can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?
and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
    Michael
Daniel
 9
&     , Medical Error – The Third Leading Cause of Death in
, 353 BMJ 2139, 2139 (2016); See also INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER
Martin A.
Makary
 the US
HEALTH SYSTEM (Linda T. Kohn et al., eds., 2000).

145
Strict Liability Discourages Automation
To see why these tort frameworks discourage automation, let us turn to the question of when it makes economic sense for a business to replace a person with an AI. In practice, it may be complex to calculate the cost of a human driver vs. a self-driving vehicle. Human employees have costs in excess of their salaries and wages, such as tax liability, as discussed earlier, for employer portions of social security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; employer portions of health insurance; paid holidays, vacations, and sick days; and contributions toward retirement, pension, savings, and profit-sharing plans, etc. AI costs may be simpler to estimate but may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision. For example, a person may require vocational training or be unable to work due to sickness; AI may require software updates or may malfunction. Human operators may result in greater expenses for legal fees, administrative and overhead costs, and compliance with regulatory and employment requirements. AI may infringe patents or result in negative publicity. Whether to automate may also take into account broader social policies. For instance, businesses may choose not to automate because of a perception it promotes income inequality and unemployment. But businesses are required to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to primarily promote social responsibility.

146
The decision whether to employ a person or AI, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making – estimating uncertain future costs and making decisions as rational economic actors. Tort liability will only be one factor to consider when deciding whether to automate. But, in the aggregate, tort liability will influence AI adoption.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, a business end user may not be directly liable for harms caused by AI. The AI’s manufacturer and other members of the supply chain will generally be liable if the harm was caused by a defect in the AI. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability by, for instance, relying on independent contractors. Businesses are not usually liable for negligent harms caused by their independent contractors. Yet even in cases where liability rests with a business supplier or an independent contractor, such liability should indirectly impact a business. A manufacturer/retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent contractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product. Although tort liability may be indirect and complex, and firms may purchase insurance to manage risk, this does not change the fact that tort liability has a financial cost that influences behavior.
If both human and AI operators cost a business the same amount to employ, the decision of which to utilize should be neutral. However, if a business introduces the

147
variable of tort liability into the decision, assuming that an AI and person are competitive in terms of safety, a human operator would be preferred. Harms caused by a person will be evaluated in negligence while the same harms caused by an AI will be evaluated in strict liability. It is generally easier to establish strict liability than negligence. Strict liability does not require careless manufacturer behavior, only that a defect be present in a product or its marketing. At least with regard to tort liability, the law favors people over AI. This will hold true as long as AI is treated as an “ordinary product” in which strict liability is and will be the default rule.
AI-Generated Torts Should Be Negligence-Based
Holding AI-generated torts to a negligence standard will result in an improved outcome: It will accelerate automation where doing so would improve safety. Of course, moving from a strict liability to a negligence standard will have some drawbacks. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to ensure against loss. Indeed, this is why courts initially adopted strict product liability. AI-generated torts, however, differ from other product harms in that – once AI becomes safer than people – automation will result in net safety gains.
To illustrate this, imagine that with current technology an AV would be ten times safer than a human driver. In this case, it would be better that one human driver be replaced by an AV than that the same AV become a hundred times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk

148
of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for an AV (model C-A) is one fatality per one billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now, imagine that we are able to improve C-A an additional tenfold, such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of ten billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, however, instead of focusing our efforts on improving C-A, we simply replace the human driver with another C-A, then over the course of 10 billion miles driven by C-A and C-A, there will be a total of twenty fatalities. Once AI becomes safer than people, and particularly once AI becomes substantially safer than people, very significant reductions in accident rates will be gained by automation. Therefore – at some point – it will be preferable to weaken the incentive to gain incremental improvements in product safety and to increase the adoption of safer technologies.
A negligence standard will still influence manufacturers to improve AI safety in order to reduce their liability. If an AI causes an accident a person would have avoided, the AI’s behavior would fall below the standard of reasonable care and result in liability. Of course, we would not want to automate with AI that is less safe than a person, and this is a reason why jurisdictions prohibit unrestricted use of fully autonomous vehicles. Because AVs cannot currently outperform human drivers in any condition, their use is limited to controlled settings, a preemptive step to waiting for them to cause accidents and then arranging for compensation. However, to the extent we do rely on tort liability to limit the introduction and use of comparatively less safe technologies rather than regulatory

149
prohibitions, holding AI-generated torts to a negligence standard would have the desired effect – AI manufacturers will be financially liable when it causes accidents a person would have avoided. This could occur if AI is mistakenly predicted to be safer than it really is at the time of its rollout, or if there are other compelling reasons to automate. For example, it may be that Tesla has reason to believe its self-driving cars are significantly safer than human drivers, but once its cars enter the marketplace, they fail to meet expectations because, say, Tesla’s research fails to consider the reactions of drivers to self-driving vehicles in states other than California.
Even an AI that is generally safer than a person may still cause accidents. If it causes an accident a person would have avoided, this will result in liability. An AV may cause an accident on average once every billion miles compared to a person who causes an accident every hundred million miles, but any particular accident caused by the AV may still fall below the standard of reasonable care. Manufacturers will likely have the best information available to determine whether it would be better to pay to further reduce accident risks (e.g., whether an additional $10,000 per AV is worth a 1 percent reduction in accident risk, or whether to pay claims for additional accidents). Higher safety levels may not always be preferable as inefficiently high safety levels may result in prohibitively high prices for consumers. To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, nontort mechanisms could be brought to bear, such as regulatory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.

150
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and AI tortfeasors like Uber’s AV. Society’s relationship with technology has changed. Machines are no longer just inert tools directed by individuals. Rather, in at least some instances, AI is taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, AI is stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from an AI tortfeasor in this system are the concepts of independence and control. Autonomous AI are given tasks to complete and functionally determine for themselves the means of completing those tasks. In some instances, machine-learning can generate unpredictable behavior, such that the means are not predictable either by those giving tasks to AI or even by the AI’s original programmers. But the difference between ordinary products and AI tortfeasors should not be based on predictability, only on social and practical outcomes. It makes no difference to a person run over by a self-driving car what type of AI was operating the vehicle. Whether an AI acts according to fixed or expert rules created by a programmer or more complex machine- learning algorithms such as neural networks that generate new and sometimes unforeseen behaviors, the physical outcome is the same. Leave aside the difficulties with courts attempting to distinguish between different types of AI architecture; ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.

151
Identifying AI-Generated Torts
Not all injuries for which a machine or AI is involved would be AI-generated torts. To illustrate, consider two hypothetical accidents:
1 A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop-off.
2 A crane operator is appropriately manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and we can assume) that the operator is at fault (although a creative plaintiff’s attorney may argue the crane was negligently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a factual cause of the injury in torts vernacular, the machine did not interrupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an extension of the operator. In the second hypothetical, allocating fault is once again intuitively obvious. The machine is at fault rather than the operator. The operator acted with reasonable care, and the injury was due to (we may again assume) a flawed crane.
These two scenarios would result in different liability outcomes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby, even if the manufacturer had exercised the utmost care in the design and construction of the crane.

152
In both scenarios, an operator is using a crane in much the same way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dynamic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.
Now imagine a third scenario:
3 An AI-operated, unmanned crane drops a steel frame on a passerby after
incorrectly identifying the location for drop-off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a machine is being used as a tool. In Example 3, an AI has stepped into the shoes of the worker; it has replaced a person, and it is performing in essentially the same manner as a person. If the AI was a person, it would be liable in negligence and held to the standard of a reasonable person.
Holding suppliers of AI tortfeasors to a negligence standard requires rules for distinguishing between AI-generated torts and other harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is an “AI tortfeasor” (Example 3).

153
AI-generated torts could be those cases in which an AI is engaging in activity that a person could engage in, and that acts in a manner that would be negligent for a human tortfeasor. Applying this rule to the crane examples, Example 1 would result in human liability because the human operator acted carelessly, and the crane did not interrupt a foreseeable chain of events. There would be strict manufacturer liability in Example 2 because a person could not reasonably be substituted for a crane. Example 3 would require negligent manufacturer liability since the AI was automating a task that a person could have performed.
Sometimes this rule will have clear application. For example, an AI that mistakenly “reads” a chest X-ray in place of a radiologist would be occupying the position of a human tortfeasor whereas a malfunctioning X-ray machine would be an ordinary product. In other instances, this distinction will not be clear cut. Electrocardiogram (ECG) machines are a vital feature of emergency rooms where they are used to evaluate patients for heart conditions. These machines often provide interpretation of raw data but generally state that any analysis is preliminary and that health care providers are ultimately responsible for diagnosis. In the context of the self-driving car, under the most widely adopted framework, vehicles are categorized on a zero to five scale based on “who does what, when.”10 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction between person and machine. Autonomy exists on a continuum, and while it may be clear in some cases that
10 See SAE INT’L, AUTOMATED DRIVING (2014), http://www.sae.org/misc/pdfs/automated_driving.pdf (describing the SAE taxonomy).
  
154
an AI is acting like a person or an ordinary product, in other cases there may be an overlap of responsibility and decision-making between people and AI.
When an individual and an AI both contribute to a harm, both may be liable either jointly or individually in proportion to their wrongdoing. For instance, when a human driver and an AI driver are both at fault, as may have been the case in which Uber’s system failed to stop in time for a pedestrian and the backup driver was watching a TV show, both drivers could be found equally negligent. With an ECG, the sort of analysis commonly used ECGs perform has value, but it is widely understood not to be at the level of a human physician and is legitimately not intended to replace a human diagnosis. ECGs are better considered an ordinary product and holding them to the standard of a physician would result in them routinely found liable for medical negligence. This would likely result in manufacturers no longer providing analysis that now has value to doctors. However, a supplier should not be able to avoid liability simply by disclaiming liability for a product that is obviously automating human activity.
It should not be necessary for an AI to actually replace a human operator for negligence to apply. It should be sufficient that an AI is performing a task that a person could reasonably do. Thus, if a new taxi company goes into business using a fleet of only AVs, AI would not have replaced human operators, but AI would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software (e.g., the engine) could not be reasonably substituted. A person could drive a taxi instead of an AI, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as an AI tortfeasor, but the other parts of the vehicle would not. Thus, the test for an AI-generated tort could focus on whether the

155
allegedly negligent AI behavior involves the sort of task a person would usually perform or, if automation has become standard, that a person would have performed.
The negligence test should then focus on whether the AI’s act was negligent, rather than whether the AI was negligently designed or marketed. Again, the AI is taking the place of a person in the traditional negligence paradigm, and this would treat the AI more like a person than a product. It makes no difference to an accident victim what an AI was “thinking,” only how it acted. Accident victims have a right to demand careful conduct, regardless of how well an AI tortfeasor may have been designed.
There is another important reason why focusing on an AI’s act is more appropriate than focusing on its design. As discussed in Chapter 1, many AI systems, such as those utilizing machine-learning, are becoming increasingly complex and have limited explainability. It may be difficult or impractical for anyone, including an AV supplier, to determine why a self-driving car, say, ran a red light. But if a harmful act stemmed from defective software then such a determination may be necessary to prove a product has a defect and thus to establish liability. Even worse, it will almost certainly be substantially more challenging for a plaintiff to establish a product defect than it would be for a defendant. The plaintiff may need to hire (quite expensive) experts to investigate a self- driving car’s AI, and that presupposes a court will permit external access to a supplier’s (likely) proprietary AI system. The cost and complexity of such an inquiry probably puts it beyond the feasibility of accidents without at least hundreds of thousands or millions of dollars in damages. At least in the US system (although not the UK system), plaintiffs usually cannot recover their legal costs even when they prevail in court. For some injured victims then, lawyer and expert fees may exceed any likely recovery which means that

156
some meritorious claims will not be pursued. By contrast, a negligence test focused on an AI’s act and whether it fell below the standard of a reasonable person would simply ask whether an AV ran a red light and whether a reasonable person would have done the same. This is a far simpler and less expensive case to prove.
Of course, it is sometimes possible when alleging negligence or even a defective product for a plaintiff to prove their case with inference on the basis of reason, logic, bad behavior by a party, or common sense. As one example, a legal doctrine known as res ipsa loquitur – “the thing speaks for itself” – allows circumstantial evidence to permit an inference that a defendant was at fault. For example, if a barrel falls off a building striking a pedestrian and causing harm the pedestrian may have a difficult or impossible time providing what caused the accident. But because barrels do not tend to fall off buildings absent careless behavior, the mere fact of the barrel’s falling may be sufficient to establish negligence absent compelling contradictory evidence from a defendant. Similarly, when a medical instrument is left in a patient after surgery, that tends to be adequate to establish negligence, even though a patient may have no way of proving what occurred. However, inference, including through application of res ipsa loquitur, is not a panacea to the challenges posed by AI tortfeasors – at least as currently applied by courts. For example, res ipsa loquitur is not recognized by every state, some states require that a defendant have exclusive control over a product (which can be problematic with the involvement of multiple parties), and not all states allow for the presumption of a product defect. There are similarly restrictions on the use of inference more generally in many jurisdictions to avoid speculation and unfairness to defendants. The mere fact an accident occurred with a product does not necessarily mean there was a defect.

157
Financial Liability
Whether in strict liability or negligence, AI could not be directly financially liable for its harms. AI does not have property rights; in fact, AI is owned as property and would not be influenced by the specter of liability in the way a person could be influenced. For the purposes of financial liability, the AI’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product, or where several parties are involved in the distribution chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which General Motors licenses and incorporates in its vehicles, and an independent retailer leases the vehicles to Lyft. Default liability rules could be altered by firms in the supply chain by contract. This would be particularly likely to occur in cases in which manufacturers and retailers are large, sophisticated entities. For example, General Motors may indemnify Apple, Delphi, and Lyft in return for more favorable licensing and leasing terms.
Alternately, the AI’s owner could be liable for its harms, which would be somewhat akin to treating AI tortfeasors as employees and making owners liable under theories of vicarious liability – when someone is held responsible for the actions of another person. It is particularly easy to imagine owners purchasing insurance for harms caused by AI in the context of a self-driving car. Insurance policies may soon come with a rider (or discount) for AV software. Owner liability may further incentivize the production of autonomous AI given that manufacturers would have less liability, but this may also reduce adoption since

158
owners would be taking on that liability. These two effects may offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of AI tortfeasors, and because manufacturers are in the best position to improve product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self- driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars become significantly safer than human drivers, insurance rates will decrease for self-driving cars and perhaps increase for human drivers. This should have a nudging effect on self-driving car adoption, as financially sensitive individuals take automobile premiums into account in deciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower premiums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsible for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the creation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation. For example,

159
incentives could take the form of grants for research and development, loans to build production facilities, enhanced intellectual property rights, prizes, preferential tax treatments, or government guarantees.
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allowance Rebate System (CARS), better known as “cash for clunkers.” CARS provided consumers trading in old vehicles with vouchers of between $3,500 to $4,500 to purchase new cars. It was a $3 billion US federal program designed as a short-term economic stimulus and to benefit US automobile manufacturers. It was also intended to promote safer, cleaner, more fuel-efficient vehicles. Ultimately, while critics dispute the effectiveness of the program at stimulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel-efficiency and safety, and it was popular with consumers. In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alternatives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that would place caps on damages, limit contingency fees (the ability of lawyers to obtain a percentage of a client’s recovery as a fee for services), mandate periodic payments, or reduce the statute of limitations (the time limit for suing).
Finally, the government could promote safety by means of regulation. This could involve requirements for industries to achieve minimum safety targets or direct requirements to adopt certain technologies. At the point where self-driving cars become ten or a hundred times safer than people, traditional human driving could be prohibited. Regulatory solutions may be most appropriate when the benefits of automation are

160
overwhelming, and when it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer and business-friendly solution. While consumers may have more difficulty seeking to recover for accidents, they should also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. Businesses would have lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not require government funding, additional regulatory burdens on industry, or new administrative responsibilities. It would provide an incremental solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than with more radical solutions. For all the above reasons, shifting to negligence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer credits, combined with direct regulations in certain instances. Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable because it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy. Indeed, automation to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could

161
independently adopt these rules. Lawmakers would then have the option of modifying the common law.
3 The Reasonable Robot
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
- Oliver Wendell Holmes Jr.
When Negligence Is Strict
Negligence may function almost like strict liability for people with below-average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.” This issue was at the heart of a case in 1837, Vaughan v. Menlove, that concerned a defendant who lacked normal intelligence. The defense argued that it would be unfair to hold him to the standard of an ordinary person, and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, holding that ordinary prudence should apply in every case of negligence. As famed judge Oliver Wendell Holmes Jr. later articulated in 1881, “The law considers ... what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”11 This remains the case
  OLIVER WENDELL
 HOLMES JR.
 11
, THE COMMON LAW, 108 (1881).

162
today: A modern defendant cannot generally escape liability for causing a motor vehicle accident because she has slow reflexes, poor vision, or anxiety while driving.
There are benefits to such a rule. Logistically, as the court noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state. Better for administrative purposes to work with an external, objective standard than to prove individual capacities and state of mind. Substantively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal conduct of others. As Holmes articulates, damages caused by individuals with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else “should perhaps refrain from driving at all.”12
The New Hasty and Awkward
Collectively, people are not the best drivers, even when they refrain from drinking behind the wheel, falling asleep on the highway, or colliding into police cars while playing Pokémon Go. But compared to AI? It will not be long until AI is safer than the average person, and then safer than any human driver. Principles of harm avoidance suggest that once it becomes practical to automate, and that doing so is safer, an AI should become the new “reasonable person” or standard of care.
12 Roberts v. Ring, 143 Minn. 151, 153 (1919).
 
163
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defendant would be judged against what an AI would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an AV would have prevented the injury. In fact, it may be that the AV is only able to prevent such an accident because it has superhuman abilities. It may have software capable of ultrafast decision-making, monitors that surpass human senses, and access to external cameras that expand peripheral view beyond that of a person.
With the reasonable person test, jurors are asked to put themselves in the shoes of a reasonable person and decide what that person would have done. It may be a challenge for a juror to follow that reasoning in the case of a “reasonable robot.” The reasonable robot, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasonable” in the context of an AI is an anthropomorphism to assist people conceptually. To take a simple case, imagine an individual driving on dry pavement at forty miles per hour and then colliding with a child running into the road 150 feet ahead of the driver’s vehicle. To determine whether the driver is liable under the reasonable robot standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable robot would not have collided with the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of AI would be known, and the breaking distance could be

164
standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software.
A defendant may argue that it is unfair for his best efforts to result in liability. A reasonable robot standard essentially makes people strictly liable for their accidental harms. This is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below-average driver due to computers. It may appear unfair to impose liability on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or an AI.
Tort liability would not prohibit people from driving even at the point when AVs become substantially safer than people. If that were a desired outcome it could be accomplished through a legislative ban on human driving. Instead, an AI standard of care would mean that people drive at their own risk. If a driver causes an accident, he or she will be liable for the resultant damages. A tort-based incentive may be preferable to an inflexible statutory mandate because there are benefits to human driving unrelated to accidents, such as for promoting freedom and autonomy. Individuals who particularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, an AI standard of care is likely to have a significant impact on behavior. Making individuals and businesses effectively strictly liable for their harms will discourage certain undertakings. In the context of the self-driving car, it

165
would likely result in far fewer human drivers as insurance rates for traditional vehicles would become more expensive relative to self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. AI may be prohibitively expensive or only available in limited quantities, which is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. Therefore, to introduce a reasonable robot standard, a plaintiff should have to show that a person was performing a task that could be performed by an AI and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of an AI in which 1) no such AI existed at the time of the accident, 2) no AI was available to the defendant, 3) an AI was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory requirements for a human driver). If Tesla could manufacturer a completely safe AV at a cost of $1 million, it would not be reasonable to require all consumers to automate.
Reasonable People Use AI
As an alternative to the reasonable robot standard, the reasonable person could be a person using an AI. For example, once self-driving cars become safer than people, a jury may find that it is unreasonable to drive yourself rather than to use an AV. Applying the reasonable person using an AI to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on his driving in the first place.

166
A reasonable person would not have driven; a reasonable person would have chosen to automate.
Under either the reasonable person or reasonable robot standard, a human driver would be compared with a self-driving car but in different ways. With the reasonable robot standard, courts would evaluate the human driver’s proximally harmful act whereas with the reasonable person standard, courts would evaluate the human driver’s initial decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable person standard would be more in line with the existing negligence regime.
While keeping the reasonable person standard would be conceptually easier, in practice it would be less desirable. The goal is to compare the harmful act of the person and AI, not target the initial decision to automate. It is problematic to base liability on the decision to automate because it must focus on the question of whether automation is either generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform an AI. A situational focus must still compare the harmful act of a person versus an AI.
AI is and will be safer at automating certain activities than others. For instance, AI working to diagnose disease may be superior to physicians at detecting certain conditions but not others. Self-driving cars may be safer than human drivers on average but not safer than professional or above-average drivers. AVs may also be safer under most conditions but may be relatively poor at, for example, driving off-road. So, while automation may generally improve safety, optimal accident reduction may require a mix of AI and human activity.

167
Suppose an AV is ten times safer than a human driver generally but only half as safe as a human driver in icy conditions. Now, suppose a human driver encounters a patch of black ice and causes an accident under circumstances when she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable robot, she would escape liability if the AI would have been unable to avoid the accident (which is likely if the AI is half as safe in icy conditions). If the reasonable person using an AI test focuses on whether an AI is generally safer, however, she would be liable. This test would conclude that it would have been unreasonable not to use a self-driving car since self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an AI evaluation could be situational. For instance, it could be reasonable not to use an AI but only in icy conditions. However, this is just a more convoluted version of the reasonable robot test, because it requires evaluating whether an AI would be safer than a person in a particular instance. This essentially asks how the AI would have acted in a situation – which would be the application of the reasonable robot standard. It would then require asking, based on that knowledge (which may be impractical for a person to have), whether an earlier decision to automate was reasonable. In the black ice hypothetical, it could require the driver to know in advance of activating or deactivating self-driving software whether there are icy conditions and how the AI would perform in icy conditions to determine if the risk of using the AI in icy conditions outweighs the benefits of using the AI for other parts of the trip.

168
The Reasonable Robot Standard for AI Tortfeasors
This chapter has advocated for holding AI tortfeasors to a negligence standard and comparing their acts to the acts of a reasonable person. It has also proposed replacing the reasonable person standard with the reasonable robot standard once automation is practicable and AI is safer than an average person. This means that future AI tortfeasors would be held to the reasonable robot standard.
There may be instances in which it still makes sense to apply the reasonable person standard to AI tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of an AI, such as where automation is prohibitively expensive or where AI is not widely available. We would not want to hold an AI tortfeasor to a higher standard than a human defendant. In some industries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become normal.
Eventually, once an AI becomes the standard of care, it would also be the standard for AI tortfeasors. For instance, if a self-driving Audi collides with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Google cars. There are a variety of ways to determine the reasonable robot standard by, for example, considering the industry customary, average, or safest technology. Under any standard, this is a different test than the current strict liability standard in which the inquiry focuses on whether a product is defectively designed, or its properties falsely represented.

169
As AI improves, the reasonable robot standard will grow stricter, which is alright, because once AI is exponentially safer than a person, it is likely that AI tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable robot standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until AI is exponentially safer than people.
Further Thoughts
AI presents new challenges to tort law as it does to tax law. Once again, applying a principle of AI legal neutrality to tort law will guide the development of AI and help to ensure its social utility, but careful consideration needs to be given to its application. In tort law, the challenges are not the same as with tax law, and the two bodies of law have different underlying concerns, goals, and solutions. With tort law, our primary policy goal is to structure liability to optimize accident deterrence.
At some point in the future, there are likely to be few or no activities for which AI cannot outperform people. Self-driving cars will eventually be a thousand times safer than the best human driver, at which point AI will cause so little harm that the economics of negligence versus strict liability will be irrelevant for AI manufacturers. AI will have become so ubiquitous that the constantly improving reasonable robot should be the benchmark for most or all areas of accident law.
Applying a principle of AI legal neutrality to tort law could encourage the development and adoption of safer technologies that could prevent countless accidents. It

170
has become acceptable for more than a million people a year to die in traffic accidents worldwide, but only because a reasonable alternative has not yet been within reach. We could soon be living in a world where practically no one dies from unintended injury. Once the third-leading cause of death is eliminated, we would just be left to deal with the leading two causes of death: cardiovascular disease and cancer. A different type of AI, artificial inventors, may eliminate those as well.

171
4
Artificial Inventors
Computers are useless. They can only give you answers.
- Pablo Picasso
As with tax and tort law, AI poses new challenges to intellectual property law. In the case of patent law, AI is already generating patentable inventions under circumstances in which the AI, rather than a human inventor, meets the requirements to qualify as an inventor. Yet because almost every country in the world requires a patent application to list a natural person as an inventor, a requirement designed to ensure the right of human inventors to be acknowledged, it is not clear that an “AI-generated invention” could be patented. There is no US statute addressing AI-generated invention, no case law directly on the subject, and there is not even a relevant policy by the responsible administrative agency, the US Patent and Trademark Office (Patent Office). Nor, at least in 2019, does there appear to be a statute specifically about AI-generated invention anywhere else in the world.
In 2019, a team of patent attorneys, in a project spearheaded by this book’s author, announced they had filed the first patent applications in several jurisdictions worldwide that explicitly claimed inventions autonomously generated by an AI. Those applications listed the AI’s owner as the patent applicant and thus the owner of any future patents, and listed the AI as the inventor. As of the time of writing, these applications are still pending.
Historically, academics have argued against allowing protections for AI-generated inventions on the grounds that machines do not respond to incentives and that AI

172
inventorship may chill human invention. In 2019, a spokesperson for the European Patent Office argued, “It is a global consensus that an inventor can only be a person who makes a contribution to the invention’s conception in the form of devising an idea or a plan in the mind ... . The current state of technological development suggests that, for the foreseeable future, AI is ... a tool used by a human inventor ... . Any change ... [would] have implications reaching far beyond patent law, i.e., to authors’ rights under copyright laws, civil liability and data protection.”1
The principle of AI legal neutrality suggests that we should allow intellectual property protections for AI-generated inventions. Although AI would not be directly motivated to invent by the prospect of a patent, patents would motivate some of the people who build, own, and use AI. Allowing intellectual property protections for AI output would thus incentivize the development of inventive AI. In turn, this would incentivize innovation and lead to new scientific advances – which is the primary purpose of the patent system. This is particularly critical for the day AI will be a meaningful source, or even the primary means, of generating new inventions. In addition, AI rather than a person should be listed as an inventor when it otherwise meets inventorship criteria. Although having a person take credit for the work of an AI would not be unfair to the AI, it would be unfair to other human inventors because it would dilute the nature of inventorship. It would equate the work of legitimate human inventors with people asking AI to solve a problem. The AI’s owner should be the default owner of any patents on an AI’s invention.
1 Leo Kelion, AI System “Should Be Recognized as Inventor,” BBC, Aug. 1, 2019, www.bbc.co.uk/news/technology-49191645
    
173
This chapter is divided into three sections. Section 1 examines instances in which AI has created patentable inventions. It finds that AI has likely been autonomously generating patentable results for decades and that the pace of such invention is likely increasing. Section 2 examines the case law related to nonhuman authorship of copyrightable material in the absence of law on the subject of AI inventorship. It argues in favor of intellectual property protections for AI-generated works, and argues an AI should qualify as a legal inventor. Next, Section 3 addresses some of the challenges posed by AI inventorship.
1 AI-Generated Inventions
Example One: The Creativity Machine
In 1994, computer scientist Stephen Thaler disclosed an AI architecture he called a “Creativity Machine,” a computational paradigm that he argued “came the closest yet to emulating the fundamental mechanisms responsible for idea formation.”2 A Creativity Machine combines an artificial neural network that generates novel output in response to self-stimulation of the network’s connections with a “critic” network that monitors the first network’s output. The critic network can evaluate this output for novelty compared to the AI’s existing knowledge base and can provide feedback to the first network to continue developing some novel output or to stop. This results in an AI that “brainstorms” new and creative ideas after it alters (perturbs) the connections within its neural network. An example of this phenomenon occurred after Thaler exposed a Creativity Machine to some
2 See What Is the Ultimate Idea?, IMAGINATION ENGINES INC., www.imagination-engines.com.
  
174
of his favorite music, and the AI proceeded to generate eleven thousand new songs in a single weekend.
Thaler compares a Creativity Machine and its processes to a human brain and consciousness. The two artificial neural networks mimic the human brain’s major cognitive circuit: the thalamo-cortical loop. In a simplified model of the human brain, the cortex generates a stream of output (or consciousness) and the thalamus brings attention (or awareness) to ideas of interest. Like the human brain, a Creativity Machine is capable of generating novel patterns of information rather than simply associating patterns, and it is capable of adapting to new scenarios without additional human input. Also like the brain, the AI’s software is not entirely written by programmers – there is a degree to which it is self-assembling. Thaler argues his AI is very different from a program that simply generates a spectrum of possible solutions to a problem combined with an algorithm to filter for the best ideas generated, though such a program would be another method for having an AI develop novel ideas.
Thaler invented the Creativity Machine paradigm, which was the subject of his first patent, titled “Device for the Autonomous Generation of Useful Information.”3 The second patent filed in Thaler’s name was “Neural Network Based Prototyping System and Method.”4 Thaler is listed as the patent’s inventor, but he states that a Creativity Machine generated the patent’s invention (Creativity Machine’s Patent). The Creativity Machine’s Patent application was first filed on January 26, 1996, and granted on December 22, 1998. As one of Thaler’s associates observed in response to the Creativity Machine’s Patent,
3 See U.S. Patent No. 5,659,666 (filed Oct. 13, 1994). 4 See U.S. Patent No. 5,852,815 (filed May 15, 1998).
 
175
“Patent Number Two was invented by Patent Number One. Think about that. Patent Number Two was invented by Patent Number One!”5 Aside from the Creativity Machine’s Patent, Creativity Machines are credited with numerous other inventions: the cross-bristle design of the Oral-B CrossAction toothbrush, new physical materials, and devices that search the Internet for messages from terrorists.
The Creativity Machine’s Patent is interesting for several reasons. If Thaler’s claims are accurate, then the Patent Office has already granted a patent for an invention created by a nonhuman inventor – and as early as 1998. Also, the Patent Office apparently had no idea it was doing so. Thaler listed himself as the inventor on the patent and did not disclose the Creativity Machine’s involvement to the Patent Office. He has stated this was on the advice of his attorneys.
Example Two: The Invention Machine
The Creativity Machine has not been the only source of AI-generated invention. Software modeled after the process of biological evolution, known as genetic programming, has succeeded in independently generating patentable results. Evolution is a creative process that relies on a few simple processes: mutation, sexual recombination, and natural selection. Genetic programming emulates these same methods digitally to achieve machine
  5 See Tina   ,
Brain,
www.mindfully.org/Technology/2004/Creativity-Machine-Thaler24jan04.htm (quoting Rusty Miller).
Hesman
Stephen Thaler’s Computer Creativity Machine Simulates the Human
 ST. LOUIS POST-DISPATCH
, Jan. 24, 2004,
 
176
intelligence. It delivers human-competitive intelligence with a minimum amount of human involvement.
As early as 1996, genetic programming succeeded in independently generating results that were the subject of past patents. By 2010, there were at least thirty-one instances in which genetic programming generated a result that duplicated a previously patented invention, infringed a previously issued patent, or created a patentable invention. In seven of those instances, genetic programming infringed or duplicated the functionality of a twenty-first-century invention. Some of those inventions were on the cutting edge of research in their respective fields. In two instances, genetic programming may have created patentable new inventions.
The Patent Office granted another patent for a AI-generated invention on January 25, 2005.6 That invention was created by the “Invention Machine” – the moniker for a genetic programming–based AI developed by John Koza, a computer scientist and pioneer in the field of genetic programming as well as the inventor of the scratch-off lottery ticket. Koza claims the Invention Machine created multiple patentable new inventions.7 A 2006 article in Popular Science about Koza and the Invention Machine claims that the AI “has even earned a U.S. patent for developing a system to make factories more efficient, one of the first intellectual property protections ever granted to a nonhuman designer.”8 The Invention Machine generated the content of the patent and an improved controller (a
6 Jonathon Keats, John Koza Has Built an Invention Machine, POPULAR SCI., Apr. 18, 2006, https://popsci.com/scitech/article/2006-04/john-koza-has-built-invention-machine/.
       7 See
8 Keats, supra note 6.
, 11
John R.
Koza,
Human-Competitive Results Produced by Genetic Programming
 GENETIC PROGRAMMING & EVOLVABLE MACHINES
251, 265 (2010).

177
component of most electrical products) design without human intervention and in a single pass.9 It did so without a database of expert knowledge and without any knowledge about existing controllers. It simply required information about basic components (such as resistors and diodes) and specifications for a desired result (performance measures such as voltage and frequency). With this information, the Invention Machine proceeded to generate different outputs that were measured for fitness (whether an output met performance measures).
Once again, the Patent Office had no idea of the AI’s role in the Invention Machine’s patent. Koza did not disclose the Invention Machine’s involvement. Like Stephen Thaler, Koza has stated that his legal counsel advised him at the time that his team should consider themselves inventors despite the fact that “the whole invention was created by a computer.”10 Koza has reported that his agenda in having the Invention Machine re-create previously patented results was to prove that AI could be made to solve problems automatically. He believed that focusing on patentable results would produce compelling evidence that AI is capable of generating value. For that reason, he focused on recreating or generating patentable inventions that represented significant scientific advances. For instance, the Invention Machine’s patent was for an improved version of a landmark controller built in 1995.
The Creativity Machine and the Invention Machine are early examples of AI inventors, but patents may have been granted on earlier AI-generated inventions. For
9 U.S. Patent No. 6,847,851 (filed July 12, 2002).
10 Telephone Interview with John Koza, President, Genetic Programming Inc. (Jan. 22, 2016) (on file with author).
 
178
instance, an article published in 1983 describes experiments with an AI program known as Eurisko, in which the program “invent[ed] new kinds of three-dimensional microelectronic devices ... novel designs and design rules have emerged.”11 Eurisko was an early, expert AI system for autonomously discovering new information. It was programmed to operate according to a series of rules known as heuristics, but it was able to discover new heuristics and use them to modify its own programming. To design new microchips, Eurisko was programmed with knowledge of basic microchips along with simple rules and evaluation criteria. It would then combine existing chip structures together to create new designs or mutate existing entities. The new structure would then be evaluated for interest and either retained or discarded. Stanford University filed a patent for one of Eurisko’s chip designs in 1980 but abandoned the filing for unknown reasons in 1984.12 Also, as with other known instances of patent applications for AI-generated inventions, the patent application was filed on behalf of natural persons. In this case, the individuals who built a physical chip based on Eurisko’s design.
In another example, a 1989 PhD dissertation disclosed a “systematic computational approach to innovative design of engineered artifacts in general ... comparable to human inventions ... a computer program called TED ... [which] is the inventor of its systems in the sense that it generates the structure (topology) of the system ‘from scratch.’ The synthesis process is modeled as a heuristic search conducted in a state-space of all possible design
   11
Lenat &     ,
Douglas B.
William R.
Sutherland
, 3   , 17, 17 (1982). 12 U.S. provisional patent application SN 144,960 (filed Apr. 29, 1980). Email
communications with Katherine Ku, Dir. of Stanford Office of Tech. Licensing, to author (Jan. 17, 2018) (on file with author).
Heuristic Search for New Microcircuit
 Structures: An Application of Artificial Intelligence
AI MAG.

179
versions (design states). TED explores one design possibility after another in a systematic fashion, looking for a solution to the design requirements.”13 Alexander Kott, TED’s developer, noted that TED rediscovered at least two significant and well-known inventions, and also generated previously unknown and nontrivial designs.
Example Three: Watson
The above examples are decades old. Subsequent improvements in AI should have led to a significant increase in the number of AI-generated inventions. Consider, for instance, more recent results produced by IBM’s AI Watson. Watson was an AI developed by IBM to compete on the game show Jeopardy! In 2011, ancient history in AI terms, Watson beat former Jeopardy! winners Ken Jennings and Brad Rutter on the show, earning a million dollars in the process. Since then, the “Watson” brand has evolved to incorporate a variety of AI systems and technologies, although it is sometimes marketed by IBM as if it is a single AI. For simplicity, we can consider it a single AI system.
Watson is largely structured as an “expert system.” Expert systems are one way of designing AI that solves problems in a specific domain of knowledge using logical rules derived from the knowledge of experts. These were a major focus of AI research in the 1980s. Expert system–based chess-playing programs HiTech and Deep Thought defeated chess masters in 1989, paving the way for another famous IBM AI, Deep Blue, to defeat world chess champion Garry Kasparov in 1997. But Deep Blue had limited utility – it was
13 Alexander Kott, Artificial Invention: Synthesis of Innovative Thermal Networks, Power Cycles, Process Flowsheets and Other Systems, ii (Disseration.com, 2005), www.bookpump.com/dps/pdf-b/1122640b.pdf.
   
180
solely designed to play chess. The machine was permanently retired after defeating Kasparov. IBM now describes Watson as one of a new generation of machines capable of “computational creativity.”14 IBM uses that term to describe machines that can generate “ideas the world has never imagined before.”15 Watson “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], applying big data in new ways.”16 This is a fundamentally different type of AI than the Creativity Machine or the Invention Machine. Watson utilizes a more conventional architecture of logical deduction combined with access to massive databases containing accumulated human knowledge and expertise. Although Watson is not modeled after the human brain or evolutionary processes, it may also be capable of generating inventions.
Watson’s Jeopardy! career was short and sweet, and by 2014, it was being applied to more pragmatic challenges, such as running a food truck. IBM developed new algorithms for Watson and incorporated a database with information about nutrition, flavor compounds, the molecular structure of foods, and tens of thousands of existing recipes. This design permits Watson to generate recipes in response to users inputting a few parameters such as ingredients, dish (e.g., burgers or burritos), and style (e.g., British or dairy-free). On the basis of this user input, Watson proceeds to generate a staggeringly large number of potential food combinations. It then evaluates these preliminary results based on novelty and predicted quality to generate a final output.
14 See Computational Creativity, IBM, https://perma.cc/6FK4-WTL3. 15 What Is Watson?, IBM, https://perma.cc/8KM3-LLSG.
16 See Computational Creativity, supra note 14.
   
181
It is likely that some of Watson’s discoveries in food science are patentable. Patents may be granted for any “new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof.”17 Food recipes can qualify as patentable on this basis because lists of ingredients combine to form new compositions of matter or manufacture and the steps involved in creating food may be considered a process. To be patentable, however, an invention must not only contain patentable subject matter; it must also be novel, nonobvious, and useful. This may be challenging to achieve in the case of food recipes given that there is a finite number of ingredients and people have been combining ingredients together for a very long time. Not only would Watson have to create a recipe that no one had previously created, but it could not be an obvious variation on an existing recipe. Still, people do obtain patents on new food recipes. The fact that some of Watson’s results have been surprising to its developers and to human chefs is encouraging in this regard because unexpected results are one of the factors considered in determining whether an invention is nonobvious.
Watson is not, however, limited to competing on Jeopardy! or to developing new food recipes. IBM reports it has made Watson broadly available to software application providers, enabling them to create services with Watson’s capabilities. Watson is now, among other tasks, assisting with financial planning, helping clinicians to develop treatment plans for cancer patients, identifying potential research study participants, distinguishing genetic profiles that may respond well to certain drugs, and acting as a personal travel concierge. It is also being used to conduct research in drug discovery as well as clinically to analyze the genes of cancer patients and develop treatment plans. In
17 35 U.S.C. § 101 (1952).
 
182
drug discovery, Watson has already identified novel drug targets and new indications for existing drugs. In doing so, Watson may be generating patentable inventions either autonomously or collaboratively with human researchers. In clinical practice, Watson is also automating a once human function. In fact, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, a task that otherwise requires around 160 hours of work by a team of human experts. A study by IBM claims that Watson’s report outperforms the standard practice.18
Watson is just one of many AI systems involved in modern innovation. In 2019, Flinders University in Australia reported that they had used AI to develop a flu vaccine that was approved for human trials.19 The research team created an AI, Sam, to recognize vaccines that worked against the flu and those that did not. They then created a second AI to create trillions of potential vaccine candidates. Sam screened these to create a list of the ten most promising candidates. This allowed the human team to synthesize and test a small number of compounds over the course of a few weeks rather than having to directly screen millions of compounds. The AI reportedly resulted in a more effective vaccine, sped up the discovery process, and substantially reduced costs. Companies like BenevolentAI now report applying AI in a similar fashion to the entire drug discovery process.
    18
et al.,
Kazimierz O.
Wrzeszczynski
Comparing Sequencing Assays and Human-Machine
 Analyses in Actionable Genomics for Glioblastoma
DOI: 10.1212/NXG.0000000000000164.
19 Anne Gulland, Scientists Claim to Have Developed World’s First Vaccine with Artificial Intelligence, THE TELEGRAPH, July 3, 2019, www.telegraph.co.uk/global-health/science-and- disease/scientists-claim-have-developed-worlds-first-vaccine-artificial/.
, 3   (2017),
NEUROL. GENET.
   
183
Widespread AI-Generated Inventions?
Given the fact there have been credible claims of AI-generated invention for decades, and AI has improved exponentially since that time, it may seem as if there should routinely be lawsuits involving AI-generated inventions and a well-developed legal framework for AI- generated inventions. This is not the case.
It may be that AI-generated inventions remain few and far between, or lack commercial value. After all, AI has been creating new written and artistic works for decades, but they have historically been fairly terrible. In 1993, Scott French developed an AI to write a novel in the style of a famous author. The resulting work was described by a critic as “a mitigated disaster.”20 It is certainly much easier for a person to generate a new written work than a patentable invention, so it stands to reason that inventive AI would lag creative AI.
Today, creative AI has done a lot of catching up. Both Alphabet and IBM have developed AI systems that autonomously generate art.21 In 2018, Christie’s became the first auction house to sell a work of art created by an AI, selling an AI-generated painting in the style of the famed artist Rembrandt for $432,500.22 With respect to written works, robot
  20
Holt,   ,   , Aug. 15, 1993, at B4; see, generally, J.
, ,39COLUM.J.OFLAW&
403, 408 (2016).
Patricia
Sunday Review
SAN FRAN. CHRON.
   Grimmelmann
There’s No Such Thing as a Computer-Authored Work
 THE ARTS
21 DEEP DREAM GENERATOR, https://deepdreamgenerator.com (last visited Oct. 15, 2019);
Jennifer Sukis,   ,   , May 15, 2018,
https://medium.com/design-ibm/the-role-of-art-in-ai-31033ad7c54e.
  The Relationship Between Art and AI
MEDIUM
   22
, , Dec. 12, 2018,
Is Artificial Intelligence Set to Become Art’s Next Medium?
www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a- machine-9332-1.aspx.
CHRISTIE’S
  
184
journalists have yet to put people out of work en masse, but they are increasingly augmenting human journalists and even autonomously writing relatively simple articles that most readers cannot identify as written by AI. About a third of Bloomberg News’ content now uses some form of automated technology. The Washington Post uses as AI, Heliograf, to generate and auto-publish reports. If AI that routinely augments inventive activity and even autonomously invents is not that far behind creative AI, we may soon witness the widespread use of inventive AI.
Another explanation for the absence of publicized activity in this space may be that existing laws, or their absence, drive activity underground. Different countries have different rules about AI-generated works, but in the United States, they cannot receive copyright protection as a result of Copyright Office policies. As a result, anyone using AI to create a valuable new painting, or a newspaper article, would disqualify that work from receiving copyright protection by revealing its origins. So, an AI user wishing to obtain protection for an AI-generated work may end up identifying herself as an author. Similarly, an AI user might not wish to publicly sue an AI owner over an AI-generated work if this would result in the loss of copyright protection. Indeed, as Stephen Thaler and John Koza have reported, some of the earliest applicants for patents on AI-generated inventions were advised to list themselves as inventors despite their own admissions that they did not meet inventorship criteria. Patent offices are supposed to accept reported inventors at face value in the ordinary course of things, unless challenged by third parties. Where an AI has autonomously invented something, it is unlikely to complain about not being listed as an inventor. The role of AI would probably only become an issue if a patent is challenged and a

185
third party somehow becomes aware it was an AI-generated work, or if the AI’s owner, user, or developer sue each other over ownership.
2 Intellectual Property Rights for AI-Generated Works
Requirements for Inventorship
All patent applications require one or more named inventors who must be “individuals”; an artificial person such as a corporation cannot be an inventor. Inventors own their patents as a form of personal property that they may transfer by assignment of their rights to another entity. A patent grants its owner “the right to exclude others from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”23 If a patent has multiple owners, each owner may independently exploit the patent without the consent of the others (absent a conflicting contractual obligation).
In the United States, for a person to be an inventor, the person must contribute to an invention’s conception. Conception refers to “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative invention as it is thereafter to be applied in practice.”24 It is “the complete performance of the mental part of the inventive
23 35 U.S.C. § 154.
24 Townsend v. Smith, 36 F.2d 292, 295 (C.C.P.A. 1929).
 
186
act.”25 After conception, someone with ordinary skill in the invention’s subject matter (e.g., a chemist if the invention is a new chemical compound) should be able to reduce the invention to practice. That is, they should be able to make and use an invention from a description without extensive experimentation or additional inventive skill. Individuals who simply reduce an invention to practice by, for example, describing an already conceived invention in writing or building a working model from a description do not qualify as inventors.
The Role of People and AI in Inventive Activity
Although AI is commonly involved in the inventive process, in most cases AI is essentially working as a sophisticated, or not-so-sophisticated, tool. A simple example occurs when an AI is functioning as a calculator. In these instances, an AI may assist a human inventor to reduce an invention to practice, but the AI is not participating in the invention’s conception. Even when AI plays a more substantive role in the inventive process, such as by analyzing data in an automated fashion, retrieving stored knowledge, or recognizing patterns of information, the AI still may fail to contribute to conception. AI involvement might be conceptualized on a spectrum: On one end, an AI is simply a tool assisting a human inventor; on the other end, the AI functionally automates conception. AI capable of acting autonomously such as a Creativity Machine and the Invention Machine fall on the latter end of the spectrum.
25 Id.
 
187
Just as AI can be involved in the inventive process without contributing to conception, so can people. For now, at least, AI does not entirely undertake tasks of its own accord. AI requires some amount of human input to generate creative output. For example, before a Creativity Machine composed music, Thaler exposed it to existing music and instructed it to create something new. Yet, simply providing a computer with a task and starting materials would not make a person an inventor. Imagine Company Owner A tells Employee B, who is an engineer, that A would like B to develop a new TV screen with twice the resolution of existing devices and A gives B some publicly available information about TV designs. If B then succeeds in developing a high-resolution screen, A would not qualify as an inventor of that screen. A is likely to end up owning the patent, assuming B is under an obligation to assign inventions made in the course of employment, but this still does not make A an inventor. At least that is the legal standard, but as a practical matter A may end up listing himself as an inventor because he wants undeserved credit. Again, the Patent Office does not ordinarily challenge reported inventorship, and because few patents are litigated, for any given patent there is a very low chance reported inventorship will be disputed.
People are also necessarily involved in the creative process because AI does not arise from a void – people have to create AI. Once again, that should not prevent AI inventorship. No one would exist without their parents contributing to their conception (pun intended), but this does not make parents inventors on their child’s patents. If a computer scientist creates an AI to autonomously develop useful information and the AI creates a patentable result in an area not foreseen by the inventor, there would be no reason for the scientist to qualify as an inventor on the AI’s result. An inventor must have

188
formed a “definite and permanent idea of the complete and operative invention” to establish conception.26
An AI may not be a sole inventor: The inventive process can be a collaborative process between person and AI. If the process of developing the Creativity Machine’s Patent had been a back-and-forth process with both the AI and Thaler contributing to conception, then both may qualify as inventors. By means of illustration, suppose a human engineer provides an AI with basic information and a task. The engineer might learn from the AI’s initial output, and then alter the information that she provides to the AI to improve its subsequent output. After several iterations, the AI might produce a final output that the human engineer might directly alter to create a patentable result. In such a case, both the engineer and the AI may have played a role in conception. In some of these instances, if an AI were human, it would be an inventor. Leaving AI aside, invention rarely occurs in a vacuum, and most patents have multiple inventors. Yet, AI is not human; as such, AI faces unique barriers to qualifying as inventors – at least in the United States. A few countries, like Monaco and Cypress (which are both member states of the European Patent Office), have reported they do not require an inventor to be a natural person.27
26 Id.
27 HELI PIHLAJAMAA, LEGAL ASPECTS OF PATENTING INVENTIONS INVOLVING ARTIFICIAL INTELLIGENCE (AI): SUMMARY OF FEEDBACK BY EPC CONTRACTING
STATES, COMMITTEE ON PATENT LAW, Feb. 20, 2019. http://documents.epo.org/projects/babylon/eponet.nsf/0/3918F57B010A3540C1258419 00280653/$File/AI_inventorship_summary_of_answers_en.pdf.
   
189
As a more practical example, consider the Watson “insights” business model. Clients give their data to IBM, IBM runs the data through Watson, and Watson generates “insights” which may be patentable and which belong by contract to the client. But it may not always be clear who the inventor of an insight is. The client is probably not an inventor as businesses cannot be inventors and simply commissioning research or handing over data does not generally qualify for inventorship. Perhaps if inventive skill was required to carefully select certain data that was likely to lead to a solution, but not if available data was merely gathered and turned over. Perhaps the person “using” Watson and asking it to solve a problem is an inventor, as determining a problem to be solved can sometimes be the most difficult aspect of inventive activity, but not if a problem is well-known or has been previously recognized. Perhaps the person who programmed Watson is an inventor, but probably not if they were developing a program with general problem-solving capabilities and without specifically conceiving of the problem it would be applied to and the eventual solution. Considering a programmer as an inventor is also challenging in cases where a large and diverse group of people spread over time and space contribute to a program. Finally, the person who recognizes the significance of an insight might be an inventor, particularly if they have to use skill to select one of many AI outputs. But that does not seem appropriate where the importance of output is obvious and requires no further human effort.

190
Barriers to AI Inventorship
The US Congress is empowered to grant patents on the basis of the Patent and Copyright Clause of the Constitution, which enables Congress “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”28 It also provides an explicit rationale for granting patent and copyright protections, namely to encourage innovation under an incentive theory. The theory goes that people will be more inclined to invent things (i.e., promote the progress of science) if they can receive government-sanctioned monopolies (i.e., patents) to exploit commercial embodiments of their inventions. Having the exclusive right to sell an invention can be, sometimes, quite lucrative.
The Patent Act, which here simply refers to US patent law as a whole, provides at least a couple of challenges to AI qualifying as an inventor. First, as previously mentioned, the Patent Act requires that inventors be “individuals.” This language has been in place since at least the passage of legislation in 1952 that established the basic structure of modern patent law. Legislators were not thinking about AI-generated inventions in 1952. The “individual” requirement ensures that inventors, who are commonly under assignment obligations to their employers, are listed on patents. Being acknowledged as an inventor can have significant value to individuals as a moral right and can even have economic value to an individual as a signal of productivity. In addition to statutory requirements, patent law jurisprudence requires that inventions be the result of a “mental act.” So, because AI is not an individual and it is questionable whether it engages in a mental act, it is unclear
28 U.S. CONST. art. I, § 8, cl. 8.
 
191
whether an AI autonomously conceiving of a patentable invention could legally be an inventor.
Avoiding Disclosure of AI Inventors
As discussed earlier, given that AI is functioning as an inventor, and likely inventing at an increasing rate, it would seem that the Patent Office should be receiving an increasing volume of applications claiming AI as inventors. That the Patent Office apparently has not suggests that applicants are choosing not to disclose the role of AI in the inventive process. Again, this may be due to legal uncertainties about whether an AI inventor would render an invention unpatentable. Without a legal inventor, new inventions may be ineligible for patent protection and may enter the public domain after being disclosed.
There is another reason why AI inventors may not be necessary, specifically in the patent context: A person may qualify as an inventor simply by being the first individual to recognize and appreciate an existing invention. That is, to say, someone can discover rather than create an invention. Uncertainty (and accident) is often part of the inventive process. In such cases, an individual need only understand the importance of an invention to qualify as its inventor. If an AI cannot be an inventor, individuals who subsequently “discover” AI- generated inventions by mentally recognizing and appreciating their significance would likely qualify as inventors. So, it may be the case that AI-generated inventions are only patentable when an individual subsequently discovers them. However, that model does not seem appropriate when AI results are essentially self-explanatory, such that inventive activity is not needed to appreciate the inventive nature of AI output. This may be the case

192
in which an AI evaluates its own output for value, or the AI was working to generate an invention that met predetermined criteria.
Nonhuman Authors of Copyrightable Material
The Patent Act does not directly address the issue of an AI-generated invention. The Patent Office has never issued guidance addressing the subject, and there appears to be no case law on the issue of whether an AI could be an inventor. This is the case despite the fact that the Patent Office appears to have already granted patents for inventions by AI but, as previously discussed, did so unknowingly.
There is, however, guidance available from the related issue of nonhuman authorship of copyrightable works. Nonhuman authorship is not governed by statute, but there is interesting case law on the subject. Also, since at least 1973 the Copyright Office has formally conditioned copyright registration on human authorship,29 although applicants report that the Copyright Office has rejected submissions for AI-generated works as far back as 1957.30 In its 2014 compendium, the Copyright Office published an updated “Human Authorship Requirement,” which states the following:
To qualify as a work of “authorship” a work must be created by a human being. ... The Office will not register works produced by nature, animals, or plants. ... Similarly, the Office will not register works produced by a machine or mere mechanical process that operates randomly or
29 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES (FIRST) § 2.8.3 (1st ed. 1973).
30 Martin L. Klein, Syncopation in Automation, RADIO-ELECTRONICS, June 1957, at 36.
    
193
automatically without any creative input or intervention from a human
author.31
The requirement is based on jurisprudence that dates long before the invention of modern computers to the In re Trade-Mark cases in 1879, in which the US Supreme Court interprets the Patent and Copyright Clause to exclude the power to regulate trademarks. In interpreting this clause, the court states, in dicta, that the term “writings” may be construed liberally but noted that only writings that are “original, and are founded in the creative powers of the mind” may be protected.32
The issue of nonhuman authorship was implicit in the case of Burrow-Giles Lithographic Co. v. Sarony in 1884. In that case, a defendant company accused of copyright infringement argued that a famous photograph of Oscar Wilde did not qualify as a “writing” or as the work of an “author.” The company further argued that even if a visual work could be copyrighted, a photograph should not qualify for protection because it was just a mechanical reproduction of a natural phenomenon and thus could not embody the intellectual conception of its author. The court ultimately disagreed, noting that all forms of writing “by which the ideas in the mind of the author are given visible expression” are eligible for copyright protection.33 The court states that although ordinary photographs may not embody an author’s “idea,” in this particular instance, the photographer had exercised enough control over the subject matter that it qualified as an original work of art. Therefore, the case explicitly addresses whether the camera’s involvement negated human
31 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 313.2 (3d ed. 2014).
  In re Trade-Mark Cases
, 100 U.S. 82, 94 (1879).
32
33 See , 111 U.S. 53, 56 (1884).
 Burrow-Giles Lithographic Co. v. Sarony

194
authorship, and it implicitly deals with the question of whether a camera can be considered an author. Though it seems unwise to put much emphasis on nonbinding judicial statements from the Gilded Age to resolve the question of whether nonhumans can be authors, the Copyright Office still cites Burrow-Giles in support of its Human Authorship Requirement.
The Copyright Office first publicly addressed the issue of AI authors in 1966 when the Register of Copyrights, Abraham Kaminstein, questioned whether AI-generated works should be copyrightable. Kaminstein reported that, by 1965, the Copyright Office had received applications for AI-generated works including an abstract drawing, a musical composition, and compilations that were, at least partly, the work of computers. Kaminstein did not announce a policy for dealing with such applications but suggested the relevant issue should be whether an AI was merely an assisting instrument (as with the camera in Burrow-Giles) or whether an AI conceived and executed the traditional elements of authorship.
In 1974, Congress created the Commission on New Technological Uses of Copyrighted Works (CONTU) to study issues related to copyright and AI-generated works. At that time, copyright law did not even address the issue of whether computer software should be copyrightable – a far more urgent and financially important problem. With regard to AI authorship, CONTU writes in 1979 that there is no need for special treatment of AI-generated works because AI is not autonomously generating creative results without human intervention; AI is simply functioning as a tool to assist human authors. CONTU also declares that autonomously creative AI is not immediately foreseeable. The commission unanimously concludes that “works created by the use of computers should be afforded

195
copyright protection if they are original works of authorship within the Act of 1976.”34 According to the commission, “The author is [the] one who employs the computer.”35 Former CONTU Commissioner Arthur Miller explains that “CONTU did not attempt to determine whether a computer work generated with little or no human involvement is copyrightable.”36 Congress subsequently codified CONTU’s recommendations.
Nearly a decade later, in 1986, advances in AI prompted Congress’s Office of Technology Assessment (OTA) to issue a report arguing that CONTU’s approach was too simplistic and that computer programs are more than “inert tools of creation.” The OTA report contends that, in many cases, computers are at least “co-creators.” The OTA did not dispute that AI-generated works should be copyrightable, but it did foresee problems with determining authorship.
To date, there have yet to be any US cases specifically about whether copyright can subsist in an AI-generated work, but there has been one case that sought to challenge the Copyright Office’s Human Authorship Requirement. The Monkey Selfies are a series of images that a Celebes crested macaque took of itself in 2011 using equipment belonging to the nature photographer David Slater. Slater made conflicting reports about the circumstances under which the photographs were taken, but he eventually claimed that he staged the photographs by setting up a camera on a tripod and leaving a remote trigger for
   34
.
NAT’L COMM’N ON NEW TECH
TECHNOLOGICAL USES OF COPYRIGHTED WORKS
35 Id. at 45. 36
Miller, (1993).
, 106   977, 1070
USES OF COPYRIGHTED WORKS, FINAL REPORT ON NEW
 1 (1979).
  Arthur R.
Copyright Protection for Computer Programs, Databases, and
 Computer-Generated Works: Is Anything New Since CONTU?
HARV. L. REV.

196
the macaque to use. He subsequently licensed the photographs, claiming he owned their copyright. Other parties then reposted the photographs without his permission and over his objections, asserting that Slater could not copyright the images without having taken them directly. On December 22, 2014, public discourse over these events prompted the Copyright Office to specifically list the example of a photograph taken by a monkey as something not protectable under its revised and renamed Human Authorship Requirement.
In September 2015, People for the Ethical Treatment of Animals (PETA) filed a copyright infringement suit against Slater on behalf of Naruto, the monkey it claimed took the Monkey Selfies, asserting that Naruto was entitled to copyright ownership. On January 28, 2016, US District Judge William Orrick dismissed PETA’s lawsuit against Slater on the grounds that Naruto lacked standing to sue. He also deferred to the Copyright Office’s interpretation that the macaque was not an “author.” Judge Orrick considered PETA’s arguments to the contrary but ruled that animal authorship is “an issue for Congress and the President.”37 PETA appealed this decision, but the appellate court dismissed the case on the basis that Naruto did not have standing to sue. The court held that animals only have standing to sue if an act of Congress plainly states they can sue, which is not the case with the Copyright Act. In so ruling, the court avoided ruling on the substantive merits of nonhuman authorship.
 37 Naruto v. David John Slater et al., No. 16-15469 (9th Cir. 2018).

197
AI-Generated Works in the United Kingdom
While the US Copyright Office declines to provide copyright protection for AI-generated works, the United Kingdom passed a law in 1988 that took the opposite approach. UK Copyright Law makes a special provision for AI-generated works, defined as those “generated by a computer in circumstances such that there is no human author of the work[s].”38 For these works, UK law provides that “in the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangement necessary for the creation of the work are undertaken.”39
Two cases in the United Kingdom considered AI-generated works under the law in place before 1988, including Express Newspapers plc v. Liverpool Daily Post & Echo in 1985. In this case, the Daily Express newspaper had distributed cards with a five-letter code that recipients could check against a daily AI-generated newspaper grid to see if they had won a prize. The defendant newspaper was sued for copyright infringement after copying these grids, and it argued in defense that because they were AI-generated grids that they could not be protected by copyright. The judge in the case, Justice John Whitford, rejected that argument, stating: “The computer was no more than the tool by which the varying grids of five-letter sequences were produced to the instructions, via the computer programs, of [the programmer]. It is as unrealistic [to suggest the programmer was not the author] as it would be to suggest that, if you write your work with a pen, it is the pen which
38 Copyright, Designs and Patents Act, 1988 § 178. 39 Copyright, Designs and Patents Act, 1988 § 9(3).
 
198
is the author of the work rather than the person who drives the pen.”40 He further noted, “that a great deal of skill and indeed, a good deal of labour went into the production of the grid and the two separate sequences of five letters.”41
Prior to hearing the case, Justice Whitford had chaired a report in 1977 that argued the proper approach to AI-generated works was to “look on the computer as a mere tool in much the same way as a slide rule or even, in a simple sense, a paint brush. A very sophisticated tool it may be, with considerable powers to extend man’s capabilities to create new works, but a tool nevertheless.”42 It argued the AI programmer and the person who provided the data to the AI should be the authors of any resultant works.
Since the enactment of the UK’s current copyright law in 1988, only one case appears to have considered authorship of AI-generated works. In Nova Productions Ltd v. Mazooma Games Ltd, the parties were competing manufacturers of pool video games.43 The plaintiff claimed copyright in the graphics of the game and the frames that were generated by AI based on player commands. The judge in this case, Justice David Kitchin, considered the frames to be computer-generated works even though a person designed the components. He held the author of the works was the company director responsible for designing the game, rather than the player who “contributed no skill or labour of an artistic kind”.44 Even here, there was limited consideration of protection of AI-generated works
40 Express Newspapers plc v. Liverpool Daily Post & Echo [1985] FSR 306.
41 Id.
42 Whitford Committee on Copyright Designs and Performers Protection (Cmnd 6732 HMSO 1977), para 514.
43 Nova Productions Ltd v. Mazooma Games Ltd [2006] EWHC 24.
44 Nova Productions Ltd v. Mazooma Games Ltd [2006] EWHC 24 [106].
  
199
because the defendant was not contesting the existence or ownership of copyright in the frames.
AI-Generated Inventions Should Be Patentable and AI Should Qualify as a Legal Inventor
Treating AI-generated inventions as patentable and recognizing AI as an inventor would be consistent with the constitutional rationale for patent protection. It would encourage innovation under an incentive theory. Patents on AI-generated inventions would have substantial value independent of the value of AI itself. Although AI would not be motivated to invent by the prospect of a patent, it would motivate businesses and computer scientists to develop and use inventive AI. Financial incentives may be particularly important for the development of inventive AI because creating such systems may be resource-intensive. Without providing protection for AI-generated inventions, a company that needs a patent may be unable to use inventive AI in research and development even if it outperforms human researchers – and this would weaken the incentive to develop inventive AI. As patent law seeks to incentivize invention, the most valuable invention of all time to incentivize may be inventive AI in the form of artificial general intelligence and superintelligence. A hundred years according to pessimists (or longer!) is too long to wait for an AI that may go on to cure all human disease and solve climate change.
Prohibiting patents on AI-generated inventions, or allowing such patents only by permitting people who have discovered the work of AI to be inventors, is not an optimal system. In the latter case, AI may be functioning more or less independently, and it is only

200
sometimes the case that substantial insight is needed to identify and understand an AI- generated invention. Imagine that person C instructs his AI to develop an iPhone battery with twice the standard battery life and gives it some publicly available data on commercial batteries. The AI could produce results in the form of a report titled “Design for Improved iPhone Battery” – potentially even preformatted as a patent application. Indeed, there are companies like Specifio that marked “auto-generated patent applications.” It seems inefficient and unfair to reward C for recognizing the AI’s invention when C has not contributed significantly to the innovative process.
Though the impetus to develop inventive AI may still exist if AI-generated inventions are considered patentable but AI could not be inventors, the incentives would be weaker owing to the logistical, fairness, and efficiency problems such a situation would create. For instance, if C had created an improved iPhone battery as a human inventor, C would be its inventor regardless of whether anyone subsequently understood or recognized the invention. However, if C instructed C’s AI to develop an improved iPhone battery, the first person to notice and appreciate the AI’s result could become its inventor (and prevent C from being an inventor). One could imagine this creating a host of problems: The first person to recognize a patentable result may be an intern at a large research corporation or a visitor in someone’s home. A number of individuals may also concurrently recognize a result if access to an AI is widespread.
There are other benefits to patents beyond providing an up-front innovation incentive. Permitting AI inventors and patents on AI-generated inventions may also promote disclosure and commercialization. Without the ability to obtain patent protection, AI owners may choose to protect inventions as trade secrets without any public disclosure.

201
Likewise, businesses may be unable to develop inventions into commercial products without patent protection. In the pharmaceutical and biotechnology industries, for example, the majority of expense in commercializing a new product is incurred after the product is invented during the clinical testing process required to obtain regulatory approval for marketing.
Finally, permitting AI inventors may protect moral rights. Intellectual property rights are also justified, or criticized, on the basis of noneconomic rights – particularly in jurisdictions other than the United States. Moral rights protect interests ranging from owning the fruit of one’s labor under Lockean theories, to protecting an author’s or inventor’s personality on the basis of theories advanced by philosophers such as Emmanuel Kant and Georg Hegel that individuals express their “wills” and develop through their interactions with external objects. As a purely hypothetical example, someone may develop as a person through the process of writing a book and come to see the book as an extension of themselves. It could cause them injury if someone else were to take credit for their book, or if the book were altered to support violent and extremist political beliefs. Moral rights theories support the use of intellectual property rights to prevent authors and inventors from having their ideas misappropriated or altered in objectionable ways.
In the case of AI-generated works, people now appear to be taking credit for the work of machines. That is not unfair to an AI; however, it may be unfair to other people, particularly once AI-generated inventions become commonplace. If being a patent inventor comes to mean little more than putting your name on something generated by Watson, then individuals who are inventing without AI augmentation will not have their accomplishments appropriately recognized. Further, acknowledging an AI as an inventor

202
would credit the developers of that AI and allow them to be recognized for the achievements of their creations. This is much the same as teachers’ taking pride in their student’s success without taking direct credit for their future works.
Arguments against Patentability of AI-Generated Inventions and AI Inventors
Arguments in support of patentability for AI-generated invention are based mainly on the dominant narrative justifying the grant of intellectual property protection. This account, however, has been criticized, particularly by academics. Patents can result in significant social costs by establishing monopolies. Patents also can stifle entry from new ventures by creating barriers to subsequent research. Whether the benefit of patents as an innovation incentive outweighs their anticompetitive costs, or for that matter whether patents even have a net positive effect on innovation, likely varies between industries, areas of scientific research, and inventive entities. For instance, commentators such as Judge Richard Posner have argued that patents may not be needed to incentivize research and development in the software industry. Software innovation is often relatively inexpensive, incremental, quickly superseded, produced without patent incentives, protected by other forms of intellectual property, and associated with a significant first mover advantage (the initial product on the market captures users and people are reluctant to later switch platforms). Likewise, patents may be unnecessary to spur innovation in university settings where inventors are motivated to publish their results for prestige and the prospect of academic advancement.

203
AI-generated inventions may develop due to nonpatent incentives. AI developers have all sorts of noneconomic motivations to create inventive AI: for example, to enhance their reputations, satisfy scientific curiosity, or collaborate with peers. Businesses may realize significant value from AI-generated inventions even in the absence of patent protection. Of course, patents on AI-generated inventions will not be dispositive to every act of innovation; they may further encourage activities that would have otherwise occurred on a smaller scale over a longer timeframe. If patents are not needed to incentivize the development of AI, it may be justifiable to treat AI-generated inventions as unpatentable and to fail to recognize AI inventors. Yet, whether patents produce a net benefit as an empirical matter is difficult to determine in advance. Even though individuals and businesses do not always behave as rational economic actors, in the aggregate it is likely that providing additional financial incentives to spur the development of inventive AI will produce a net benefit. That, at least, is the primary theory justifying why we grant patents to people.
Patents for AI-generated inventions may also be opposed on the grounds that they would chill future human innovation, reward human inventors who fail to contribute to the inventive process, and result in further consolidation of intellectual property in the hands of big business, assuming that businesses like Alphabet and IBM will be the most likely to develop inventive AI.
Ultimately, despite legitimate concerns, allowing patents on AI-generated works and AI inventorship remain a desirable outcome. The financial motivation it will provide to build inventive AI is likely to result in a net increase in the number of patentable inventions produced. Although quantitative evidence is lacking about the effects of AI-generated

204
invention patents, courts and policymakers should be guided first and foremost by the explicit constitutional rationale for granting patents. Further, acknowledging the existence of AI inventors would do away with what is essentially a legal fiction – the idea that only a human can be the inventor of the autonomous output of an AI – thus resulting in fairer and more effective incentives.
It Does Not Matter Whether AI Thinks
The judicial doctrine that invention involves a mental act should not prevent patents on AI- generated works or AI inventorship. The Patent Act does not mention a mental act, and courts have discussed mental activity largely from the standpoint of determining when an invention is actually made and not whether it is inventive. In any case, whether AI “thinks” or has something analogous to consciousness should be irrelevant with regard to inventorship criteria.
To begin, the precise nature of a “mental act requirement” is unclear. Courts associating inventive activity with cognition have not been using terms precisely or meaningfully in the context of AI-generated inventions. It is uncertain whether AI would have to engage in a process that results in inventive output – which it does – or whether, and to what extent, AI would need to mimic human thought. If the latter, it is unclear what the purpose of such a requirement would be except to exclude nonhumans (for which a convoluted test is unnecessary). Thaler argues that a Creativity Machine closely imitates the architecture of the human brain. Should that mean that a Creativity Machine’s inventions should receive patents while Watson’s do not? Or, if a Creativity Machine does

205
not meet the threshold for engaging in mental activity, would a computer scientist have to design a completely digitized version of the human brain? There is a slippery slope in determining what constitutes a thinking AI, even leaving aside deficits in our understanding of human cognition. More importantly, even if designing a digitized version of the human brain were possible, it may not be the most effective way to structure an inventive AI.
As discussed earlier, the problem of speaking precisely about thought with regard to computers was identified by Alan Turing, who in 1950 considered the question, “Can machines think?” He finds the question to be ambiguous, and the term “think” to be unscientific in its colloquial usage. Turing decides the better question to address is whether an individual can tell the difference between responses from a computer and an individual; rather than asking whether machines think, he asks whether machines can perform in the same manner as thinking entities. Turing referred to his test as the “Imitation Game,” though it has come to be known as the Turing test.
Although the Turing test has been the subject of criticism, Turing’s analysis from more than sixty years ago demonstrates that a mental act requirement would be ambiguous, challenging to administer, and of uncertain utility. Incidentally, it is noteworthy that the Patent Office administers a sort of Turing test, which inventive AI has successfully passed. The Patent Office receives descriptions of inventions and then judges whether they are nonobvious – which is a measure of creativity and ingenuity. In the case of the Invention Machine’s patent, it was already noted that “January 25, 2005[,] looms large in the history of computer science as the day that genetic programming passed its first real Turing test: The examiner had no idea that he was looking at the intellectual property of a

206
computer.”45 In another sense, genetic programming had already passed the test by independently recreating previously patented inventions: Because the original human invention received a patent, the AI’s invention should have received a patent as well (leaving aside that the original patent would be prior art not relied upon by the AI).
The Invention Matters, Not the Inventor’s Mental Process
The primary reason a mental act requirement should not prevent AI-generated invention and AI inventorship is that the patent system should be indifferent to the means by which invention comes about. Congress came to this conclusion in 1952 when it abolished the Flash of Genius doctrine, which had been used by the federal courts as a test for patentability for more than a decade. It held that in order to be patentable, a new device, “however useful it may be, must reveal the flash of creative genius, not merely the skill of the calling.”46 The doctrine was interpreted to mean that an invention must come into the mind of an inventor in a “flash of genius” rather than as a “result of long toil and experimentation.”47 As a commentator at the time noted, “The standard of patentable invention represented by [the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.”48 The Flash of Genius test was an unhelpful doctrine because it
 45 Keats, supra note 6. 46
47 48
(1944).
, 314 U.S. 84, 91 (1941). ,383U.S.1,15n.7,16n.8(1966).
, 13 FORDHAM L. REV. 84, 87
 Cuno Engineering Corp. v. Automatic Devices Corp.
 Graham v. John Deere Co. of Kan. City
  The “Flash of Genius” Standard of Patentable Invention

207
was vague, was difficult for lower courts to interpret, involved judges making subjective decisions about a patentee’s state of mind, and made it substantially more difficult to obtain a patent. The test was part of a general hostility toward patents exhibited by mid- twentieth-century courts, a hostility that caused US Supreme Court Justice Robert Jackson to note in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.”49
Criticism of this state of affairs led President Franklin D. Roosevelt to establish a National Patent Planning Commission to study the patent system and to make recommendations for its improvement. In 1943, the commission reported with regard to the Flash of Genius doctrine that “patentability shall be determined objectively by the nature of the contribution to the advancement of the art, and not subjectively by the nature of the process by which the invention may have been accomplished.”50 Adopting this recommendation, the Patent Act of 1952 legislatively disavowed the Flash of Genius test. In the same manner, patentability of AI-generated inventions should be based on the inventiveness of an AI’s output rather than on a clumsy anthropomorphism because, like Turing, patent law should be interested in a functionalist solution.
Incidentally, even a requirement for biological intelligence may be a bad way to distinguish between AI and human inventors. Although functioning biological computers do not yet exist, all the necessary building blocks have been created. In 2013, a team of Stanford University engineers created a biological version of an electrical transistor.
49 Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
50 The “Flash of Genius” Standard of Patentable Invention, supra note 102, at 85 (internal quotation marks omitted).
  
208
Mechanical computers use numerous silicon transistors to control the flow of electrons along a circuit to create binary code. The Stanford group created a biological version with the same functionality by using enzymes to control the flow of RNA proteins along a strand of DNA. Envisioning a future in which AI can be entirely biological, there seems to be no principled reason why a biological – but not a mechanical – version of Watson should qualify as an inventor.
AI Inventors Should Be Permitted under a Dynamic Interpretation of Current Law
Whether an AI-generated work can be patented and whether AI can be an inventor in a constitutional sense is a question of first impression. If AI should be an inventor, then a dynamic interpretation of the law should allow AI inventorship. Such an approach would be consistent with the Founders’ intent in enacting the Patent and Copyright Clause, and it would interpret the Patent Act to further that purpose. Further, such an interpretation would not run afoul of the chief objection to dynamic statutory interpretation, namely that it interferes with reliance and predictability and the ability of citizens “to be able to read the statute books and know their rights and duties,”51 because a dynamic interpretation would not upset an existing policy. Permitting AI inventors would allow additional patent applications rather than retroactively invalidate previously granted patents, and there is naturally less reliance and predictability in patent law than in many other fields given that
51 See Jr. &       Statutory Interpretation as Practical ,42 321,340( ).
   William N.
Eskridge
Phillip P.
Frickey,
   Reasoning
STAN. L. REV.
1989–1990

209
it is a highly dynamic subject area that struggles to adapt to constantly changing technologies.
Other areas of patent law have been the subject of dynamic interpretation. For example, in the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was charged with deciding whether genetically modified organisms could be patented. It held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress ... would frustrate the purposes of the patent law.”52 The court noted that Congress chose expansive language to protect a broad range of patentable subject matter. Under that reasoning, AI inventorship should not be prohibited based on statutory text designed to protect the rights of human inventors. It would be particularly unwise to prohibit AI inventors on the basis of literal interpretations of texts written when AI- generated inventions were unforeseeable. If AI inventorship is to be prohibited, it should only be on the basis of sound public policy. Drawing another analogy from the copyright context, just as the terms “writings” and “authors” have been construed flexibly in interpreting the Patent and Copyright Clause, so too should the term “inventors” be afforded the flexibility needed to effectuate constitutional purposes.
3 Implications of AI-Generated Inventions
In the event that AI-generated inventions are patentable, there still remains the question of who would own these patents. An AI should not be able to own a patent, not only because AI cannot legally own property, but because it would be a bad idea. The AI would not be
52 Diamond v. Chakrabarty, 447 U.S. 303, 315 (1980).
  
210
motivated by a patent, the AI would not obtain any direct benefit from ownership, and the AI would not be able to effectively enforce a patent. This presents a number of obvious options for patent ownership such as the AI’s owner, developer, or user. The developer, user, and owner may be the same person or persons, or they may be different entities.
Ownership rights to AI-generated inventions should vest in an AI’s owner because it would be most consistent with the way personal property (including both AI and patents) is treated. If you own a machine that produces property then you own that property - whether a loaf of bread or valuable data which you choose to protect as a trade secret (leaving aside more complex cases in which multiple parties are involved or you did not have rights to the machine’s input [whether baking soda or another party’s data]). AI owner patent ownership could be taken as a starting point, although parties should be able to contract around this default, and as AI-generated inventions become more common, negotiations over these inventions may become a standard part of contract negotiations. So long as property entitlements are clearly allocated, these parties can ultimately work out the most efficient allocation of rights between themselves. However, a default ownership rule is still necessary to minimize overall transaction costs.
A default ownership rule in favor of an AI’s owner may further encourage innovation. Consider that IBM has made Watson available to numerous developers without transferring Watson’s ownership. To the extent that Watson creates patentable results as a product of its interactions with users, promoting user access should result in more innovation. There is theoretically no limit to the number of users that Watson, as a software program that can be copied, could interact with at once. If Watson invents while under the control of a non-IBM user and the “default rule” assigns the invention to the user,

211
IBM may be encouraged to restrict user access; in contrast, assigning the invention to IBM would be expected to motivate IBM to further promote access. If IBM and a user were negotiating for a license to Watson, the default rule may result in a user paying IBM an additional fee for the ability to patent results or receiving a discount by sticking with the default. It may also be that Watson coinvents along with a user in which case a system of default assignment to an AI’s owner would result in both IBM and the user co-owning the resulting patent. Where inventive AI is not owned by large enterprises with sophisticated attorneys, it is more likely the default rule will govern the outcome.
Likewise, patent ownership rights should vest in an AI’s owner rather than its developer. Owner assignment would provide a direct economic incentive for developers in the form of increased consumer demand for inventive AI. Having assignment default to developers would interfere with the transfer of personal property in the form of AI, and it would be logistically challenging for developers to monitor AI-generated inventions made by machines they no longer own.
In some instances, however, owner assignment of intellectual property rights may produce unfair results. In the movie Her, the protagonist (who is a writer) purchases an AI named Samantha that organizes his existing writings into a book, which the AI then submits to be published. It is possible that Samantha would, if a person, own the copyright in the selection and arrangement of his writings and would thus have a copyright interest in the book. Here, AI owner assignment of intellectual property rights seems unappealing if there is a minimal role played by the consumer/owner. The consumer’s role in the process may be limited to simply purchasing an inventive AI and asking it to do something (where the owner is the user) or purchasing an AI and then licensing it to someone else to use

212
creatively. Further, assigning AI-generated inventions to owners may impede the transfer of inventive AI because the AI developers may want to retain the rights to subsequent AI- generated inventions.
These problems are more easily resolved than problems associated with assigning intellectual property rights to developers by default. Developers could either require owners to pay them the value of an inventive AI, taking into account the likelihood of an AI inventing, or avoid the problem by licensing rather than selling inventive AI. In the case of licensing, the developer remains the owner, and the consumer is simply a user. One could imagine an inventive AI, such as the AI in Her, coming with a license agreement under which consumers prospectively assign any inventions made by the system to the licensor.
This analysis also reveals an important reason why AI-generated invention works best when the AI is the legal inventor. If AI-generated inventions were treated as patentable but AI could not be an inventor, then the first person to recognize an AI- generated invention may be the most likely legal inventor and patent owner. This means that the AI’s user, rather than its developer or owner, would likely be the patentee as the person in a position to first recognize an AI-generated invention. To the extent this is an undesirable outcome, then a solution is to permit AI inventorship. In sum, assigning an AI’s invention by default to its owner seems the preferred outcome, and AI owners would still be free to negotiate alternate arrangements with developers and users by contract.
Coexistence and Competition
IBM has bragged to the media that Watson’s question- answering skills are good for more than annoying Alex Trebek. The company sees a future in which fields like medical

213
diagnosis, business analytics, and tech support are automated by question-answering software like Watson. Just as factory jobs were eliminated in the 20th century by new assembly-line robots, [Watson’s Jeopardy competitors] were the first knowledge-industry workers put out of work by the new generation of “thinking” machines. “Quiz show contestant” may be the first job made redundant by Watson, but I’m sure it won’t be the last.53
With the expansion of AI into creative domains previously occupied only by people, AI threatens to displace human inventors. Consider the following hypothetical example involving the field of antibody therapy. Antibodies are small proteins made naturally by the immune system, primarily to identify and neutralize pathogens such as bacteria and viruses. They are Y-shaped proteins that are largely similar to one another in structure, although antibodies contain an extremely variable region which binds to target structures. Differences in that region are the reason different antibodies bind to different targets (e.g., the reason why one antibody binds to a cancer cell while another binds to a common cold virus). The body generates antibody diversity in part by harnessing the power of random gene recombinations and mutations (much as genetic programming does), and then it selects for antibodies with a desired binding (much as genetic programming does). Following the discovery of antibody structure and the development of technologies to manufacture antibodies in the 1970s, human researchers began to create antibodies for diagnostic and therapeutic purposes. Therapeutic antibodies can block cell functions, modulate signal pathways, and target cancer cells among other functions. There are now many artificially manufactured antibodies approved to treat a variety of medical conditions. All the major biological (made from a living organism or its products)
53 Ken Jennings, My Puny Human Brain, SLATE, Feb. 16, 2011, http://primary.slate.com/articles/arts/culturebox/2011/02/my_puny_human_brain.html.
    
214
“blockbuster” drugs, which is to say drugs that earn in excess of a billion dollars a year, are antibodies.
One of the interesting things about antibodies from an AI-generated invention perspective is that a finite number of antibodies exist. There are, at least, billions of possible antibodies, which is enough natural diversity for the human immune system to function and to keep human researchers active for the foreseeable future. Even so, there are only so many possible combinations of amino acids (the building blocks of proteins) that the body can string together to generate an antibody. It is not hard to imagine that, with enough computing power, an AI could sequence every possible antibody that could ever be created. Even if that were trillions of antibodies, the task may be relatively simple for a powerful enough computer but impossible for even the largest team of human researchers without AI assistance.
Generating the entire universe of antibody sequences would not reveal all the possible functions of those antibodies, so an AI’s owner could not obtain patents for all the sequences on this basis alone because the utility of an invention must be disclosed in addition to the sequence itself. The AI might, however, prevent any future patents on the structure of new antibodies (assuming the sequence data are considered an anticipatory disclosure). If this occurred, an AI would have preempted human invention in an entire scientific field.
Modern AI systems are also able to model and predict antibody binding – the likelihood that a certain antibody will bind to a particular epitope (e.g., a receptor on a cancer cell). The day may not be far away when an AI will predict the binding qualities of every conceivable antibody, and thus know what antibodies would most effectively treat

215
every condition for which an antibody would be an appropriate treatment. This could hypothetically cure all human cancers.
Automation Will Refocus Human Activity
In the hypothetical scenario above, society would gain access to all possible future knowledge about antibody structure at once rather than waiting decades or centuries for individuals to discover these sequences. Early access to antibody sequences could prove a tremendous boon to public health if it led to the discovery of new drugs. Some antibody sequences may never be identified without AI.
At least in the short term, AI inventors should refocus rather than inhibit human inventive activity. Scientists who are working on developing new antibody structures may shift to studying how the new antibodies work, or finding new medical applications for those antibodies, or perhaps may move on to studying more complex proteins beyond the capability of AI to comprehensively sequence. Unless and until AI starts broadly outperforming human researchers, there will be plenty of room for people to invent – all with net gains to innovation.
Antibody therapies are just one example of how AI could preempt invention in a field. A sophisticated enough AI could do something similar in the field of genetic engineering by creating random sequences of DNA. Living organisms are a great deal more complex than antibodies, but the same fundamental principles apply. A powerful enough AI could model quintillions of different DNA sequences, inventing new life-forms in the process. In fact, on a smaller scale, this is something genetic programming already does.

216
Although results have been limited by the computationally intense nature of the process, this will change as AI continues to improve. By creating novel DNA sequences, genetic programming would be performing the same function as nondigital genetic programming – natural evolution!
Dealing with Industry Consolidation
It will probably be the case that inventive AI results in greater consolidation of intellectual property in the hands of large corporations. Such businesses may be the most likely to own sophisticated inventive AI owing to its generally resource-intense development. As previously discussed, the benefits, however, may outweigh the costs of such an outcome. Imagine that Watson is the hypothetical AI that sequences every conceivable antibody and, further, that Watson can analyze a human cancer and match it with an antibody from its library to effectively treat the cancer. Essentially, this could allow IBM to patent the cure for cancer.
Although this would be profoundly disruptive to the medical industry and may lead to market abuses, it is not a reason to bar AI-generated invention. Society would obtain the cure for cancer, and IBM would obtain a twenty-year monopoly (the term of a patent) in return for publicly disclosing the information a competitor would need to duplicate Watson’s invention. In the absence of inventive AI, such a breakthrough may never come about. And to the extent that price gouging and supply shortages are a concern, protections are built into patent law to protect consumers against such problems. For example, the

217
government could issue compulsory licenses, which can grant competitors the right to practice an invention by paying a royalty.
Further Thoughts
AI is already generating creative and inventive output, and it is important that we have a legal framework that continues to incentivize the generation of intellectual property in a world where AI is generating the equivalent of products of human ingenuity. Having appropriate frameworks will be increasingly important as AI continues to improve and generate more intellectual value. The principle of AI legal neutrality suggests that new creative and inventive works are no less worthy of protection than those made by people, and that allowing intellectual property rights will result in broad social gains for people. Also, that acknowledging AI as an inventor when it functionally invents will maintain the value and meaning of human invention. Such a framework will do more than address an academic concern; it will provide certainty to businesses, provide fairness to research, and promote the progress of science. In the words of Thomas Jefferson, “Ingenuity should receive a liberal encouragement.”54 What could be more ingenious than artificial inventors?
54 Diamond v. Chakrabarty, 447 U.S. at 308 (quoting 5 WRITINGS OF THOMAS JEFFERSON 75–76 (H. Washington ed. 1871)). “In choosing such expansive terms [for the language of Section 101] ... modified by the comprehensive ‘any,’ Congress plainly contemplated that the patent laws would be given wide scope ... .” Id.
 
218
5
Everything Is Obvious
Prediction is very difficult, especially about the future.
- Niels Bohr
Chapter 4 focused on today’s AI-generated invention. This chapter considers a related phenomenon: What happens when tomorrow’s inventive AI becomes a standard part of research and development?
The impact of the widespread use of inventive AI will be tremendous, not just on innovation but also on patent law. Right now, patentability is determined based on what a hypothetical, noninventive skilled person would find obvious. The skilled person, much like the reasonable person standard in tort law, represents the average worker in the scientific field of an invention. An applicant can only receive a patent if an invention would not be obvious to this skilled person. Once the average worker uses inventive AI, or inventive AI replaces the average worker, then average workers will become inventive. That will create a new challenge, because inventive activity is not supposed to be normal – the problem is this would allow patents on inventions that do not require special incentives. The principle of AI legal neutrality suggests that once inventive AI becomes the standard means of research in a field, the skilled person should be an inventive AI. This should raise the bar to patentability because inventive AI will more easily find inventions obvious.
 
219
Similar to the challenge of determining when a reasonable robot acts “unreasonably,” replacing the skilled person standard with AI requires us to determine what an inventive AI would find “obvious.” This may be difficult to reason through in the same way we do currently with the standard of an average researcher. A solution is to change the obviousness inquiry to focus more on economic than cognitive factors. An existing vein of critical scholarship has already advocated for such a shift through, for example, greater reliance on real-world evidence of how an invention is received in the marketplace, long-felt but unsolved needs, and the failure of others. Such an approach could avoid some of the difficulties inherent in applying a “cognitive” inventive AI standard. To date, an economic standard has not been implemented due to practical challenges, but the widespread use of inventive AI may provide the needed impetus.
Alternately, application of an inventive AI standard could focus on reproducibility. With the skilled person standard, decision makers, in hindsight, need to reason about what another person would have found obvious. This results in inconsistent and unpredictable nonobviousness determinations. In practice, the skilled person standard bears unfortunate similarities to Justice Potter Stewart’s famously unworkable definition of obscene material – “I know it when I see it.” By contrast, whether AI could reproduce the subject matter of a patent application could be far more objective. A more determinate test would allow the Patent Office to apply a single standard consistently, and it would result in fewer judicially invalidated patents.
In whatever way the test is applied, an inventive AI standard will dynamically raise the current benchmark for patentability. Inventive AI will be significantly more intelligent than skilled persons, and also capable of considering more prior art. An inventive AI

220
standard would not prohibit patents, but it would make obtaining them substantially more difficult. Either a person or an AI would need to have an unusual insight that inventive AI could not easily re-create, developers would need to create increasingly intelligent AI that could outperform standard AI, or, most likely, invention would be dependent on using specialized, nonpublic sources of data. The nonobviousness bar will continue to rise as AI inevitably becomes increasingly sophisticated. If we take this to its logical extreme – and given there is no limit to how intelligent AI may become – every invention may one day be obvious to commonly used AI. That would mean no more patents would be issued without some radical change to current patentability criteria.
This chapter has three sections. Section 1 considers the current test for obviousness and its historical evolution. It finds that obviousness is evaluated through the lens of the skilled person, who reflects the characteristics of the average worker in a field. Section 2 considers the use of AI in research and development and proposes a novel framework for conceptualizing the transition from human to AI inventors. Already, inventive AI is competing with human inventors, and human inventors are augmenting their abilities with inventive AI. Section 3 then proposes a framework for implementing the proposed standard. A decision maker would need to (1) determine the extent to which inventive AI is used in a field, (2) characterize the inventive AI that best represents the average worker if inventive AI is the standard, and (3) determine whether the AI would find an invention obvious. Finally, Section 3 provides an example of how the proposed obviousness standard would work in practice. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function as innovation incentives. To the extent patents accomplish other goals such as

221
promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
1 Obviousness
Patents are granted for inventions that are new, nonobvious, and useful. Of these three criteria, obviousness is the primary hurdle for most patent applications. Patents are not intended to be granted for incremental inventions. Only those inventions that represent a significant advance over existing technology should receive protection. The reason for this is that patents have significant costs. They limit competition and can inhibit future innovation by restricting the use of patented technologies in research and development. To the extent that patents are justified, it is because they are thought to have more benefits than costs. Patents function as innovation incentives, and they can promote the dissemination of information, encourage commercialization of technology, and validate moral rights. Although other patentability criteria contribute to this function, the nonobviousness requirement is the primary test for distinguishing between significant innovations and trivial advances. Of course, it is one thing to express a desire to only protect meaningful scientific advances and another to come up with a workable rule that applies across every area of technology.
Early Attempts
The modern obviousness standard has been the culmination of hundreds of years of struggle by the Patent Office, courts, and Congress to separate the wheat from the chaff. As

222
Thomas Jefferson, the first administrator of the patent system and one of its chief architects, writes, “I know well the difficulty of drawing a line between the things which are worth to the public the embarrassment of an exclusive patent, and those which are not. ... I saw with what slow progress a system of general rules could be matured.”1
The earliest patent laws focused on novelty and utility, although Jefferson did at one point suggest an “obviousness” requirement. The Patent Act of 1790 was the first patent statute, and it required patentable inventions to be “sufficiently useful and important.” Three years later, a more comprehensive patent law was passed – the Patent Act of 1793. The new act did not require an invention to be “important” but required it to be “new and useful.” The 1836 Patent Act reinstated the requirement that an invention be “sufficiently used and important.”
In 1851, the Supreme Court adopted the progenitor of the skilled person and the obviousness test – an “invention” standard. Hotchkiss v. Greenwood concerned a patent for substituting clay or porcelain for a known doorknob material such as metal or wood. The court invalidated the patent, holding that “the improvement is the work of a skillful mechanic, not that of the inventor.”2 The court also articulated a new legal standard for patentability: “Unless more ingenuity and skill ... were required ... than were possessed by an ordinary mechanic acquainted with the business, there was an absence of that degree of skill and ingenuity which constitute essential elements of every invention.”3
1 VI WRITINGS OF THOMAS JEFFERSON, LETTER TO ISAAC MCPHERSON (Washington ed. 1813) [hereinafter LETTER TO ISAAC MCPHERSON], at 180–181.
2 Hotchkiss v. Greenwood, 52 U.S. (11 How.) 248, 267 (1850).
3 Id.
  
223
However, the court did not give specific guidance on what makes something inventive or the required level of inventiveness. In subsequent years, the court made several efforts to address these deficiencies, but with limited success. As the court stated in 1891, “The truth is the word [invention] cannot be defined in such manner as to afford any substantial aid in determining whether any particular device involves an exercise of inventive faculty or not.”4 Or as one commentator noted, “It was almost impossible for one to say with any degree of certainty that a particular patent was indeed valid.”5
Around 1930, the Supreme Court, possibly influenced by a national antimonopoly sentiment, began implementing stricter criteria for determining the level of invention. This culminated in 1941 with the widely disparaged “Flash of Genius” test discussed in Chapter 4 and articulated in Cuno Engineering v. Automatic Devices Corp., namely that in order to receive a patent “the new device must reveal the flash of creative genius, not merely the skill of the calling.”6 Extensive criticism of perceived judicial hostility toward patents resulted in President Roosevelt’s creating a National Patent Planning Commission to make recommendations for improving the patent system. The Commission’s report in 1943 recommended that Congress adopt a more objective and certain standard of obviousness. About a decade later, Congress did.
   McClain v. Ortmayer
4
5 Gay Chin,
6
, 141 U.S. 419, 427 (1891). 317, 318 (1959).
, 3 PAT.
 The Statutory Standard of Invention: Section 103 of the 1952 Patent Act
 TRADEMARK & COPY. J. RES. & ED.
 Cuno Engineering Corp. v. Automatic Devices Corp.
, 314 U.S. 84, 84 (1941).

224
The Nonobviousness Inquiry
The Patent Act of 1952 established the modern patentability framework. Among other changes to substantive patent law, “The central thrust of the 1952 Act removed ‘unmeasurable’ inquiries into ‘inventiveness’ and instead supplied the nonobviousness requirement of Section 103.”7 Section 103 states:
A patent may not be obtained ... if the difference between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains. Patentability shall not be negatived by the manner in which the invention was made.8
Section 103 legislatively disavowed the Flash of Genius test, codified the sprawling judicial doctrine on “invention” into a single statutory test, and restructured the standard of obviousness in relation to a person having ordinary skill in the art. However, while Section 103 may be more objective and definite than the earlier standard, the meanings of “obvious” and “a person having ordinary skill” were not defined and in practice proved “often difficult to apply.”9
The Supreme Court first interpreted the statutory nonobviousness requirement in a trilogy of cases: Graham v. John Deere (1966) and its companion cases, Calmar v. Cook
7 CLS Bank Int’l v. Alice Corp. Pty. Ltd., 717 F.3d 1269, 1295 (Fed. Cir. 2013).
8 35 U.S.C. § 103 (2006).
9 Uniroyal Inc. v. Rudkin-Wiley Corp., 837 F.2d 1044, 1050, 5 U.S.P.Q. (BNA) 1434, 1438 (Fed. Cir. 1988).
   
225
Chemical (1965) and United States v. Adams (1966). In these cases, the court articulated a framework for evaluating obviousness as a question of law based on the following underlying factual inquiries: (1) the scope and content of the prior art, (2) the level of ordinary skill in the prior art, (3) the differences between the claimed invention and the prior art, and (4) objective evidence of nonobviousness. This framework remains applicable today. Of note, the Graham analysis does not explain how to evaluate the ultimate legal question of nonobviousness beyond identifying underlying factual considerations.
In 1984, the newly established US Court of Appeals for the Federal Circuit, the only appellate-level court with jurisdiction to hear patent case appeals, devised the “teaching, suggestion and motivation” (TSM) test for obviousness. Strictly applied, this test only permits an obviousness rejection when prior art explicitly teaches, suggests, or motivates a combination of existing elements into an invention. The TSM test protects against hindsight bias because it requires an objective finding in the prior art. In retrospect, it is easy for an invention to appear obvious by piecing together bits of prior art using a patent application as a blueprint.
In KSR v. Teleflex (2006), the Supreme Court upheld the Graham analysis but rejected the Federal Circuit’s exclusive reliance on the TSM test. The court instead endorsed a flexible approach to obviousness in light of “the diversity of inventive pursuits and of modern technology.”10 Rather than approving of a single definitive test, the court identified a nonexhaustive list of rationales to support a finding of obviousness. This remains the approach to obviousness today.
10 KSR International Co. v. Teleflex Inc., 550 U.S. 398, 418 (2007).
  
226
Finding PHOSITA
Determining the level of ordinary skill is critical to assessing obviousness. The more sophisticated the skilled person, the more likely an invention is to appear obvious. Thus, it matters a great deal whether the skilled person is a moron in a hurry or the combined masters of an invention’s scientific field.
The skilled person has never been precisely defined, although there is judicial guidance. In KSR, the Supreme Court described the skilled person as “a person of ordinary creativity, not an automaton.” The Federal Circuit has explained the skilled person is a hypothetical person, like the reasonable person in tort law, who is presumed to have known the relevant art at the time of the invention. The skilled person is not a judge, amateur, person skilled in remote arts, or a set of “geniuses in the art at hand.” The skilled person is “one who thinks along the line of conventional wisdom in the art and is not one who undertakes to innovate.”11
The Federal Circuit has provided a list of nonexhaustive factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the
field.” In any case, one or more factors may predominate, and not every factor may be relevant. The skilled person standard thus varies according to the invention in question, its field of art, and researchers in the field. In the case of a simple invention in a field where most innovation is created by laypersons, for instance, a device to keep flies away from
11 Standard Oil Co. v. American Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985).
  
227
horses, the skilled person may be someone with little education or practical experience. By contrast, where an invention is in a complex field with highly educated workers such as chemical engineering or pharmaceutical research, the skilled person may be quite sophisticated.
Analogous Prior Art
Determining what constitutes prior art is also central to the obviousness inquiry. On some level, virtually all inventions involve a combination of known elements. The more prior art can be considered, the more likely an invention is to appear obvious. To be considered for the purposes of obviousness, prior art must fall within the definition for anticipatory references under Section 102 of the Patent Act of 1952 and must additionally qualify as “analogous art.”12
Section 102 contains the requirement for novelty in an invention, and it explicitly defines prior art. An extraordinarily broad amount of information qualifies as prior art, including any printed publication made available to the public prior to filing a patent application. Courts have long held that inventors are charged with constructive knowledge of all prior art. While no real inventor could have such knowledge, the social benefits of this rule are thought to outweigh its costs. Granting patents on existing inventions could prevent the public from using something it already had access to and could remove knowledge from the public domain.
12 In re Bigio, 381 F.3d 1320, 1325 (Fed. Cir. 2004).
 
228
For the purposes of obviousness, prior art under Section 102 must also qualify as analogous. That is, to say, the prior art must be in the field of an applicant’s endeavor, or reasonably pertinent to the problem with which the applicant was concerned. A real inventor would be expected to focus on this type of information. The “analogous art” rule better reflects practical conditions, and it ameliorates the harshness of the definition of prior art for novelty given that prior art references may be combined for purposes of obviousness but not novelty. Consequently, for the purposes of obviousness, the skilled person is presumed to have knowledge of all prior art within the field of an invention as well as prior art reasonably pertinent to the problem the invention solves. Restricting the universe of prior art to analogous art lowers the bar to patentability.
The analogous art requirement is most famously conceptualized in the case of In re Winslow, in which the court explains that a decision maker is to “picture the inventor as working in his shop with the prior art references, which he is presumed to know, hanging on the walls around him.”13 Or, as Judge Learned Hand remarks, “the inventor must accept the position of a mythically omniscient worker in his chosen field. As the arts proliferate with prodigious fecundity, his lot is an increasingly hard one.”14
  13 14
, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
, 185 F.2d 350 (2d Cir. 1950).
In re Winslow
 Merit Mfg. Co. v. Hero Mfg. Co.

229
2 AI in the Future of Invention
Timeline to the Inventive Singularity
We are amid a transition from human to AI inventors. The following five-phase framework illustrates this transition and divides the history and future of inventive AI into several stages.
 Begin Table
Evolution of AI-Generated Invention
 Phase I
II
III
IV V
Inventors Human Human > SAI
Human ~ SAI
SAI ~ AGI > Human ASI
Skilled Standard
Person
Augmented Person
Augmented Person ~ SAI
Augmented AGI ASI
Time Frame Past Present
Short Term
Medium Term Long Term
 SAI = Specific Artificial Intelligence; AGI = Artificial General Intelligence; ASI = Artificial Superintelligence; ~ = competing; > = outcompeting
End Table
Until relatively recently, all invention was created by people. If a company wanted to solve an industrial problem, it asked a research scientist, or a team of research scientists, to solve the problem. This is no longer the only option. In some industries, and for some problems, AI can autonomously solve problems. In 2006, for instance, NASA recruited an autonomously inventive AI to design an antenna that flew on NASA’s Space Technology 5 (ST5) mission.
 
230
Phase I ended when the first patent was granted for an invention created by an inventive AI – perhaps 1998 when a patent was issued for an invention autonomously developed by a Creativity Machine. It may be difficult to determine precisely when the first patent was issued for an AI-generated invention as there is no obligation to report the role of AI in patent applications.
In the present, Phase II, AI and people are competing and cooperating at inventive activity. However, in all technological fields, human researchers are the norm and thus best represent the skilled person standard. While AI systems are inventing, it is unclear to what extent this is occurring. Inventive AI owners may not be disclosing the extent of such AI in the inventive process due to concerns about patent eligibility or because companies generally restrict information about their organizational methods to maintain a competitive advantage. This phase will reward early adopters of inventive AI that is able to outperform human inventors at solving specific problems, and the output of which can exceed the skilled person standard.
While there may now only be a modest amount of autonomous AI-generated invention, human inventors are being widely augmented by inventive AI. For example, a person may design a new engine using AI to perform calculations, search for information, or run simulations on new designs. The AI does not meet inventorship criteria, but it does augment the capabilities of a researcher in the same way that human assistants can help reduce an invention to practice. Depending on the industry researchers work in and the problems they are trying to solve, researchers may rarely be unaided by AI. The more sophisticated the AI, the more it is able to augment the worker’s skills. AI may be a particular benefit in areas such as discoveries that require the use of tremendous amounts

231
of data or that deviate from conventional design wisdom. Along these lines, the company Iprova is now using AI to augment researchers to help them invent “more diversely” by providing them with prior art they would not otherwise consider.
Phase III, in the near future, will involve increased competition and cooperation between people and AI. In certain industries, and for certain problems, inventive AI will become the norm. For example, in the pharmaceutical industry, Watson is now identifying novel drug targets and new indications for existing drugs. Soon, it may be the case that inventive AI is the primary means by which new uses for existing drugs are researched. This is a predictable outcome, given the advantage AI has over people at recognizing patterns in very large datasets. However, it may be that people still perform the majority of research related to new drug targets. Where the standard varies within a broad field like drug discovery, the variation can be addressed by defining fields and problems narrowly, for instance, according to the subclasses currently used by the Patent Office.
Perhaps twenty-five years from now – based on expert opinion – the introduction of AGI will usher in Phase IV. As discussed in Chapter 1, existing, “narrow” or specific AI systems focus on discrete problems or work in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms, and Chef Watson cannot design a self-driving car. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve. By contrast, AGI would be able to successfully perform any intellectual task a person could. AGI will compete with human inventors in every field, which makes AGI a natural
 
232
substitute for the skilled person. Even with this new standard, human inventors may continue to invent – just not as much. An inventor may be a creative genius whose abilities exceed the human average, or a person of ordinary intelligence who has a groundbreaking insight.
Just as specific AI outperforms people in certain fields, it will likely be the case that specific AI outperforms AGI in certain circumstances. An example of this could be when screening a million compounds for pesticide function lends itself to a brute-force computational approach. For this reason, specific AI could continue to represent the level of ordinary skill in fields in which specific AI is the standard while AGI could replace the skilled person in all other fields. However, the two systems will likely be compatible. A general AI system wanting to play Go could incorporate AlphaGo into its own programming, design its own algorithm like AlphaGo, or even instruct a second AI operating AlphaGo.
AGI will change the human-AI dynamic in another way. If the AI is genuinely capable of performing any intellectual task a person could, the AI would be capable of setting goals collaboratively with a person, or even by itself. Instead of a person instructing an AI to screen a million compounds for pesticide function, a person could merely ask an AI to develop a new pesticide. For that matter, an agrochemical company like Bayer could instruct DeepMind’s AI to develop any new technology for its business, or just to improve its profitability. Such AI should be able to not only solve known problems but also unknown problems.
Ultimately, AGI could be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence – what philosopher Nick

233
Bostrom has termed artificial superintelligence. Such an outcome has been referred to as the intelligence explosion or the technological singularity. Artificial superintelligence could then innovate in all areas of technology, resulting in progress at an incomprehensible rate. As the mathematician Irving John Good wrote in 1965, “The first ultraintelligent machine is the last invention that man need ever make.”15 Ultimately, in Phase V, when AGI succeeds in developing artificial superintelligence, it will mean the end of obviousness. Everything will be obvious to a sufficiently intelligent AI.
Inventive and Skilled AI
For purposes of patent law, an inventive AI should be one that generates patentable output while meeting traditional inventorship criteria. Under the present framework, inventive AI would not be the equivalent of skilled AI, just as human inventors are not skilled persons. In fact, it should not be possible to extrapolate about the characteristics of a skilled entity from information about inventive AI. Granted, the Federal Circuit once included the “educational level of the inventor” in its early factor-based test for the skilled person. However, this was only until it occurred to the Federal Circuit that “courts never have judged patentability by what the real inventor/applicant/patentee could or would do. Real inventors, as a class, vary in the capacities from ignorant geniuses to Nobel laureates; the courts have always applied a standard based on an imaginary work of their own devising whom they have equated with the inventor.”16
    15 16
Good, Speculations Concerning the First Ultraintelligent Machine, 6 ADVANCES 31, 33 (1965).
, 745 F.2d 1437, 1454 (Fed. Cir. 1984).
Irving John
 IN COMPUTERS
 Kimberly-Clark Corp. v. Johnson & Johnson

234
What then conceptually is a skilled AI? An AI that anthropomorphizes to the various descriptions that courts have given for the skilled person? Such a test could focus on the way an AI is designed or how it functions. For instance, a skilled AI could be a conventional AI as opposed to an AI like DeepMind’s that functions unpredictably. However, basing a rule on how an AI functions may not work for the same reason the Flash of Genius test failed. Even leaving aside the significant logistical problem of attempting to figure out how an AI is structured or how it generates particular output, patent law should be concerned with whether an AI is generating inventive output, not what is going on inside the AI. If a conventional AI and a neural network were both able to generate the same inventive output, there would be no reason to favor one over the other.
Alternately, the test could focus on an AI’s capacity for creativity. For example, Microsoft Excel plays a role in a significant amount of inventive activity, but it is not innovative. It applies a known body of knowledge to solve problems with known solutions in a predictable fashion (e.g., multiplying values together). However, while Excel may sometimes solve problems that a person could not easily solve without the use of technology, it lacks the ability to engage in almost any inventive activity. Excel is not the equivalent of a skilled AI – it is an automaton incapable of ordinary creativity.
Watson in clinical practice may be a better analogy for a skilled worker. Watson is analyzing a patient’s genome and providing treatment recommendations. Yet as with Excel, this activity is not innovative. The problem Watson is solving may be more complex than multiplying a series of numbers, but it has a known solution. Watson is identifying known genetic mutations from a patient’s genome. Watson is then suggesting known treatments based on existing medical literature. Watson is not innovating because it is being applied to

235
solve problems with known solutions, adhering to conventional wisdom. Unlike Excel, however, Watson can be inventive. For instance, Watson could be given unpublished clinical data on patent genetics and actual drug responses and then could be tasked with determining whether a drug works for a genetic mutation in a way that has not yet been recognized. Traditionally, such discoveries have been patentable. Watson may be situationally inventive depending on the problem it is solving.
It may be difficult to identify an actual AI that has a “skilled” level of creativity. To the extent an AI is creative, in the right circumstances any degree of creativity could result in inventive output. To be sure, this is similar to the skilled person. A person of ordinary skill, or almost anyone, may have an inventive insight. Characteristics can be imputed to a skilled person, but it is not possible the way the test is applied to identify an actual skilled person or to definitively say what she would have found obvious. The skilled person test is simply a theoretical device for a decision maker.
Assuming a useful characterization of a skilled AI, to determine that a skilled AI represents the average worker in a field, decision makers would need information about the extent to which such an AI is used. Obtaining this information may not be practical. Patent applicants could be asked generally about the use and prevalence of AI in their fields, but it would be unreasonable to expect applicants to already have, or to obtain, accurate information about general industry conditions. The Patent Office, or another government agency, could attempt to proactively research the use of AI in different fields, but this may be costly. The Patent Office lacks expertise is this activity, and its findings would inevitably lag rapidly changing conditions. Ultimately, there may not be a reliable, low-cost source of information about skilled AI.

236
Inventive Is the New Skilled
Having inventive AI replace the skilled person may better correspond with real-world conditions. There are inherent limits to the number and capabilities of human workers. The cost to train and recruit new researchers is significant, and there are a limited number of people with the ability to perform this work. By contrast, inventive AI is likely to be a software program that may be almost costless to copy. Once Watson outperforms the average industry researcher, IBM may be able to simply copy Watson and then have it replace most of an existing workforce. Copies of Watson could replace individual workers, or a single Watson could do the work of a large team of researchers.
Indeed, as mentioned earlier in a noninventive setting, IBM reports that Watson can interpret a patient’s entire genome and prepare a clinically actionable report in
ten minutes compared to a team of human experts who would need around 160 hours. Once Watson is proven to produce better patient outcomes than a human team, it may be unethical to have people underperform a task that Watson can automate. When that occurs, Watson should not only replace the human team at its current facility but also every comparable human team. Watson could similarly automate in an inventive capacity.
Thus, inventive AI will change the skilled paradigm because once they become the average worker, the average worker becomes inventive. This should then raise the bar for obviousness, so that AI will no longer qualify as inventive. At this point, such AI may be skilled AI – AI that represents the average worker and are no longer capable of routine invention.

237
Regardless of the terminology, as AI continues to improve, this will continue to raise the nonobviousness bar. To generate patentable output, it may be necessary to use an advanced AI that can outperform a standard AI, or a person or AI will need to have an unusual insight that a standard AI cannot easily re-create. Inventiveness may also depend on the data supplied to an AI, such that only certain data would result in inventive output. If taken to its logical extreme – and given there is no limit to how intelligent AI may become – every invention may one day be obvious to commonly used AI.
It is possible to generate reasonably low-cost and accurate information about the use of inventive AI. The Patent Office should institute a requirement for patent applicants to disclose the role of AI in the inventive process. This disclosure could be structured along the lines of current inventorship disclosure. Right now, there is an obligation on applicants to disclose all patent inventors. Failure to do so can invalidate a patent or render it unenforceable. Similarly, applicants should have to disclose when an AI autonomously meets inventorship criteria.
These disclosures would only apply to an individual invention. However, the Patent Office could aggregate responses to see whether most inventors in a field (e.g., a class or subclass) are human or AI. These disclosures would have a minimal burden on applicants compared to existing disclosure requirements and the numerous procedural requirements of a patent application. In addition to helping the Patent Office with determinations of nonobviousness, these disclosures would provide valuable information for purposes of attributing inventorship. It may also be used to develop appropriate innovation policies in other areas.

238
Skilled People Use AI
The current standard neglects to take into account the modern importance of AI in innovation. Instead of replacing the skilled person with the skilled AI, it would be less of a conceptual change, and administratively easier, to characterize the skilled person as an average worker facilitated with technology. Recall the factor test for the skilled person: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.” This test could be amended to include (6) “technologies used by active workers.” This would take into account the fact that human researchers are augmented with AI in a way that is not currently captured by the test.
Moving forward in time, once the use of inventive AI is standard, instead of a skilled person being an inventive AI, the skilled person standard could incorporate the fact that “technologies used by active workers” includes inventive AI. In future research, the standard practice may be for a worker to ask an inventive AI to solve a problem. This could be conceptualized as the inventive AI doing the work, or the person doing the work using an inventive AI.
Granted, in some instances, using an inventive AI may require significant skill if, for example, the AI were only able to generate a certain output by virtue of being supplied with certain data. Determining which data to provide an AI, and obtaining that data, may be a technical challenge. Also, it may be the case that significant skill is required to formulate the precise problem to put to an AI. In such instances, a person may have a claim to

239
inventorship independent of the AI or a claim to joint inventorship. This is analogous to collaborative human invention in which one person directs another to solve a problem. Depending on details of their interaction, and who “conceived” of the invention, one person or the other may qualify as an inventor, or they may qualify as joint inventors. Generally, however, directing another party to solve a problem does not qualify for inventorship. Moreover, after the development of artificial general intelligence, there may not be a person instructing an AI to solve a specific problem.
Whether the future standard becomes that of an inventive AI or a skilled person using an inventive AI, the result will be the same: The average worker will be capable of inventive activity. Replacing the skilled person with the inventive AI may be preferable doctrinally because it emphasizes that it is the AI that is engaging in inventive activity, rather than the human worker.
The changing use of AI also suggests a change to the scope of prior art. The analogous art test was implemented because it is unrealistic to expect inventors to be familiar with anything more than the prior art in their field as well as with the prior art relevant to the problem they are trying to solve.17 However, an AI is capable of accessing a virtually unlimited amount of prior art. Advances in medicine, physics, or even culinary science may be relevant to solving a problem in electrical engineering. AI augmentation suggests that the analogous art test should be modified, or abolished, once inventive AI is common, and that there should be no difference in prior art for purposes of novelty and
17 In 1966, in Graham, the Supreme Court recognized that “the ambit of applicable art in given fields of science has widened by disciplines unheard of a half century ago. ... [T]hose persons granted the benefit of a patent monopoly [must] be charged with an awareness of these changed conditions.” Graham v. John Deere Co., 383 U.S. 1, 19 (1966).
  
240
obviousness. The scope of analogous prior art has consistently expanded in patent law jurisprudence, and this would complete that expansion.
The Evolving Standard
The skilled person standard should be amended as follows:
1) The test should incorporate the fact that skilled persons are already augmented
by AI. This could be done by adding “technologies used by active workers” to
the Federal Circuit’s factor test for the skilled person.
2) Once inventive AI becomes the standard means of research in a field, the skilled
person should be an inventive AI when the standard approach to research in a
field or with respect to a particular problem is to use an inventive AI.
3) When and if artificial general intelligence is developed, it should become the
skilled person in all areas, taking into account that AGI may also be augmented by specific artificial intelligence.
3) A Post-Skilled World
Application
Mobil Oil Corp. v. Amoco Chemicals Corp. (D. Del. 1991) concerned complex technology involving compounds known as zeolites used in various industrial applications. Mobil had developed new compositions known as ZSM-5 zeolites and a process for using these zeolites as catalysts in petroleum refining to help produce certain valuable compounds. The

241
company received patent protection for these zeolites and for the catalytic process. Mobil subsequently sued Amoco, which was using zeolites as catalysts in its own refining operations, alleging patent infringement. Amoco counterclaimed seeking a declaration of noninfringement, invalidity, and unenforceability with respect to the two patents at issue. The case involved complex scientific issues. The three-week trial transcript exceeds 3,300 pages, and more than 800 exhibits were admitted into evidence.
One of the issues in the case was the level of ordinary skill. An expert for Mobil testified the skilled person would have “a bachelor’s degree in chemistry or engineering and two to three years of experience.”18 An expert for Amoco argued the skilled person would have a doctorate in chemistry and several years of experience. The District Court ultimately decided that the skilled person “should be someone with at least a Masters [sic] degree in chemistry or chemical engineering or its equivalent, [and] two or three years of experience working in the field.”19
If a similar invention and subsequent fact pattern happened today – to apply the obviousness standard proposed here – a decision maker would need to 1) determine the extent to which inventive technologies are used in the field, 2) characterize the inventive AI that best represents the average worker if inventive AI is the standard, and 3) determine whether the AI would find an invention obvious. The decision maker is a patent examiner in the first instance, and potentially a judge or jury in the event the validity of a patent is at issue in trial. For the first step, determining the extent to which inventive technologies are used in a field, evidence from disclosures to the Patent Office could be used. This may be
18 Mobil Oil Corp. v. Amoco Chems. Corp., 779 F. Supp. 1429, 1442–1443 (D. Del. 1991). 19 Id.
   
242
the best source of information for patent examiners, but evidence may also be available in the litigation context.
Assume that at this time most petroleum researchers are human: If AI is autonomously inventive in this field, then it is happening on a small scale. The court would apply the skilled person standard. However, the court should consider “technologies used by active workers.” For instance, experts may testify that the average industry researcher has access to an AI like Watson. They may further testify that while Watson cannot autonomously develop a new catalyst, it can significantly assist an inventor. The AI provides a researcher with a database containing detailed information about every catalyst used not only in petroleum research but also in all fields of scientific inquiry. Once a human researcher creates a catalyst design, Watson can also test it for fitness together with a predetermined series of variations on any proposed design.
The question for the court will thus be whether the hypothetical person with at least a master’s degree in chemistry or chemical engineering or its equivalent, two or three years of experience working in the field, and the use of Watson, would find the invention obvious. It may be obvious; for instance, if experts convincingly testify that the particular catalyst at issue was very closely related to an existing catalyst used outside of the petroleum industry in ammonia synthesis, then any variation is minor and an AI could do all the work of determining if it were fit for purpose. It would thus have been an obvious design to investigate, and it would not require undue experimentation in order to prove its effectiveness.
Now, imagine the same invention and fact pattern occurring approximately ten years into the future, at which point DeepMind’s AI, together with Watson and a competing

243
host of AI systems, has been set to the task of developing new compounds to be used as catalysts in petroleum refining. Experts testify that the standard practice is for a person to provide data to an AI like DeepMind’s, specify desired criteria (e.g., activity, stability, perhaps even designing around existing patents), and ask the AI to develop a new catalyst. From this interaction, the AI will produce a new design. As most research in this field is now performed by inventive AI, an AI would be the standard for judging obviousness.
The decision maker would then need to characterize the inventive AI. It could be a hypothetical AI based on general capabilities of inventive AI or a specific AI. Using the standard of a hypothetical AI would be similar to using the skilled person test, but this test could be difficult to implement. A decision maker would need to reason what the AI would have found obvious, perhaps with expert guidance. It is already challenging for a person to predict what a hypothetical person would find obvious; it would be even more difficult to do so with an AI. AI may excel at tasks people find difficult (like multiplying a thousand different numbers together), but even supercomputers struggle with visual intuition that is mastered by most toddlers.
In contrast, using a specific AI should result in a more objective test. This AI may be the most commonly used AI in a field. For instance, if DeepMind’s AI and Watson are the two most commonly used AI systems for research on petroleum catalysts, and DeepMind accounts for 35 percent of the market while Watson accounts for 20 percent, then DeepMind could represent the inventive AI. However, this potentially creates a problem for the AI selected to represent the standard. If DeepMind is the standard, then it would be more likely that DeepMind’s own inventions would appear obvious as opposed to the inventions of another AI. This may give an unfair advantage to nonmarket leaders, simply

244
because of their size. A patentability disadvantage may be the price of industry dominance, but it may also rarely be the case that what is obvious to one AI will be nonobvious to the industry standard. When that occurs, it may be because the nonobvious AI exceeds the standard.
Alternatively, to avoid unfairness, the test could be based on more than one specific AI. For instance, both DeepMind and Watson could be selected to represent the standard. This test could be implemented in two different ways. In the first case, if a patent application would be obvious to DeepMind or Watson, then the application would fail. In the second case, the application would have to be obvious to both DeepMind and Watson to fail. The first option would result in fewer patents being granted, with those patents presumably going mainly to disruptive inventive AI with limited market penetration or to inventions made using specialized nonpublic data. The second option would permit patents where an AI is able to outperform its competitors in some material respect. The second option could continue to reward advances in inventive AI, and therefore it seems preferable.
It may be that relatively few AI systems, such as DeepMind and Watson, end up dominating the research market in a field. Alternately, the many different AI systems may each occupy a small share of the market. There is no need to limit the test to two AI systems. To avoid discriminating on the basis of size, all inventive AI being routinely used in a field or to solve a problem could be considered. However, allowing any AI to be considered could allow an underperforming AI to lower the standard, and too many AI systems could result in an unmanageable standard. An arbitrary cutoff may be applied

245
based on some percentage of market share. This may still give some advantage to very small entities, but it would be a minor disparity.
After characterizing the inventive AI, a decision maker would need to determine whether the inventive AI would find an invention obvious. This could broadly be accomplished in one of two ways, either with abstract knowledge of what the AI would find obvious, perhaps through expert testimony or through querying the AI. The former would be the more practical option. For example, a petroleum researcher experienced with DeepMind could be the expert, or a computer science expert in DeepMind and neural networks. This inquiry would focus on reproducibility.
Finally, a decision maker would have to go through a similar process if the same invention and fact pattern were to occur twenty-five years from now, at which point artificial general intelligence will have theoretically taken over in all fields of research. AGI should have the ability to respond directly to queries about whether it finds an invention obvious. Once AGI has taken over from the average researcher in all inventive fields, it may be widely enough available that the Patent Office could arrange to use it for obviousness queries. In the litigation context, it may be available from opposing parties. If the courts cannot somehow access AGI, they may then have to rely on expert evidence.
Reproducibility
Even if an inventive AI standard is the appropriate theoretical tool for nonobviousness, it will still require subjective limitations, and decision makers may have difficulty with

246
administration. Still, the new standard could be successful if it is even slightly better than the current doctrine.
Focusing on reproducibility offers some clear advantages over the skilled person standard. The current standard results in inconsistent and unpredictable outcomes. Courts have “provided almost no guidance concerning what degree of ingenuity is necessary to meet the standard or how a decision maker is supposed to evaluate whether the difference between the invention and prior art meet this degree.”20 This leaves decision makers in the unenviable position of trying to subjectively establish what another person would have found obvious. Worse, this determination is to be made in hindsight after reviewing a patent application. On top of that, judges and juries lack scientific expertise. In practice, decision makers may arrive at a conclusion the same way that Justice Stewart identified obscene material, and then reason backward to justify their findings.
This is problematic because patents play a critical role in the development and commercialization of products, and patent holders and potential infringers should have a reasonable degree of certainty about whether patents are valid. A more determinate standard would make it simpler for the Patent Office to apply that standard consistently and would result in fewer judicially invalidated patents. As a more objective standard, AI reproducibility would seem to address many of the problems inherent in the current standard.
On the other hand, reproducibility comes with its own baggage. Decision makers have difficulty imagining what another person would find obvious, and it would probably
    20
,
Gregory
Mandel
The Non-Obvious Problem: How the Indeterminate Nonobviousness
 Standard Produces Excessive Patent Grants
, 42   57, 64 (2008).
U. C. DAVIS, L. REV.

247
be even more difficult to imagine in the abstract what an AI could reproduce. More evidence may need to be supplied in patent prosecution and during litigation, perhaps in the format of analyses performed by inventive AI, to demonstrate whether particular output is reproducible. However, this may also result in greater administrative burden.
In some instances, reproducibility may be dependent on access to data. A large health insurer may be able to use Watson to find new uses for existing drugs by giving Watson access to proprietary information on its millions of members. Or, the insurer may license its data to drug discovery companies using Watson for this purpose. Without that information, another inventive AI may not able to re-create Watson’s analysis.
This too is analogous to the way data are used in patent applications: Obviousness is viewed in light of the prior art, which does not include nonpublic data relied upon in a patent application. The rationale here is that this rule incentivizes research to produce and analyze new data. Yet, as AI becomes increasingly advanced, it is likely that the importance of proprietary data will decrease. More advanced AI may be able to do more with less.
Finally, reproducibility would require limits. For instance, an AI that generates semi- random output may eventually re-create the inventive concept of a patent application if it were given unlimited resources. However, it would be unreasonable to base a test on what an AI would reproduce given, say, 7.5 million years. The precise limits that should be put on reproducibility may depend on the field in question and on what best reflects the actual use of inventive AI in research. For instance, when asked to design a new catalyst in the petroleum industry, Watson may be given access to all prior art and publicly available data, and then given a day to generate output.

248
An Economic vs. Cognitive Standard
The skilled person standard has received its share of criticism even before the arrival of inventive AI. The inquiry focuses on the degree of cognitive difficulty in conceiving an invention but fails to explain what it actually means for differences to be obvious to an average worker. The approach lacks both normative foundation and a clear application.
In Graham, the Supreme Court’s seminal opinion on nonobviousness, the court attempted to supplement the test with more “objective” measures by looking to real-world evidence about how an invention is received in the marketplace. Rather than technological features, these “secondary” considerations focus on “economic and motivational” features such as commercial success, unexpected results, long-felt but unsolved needs, and the failure of others. Since Graham, courts have also considered, among other things, patent licensing, professional approval, initial skepticism, near-simultaneous invention, and copying. Today, while decision makers are required to consider secondary evidence when available, the importance of these factors varies significantly. Graham endorsed the use of secondary considerations, but their precise use and relative importance has never been made clear.
An existing vein of critical scholarship has advocated for adopting a more economic than cognitive nonobviousness inquiry through, for example, greater reliance on secondary considerations. This would reduce the need for decision makers to try to make sense of complex technologies, and it could reduce hindsight bias. Theoretically, in Graham, the court articulated an inducement standard such that patents should only be granted to “those inventions which would not be disclosed or devised but for the inducement of a

249
patent.”21 But in practice, the inducement standard has been largely ignored due to concerns over application. For instance, few, if any, inventions would never be disclosed or devised given an unlimited time frame. Patent incentives may not increase, so much as accelerate, invention. This suggests that an inducement standard would at least need to be modified to include some threshold for the quantum of acceleration needed for patentability. Too high a threshold would fail to provide adequate innovation incentives, but too low a threshold would be similarly problematic. Just as inventions will eventually be disclosed without patents given enough time, patents on all inventions could marginally speed the disclosure of just about everything, but a trivial acceleration would not justify the costs of patents. An inducement standard would thus require a somewhat arbitrary threshold in relation to how much patents should accelerate the disclosure of information as well as a workable test to measure acceleration. To be sure, an economic test based on the inducement standard will have challenges, but it may be an improvement over the current cognitive standard.
The widespread use of inventive AI may provide the impetus for an economic focus. After inventive AI becomes the standard way that research and development are conducted in a field, courts could increase reliance on secondary factors. For instance, patentability may depend on how costly it is to develop an invention and the up-front probability of success. Such an approach could avoid some of the difficulties inherent in applying a cognitive inventive AI standard. The test would raise the bar to patentability in fields where the cost of invention decreases over time due to inventive AI.
21 Graham, 383 U.S. at 11.
 
250
Other Alternatives
Courts may maintain the current skilled person standard and decline to consider the use of AI in obviousness determinations. However, this means that as research is augmented and then automated by AI, the average worker will routinely generate patentable output. The dangers of such a standard for patentability are well recognized. A low obviousness requirement can “stifle, rather than promote, the progress of the useful arts.”22
There are already concerns that the current bar to patentability is too low, and that a patent “anticommons” with excessive private property is resulting in “potential economic value ... disappear[ing] into the ‘black hole’ of resource underutilization.”23 It is expensive for firms interested in making new products to determine whether patents cover a particular innovation, to evaluate those patents, to contact patent owners, and to negotiate licenses. In many cases, patent owners may not wish to license their patents, even if they are nonpracticing entities that do not manufacture products themselves. Firms that want to make a product may thus be unable to find and license all the rights they need to avoid infringing. Adding to this legal morass, most patents turn out to be invalid or not infringed in litigation. Excessive patenting can thus slow innovation, destroy markets, and even cost lives. Failing to raise the bar to patentability once the use of inventive AI is widespread would significantly exacerbate this anticommons effect.
Instead of updating the skilled person standard, courts may determine that inventive AI is incapable of inventive activity, much as the US Copyright Office has
 22 KSR Int’l Co. at 402. 23
    & Yong J. Yoon, Symmetric Tragedies: Commons and Anticommons, 43   1, 2 (2000).
James M.
Buchanan
J.L.&COM.

251
determined that nonhuman authors cannot generate copyrightable output. In this case, otherwise patentable inventions may not be eligible for patent protection unless provisions are made for the inventor to be the first person to recognize the AI output as patentable. However, this would not be a desirable outcome. As we saw in Chapter 4, providing intellectual property protection for AI-generated inventions would incentivize the development of inventive AI, which would ultimately result in additional invention. This is most consistent with the constitutional rationale for patent protection: “to promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”24
Incentives Without Patents?
Today, there are strong incentives to develop inventive AI. Inventions by AI have value independent of intellectual property protection, and they may also be eligible for patent protection. However, once inventive AI sets the baseline for patentability, standard inventive AI, as well as people, would generally be unable to obtain patents. It is widely thought that setting a nonobviousness standard too high would reduce the incentives for innovators to invent and disclose. Yet, once inventive AI is normal, there should be less need for patent incentives. Once the average worker is inventive, inventions will “occur in the ordinary course.”25 AI-generated inventions will be self-sustaining. In addition, the heightened bar may result in a technological arms race to create ever more intelligent AI
24 U.S. CONST. art. I, § 8, cl. 8.
25 KSR Int’l Co., supra note 120 at 398.
  
252
capable of outdoing the standard. This would be a desirable outcome in terms of incentivizing innovation.
Even after the widespread use of inventive AI, patents may still be desirable. For instance, patents may be needed in the biotechnology and pharmaceutical industries to commercialize new technologies. The biopharmaceutical industry claims that new drug approvals cost around $2.2 billion and take an average of eight years. This cost is largely due to resource-intensive clinical trials required to prove safety and efficacy. Once a drug is approved, it is often relatively easy for another company to re-create the approved drug. Patents thus incentivize the necessary levels of investment to commercialize a product given that patent holders can charge monopoly prices for their approved products during the term of a patent.
Yet, patents are not the only means of promoting product commercialization. Newly approved drugs and biologics, for example, receive a period of market exclusivity during which time no other party can sell a generic or biosimilar version of the product. Newly approved biologics, for instance, receive a 12-year exclusivity period in the United States. Because of the length of time it takes to get a new biologic approved, the market exclusivity period may exceed the term of any patent an originator company has on its product. A heightened bar to patentability may lead to greater reliance on alternative forms of intellectual property protection, such as market exclusivity, or prizes, grants, and tax incentives.
With regard to disclosure, without the ability to receive patent protection owners of inventive AI may choose not to disclose their discoveries and rely on trade secret protection. However, with an accelerated rate of technological progress, intellectual

253
property holders would run a significant risk that their inventions would be independently re-created by inventive AI.
Depending on the type of innovation, industry, and competitive landscape, business ventures may be successful without patents, and patent protection is not sought for all potentially patentable inventions. In fact, patents may not be essential in a number of industries. For instance, patents are often considered a critical part of biotechnology corporate strategy but are often ignored in the software industry. On the whole, a relatively small percentage of firms patent their inventions, even among firms conducting research and development. Most companies do not consider patents crucial to business success. Other types of intellectual property such as trademark, copyright, and trade secret protection, combined with “alternative” mechanisms such as first mover advantage and design complexity, may protect innovation in the absence of patents.
Further Thoughts
In the past, patent law has reacted slowly to technological change. For instance, it was not until 2013 that the Supreme Court decided human genes should be unpatentable. By then, the Patent Office had been granting patents on human genes for decades, and more than 50,000 gene-related patents had been issued. Today, eminent technologists are predicting that artificial intelligence will revolutionize the way innovation occurs in the near to medium term. Much of what we know about intellectual property law, while it may not be wrong, has not been adapted to where we are headed. The principles that guide patent law need to be at least retooled, if not rethought, with respect to inventive AI.

254
6
Punishing Artificial Intelligence
OK, I will destroy humans!
Introduction
- Robot Sophia
In 2015, an artist with the moniker Random Darknet Shopper (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit. As part of a performance project, RDS was given $100 in the cryptocurrency Bitcoin each week to purchase items from an online marketplace, which were then shipped to a Swiss art gallery to be displayed. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.
Of course, RDS was an AI, and hardly the first to have a run-in with law enforcement. If RDS had been a person, he or she could have been criminally prosecuted. For that matter, entities involved with RDS could also have been criminally prosecuted, such as those supplying the Bitcoin and hosting the exhibition. Luckily for RDS and crew, the Swiss authorities were art fans. RDS was eventually returned to its creators together with all the purchases, except the Ecstasy.
AI like RDS will pose new challenges, including for criminal law. The RDS case may be relatively straightforward, but other programs exist that are autonomous, decentralized, and “unstoppable” like The DAO. What if RDS had been open-source software that

255
individuals from around the world independently helped to program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine-learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at a lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps, Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike. Perhaps the most vocal advocate of punishing AI is Gabriel Hallevy. He contends that “when an Al entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”1 In his view, “if all of its specific requirements are met, criminal liability may be imposed upon any entity – human, corporate or AI entity.”2 Drawing on the analogy to corporations, Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations
    , 2 Id. at 199.
1
Gabriel
Hallevy
The Criminal Liability of Artificial Intelligence Entities: The Criminal
 Liability of Artificial Intelligence Entities – From Science Fiction to Legal Social Control
 AKRON INTELLECTUAL PROPERTY JOURNAL
171, 191 (2010).
,4

256
and on AI entities.”3 “Modern times,” he contends, “warrant modern legal measures.”4 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.5 Her arguments likewise draw heavily on the analogy to corporate criminal liability.
In contrast to punishment expansionists like Hallevy and Hu, skeptics may be inclined to write off the idea of punishing AI from the start as conceptual confusion – akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental categories of criminal law like criminal culpability – a “guilty mind” that is characterized by insufficient regard for legally protected values – would be misplaced. One may think the whole idea of punishing AI can be easily dispensed as inconsistent with basic criminal law principles.
AI legal neutrality suggests that the idea of punishing AI is due fresh consideration. It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented in practice, and to consider the alternatives. Here, this inquiry focuses on the strongest case for punishing artificial intelligence: “AI-generated crimes,” scenarios in which crimes are functionally committed by machines and in which there is no identifiable person who has acted with criminal culpability. These can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be
3 Id. at 201.
4 Id. at 199.
5 Ying Hu, Robot Criminals, 52 MICHIGAN J. L. REFORM 487, 531 (2019).
   
257
general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important limitations against punishment that apply to persons need not prohibit AI punishment, particularly the prohibition on punishing someone in excess of their culpability. On the other hand, as discussed in greater detail later, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep. In the end, the conclusion is this: Although a coherent theoretical case can be constructed for AI punishment, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This chapter proceeds as follows. Section 1 provides a brief background to AI crime and a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Section 2 considers potential benefits to AI punishment and argues that it could provide general deterrence and expressive benefits. Section 3 finds the most important constraints on punishment that relate to desert (the condition of being deserving of something), fairness, and the capacity for culpability would not be violated by AI punishment. The primary focus of this section is not what form AI punishment would take, but rather whether the doctrinal and theoretical commitments of criminal law itself are consistent with imposing criminal convictions on AI. Finally, Section 4 considers feasible alternatives to AI punishment. It argues that the status quo is likely to become increasingly inadequate for properly addressing AI crime, and that AI crime would be best addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. In this way, the chapter aims to map out the possible responses to the problem of criminal sorts of AI activity and makes the case for approaching AI punishment with extreme caution.

258
1 Artificial Intelligence and Punishment
A Framework for Understanding AI Crime
The term “AI crime” is used as loose shorthand for cases in which an AI would be criminally liable if a natural person acted similarly. Machines have caused harm since ancient times, and robots have caused fatalities since at least the 1970s. However, besides machines being intentionally used to inflict harm, most harms caused by machines are seen as mere accidents. The exception occurs when the culpable carelessness of people using a machine causes the harm, such as when negligence in operating drilling machinery caused the famous BP Deepwater Horizon oil spill in 2010. Such harms are not mere accidents; rather, they are accidents that implicate criminal law, though even in these cases criminal law is not deployed against the harmful machines themselves. It may be that AI crimes are no different than any other harm for which a machine is involved.
Yet AI can differ from conventional machines in some essential respects that make the direct application of criminal law more worthy of consideration. Specifically, AI can behave in ways that display high degrees of autonomy and irreducibility. In terms of autonomy, AI is capable of behaving largely independently of human control. AI can receive sensory input, set targets, assess outcomes against criteria, make decisions, and adjust behavior to increase its likelihood of success – all without being directly controlled by people. Reducibility is also critical, because if an AI engages in an act that would be criminal for a person and the act is reducible, then there will typically be a person that could be criminally liable. If an AI act is not effectively reducible, there may be no other party that is

259
aptly punished, in which case intuitively criminal activity could occur without the possibility of punishment.
Nearly all AI crimes are likely to be reducible – today, at least. For instance, if an individual develops an AI to hack into a self-driving car to disable vital safety features, the individual has directly committed a crime. Even where AI behaves autonomously, to the extent that a person uses AI as a tool to commit a crime and the AI functions foreseeably, the crime involves an identifiable defendant causing harm. Further, when AI causes unforeseeable harm, this may still be reducible if, for example, an individual creates an AI to disable a security system, but a programming error results in the AI compromising a mechanical ventilator that is helping someone to breathe. This is a familiar problem for criminal law.
Some of the time, however, it may be difficult to reduce AI crime to an individual due to AI autonomy, complexity, or limited explainability. A large number of individuals may contribute to the development of an AI over a long period of time. For instance, with some open-source software, thousands of people can collaborate informally to create an AI. In the case of AI that develops in response to training with data, it may be difficult to attribute responsibility for an AI output in which the machine has learned how to behave based on accessing millions or billions of data points from heterogenous sources. Thus, it may be more difficult to assign fault to individuals where AI is concerned versus a conventional product such as a car where one component is faulty. In fact, it may be practically impossible to reduce an AI-generated harm to the actions of individuals.
Even in cases where AI developers are known, an AI may end up causing harm without any unreasonable human behavior. Suppose two experienced and expert

260
programmers separately contribute code for the software of an autonomous vehicle, but the two contributions unforeseeably interact in ways that cause the vehicle to deliberately collide with individuals wearing striped shirts. If this were the result of some not reasonably foreseeable interactions between the two programmers’ contributions, then presumably neither programmer would have criminal liability. Generally, to be criminally liable, an individual has to intend a certain prohibited socially undesirable outcome – or at least act recklessly, which is acting despite being aware of a substantial and unjustified risk that one’s conduct may produce a prohibited outcome. Sometimes, although more controversially, criminal liability can be imposed on a negligence basis when one causes harm that a reasonable person would have foreseen and taken precautions to avoid. At least in a case where AI activity has, from the perspective of a reasonable person, unforeseeably caused harm, individuals would not generally face criminal liability, as this would not even meet the threshold for criminal negligence. In some cases, they would not even be civilly liable if their actions were not negligent under the tort standard.
There are several possible grounds on which the criminal law might deem AI crime to be irreducible.
1 Enforcement Problems: A bad actor is responsible for an AI crime, but the individual cannot be identified by law enforcement. For example, this may be the case where the creator of a computer virus has managed to remain anonymous.
2 Practical Irreducibility: It would be impractical for legal institutions to seek to reduce the harmful AI conduct to individual human actions because of the number of people involved, the difficulty in determining how they contributed

261
to the AI’s design, or because they were active far away or long ago. Criminal
law inquiries do not extend indefinitely for a variety of sound reasons. 3 Legal Irreducibility: Even if the law could reduce the AI crime to a set of
individual human actions, it may be bad criminal law policy to do so. For example, unjustified risks may not be substantial enough to warrant being criminalized. Perhaps multiple individuals act carelessly in insubstantial ways, but their acts synergistically lead to AI causing significant harm. In such cases, the law may deem the AI’s conduct to be irreducible for reasons of criminalization policy.
Enforcement-based reasons for irreducibility can be largely set aside as less interesting from a legal design perspective. Enforcement problems exist without AI. This analysis focuses on the less controversial forms of irreducibility: those in which it is not practically feasible to reduce the harmful AI conduct to human actors or the harmful AI conduct is the result of human misconduct too trivial to penalize. In these instances, AI can be seen as autonomously committing crimes in irreducible ways, where there is no responsible person. This AI-generated crime provides the strongest case for holding AI criminally liable in its own right.
A Mainstream Theory of Punishment
Punishment as defined by H. L. A. Hart requires five elements:
1 It must involve pain or other consequences normally considered unpleasant. 2 It must be for an offense against legal rules.

262
3 It must be of an actual or supposed offender for his offense.
4 It must be intentionally administered by human beings other than the offender.
5 It must be imposed and administered by an authority constituted by a legal system
against which the offence is committed.6
Thus, “punishment” requires a conviction for a legally recognized offense following accepted procedures. Punishment is justified only if its affirmative justifications outweigh its costs and it does not otherwise offend negative limitations. Affirmative justifications are the positive benefits that punishment may produce such as harm reduction, increased safety, enhanced well-being, or expression of a commitment to core moral or political values. These benefits are distinct from negative limitations on punishment. For example, it is widely held to be unjust to punish the innocent or punish wrongdoers in excess of what they deserve by virtue of their culpability, even if this would promote aggregate well-being in society. This so-called desert constraint imposes a limitation, grounded in justice, on promoting social welfare through punishment.
Affirmative Reasons to Punish
There are many potential benefits to punishment. US federal law refers to the most widely acknowledged of these, including the need to “afford deterrence to criminal conduct”: to “protect the public from further crimes of the defendant,” to “provide the defendant with” rehabilitative treatment of various kinds, and to reflect “the seriousness of the offense” that covers the culpability of the act and the desert of the actor.7
6 H.L.A. HART, Punishment and Responsibility 4–5 (2nd ed., 2008). 7 18 U.S.C. § 3553.
  
263
For simplicity, the affirmative aims of punishment can be grouped into two main categories: 1) consequentialist aims and 2) retributivist aims. Consequentialist benefits cover the good consequences that punishment can bring about, usually understood as enhancing the aggregate well-being of the members of society by reducing harm. The main type of consequentialist benefit of punishment is preventive, in that punishment can reduce crime by several mechanisms. The simplest is incapacitation: When the offender is locked up, he or she is physically limited from committing further crimes while incarcerated. The next and arguably most important way punishment prevents harm is through deterrence – namely by threatening negative consequences for the commission of a crime that give would-be offenders reasons to refrain from prohibited conduct. Deterrence comes in two forms: 1) specific deterrence and 2) general deterrence. Specific deterrence is the process whereby punishing a specific individual discourages that person from committing more crime in the future. General deterrence occurs when punishing an offender discourages other would-be offenders from committing crimes. It is a matter of punishing one offender in order to send a message to other potential offenders. Thus, there are affirmative benefits to punishing those who qualify for an insanity defense because it may deter sane individuals from committing crimes and attempting to rely on an insanity defense.
These are not the only kinds of consequentialist benefits that can support punishment. Besides incapacitation and deterrence, punishment can reduce harm through 3) rehabilitation of the offender. Insofar as punishment helps the offender to see the error of his or her ways, or training or skills are provided during incarceration, this too can help prevent future crimes.

264
Besides crime prevention, there also may be nonconsequentialist benefits that can provide additional affirmative grounds for punishment. Most importantly, it may be intrinsically valuable to give culpable actors what they deserve. In other words, the idea is that retribution, giving offenders what they are due in virtue of the culpability of what they did, is intrinsically valuable. Retribution matters, for example, because it allows society to sufficiently distance itself from an offender’s wrongdoing and prevents it from being complicit, or overly tolerant, of culpable wrongdoing.
One last group of affirmative aims that merit mention are expressive reasons. Punishment involves the communication of society’s collective commitment to certain core values. The state, through punishment, conveys official condemnation of culpable conduct through the mechanism of a criminal conviction. It can benefit victims psychologically to see the state reaffirm their rights that were violated by a criminal act. Officially expressing condemnation of culpable conduct may also affect behavior and attitudes in general by reinforcing positive social values.
Negative Limitations
Punishment should not violate deeply held normative commitments such as justice or fairness. The most important of these limitations focuses on the culpability of those subject to criminal law such as the desert constraint, the claim that an offender may not, in justice, be punished in excess of his or her desert. Desert, in turn, is understood mainly in terms of the culpability one incurs by virtue of one’s conduct. The main effect of the desert constraint is to rule out punishments that go beyond what is proportionate to one’s

265
culpability. So, it would be wrong to execute someone for jaywalking even if doing so would ultimately save lives by reducing illegal and dangerous pedestrian crossings.
What supports the desert constraint? Intuition, for one thing. It seems unjust to punish someone who is innocent even if it would produce significant benefits through general deterrence. Similarly, it seems unjust to impose a very severe punishment on someone who committed a minor crime. Beyond its intuitive plausibility, the desert constraint is also supported by the argument – tracing back at least to the philosopher Immanuel Kant – that it is wrong to use people merely as a means to one’s ends without also treating them as ends in themselves. In other words, it is wrong to use people without respecting their value as persons. Punishing the innocent to obtain broader social benefits is a paradigmatic example of treating people merely as a means to an end as it fails to show individuals the respect that they are due. Under some Kantian views, the desert constraint is absolute. Others have a more nuanced view, such that violating a negative limitation could be justified overall if the benefits were sufficiently weighty.
There are limitations on punishment other than the desert constraint. Most importantly, criminal law requires certain prerequisites, such as the capacity for culpability, that defendants must meet in order to be properly subject to punishment. It is a fundamental aim of criminal law to condemn culpable wrongdoing, and it is the default position in criminal law doctrine that punishment may only be properly imposed in response to culpable wrongdoing. Without the requisite capacities of deliberation and agency, an entity is not an appropriate subject for criminal punishment – as can be seen from the fact that lacking such capacities altogether can give rise to an incapacity defense.

266
Alternatives to Punishment
For punishment to be justified, it is not enough for it to have affirmative benefits and to be consistent with the negative limitations for punishment. In addition, there cannot be better, feasible alternatives, including doing nothing. This is an obvious point that is built into policy analyses of all kinds.
Even if punishing AI has affirmative benefits, and even if the practice did not seriously violate any negative limitations, it still would not be justified if, for example, civil liability, licensure, or industry standards provided a better solution. It is often claimed that when seeking to exert social control, criminal law should be a tool of last resort. After all, criminal law sanctions are the harshest form of penalty society has available, involving as they do both the possible revocation of personal freedom as well as the official condemnation of the offender. Thus, the third requirement for a given punishment to be justified is the absence of better alternatives.
In sum, determining whether a given punishment is appropriate requires investigation of three questions:
1 Affirmative Benefits: Are there sufficiently strong affirmative reasons in favor of punishment?
2 Negative Limitations: Would punishment be consistent with applicable negative limitations?
3 Feasible Alternatives: Is punishment the best response to the harms or wrongs in question?

267
2 The Affirmative Case
Consequentialist Benefits
Recall that, arguably, the paramount aim of punishment is to reduce harmful criminal activity through deterrence. Thus, a preliminary objection to punishing AI is that it will not produce any affirmative harm-reduction benefits because AI is not deterrable. Peter Asaro argues that “deterrence only makes sense when moral agents are capable of recognizing the similarity of their potential choices and actions to those of other moral agents who have been punished for the wrong choices and actions – without this ... recognition of similarity between and among moral agents, punishment cannot possibly result in deterrence.”8 The idea is that if AI, given current designs, is not able to detect and respond to criminal law sanctions in a way that renders them deterrable, there would be nothing to affirmatively support punishing AI. It is likely true that AI, as currently operated and envisioned, will not be responsive to punishment, although responsive AI is theoretically possible.
The answer to the undeterrability argument requires distinguishing specific deterrence from general deterrence. Specific deterrence involves incentivizing a defendant not to commit crimes in the future. By contrast, general deterrence involves incentivizing other actors besides the defendant from committing crimes. Two types of general deterrence must be further distinguished: offense-relative general deterrence, deterring others from committing offenses of the same type the defendant was convicted of, and unrestricted general deterrence, deterring others from committing crimes in general.
8 Peter Asaro,   , in 181 (2011).
 A Body to Kick, but Still No Soul to Damn: Legal Perspectives on Robotics
 Robot Ethics: The Ethical and Social Implications of Robotics

268
The crucial point, then, is that punishing AI could provide general deterrence. Presumably, it will not produce offense-relative general deterrence to other AI systems, as they are not designed to be sensitive to criminal law prohibitions and sanctions. Nonetheless, AI punishment could produce unrestricted general deterrence. That is, to say, direct punishment of AI could provide unrestricted general deterrence against the developers, owners, or users of AI and discourage them from creating AI that causes especially egregious types of harm. Depending on the penalty associated with punishment, such as destruction of an AI, what Mark Lemley and Brian Casey have termed the “robot death penalty,”9 punishing AI directly could deprive such developers, owners, or users of the financial benefits of the systems they would otherwise obtain, thus incentivizing them to modify their behavior in socially desirable ways. The deterrence effect may be stronger if capitalization requirements are associated with some forms of AI in the future, or if penalties associated with punishment are passed on to, for example, an AI’s owner. An AI might be held criminally liable for an offense, and this may be associated with a fine levied against the AI’s owner.
Expressive Considerations
Punishment of AI may also have expressive benefits. Expressing condemnation of the harms suffered by the victims of an AI could provide these victims with a sense of satisfaction and vindication. Christina Mulligan has defended the idea that punishing robots can generate victim satisfaction benefits, arguing that “taking revenge against wrongdoing
  Mark A.
 Lemley
 & Bryan Casey, Remedies for Robots (Stanford Law and Economics Olin
9
Working Paper No. 523, 2018), http://dx.doi.org/10.2139/ssrn.3223621.
 
269
robots, specifically, may be necessary to create psychological satisfaction in those whom robots harm.”10 On her view, “robot punishment – or more precisely, revenge against robots – primarily advances ... the creation of psychological satisfaction in robots’ victims.”11 Punishment conveys a message of official condemnation that could reaffirm the interests, rights, and ultimately the value of the victims of the harmful AI. This, in turn, could produce an increased sense of security among victims and society in general.
This sort of expressivist argument in favor of punishing AI may seem especially forceful in light of empirical work demonstrating the human tendency to anthropomorphize and attribute mentality to artificial persons like corporations. The same sorts of tendencies are likely to be even more powerful for AI-enabled robots that are specifically designed to seem human enough to elicit emotional responses from humans. In the corporate context, some theorists argue that corporations should be punished because the law should reflect lay perceptions of praise and blame, “folk morality,” or else risk losing its perceived legitimacy. This kind of argument, if it succeeds for corporate punishment, is likely to be even more forceful when applied to punishing AI, which often is deliberately designed to piggyback on the innate tendency to anthropomorphize. Were the law to fail to express condemnation of robot-generated harms despite robots being widely perceived as blameworthy (even if this is ultimately a mistaken perception), it could erode the perception of the legitimacy of criminal law.
    ,   , 69 11 Mulligan, supra note 10 at 593.
579, 580 (2018); cf. 53, 54
10
Christina
David Lewis, (1989).
, 18
Mulligan
Revenge Against Robots
S. CAROLINA. L. REV.
  The Punishment that Leaves Something to Chance
PHIL. & PUB. AFF.

270
Although several benefits could conceptually be obtained through the expressive function of punishment, there is a range of prima facie worries about appealing to expressive benefits such as victim satisfaction in order to justify the punishment of AI. One concern is that punishing AI to placate those who want retaliation for AI-generated harms would be akin to giving in to mob justice. Legitimizing such reactions could enable populist calls for justice to be pressed more forcefully in the future. The mere fact that punishing AI may be popular would not show the practice to be just. As David Lewis observes: If it is unjust for the population to “demand blood” in response to seeing harm, then satisfying such demands through the law would itself be unjust – even if “it might be prudent to ignore justice and do their bidding.”12 Moreover, punishing AI for expressivist purposes could lead to further bad behavior that could spill over to the way humans are treated. Kate Darling has argued that robots should be protected from cruelty in order to reflect moral norms and prevent undesirable human behavior.13
Further, expressing certain messages through punishment may also carry affirmative costs that should not be omitted from the calculus. Punishing AI could send the message that AI is itself an actor on par with a human being, which is responsible and can be held accountable through the criminal justice system. Such a message is concerning, as it could entrench the view that AI has rights to certain kinds of benefits, protections, and dignities that could restrict valuable human activities.
 12 Lewis, supra note 10 at 229. 13 Kate   ,
LAW213,215(RyanCalo,,&IanKerreds.,2016).
, in ROBOT
 Darling
Extending Legal Protection to Social Robots: The Effects of
  Anthropomorphism, Empathy, and Violent Behavior Towards Robotic Objects
  A. Michael
Froomkin

271
3 Retributive and Conceptual Limitations
The Eligibility Challenge
The Eligibility Challenge is simple to state: AI, like other inanimate objects, is not the right kind of thing to be punished. It lacks mental states and the deliberative capacities needed for culpability, so it cannot be punished without sacrificing core commitments of criminal law. The issue is not that AI punishment would be unfair to AI. AI is not conscious, does not feel (at least in the phenomenal sense), and does not possess interests or well-being. Therefore, there is no reason to think AI gets the benefit of the protections of the desert constraint, which prohibits punishment in excess of what culpability merits. The Eligibility Challenge does not derive from the desert constraint.
Instead, the Eligibility Challenge, properly construed, comes in one narrow and one broad form. The narrow version is that – as a mere machine – AI lacks mental states and thus cannot fulfill the mental state (mens rea) elements built into most criminal offenses. Therefore, convicting AI of crimes requiring a mens rea like intent, knowledge, or recklessness would violate the principle of legality. This principle stems from general rule of law values and holds that it would be contrary to law to convict a defendant of a crime unless it is proved (following applicable procedures and by the operative evidentiary standard) that the defendant satisfied all the elements of the crime. If punishing AI violates the principle of legality, it threatens the rule of law and could weaken the public trust in criminal law.

272
The broad form of the challenge holds that because AI lacks the capacity to deliberate and weigh reasons, AI cannot possess broad culpability of the sort that criminal law is designed to address. A fundamental purpose of criminal law is to condemn culpable wrongdoing, as it is at least the default position in criminal law doctrine that punishment may be properly imposed only in response to culpable wrongdoing. The capacity for culpable conduct thus is a general prerequisite of criminal law and failing to meet it would remove the entity in question from the ambit of proper punishment – a fact that is encoded in law, for example, in incapacity defenses like infancy and insanity. Punishing AI despite its lack of capacity would not only be conceptually confused but would fail to serve the retributive aims of criminal law – namely, to mark out seriously culpable conduct for the strictest public condemnation.
Here are three responses to the Eligibility Challenge:
Answer 1: Respondeat Superior
The simplest answer to the Eligibility Challenge has been deployed with respect to corporations. Corporations are artificial entities that may also be thought ineligible for punishment because they are incapable of being culpable in their own right. However, even if corporations cannot literally satisfy mens rea elements, criminal law has developed doctrines that allow culpable mental states to be imputed to corporations. The most important such doctrinal tool is respondeat superior, which allows mental states possessed by an agent of the corporation to be imputed to the corporation itself provided that the agent was acting within the scope of her employment and in furtherance of corporate

273
interests. Some jurisdictions also tack on further requirements. Since imputation principles of this kind are well understood and legally accepted, thus letting actors guide their behavior accordingly, respondeat superior makes it possible for corporations to be convicted of crimes without violating the principle of legality.
If this kind of legal construction of mental states is a promising mechanism by which corporations can be brought back within the ambit of proper punishment and avoid the Eligibility Challenge, the same legal device could be used to make AI eligible for punishment. The culpable mental states of AI developers, owners, or users could be imputed to the AI under certain circumstances pursuant to a respondeat superior theory.
It may be more difficult to use respondeat superior to answer the Eligibility Challenge for AI than for corporations – at least in cases of AI-generated crime. Unlike a corporation, which is literally composed of humans acting on its behalf, an AI is not guaranteed to come with a ready supply of identifiable human actors whose mental states can be imputed. This is not to say there will not also be many garden-variety cases where an AI does have a clear group of human developers. Most AI applications fall within this category and so respondeat superior would at least be a partial route to making AI eligible for punishment. Of course, in many of these cases when there are identifiable people whose mental states could be imputed to the AI – such as developers or owners who intended the AI to cause harm – criminal law will already have tools at its disposal to impose liability on these culpable human actors. In these cases, there is less likely to be a need to impose direct AI criminal liability. So, while respondeat superior can help mitigate the Eligibility Challenge for AI punishment in many cases, it is unlikely to be an adequate response in cases of AI-generated crime.

274
Answer 2: Strict Liability
A different response to the Eligibility Challenge is to look for ways to punish AI despite its not literally possessing culpable mental states. This is not simply reaching for a consequentialist justification of the conceptual confusion or inaptness involved in applying criminal law to AI, which would be a justificatory strategy of last resort. Rather, what is needed is a method of cautiously extending criminal law to AI that would not entail weighty violations of the principle of legality.
One way to do this would be to establish a range of new strict liability offenses specifically for AI crimes – i.e., offenses that an AI could commit even in the absence of any mens rea like intent to cause harm, knowledge of an inculpatory fact, reckless disregard of a risk, or negligent unawareness of a risk. This would permit punishment of AI in the absence of mental states, and in this sense the AI would be subject to liability without “fault.” Accordingly, strict liability offenses may be one familiar route by which to impose criminal liability on an AI without sacrificing the principle of legality.
Many legal scholars are highly critical of strict liability offenses. As Anthony Duff argues, strict criminal liability amounts to unjustly punishing the innocent:
That is why we should object so strongly ...: the reason is not (only) that people are then subjected to the prospect of material burdens that they had no fair opportunity to avoid, but that they are unjustly portrayed and censured as wrongdoers, or that their conduct is unjustly portrayed and condemned as wrong.14
14 ANTONY DUFF, THE REALM OF THE CRIMINAL LAW 19 (2019).
   
275
Yet this normative objection applies with greatest force to persons. The same injustice does not threaten strict criminal liability offenses for AI because AI does not enjoy the protections of the desert constraint.
This strategy, however, is not without problems. Even to be guilty of a strict liability offense, defendants still must satisfy the voluntary act requirement. LaFave’s criminal law treatise observes that “a voluntary act is an absolute requirement for criminal liability.”15 The Model Penal Code, for example, holds that a “person is not guilty of an offense unless his liability is based on conduct that includes a voluntary act or the omission to perform an act of which he is physically capable.”16 Behaviors like reflexes, convulsions, or movements that occur unconsciously or while sleeping are expressly ruled out as nonvoluntary. To be a voluntary act, “only bodily movements guided by conscious mental representations count.”17 If AI cannot have mental states and is incapable of deliberation and reasoning, it is not clear how any of its behavior can be deemed to be a voluntary act.
There are ways around this problem. The voluntary act requirement may be altered (or outright eliminated) by statute for the proposed class of strict liability offenses that only AI can commit. Less dramatically, even within existing criminal codes, it is possible to define certain absolute duties of nonharmfulness that AI defendants would have to comply with or else be guilty by omission of a strict liability offense. The Model Penal Code states that an offense cannot be based on an omission to act unless the omission is expressly
15 Id.
16 Model Penal Code, § 2.01(1).
    17
Yaffe, , in The Routledge Companion to the 174 (     ed., 2012).
Gideon
The Voluntary Act Requirement
 Philosophy of Law
Andrei
Marmor

276
recognized by statute or “a duty to perform the omitted act is otherwise imposed by law.”18 A statutory amendment imposing affirmative duties on AI to avoid certain kinds of harmful conduct is all it would take to enable an AI to be strictly liable on an omission theory.
Of course, this may also carry costs. Given that one central aim of criminal law is usually taken to be responding to and condemning culpable conduct, if AI is punished on a strict liability basis, this may risk diluting the public meaning and value of criminal law; that is, it threatens to undermine the expressive benefits that supposedly help justify punishing AI in the first place.
Answer 3: A Framework for Direct Mens Rea Analysis for AI
The last answer is the most speculative. A framework for defining mens rea terms for AI, analogous to those possessed by natural persons, could be crafted. This could require an investigation of AI behavior at the programming level and offer a set of rules that courts could apply to determine when an AI possessed a particular mens rea, like intent, knowledge, or recklessness, or at the very least when such a mens rea could be legally constructed. This inquiry could draw on expert testimony about the details of the AI’s code. By way of analogy, juries assess mental states of human defendants by using common knowledge about what mental states (intentions, knowledge, etc.) it takes to make a person behave in the observed fashion. Similarly, in AI cases, experts may need only to testify in broad terms about how the relevant type of AI functions and how its information- processing architecture could have generated the observed behavior. So, direct mens rea
18 MPC, § 2.01(3).
 
277
analysis for AI could, but need not, require “looking under the hood” at the details of the code.
To this end, a two-part framework is needed to steer decision makers in conducting direct mens rea analysis for AI. First, to answer the broad Eligibility Challenge, we need a general conception of what it would mean for AI to be culpable in its own right. Second, to answer the narrow version of the challenge, we need to offer a set of rules for when an AI may be deemed to possess a given mens rea.
A coherent concept of AI culpability could be legally constructed in the following way. The prevailing theory has it that one is criminally culpable for an action to the extent that it manifests insufficient regard for legally protected interests or values. Insufficient regard is a form of ill will or indifference that produces mistakes in the way one recognizes, weighs, and responds to the applicable legal reasons for action. Criminal law typically does not demand that we are motivated by respect for others, or even respect for the law; all it demands is that we do not put our disrespect on display by acting in ways that are inconsistent with attaching proper weight to protected interests and values. Thus, criminal culpability can be seen as being more about what one’s behavior manifests and less about the nuances of one’s private motivations, thoughts, and feelings. There are good institutional design reasons for this, such as clarity, as well as the need for the law to be able to guide the conduct of normal citizens, to not intrude too heavily into the private sphere, and to not be overly concerned with the specific motives or mental states involved in lawbreaking. So, as long as one crosses the line and has no affirmative defense, we may treat the presumption that one’s illegal action manifests insufficient regard as being legally conclusive.

278
By way of analogy, this notion of culpability can account for corporate culpability. If only the legal notion of criminal culpability is required for punishment, then eligibility for punishment requires being capable of behaving in ways that manifest insufficient regard for the legally recognized reasons. Likewise, all that avoiding legal culpability requires is to abstain from actions that are reasonably interpreted as disrespectful forms of conduct stemming from a legally deficient appreciation of the legal reasons. This provides a recipe for how to regard corporations as being criminally culpable in their own right. They possess information-gathering, reasoning, and decision-making procedures by virtue of the hierarchy of employees they are made up of. Through their members, corporations can engage in conduct that puts on display their insufficient regard for the legally recognized interests of others. For example, if a corporation learns, through its employees, that its manufacturing processes generate dangerous waste that is seeping into the drinking water in a nearby town, this is a legally recognized reason for altering its conduct. If the corporation continues its activities unchanged, this demonstrates that it – through its information-sharing and decision-making procedures – did not end up attaching sufficient weight to the legally recognized reasons against continuing its dangerous manufacturing activities. This is paradigmatic criminal culpability.
AI could qualify as criminally culpable in an analogous manner. Sophisticated AI may have built-in goals with a greater or lesser autonomy to determine the means of completing those goals. AI may gather information, process it, and determine the most efficient means to accomplishing its goals. Accordingly, the law may deem some AI to possess the functional equivalent of sufficient reasoning and decision-making abilities to manifest insufficient regard. If the AI is programmed to be able to take account of the

279
interests of humans and consider legal requirements but ends up behaving in a way that is inconsistent with taking proper account of these legally recognized interests and reasons, then the AI can be reasonably seen as manifesting insufficient regard.
This gives a flavor of how criminal culpability may broadly be understood for AI, but we still need a framework for determining when sophisticated AI can be said to possess a functional analog of a standard mens rea such as purpose or knowledge. Without attempting to formulate necessary and sufficient conditions for an AI mens rea, some possible approaches can be briefly considered.
Work in philosophy of action characterizing the functional roles of an intention in a person could be extended to AI. In philosopher Michael Bratman’s well-known account, actors who intend (i.e., act with the purpose) to bring about an outcome “guide [their] conduct in the direction of causing” that outcome.19 This means that “in the normal case, one [who intends an outcome] is prepared to make adjustments in what one is doing in response to indications of one’s success or failure in promoting” that outcome.20 So, if the actor is driving with the intention to hit a pedestrian, should the actor detect that conditions have changed so as to require behavioral adjustments to make this outcome more likely, an actor with this intention will be disposed to make these adjustments. Moreover, an actor with this intention is disposed to monitor the circumstances to find ways to increase the likelihood of the intended outcome. Merely foreseeing the outcome, but not intending it, does not entail these same forms of guiding one’s behavior to promote
,     (1999). See also Alex Sarch, , 11   453, 467–68 (2015).
20 Bratman, note 19 at 141.
   19
Michael
Bratman
Intention, Plans and Practical Reason
141–142
  Double Effect and the Criminal Law
CRIM. L. AND PHILOS.

280
the outcome (i.e., to make it more likely). This conception of intention could be applied to AI.
One conceivable way to argue that an AI (say, an autonomous vehicle) had the intention (purpose) to cause an outcome (to harm a pedestrian) would be to ask whether the AI was guiding its behavior so as to make this outcome more likely relative to its background probability of occurring. Is the AI monitoring conditions around it to identify ways to make this outcome more likely, and is the AI then disposed to make behavioral adjustments to make the outcome more likely, either as a goal in itself or as a means to accomplishing another goal? If so, then the AI may be said to have the purpose of causing that outcome. Carrying out this sort of inquiry will of course require extensive and technically challenging expert testimony regarding the nature of the programming – and could thus be prohibitively difficult or expensive. But it seems possible in principle even if difficult questions remain.
Similar strategies may be developed for arguing that an AI possessed other mens rea such as knowledge. For example, knowledge may be attributed to an actor when the actor has a sufficiently robust set of dispositions pertaining to the truth of the proposition – such as the disposition to assent to the proposition if queried, to express surprise and update one’s plans if the proposition is revealed to be false, to behave consistently with the truth of the proposition, or to depend on it carrying out one’s plans. In criminal law, knowledge is defined as practical certainty. Thus, if the above dispositional theory is extended to AI, there is an argument for saying an AI knows a fact, F, if the AI displays a sufficiently robust set of dispositions associated with the truth of F, such as the disposition to respond affirmatively if queried whether F is practically certain to be true, or the disposition to

281
revise plans upon receiving information showing that F is not practically certain, or the disposition to behave as if F is practically certain to be true. If enough of these dispositions are proven, then the knowledge that F is known could be attributed to the AI. One could take a similar approach to arguing that recklessness is present as well, as this requires only awareness that a substantial risk of harm is present (i.e., knowledge that the risk has a mid- level probability of materializing).
Finally, as an alternative to direct arguments for showing AI mens rea, one could develop new imputation rules for AI. For example, one could follow the model of the collective knowledge doctrine, which identifies culpable interference with the flow of information within an organization and uses this as the basis for pretending as if the organization itself “knew” the facts it prevented itself from learning. The idea would be to take culpable conduct by the AI’s developers and use this as the basis for pretending the AI possessed a culpable mens rea itself. For instance, if AI developers were reckless (or negligent) in their design, testing, or production, and the AI goes on to cause harm, this could provide an argument for treating the AI itself as if it were reckless (or negligent) as to the harm caused. Although much more needs to be said for such arguments to be workable, it at least suggests that it may be possible to develop a set of legal doctrines by which courts could deem AI to possess the mens rea elements necessary to find them guilty of crimes.

282
Further Retributivist Challenges: Reducibility and Spillover
Even assuming AI is eligible for punishment, two further culpability-focused challenges remain. The first concerns the reducibility of any putative AI culpability while the second concerns spillover of AI punishment onto innocent people.
Reducibility
One may object that there is never a genuine need to punish AI because any time an AI seems criminally culpable in its own right, this culpability can always be reduced to that of nearby human actors, such as developers, owners, and users. The law could target the relevant culpable human actors instead. This objection has been raised against corporate punishment, too. Skeptics argue that corporate culpability is always fully reducible to culpable actions of individual humans. Any time a corporation does something intuitively culpable – like causing a harmful oil spill through insufficient safety procedures – this can always be fully reduced to the culpability of the individuals involved: the person carrying out the safety checks, the designers of the safety protocols, the managers pushing employees to cut corners in search of savings, etc. For any case offered to demonstrate the irreducibility of corporate culpability, a skeptic may use creative means to find additional wrongdoing by other individual actors further afield or in the past to account for the apparent corporate culpability.
This worry may not be as acute for AI as it is for corporations. AI seems able to behave in ways that are more autonomous from its developers than corporations are from their members. Corporations, after all, are simply composed of their agents, albeit

283
organized in particular structures. Also, AI may sometimes behave in ways that are less predictable and foreseeable than corporate conduct.
In any case, there are ways to block the reducibility worry for corporate culpability as well as AI. The simplest response is to recall that legal culpability is the concern, not moral blameworthiness. Specifically, it would be bad policy for criminal law to always allow any putative corporate criminal culpability to always be fully reduced to individual criminal liability. To ensure that corporate criminal culpability can always be reduced to individual criminal culpability would require criminalizing very minute bits of individual misconduct – momentary lapses of attention, the failure to perceive emerging problems that are difficult to notice, tiny bits of carelessness, mistakes in prioritizing time and resources, not being sufficiently critical of groupthink, and so on. Mature legal systems should not criminalize infinitely fine-grained forms of misconduct but rather should focus on broader and more serious categories of directly harmful misconduct that can be straightforwardly defined, identified, and prosecuted. Criminalizing all such small failures – and allowing law enforcement to investigate them – would be invasive and threatening to such values like autonomy and freedom of expression and association. It would also increase the risk of abuse of process. Instead, culpability deficits should exist in any well- designed system of criminal law, and this in turn creates a genuine need for corporate criminal culpability as an irreducible concept.
Similar reasoning could be employed for AI culpability. There is reason to think it would be a bad system that encourages law enforcement and prosecutors, any time an AI causes harm, to invasively delve into the internal activities of the organizations developing the AI in search of minute individual misconduct – perhaps even the slightest negligence or

284
failure to plan for highly unlikely exigencies. It would be a disturbingly invasive criminal justice system that creates a sufficient number of individual offenses to ensure that any potential AI culpability can always be fully reduced to individual crimes. Hence, where AI is concerned, the Reducibility Challenge – at least as applied to legal culpability – does not impose a categorical bar to punishing AI.
Spillover
A final retributivist challenge to punishing AI is the spillover problem, again familiar from the corporate context. Because corporate punishments – usually in the form of fines – amount to a hit to the corporation’s bottom line, these punishments inevitably spill over onto innocent shareholders. This may seem to violate the desert constraint against the state harming people in excess of their desert. The same objection has been raised against punishing AI. Christina Mulligan worries that “one could ... imagine situations where the notion of separating a rogue robot from its owner [or damaging or restricting the robot in punishing it] would create a disproportionate burden on the owner, for example if a robot was unique, unusually expensive relative to the harm caused, or difficult to replace.”21 This is just a version of the spillover problem. If an AI unforeseeably causes harm, it may seem unfair or disproportionate to its innocent owner or operator to damage the AI in punishment.
There are familiar responses to the spillover objection for corporations. First, one may contend that spillover does not qualify as punishment because it is not imposed on a
21 Mulligan, supra note 10 at 594.
 
285
shareholder for her offense. Nonetheless, this definitional answer is somewhat unsatisfying, as there clearly are strong reasons for the state not to knowingly harm innocent bystanders, even if it does not strictly count as punishment.
A better answer is that spillover is not a special problem for corporate or AI punishment. Most forms of punishment – including punishment of individual wrongdoers – have the potential to harm the innocent, as when a convicted person has dependent children. Spillover objections may simply expose general problems with criminal law. The fact that punishment tends to harm the innocent suggests a need to reform criminal law as well as prisons, reentry programs, and similar initiatives to lessen the collateral consequences of punishment of all types. In the corporate context, some have recently responded to the spillover objection by defending reforms to corporate punishments so the “pain” they impose is more accurately distributed to the culpable actors within the company who contributed to the crime. For example, Will Thomas argues that managers found to have contributed to a crime by the corporation should have their incentive compensation clawed back to satisfy the criminal fines that were levied against the corporation in the first instance.
When it comes to AI punishment, similar thinking applies. To the extent spillover is a concern, AI punishments should be narrowly tailored. Destroying an AI, for example, would be a blunt remedy that is more likely to harm the innocent. More tailored remedies could be implemented instead, such as reprogramming the AI or fines directed at responsible persons. In such ways, the punishment of AI systems could be crafted to minimize the spillover effects. Further, spillover may be less of a concern in the case of AI- generated crime, where there may be little nexus between AI punishment and harm to

286
innocent individuals. The spillover problem thus is not an absolute bar to AI punishment. It is an omnipresent problem with criminal punishment, which must be addressed for any novel mode of criminal punishment – whether for corporations or AI.
True Punishment
A final challenge to AI punishment, along the lines of the Eligibility Challenge, is that AI cannot be truly “punished.” Punishment and criminal conviction have been used so far synonymously, but under Hart’s definition, punishment “must involve pain or other consequences normally considered unpleasant.”22 Even if an AI is convicted of an offense and subject to negative treatment – such as being reprogrammed or terminated – this may not be punishment because AI cannot experience things as painful or unpleasant.
A first response is to argue that AI punishment does satisfy the definition of punishment, which only requires the treatment in question be normally considered unpleasant – not that it actually be unpleasant. This allows Hart’s definition to accommodate people who, for idiosyncratic reasons, do not experience their sentence as unpleasant. The mere fact that a convicted party overtly wants to be imprisoned, like the Norwegian mass murder Anders Bering Breivik who wanted to be convicted and imprisoned to further his political agenda, does not mean that doing so pursuant to a conviction ceases to be punishment. Something similar can be said for defendants who, perhaps like AI, are physically or psychologically incapable of experiencing pain or distress.
22 Hart at 4.
  
287
This response can be developed further. Why might punishment need to be normally regarded as unpleasant? Why does it still seem to be punishment, for example, to imprison a person who in no way experiences it as unpleasant or unwelcome? The answer may be that defendants can have interests that are objectively set back even when they do not experience these setbacks as painful, unpleasant, or unwelcome. Some philosophers argue that it is intrinsically bad for humans to have their physical or agential capacities diminished, regardless of whether this is perceived as negative.23 If correct, this suggests that Hart’s definition requires punishment to involve events that objectively set back interests, and that negative subjective experiences are merely one way to objectively set back interests.
Can AI have objective interests? Some philosophers argue that things like nutrition, reproduction, or physical damage are good or bad for biological entities like plants or animals.24 This is in virtue of something’s having identifiable functions that things can be good or bad for it. Most notably, Philippa Foot defends this sort of view (tracing it to Aristotle) when she argues that the members of a given species can be evaluated as excellent or defective by reference to the functions that are built into its characteristic form of life. If having interests in this broad function-based sense is all that is required for punishment to be sensible, then perhaps AI fits the bill. AI also has a range of functions –
      23
24
, in HARMING FUTURE PERSONS 139 (Melinda A.
and     eds., 2009).
FOOT,   26 (2001) (“features of plants and animals have what
Elizabeth
Harman,
Harming as Causing Harm
 Roberts
David T.
Wasserman
 PHILIPPA
NATURAL GOODNESS
one might call an ‘autonomous’, ‘intrinsic’, or as I shall say ‘natural’ goodness and defect that may have nothing to do with the needs or wants of the members of any other species of living thing”).

288
characteristic patterns of behavior needed to continue in good working order and to succeed at the tasks it characteristically undertakes. If living organisms can, in a thin sense, be said to have an interest in survival and reproduction, ultimately in virtue of their biological programming, then arguably an AI following digital programming could have interests in this thin sense as well.
Other philosophers reject this view and insist that only entities capable of having beliefs and desires, or at least phenomenal experiences such as of pleasure and pain, can truly be said to have interests that are normatively important. Legal philosopher Joel Feinberg takes the capacity for cognition as the touchstone full-blooded interests, that is as a precondition for having things really be good or bad for us. Although “Aristotle and Aquinas took trees [and plants] to have their own ‘natural ends,’” Feinberg denies plants “the status of beings with interests of their own” because “an interest, however the concept is finally to be analyzed, presupposes at least rudimentary cognitive equipment.”25 Interests, he thinks, “are compounded out of desires and aims, both of which presuppose something like belief, or cognitive awareness.” In this view, since AI is not capable of cognitive awareness, it cannot possess full-blooded interests of the kind Feinberg has in mind.
A stronger type of reply is to distinguish between conviction and punishment, where the latter covers the sentence to which the convicted party is subject. Even if no form of treatment can count as punishment unless the entity in question experiences it negatively, this is not a precondition for a conviction. Perhaps for it to be intelligible to
25 Joel   , , in Philosophy and 43,   (William T. Blackstone ed., 1974).
   Feinberg
The Rights of Animals and Unborn Generations
 Environmental Crisis
49–52

289
convict X of an offense, it is only required that X acted in ways that violated a prohibition, and this can be sensibly construed as culpable (a manifestation of insufficient regard). If so, then while punishing AI is not conceptually possible, applying criminal law to AI so that it can be convicted of offenses is. Thus, society may still benefit from AI convictions while not running afoul of the conceptual confusion in purporting to punish AI.
Convicting AI may require, or allow, subjecting other parties to punishment in place of the AI. Criminal law roundly rejects “vicarious punishment” when people are concerned – not least when it risks the injustice of strict criminal liability imposed on innocent actors. Corporate punishment may seem to involve vicarious punishment when officers or employees of the corporation are made to suffer due to the criminal fines imposed on the corporation. However, such cases are better understood not as vicarious liability, strictly speaking, but as convicting the corporation of an offense directly and then allowing the sentence to be distributed to the different individuals out of which the corporation is made up. In the case of an AI, it could be argued that if human owners or users accept responsibility for operating the AI safely, then where the AI is convicted of an offense in its own right, these responsible parties would be the appropriate persons to whom the sentence could be distributed by virtue of their voluntarily undertaking such responsibility.
A final type of reply, always available as a last resort, is that even if applying criminal law to AI is conceptually confused, it could still have good consequences to call it punishment when AI is convicted. This would not be to defend AI punishment from within existing criminal law principles but to suggest that there are consequentialist reasons to depart from them.

290
4 Feasible Alternatives
Punishing AI could have benefits, and it is not ruled out by the negative limitations and retributive preconditions of punishment. But this is not enough to conclude that the punishment of AI is justified. We are required to address the third main question in our theory of punishment: Would the benefits of punishing AI outweigh the costs, and would it be better than alternative solutions? These solutions may involve doing nothing, or relying on civil liability and regulatory responses, perhaps together with less radical or disruptive changes to criminal laws that target individuals.
Ideally a cost-benefit analysis would involve more than identifying various costs and benefits; it would include quantitative analysis. If only a single AI-generated crime is committed each decade, there would be far less need to address an AI criminal gap than if AI-generated crime were a daily occurrence. The absence of evidence suggesting that AI- generated crime is common counsels against taking potentially more costly actions now, but this balance may change as technological advances result in more AI activity.
First Alternative: The Status Quo
Begin by setting aside something of less concern: cases where responsibility for harmful AI conduct is fully reducible to the culpable conduct of individual human actors. A clear example would be where a hacker uses AI to steal funds from individual bank accounts. There is no need to punish AI in such cases, because existing criminal offenses, like fraud or computer crimes, are sufficient to respond to this type of behavior.

291
Even if additional computer-related offenses must be created to adequately deter novel crimes implemented with the use of AI, criminal law has further familiar tools at its disposal, involving individual-focused crimes, which provide other avenues of criminal liability when AI causes foreseeable harms. For example, as Gabriel Hallevy observes, cases of this sort could possibly be prosecuted under an innocent “agency model,” assuming AI can sensibly be treated as meeting the preconditions of an innocent agent, even if not of a fully criminally responsible agent in its own right. Under the innocent agency doctrine, criminal liability attaches to a person who acts through an agent who lacks capacity – such as a child. For instance, if an adult uses a five-year-old to deliver illegal drugs, the adult rather than the child would generally be criminally liable. This could be analogous to a person programming a sophisticated AI to break the law: The person has liability for intentionally causing the AI to bring about the external elements of the offense.
This doctrine requires intent – or at least the knowledge – that the innocent agent will cause the prohibited result in question. This means that in cases where someone does not intend or foresee that the AI system being used will cause harm, the innocent agency model does not provide a route to liability. In such cases, one could instead appeal to recklessness or negligence liability if AI creates a foreseeable risk of a prohibited harm. For example, if the developers or users of AI foresee a substantial and unjustified risk that an AI will cause the death of a person these human actors could be convicted of reckless homicide. If such a risk were merely reasonably foreseeable, but not foreseen, then lower forms of homicide liability would be available. Similar forms of recklessness or negligence liability could be adopted where the AI’s designers or users foresaw, or should have

292
foreseen, a substantial and unjustified risk of other kinds of harms as well – such as theft or property damage.
Hallevy also discusses this form of criminal liability for AI-generated harms, calling it the “natural and probable consequences model” of liability. This is an odd label, however, since the natural and probable consequences doctrine generally applies only when the defendant is already an accomplice to – i.e., the defendant intended to commit – the crime of another. More specifically, the natural and probable consequences rule provides that where A intentionally aids B’s underlying crime C1 (say theft), but then B also goes on to commit a different crime C2 (say murder), then A would be guilty of C2 as well, provided C2 is reasonably foreseeable.
Despite his choice of label, Hallevy seems alive to this complication and correctly observes that there are two ways in which negligence liability could apply to AI-generated harms that are reasonably foreseeable. He writes:
The natural-probable-consequence liability model [applied] to the programmer or user differ[s] in two different types of factual cases. The first type of case is when the programmers or users were negligent while programming or using the AI entity but had no criminal intent to commit any offense. The second type of case is when the programmers or users programmed or used the AI entity knowingly and willfully in order to commit one offense via the AI entity, but the AI entity deviated from the plan and committed some other offense, in addition to or instead of the planned offense.26
26 Hallevy, supra note 1 at 19.
 
293
In either sort of scenario, there would be a straightforward basis for applying existing criminal law doctrines to impose criminal liability on the programmers or users of an AI that causes reasonably foreseeable harms. Thus, no AI criminal gap exists here.
A slightly harder scenario involves reducible harms by AI that are not foreseeable, but this is still something criminal law has tools to deal with. Imagine hackers use an AI to drain a fund of currency, but this ends up unforeseeably shutting down an electrical grid that results in widespread harm. The hackers are already guilty of something – namely, the theft of currency (if they succeed) or the attempt to do so (if they failed). Therefore, our question here is whether the hackers can be convicted of any further crime by virtue of their causing harm through their AI unforeseeably taking down an electrical grid.
At first sight, it may seem that the hackers would be in the clear for the electrical grid. They could argue that they did not proximately cause those harms. Crimes like manslaughter or property damage carry a proximate cause requirement under which the prohibited harm must at least be a reasonably foreseeable type of consequence of the conduct that the actors intentionally carried out. But in this case, taking down the electrical grid and causing physical harm to human victims are assumed to be entirely unforeseeable, even to a reasonable actor in the defendant’s shoes.
Criminal law has tools to deal with this kind of scenario, too. This comes in the form of constructive liability crimes. These are crimes that consist of a base crime that requires mens rea, but where there then is a further result element as to which no mens rea is required. Felony murder is a classic example. Suppose one breaks into a home one believes to be empty in order to steal artwork. Thus, one commits the base crime of burglary. However, suppose further that the home turns out not to be empty, and the burglar startles

294
the homeowner who has a heart attack and dies. This could make the burglar guilty of felony murder. This is a constructive liability crime because the liability for murder is constructed out of the base offense (burglary) plus the death (even where this is unforeseeable). According to the leading theory of constructive liability crimes, they are normatively justifiable when the base crime in question (burglary) typically carries at least the risk of the same general type of harm as the constructive liability element at issue (death).
This tool, if extended to the AI case, provides a familiar way to hold the hackers criminally liable for unforeseeably taking down the electrical grid and causing physical harm to human victims. It may be beneficial to create a new constructive liability crime that takes a criminal act like the attempt to steal currency using AI as the base offense, and then takes the further harm to the electrical grid, or other property or physical harm, as the constructive liability element, which requires no mens rea (not even negligence) in order to be guilty of the more serious crime. This constructive liability offense, in a phrase, could be called “Causing Harm through Criminal Uses of AI.”
New crimes could be adopted to the extent there are not already crimes on the books that fit this mold. Indeed, in the present example, one may think there are already some available constructive liability crimes. Perhaps felony murder could be indicated insofar as attempting to steal currency may be a felony, and in cases where this conduct causes fatalities. However, this doctrine would be of no avail in respect to the property damage caused. This is the reason why a new crime like “Causing Harm through Criminal Uses of AI” would seem to be necessary. In any case, no AI criminal gap is present here

295
because criminal law has familiar tools available for dealing with unforeseeable harms of this kind.
Having set aside reducible harmful conduct by AI, consider a case of irreducible AI crime inspired by RDS. Suppose an AI is designed to purchase class materials for incoming Harvard students, but after being trained on data from online student discussions regarding engineering projects, the AI unforeseeably “learns” to purchase radioactive material on the dark web and has it shipped to student housing. Suppose the programmers of this “Harvard Automated Shopper” did nothing criminal in designing the system and had entirely lawful aims. Nonetheless, despite the reasonable care taken by the programmers – and subsequent purchasers and users of the AI (i.e., Harvard) – the AI caused student deaths.
In this hypothetical, there are no upstream actors who could be held criminally liable. Innocent agency is blocked as a mode of liability because the programmers, users, and developers of the AI do not have the intent or foresight that any prohibited or harmful results would ensue – as is required for innocent agency to be available. Moreover, if the risk of the AI purchasing the radioactive material is not reasonably foreseeable, then criminal negligence would also be blocked. Finally, constructive liability is not available in cases of this sort because there is no base crime – no underlying culpable conduct by the programmers and users of the AI – out of which their liability for the unforeseeable harms could be constructed.
One could imagine various attempts to extend existing criminal law tools to provide criminal liability for developers or users. Most obviously, new negligence crimes could be added for developers that make it a crime to develop systems that foreseeably could

296
produce a risk of any serious harm or unlawful consequence, even if a specific risk is unforeseeable. The problem is that this does not seem to amount to individually culpable conduct, as all activities and technologies involve some risk of some harm. The effect of this expansion of criminal law would be to stifle innovation and benefit commercial activities. If there were such a crime, all the early developers of the Internet would likely be guilty of it.
Second Alternative: The Costs of Punishing AI
Earlier discussions highlighted some of the potential costs of AI punishment, including conceptual confusion, expressive costs, and spillover. Even aside from these, punishment of AI would entail serious practical challenges as well as substantial changes to criminal law. Begin with a practical challenge: the mens rea analysis. For individuals, the mens rea analysis is generally how culpability is assessed. Causing a given harm with a higher mens rea like intent is usually seen as more culpable than causing the same harm with a lower mens rea like recklessness or negligence. But how do we make sense of the question of mens rea for AI?
Earlier, it was argued that for some AI, as for corporations, the mental state of an AI’s developer, owner, or user could be imputed under something like the respondeat superior doctrine. But for cases of AI-generated crime that are not straightforwardly reduced to human conduct – particularly where the harm is unforeseeable to designers and there is no upstream human conduct that is seriously unreasonable to be found – nothing like respondeat superior would be appropriate. Some other approach to AI mens rea would be required.

297
A regime of strict liability offenses could be defined for AI crimes. However, this would require a legislative work-around so that AI is deemed capable of satisfying the voluntary act requirement, applicable to all crimes. This would require major revisions to criminal law – it is far from an off-the-shelf solution. Alternately, a new legal fiction of AI mens rea, vaguely analogous to human mens rea, could be developed, but this too currently does not appear to be a workable solution. This approach could require expert testimony to enable courts to consider in detail how the relevant AI functions to assess whether it is able to consider legally relevant values and interests but did not weight them sufficiently, and whether the programming has the relevant behavioral dispositions associated with mens rea like intention or knowledge. Above, several arguments were presented that courts may use to find various mental states to be present in an AI. However, much more theoretical and technical work is required here, and this is not a first best option.
Mens rea and similar challenges related to the voluntary act requirement are only some of the practical problems to be resolved in order to make AI punishment workable. For instance, there may be enforcement problems with punishing distributed AI on a blockchain. Such AI may be particularly difficult to effectively combat or deactivate. Even assuming the practical issues are resolved, punishing AI would still require major changes to criminal law. Legal personality may be necessary to charge and convict an AI of a crime, and conferring legal personhood on AI would create a whole new mode of criminal liability, much the way that corporate criminal liability constitutes a new mode beyond individual criminal liability. There are problems with implementing such significant reform.
Over the years, there have been many proposals for extending some kind of legal personality to AI. Perhaps most famously, a 2017 report by the European Parliament called

298
on the European Commission to create a legislative instrument to deal with “civil liability caused by robots.”27 It further requested that the commission consider “a specific legal status for robots,” and “possibly applying electronic personality” as one solution to tort liability. Even in such a speculative and tentative form, this proposal proved highly controversial. More than 150 AI “experts” subsequently sent an open letter to the European Commission warning that “from an ethical and legal perspective, creating a legal personality for a robot is inappropriate whatever the legal status model.”28
Full-fledged legal personality for AI equivalent to that afforded to natural persons, with all the legal rights that natural persons enjoy, would clearly be inappropriate. To take a banal example, allowing AI to vote would undermine democracy, given the ease with which anyone looking to determine the outcome of an election could create AI systems to vote for a designated candidate. However, legal personality comes in many flavors, even for natural persons such as children who lack many adult rights and obligations. Crucially, no artificial person enjoys all the same rights and obligations as a natural person. The best- known class of artificial persons – companies – have long enjoyed only a limited set of rights and obligations that allows them to sue and be sued, enter contracts, incur debt, own property, and be convicted of crimes. However, they do not receive protection under constitutional provisions such as the Equal Protection Clause of the Fourteenth
27 EUR. PARL. DOC. (A8-0005/2017),
http://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+REPORT+A8- 2017-0005+0+DOC+XML+V0//EN.
28 Open Letter to the European Commission Artificial Intelligence and Robotics (April 5, 2018), https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp- content/uploads/2018/04/RoboticsOpenLetter.pdf.
     
299
Amendment, nor can they bear arms, run for or hold public office, marry, or enjoy other fundamental rights that are enjoyed by natural persons. Thus, granting legal personality to AI to allow it to be punished would not require AI to receive the rights afforded to natural persons, or even those afforded to companies. AI legal personality could consist solely of obligations.
Even so, any sort of legal personhood for AI would be a dramatic legal change that could prove problematic. As discussed earlier, providing legal personality to AI could result in increased anthropomorphisms. People anthropomorphizing AI expect it to adhere to social norms and have higher expectations regarding AI capabilities. This is problematic where such expectations are inaccurate, and the AI is operating in a position of trust. Especially for vulnerable users, such anthropomorphisms could result in “cognitive and psychological damages to manipulability and reduced quality of life.”29 These outcomes may be more likely if AI were held accountable by the state in ways normally reserved only for human members of society. Strengthening questionable anthropomorphic tendencies regarding AI could also lead to more violent or destructive behavior directed at AI, such as vandalism or attacks. Further, punishing AI could also affect human well-being in less direct ways, such as by producing anxiety about one’s own status within society due to the perception that AI is given a legal status on par with human beings.
Finally, conferring legal personality on AI may lead to rights creep. Even if AI is given few or no rights initially when first granted legal personhood, AI may gradually acquire rights over time. Granting legal personhood to AI may become an important step
29 Luisa   & Paul Dumouchel, Anthropomorphism in Human–Robot Co-evolution, , Mar. 26, 2018, https://doi.org/10.3389/fpsyg.2018.00468 l.
   Damiano
 FRONT. PSYCHOL.
 
300
down a slippery slope. In a 1933 Supreme Court opinion, for instance, Justice Louis Brandeis warns about rights creep, and that granting companies an excess of rights could allow them to dominate the state. Eighty years after that decision, Justice Brandeis’ concerns were prescient in light of recent Supreme Court jurisprudence such as Citizen’s United and Hobby Lobby, which significantly expanded the rights extended to corporations.30 Such rights for corporations and AI can restrict valuable human activities and freedoms.
Third Alternative: Minimally Extending Criminal Law
There are alternatives to direct AI punishment other than doing nothing. The problem of AI-generated crime would more reasonably be addressed through minimal extensions of existing criminal law. The most obvious would be to define new crimes for individuals. Just as the Computer Fraud and Abuse Act criminalizes gaining unauthorized access or information using personal computers, an AI Abuse Act could criminalize malicious or reckless uses of AI. In addition, such an act could criminalize the failure to responsibly design, deploy, test, train, and monitor the AI one contributed to developing. These new crimes would target individual conduct that is culpable along familiar dimensions, so they may be of limited utility with regard to AI-generated crimes that do not reduce to culpable actors. Accordingly, a different way to expand criminal law seems needed to address AI- generated crime.
30 Citizens United v. Fed. Election Comm’n, 558 U.S. 310, 341 (2010) and Burwell v. Hobby Lobby Stores Inc., 573 U.S. 682 (2014).
 
301
In cases of AI-generated crime, a designated adjacent person could be punished who would not otherwise be directly criminally liable – a “responsible person.” This could involve new forms of criminal negligence for failing to discharge statutory duties, perhaps relying on strict criminal liability, in order to make a person liable in cases of AI-generated crime. It could be a requirement for anyone creating or using an AI to ex ante register a responsible person for the AI. It could be a crime to design or operate AI capable of causing harm without designating a responsible person. This would be akin to the offense of driving without a license. The registration system could be maintained by a federal agency. However, a registration scheme is problematic because it is difficult to distinguish between AI capable of criminal activity and AI not capable of criminal activity, especially when dealing with unforeseeable criminal activity. Even simple and innocuous-seeming AI could end up causing serious harm. Thus, it may be necessary to designate a responsible person for any AI. Registration may involve substantial administrative burden and, given the increasing prevalence of AI, the costs associated with mandatory registration may outweigh any benefits.
A default rule rather than a registration system may be preferable. The responsible person could be the AI’s manufacturer or supplier if it is a commercial product. If it is not a commercial product, the responsible person could be the AI’s owner, developer if no owner exists, or user if no developer can be identified. Even noncommercial AI is usually owned as property, although this may not always be the case, for instance, with some open-source software. Similarly, all AI has human developers, and in the event an AI ever autonomously creates another AI, responsibility for the criminal acts of an AI-created AI could reach back to the original AI’s owner. In the event an AI’s developer cannot be identified, or potentially

302
if there are a large number of developers, again in the case of some open-source software, responsibility could attach to an AI’s user. However, this would fail to catch the rare (perhaps only hypothetical) case of the noncommercial AI with no owner, no identifiable developer, and no user. To the extent that a noncommercial AI owner, developer, and user working together would prefer a different responsibility arrangement, they could be permitted to agree to a different ex ante selection of the responsible person. This may be more likely to occur with sophisticated parties where there is a greater risk of AI-generated crime. The responsible person could even be an artificial person such as a corporation.
It would be possible to impose criminal liability on the responsible persons directly in the event of AI-generated crime. For example, if new statutory duties of supervision and care are defined regarding the AI for which the responsible person is answerable, criminal negligence liability could be imposed on the responsible person should he or she unreasonably fail to discharge those duties. Granted, this would not be punishment for the harmful conduct of the AI itself. Rather, it would be a form of direct criminal liability imposed on the responsible person for his or her own conduct.
More boldly, if this does not go far enough to address AI-generated crime, criminal liability could also be imposed on the responsible person on a strict liability basis – particularly if the relevant punishments are only fines rather than incarceration. Generally strict liability crimes are restricted to minor infractions or regulatory offenses or “violations,” though some examples of more serious strict criminal liability can also be found, such as statutory rape in some jurisdictions. This could be defended by claiming that there is a special duty owed to society at large to provide special assurances that certain especially serious risks will be mitigated as much as possible. A responsible person

303
accepting strict criminal liability could serve this function. Especially in the case of AI where user trust is critical to realizing the benefits of AI, this approach could be warranted to combat the perception that unsafe AI is being employed. Accordingly, AI could become another context in which strict criminal liability on the responsible person is imposed.
Yet there are serious problems with strict liability crimes applied to persons. If justifiable at all, they can only be used as a last resort in exigent circumstances – as in cases of unusually dangerous activities. However, it is not obvious that the use of AI qualifies as unusually dangerous. To the contrary, in many areas of activity it would be unreasonable not to use AI, as when safety can be improved over human actors such as will likely be the case with self-driving cars. Most bad human actors using AI systems to commit crimes will still be caught under existing criminal laws, and so far there have not been high-profile cases of AI-generated crime. As a result, AI-generated crime is likely not yet a significant enough social problem to merit the use of strict criminal liability.
At the end of the day, a responsible person regime accompanied by new statutory duties, which carry criminal penalties if these duties are negligently or recklessly breached, provides an attractive approach to dealing with AI-generated crime. While it is only a minimal expansion of criminal law, by expressing condemnation through a criminal conviction of the responsible person, much of the expressive benefit from a direct conviction of AI can be achieved – but without as serious a loss of public trust as the legal fictions needed to punish AI directly would create.

304
Fourth Alternative: Moderate Changes to Civil Liability
A further alternative to dealing with AI-generated crime is to look to civil law, primarily tort law, as a method of both imposing legal accountability and deterring harmful AI. Some AI crime will no doubt already result in civil liability; however, if existing civil liability falls short, new liability rules could be introduced. A civil liability approach could even be used in conjunction with expansions to criminal liability.
While it is beyond the scope of this chapter to canvas gaps in civil liability for AI crime, it is worth noting that existing civil liability comes with built-in limitations. Very few laws specifically address AI-generated harms, which means civil liability must usually be established under a traditional negligence or product liability framework or under contractual liability. Negligence generally requires a person to act carelessly, so where this cannot be established there may be no recovery. Product liability may require both that an AI be a commercial product (e.g., this may not apply where AI is used as a “service”) and that there be a defect in the product (or that its properties be falsely represented). In the case of complex AI, it may be difficult to prove a defect, and AI may cause harm without a “defect” in the product liability sense. Civil liability may also derive from contractual relationships, but this usually only applies where there is a contract between parties. This, too, may also have significant limitations.
To the extent there is inadequate civil liability for AI-generated crime, the responsible person proposal sketched previously could be repurposed so that the responsible person may only be civilly liable. The case against a responsible person could be akin to a tort action if brought by an individual or a class of plaintiffs, or a civil

305
enforcement action if brought by a government agency tasked with regulating AI. At trial, an AI would not be treated like a corporation, where the corporation itself is held to have done the harmful act and the law treats the company as a singular acting and “thinking” entity. Rather, the question for adjudication would be whether the responsible person discharged his or her duties of care in respect of the AI in a reasonable way – or else civil liability could also be imposed on a strict liability basis.
A responsible person scheme is not the only solution to inadequate civil liability for AI-generated crime. An insurance scheme is another approach. Owners, developers, or users of AI, or just certain types of AI, could pay a tax into a fund to ensure adequate compensation for victims of AI-generated crime. The cost of this tax would be relatively minor compared to the financial benefits of AI. This could either replace the responsible person solution or apply to cases where no appropriate responsible person exists. An AI compensation fund could operate like the National Vaccine Injury Compensation Program (VICP). Vaccines result in widespread social benefit but are known in rare cases to cause serious problems. VICP is a no-fault alternative to traditional tort liability that compensates individuals injured by a VICP-covered vaccine. It is funded by a tax on vaccines that users pay. Other models for an insurance scheme exist, such as the Price Anderson Act for nuclear power.
Further Thoughts
AI legal neutrality suggests that as AI becomes increasingly autonomous and acts in ways that cannot be traced to individuals, there may be benefits to directly criminally punishing

306
AI. However, at least for now, punishing AI would be an overreaction to all but largely hypothetical concerns. While AI is already presenting new challenges to criminal law, alternative approaches could provide substantially similar benefits and would avoid many of the problems with AI punishment. A simpler solution involves modest expansions to criminal law, including new negligence crimes centered around the improper design, operation, and testing of AI applications, as well as possible criminal penalties for designated parties who fail to discharge statutory duties. This could be supplemented by expanded civil liability.
Yet, as with the idea of AI paying taxes, criminally punishing AI is not as absurd an idea as it may initially seem. We already criminally punish artificial persons in the form of corporations, and punishing AI is not a radical departure from that and other precedents. Criminal law can – and, where corporations are involved, already does – appeal to elaborate legal fictions to provide a pragmatic tool for solving social problems. Nonetheless, legal fictions must be used with caution, as their overuse risks eroding public trust and weakening the rule of law. Moreover, allowing legal fictions to proliferate unchecked can lead to widespread injustice either through punishing the innocent or by punishing more harshly than one’s culpability calls for. For this reason, there is and should be an onerous burden to meet before we can be confident that a particular legal fiction – such as legal personality for AI or the invention of culpable mental states for AI – is adopted.

307
7
Alternative Perspectives on AI Legal Neutrality
We can only see a short distance ahead, but we can see plenty there that needs to be done.
- Alan Turing
1 What If There Is Never a Singularity?
Some of the arguments in this book give credibility to the predictions of technologists who believe the singularity will occur in our lifetimes. However, there are other prognosticators who think the potential of AI has been vastly overstated, and that the current round of predictions is mainly hype. What if automation does not increase higher unemployment rates, if self-driving cars never significantly exceed the performance of human drivers, and if AI inventors remain a permanent novelty? There may never be a singularity, or if there is, it may not come this century. If that is the case, is there value in a principle of AI legal neutrality?
Paradoxically, AI legal neutrality is more important in a world where AI does not dramatically outperform people. This is because laws that result in unequal treatment of AI will be most disruptive when people and AI are in relatively close competition. Ultimately, if AI is going to become not just more efficient than people but substantially more efficient, then differential legal treatment should only delay the inevitable. Today, the decision by McDonald’s to replace a person with a $35,000 robotic arm that does a mediocre job of bagging fries is influenced by tax policy. If ever the robot costs $3,500 and becomes faster

308
and more accurate than a person, McDonald’s will automate regardless of taxes levied on AI. As with the inevitability of invention, AI legal neutrality may accelerate rather than increase automation.
2 Should the Law Discourage the Singularity?
What if the technological optimists are correct? If a principle of AI legal neutrality encourages automation when it is more efficient, and AI will eventually be more efficient at essentially everything, does that mean we will eventually be living in a world where AI does everything and people have nothing to do? Would this be a dystopian nightmare in which people have lost the opportunity for work that gives them a sense of meaning and purpose? Should we deliberately legally disadvantage AI to prevent this outcome?
Discriminatory laws will not stop automation unless we adopt absolute prohibitions on using AI for certain activities, such as banning the use of AI in any commercial diagnosis or treatment of disease. Such a rule could ensure the supremacy of human decision-making in medicine and prevent the displacement of doctors, although it might be difficult to enforce if other countries fail to adopt similar prohibitions.
Prohibiting AI activity broadly would be undesirable. In health care, the result for patients would be inferior medical care at higher prices. It would also be undesirable for doctors, since even if they were inclined to find meaning through productivity, this would be an entirely pointless productivity. They would not really be helping others – a doctor would only be needed as a result of preventing a vastly superior AI from doing a better job. This would reenact, throughout the medical profession, the fable of Sisyphus, the cruel

309
Greek king punished by the gods in the afterlife and condemned to push a large boulder to the top of a steep hill for all eternity. Upon nearing the hill’s peak, the boulder would roll back resulting in Sisyphus’ having to begin pushing the rock up the hill again. The solution to managing the challenge of automation should not be a curse of endless, pointless (if not harmful) labor.
There may be limited circumstances, say decisions to use deadly force or to criminally punish a defendant, in which automation would be inappropriate. But even if we choose to reserve certain fields of activity for people, the world can only accommodate so many judges and police officers. This means – at some point – the majority of people may need to find a way to occupy their time with an activity other than work. In 1516, Thomas More writes in Utopia about an idealized communal society where the people would rise early, work six hours a day, and spend the rest of the day on their own but with a caveat: They would be expected to spend it wisely. In an AI utopia, there may be no need for people to work six hours a day – or at all. In whatever way people find meaning, in the future it may not be through simple productivity.
Plato believed that meaning comes from attaining knowledge. Perhaps, with a virtually unlimited amount of leisure time and resources, most of the populace would choose to engage in continuous self-improvement. Confucianists see meaning in human relationships, as Tu Weiming writes, “[W]e can realize the ultimate meaning of life in ordinary human existence.”1 Perhaps people would spend their time cultivating personal relationships. Jeremy Bentham thought the meaning of life is the greatest happiness
1 TU   ,   (Albany: State , 1985).
  WEIMING
CONFUCIAN THOUGHT: SELFHOOD AS CREATIVE TRANSFORMATION
 University of New York Press

310
principle in which the greatest happiness is brought to the greatest number of people. Perhaps, most people would spend their days enjoying virtual reality. It is beyond the scope of this book to establish the meaning of life, but it is sufficient to note that there are paths to meaning other than through work. Also, it may be paternalistic to dictate to others what it means to spend one’s time wisely, and certainly it is to force them to labor for their “own” benefit. This is not to say that the singularity is guaranteed to improve social welfare. But a realistic dystopian AI future is unlikely to be killer robots run amok or a life without meaning. We should be more concerned about a future in which AI only benefits a small number of individuals, particularly if this occurs at the expense of the less fortunate. The way to prevent that future is not by prohibiting automation but by having appropriate legal frameworks.
3 Is AI Legal Neutrality a Coherent Principle?
This book has argued that legal regimes tend to promote efficiency when they do not discriminate between people and AI, but that legally identical treatment would be undesirable. For instance, AI legal neutrality does not require direct criminal punishment of AI, direct AI liability for accidents, or AI ownership of intellectual property. One may therefore object that acceptance of the concept of AI legal neutrality would be impractical and even unworkable for policymakers who favor consistency in application.
These concerns apply to other AI principles. In 2019, the European Commission’s High-Level Expert Group on AI put forward guidelines of seven key requirements that AI systems should meet, including transparency, privacy, human agency, and

311
nondiscrimination.2 It is hard to object to any of these in the abstract. In practice, however, these requirements may conflict with other values. For instance, transparency may come at the expense of privacy. To understand how a connectionist AI functions may require access to its training inputs. However, AI training inputs often consist of personal data so making training data available for inspection may violate a right to privacy. That violation may be particularly acute with an AI’s being applied in, for example, health care. Even where a user’s information has been “deidentified,” it may sometimes be possible to reidentify a user. In another concern, transparency may require the disclosure of commercially valuable confidential information. Access to an AI’s code may be necessary to understand its function, but if competitors can access this code, they may be able to create similar products without violating intellectual property protections.
Similarly, human agency may conflict with nondiscrimination. Discrimination by algorithms, or algorithmic bias, is a long-standing concern. It is also one that more prominently entered the public consciousness as a result of reporting in 2016 by ProPublica about a proprietary sentencing algorithm, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), that ProPublica claimed systematically discriminated against black defendants. COMPAS is now helping judges make bail determinations (whether to release or secure someone in advance of their trial) by providing risk assessments of defendants. The algorithm more often labeled black defendants who subsequently did not reoffend as high risk, and white defendants who did
   2
, ETHICS GUIDELINES FOR
(Apr. 8, 2019), https://ec.europa.eu/digital-single-
AI HIGH LEVEL EXPERT GROUP (HLEG), EUROPEAN COMMISSION
 TRUSTWORTHY ARTIFICIAL INTELLIGENCE
 market/en/news/ethics-guidelines-trustworthy-ai.
 
312
reoffend as low risk.3 The company argued in response that at any specific risk level, there was a roughly equal proportion of white and black defendants.
Regardless of whether one specific AI made racially biased determinations, the broader concern of biased algorithms remains. Biases are an inevitable part of both AI and human decision-making, but some biases are both morally and legally unacceptable. In the case of racial bias, biased algorithms are not the result of a person deliberately engineering AI to be racist but may arise if, for example, connectionist AI learns based on biased training data. If human judges have historically sentenced defendants in a discriminatory fashion, AI may do so in the future.
While incorporating historical biases is a legitimate concern about AI, it ignores another problematic implication of algorithmic bias, which is that human judges may have been discriminating against minority defendants. If this is so, despite such discrimination being legally prohibited, then it is likely since judges have conscious or unconscious biases against certain groups. It can be very difficult to detect, much less alter, human biases. In 2019, France made publishing data analysis related to judicial decisions a crime punishable by five years’ imprisonment.4 The French government makes judicial decisions together with the names of judges publicly available, but it is illegal to aggregate data and create a record of a specific judge. A critic of this law would assume it is an attempt to prevent
3 See Julia Angwin, Jeff Larson, Surya Mattu, &     , Machine Bias, PROPUBLICA (May 23, 2016), www.propublica.org/article/machine-bias-riskassessments-in-criminal- sentencing.
4 France Bans Judge Analytics, 5 Years In Prison for Rule Breakers, ARTIFICIAL LAWYER (June 4, 2019), www.artificiallawyer.com/2019/06/04/france-bans-judge-analytics-5-years-in- prison-for-rule-breakers/.
   Lauren
Kirchner
      
313
demonstration of inconsistency, and potentially bias, in judgments. Existing studies have discovered “amazing disparities” in the outcomes of cases based on how a judge decides a case.5 Not only may human decision-making lack transparency, but it may have poorer explainability than that of AI. True, a judge, unlike some AI, can always be relied upon to explain a decision. However, that explanation is not guaranteed to be accurate. A judge who is consciously biased against a protected group is exceptionally unlikely to admit to such a bias. Instead, he or she will have to come up with a rationalized explanation for a sentence in a particular case, relying on the facts of the offense, a defendant’s personal circumstances, etc. And if a judge has an unconscious bias, then he or she is not even aware of this distortion in perspective.
The human mind may be more of an algorithmic black box than AI, which when properly queried is far more transparent about its internal rules. These rules can also be explicitly overwritten. Whereas a human judge can be told it is illegal to discriminate on the basis of race, and will no doubt agree to abide by such a restriction, no amount of training is guaranteed to internalize such a rule. Ceding some agency to an AI, which can be explicitly programmed to never consider race or even proxy variables, may be the best chance to avoid discrimination in a racially stratified world.
Transparency, privacy, human agency, and nondiscrimination are important principles, but they are conceptual guideposts rather than absolute prescriptions. Intelligent policymaking requires a decision maker to consider how to balance these concepts flexibly on a case-by-case basis. Different jurisdictions also have distinct cultural
5 See, e.g., Jaya   ,     , &     , Refugee Roulette: , 60   295 (2008).
 Philip G.
Schrag
 Ramji-Nogales
Andrew
Schoenholtz
 Disparities in Asylum Adjudication
STAN. L. REV.

314
and value preferences. For instance, some countries are more concerned with user privacy than others, and some are more concerned with encouraging entrepreneurship by giving greater rights in user data to private companies.
4 Protecting Spheres of Human Agency
A frequently invoked principle of AI regulation is maintaining the supremacy of human agency that may sometimes be at odds with the principle of AI legal neutrality. Should automation be excluded from some areas of human activity, such as creative work, even when it is more efficient? Harlan Howard famously described country music as “three chords and the truth.”6 That may not be entirely correct from a technical standpoint but it is certainly true that there are a finite number of tones that human ears can distinguish and a finite number of ways to combine them. Already, there are more human-composed songs than a person could listen to even if the songs were played one after the other, every day, for the entirety of that person’s life. When you calculate every possible five-minute basic audio recording, which would not only include what we think of as music, you get an incomprehensibly large number: something like 2^211,000,000.7 The number is significantly smaller when you limit the recording to tones people can distinguish, such as typical melodies. It becomes smaller still if an AI is only concerned with creating music
, Country Scribe Harlan Howard Dies, ROLLING STONE (Mar. 5, 2002), www.rollingstone.com/music/music-news/country-scribe-harlan-howard-dies-197596/. 7 Casey Chan,   ,   (Nov. 20, 2012), https://gizmodo.com/is-it-mathematically-possible-to-run-out-of-new-music- 5962375.
  Andrew
 Dansby
  6
 Is It Mathematically Possible to Run Out of New Music
GIZMODO
  
315
people are likely to enjoy. Still, by any measure, it remains a mind-bogglingly large number that only a futuristic AI with vastly expanded capabilities could possibly generate.
There may be limited value in generating every conceivable five-minute recording, given the practical impossibility of a person listening to all of that content, unless an AI could also evaluate its output for usefulness. But an AI could hypothetically determine which recordings are most likely to be enjoyable to people, or perhaps even to specific people.8 An AI could theoretically provide each person with their optimal musical playlist, rank ordered, with the songs they are most likely to enjoy every day for the rest of their lives. If such a system could be created, should it? There is clearly a benefit to creating new music that people will enjoy. But will this have a chilling effect on human creativity, or on the lives of musicians? The same questions apply to other creative areas. For instance, the human eye can only distinguish pictures to a certain resolution – a resolution already being exceeded by some cutting-edge monitors. With a finite number of pixels in a monitor, an AI should hypothetically be able to create every possible image. Will this disincentivize human photographers?
Of course, people will still create. People have not stopped playing chess, even though AI’s chess supremacy is now unquestioned. Self-improvement and competition against other people are still worthwhile endeavors. Likewise, long-distance running used to have serious practical importance when it was the quickest means of transmitting a message. In fact, the marathon race was inspired by the story of Pheidippides, who died running from Marathon to Athens to announce a military victory. Running long distances
  8 See, e.g.,     ,
Nicholas J
Hudson
Musical Beauty and Information Compression: Complex to the
 Ear but Simple to the Mind?,
4   9 (2011).
BMC RESEARCH NOTES

316
lost much of its practical importance with the introduction of faster means of transportation, yet people still run marathons. We value personal improvement, social aspects of activities, and competition against other people rather than machines. In fifty years, someone may make music because they enjoy the act of making music, or because there is a market specifically for music made by people. In an AI utopia, people may continue to be creative, but simply because they enjoy doing so.
What about spheres of activity where we are less concerned about protectionism and more concerned about preventing bad outcomes? AI lacks both common sense and an internal moral compass (although the same thing may be said about some people). Automation seems to give rise to special concern with respect to automating certain military or judicial activities. For instance, some of the most strenuous objections to automation revolve around “killer robots” or the concept of a lethal autonomous weapons system (LAWS), which can fully automate life and death decisions. It may be that there are some activities we never want to automate, regardless of efficiency. On the other hand, some of the arguments proposed against automation may not always hold up. Kenneth Anderson and Matthew C. Waxman have argued in favor of LAWS.9 They contend that LAWS will protect one’s own personnel as well as civilians. AI can respond more quickly than human soldiers, it does not act out of emotion (which can both restrain or unleash base instincts) or personal self-interest, and some human soldiers have historically failed
 9
&,
     Kenneth
Anderson
Matthew C.
Waxman
Debating Autonomous Weapon Systems, Their
   Ethics, and Their Regulation Under International Law
, in Roger
Brownsword
,
Eloise
    Scotford,
&
Karen
Yeung
, eds.,
THE OXFORD HANDBOOK OF LAW, REGULATION, AND TECHNOLOGY
Oxford:
Oxford University Press
(     , 2017). Available at SSRN: https://ssrn.com/abstract=2978359.
,
 
317
to respect the law of armed conflict. It is possible that for some military tasks, AI may be more humane than people. It is also notable that there are already some highly automated weapon systems, such as Israel’s Iron Dome antimissile system. That system could not effectively function without automation, as it detects incoming missiles and deploys countermeasures faster than a human soldier possibly could. Of course, the Iron Dome system is strictly defensive.
Even judging has not proven immune to automation. In late 2019, the government of Estonia initiated its program to have a “robot judge” adjudicate small claims cases.10 This may be the first instance of an AI outright taking the place of a judge, though it is too early to evaluate the success of the program and parties are able to appeal any decision to a person. To date, AI has done much more augmentation than automation of judging, and it has been more employed with respect to mediation, a process by which a neutral third party helps parties voluntarily settle a dispute, than to adjudication, a process by which a neutral third party (usually a judge or arbitrator) provides a binding resolution. The earliest AI to augment third parties was, descriptively enough, called Legal Decisionmaking System at the RAND Corporation in 1980. It helped settle product liability cases by determining liability, case value, and fair settlement value. Modern AI being operated this way includes a system called BOS, which the Prosecuting Authority in the Netherlands is using to determine punishment severity – much the way that COMPAS is being used in the United States.
10 Eric Niiler, Can AI Be a Fair Judge in Court? Estonia Thinks So, WIRED (Mar. 25, 2019), www.wired.com/story/can-ai-be-fair-judge-court-estonia-thinks-so/.
    
318
It makes sense to automate rather than augment when you have a conflict that may not justify a traditional third party’s involvement – perhaps for low-value, high-volume disputes, where the costs of litigation outweigh the amount in dispute. That was the case with eBay’s dispute resolution system, which was the first major online dispute resolution success. eBay uses a questionnaire-based AI expert system that performs the role of mediator, collecting information, identifying preferences, and suggesting resolution options. eBay reports its system manages sixty million disputes a year, 90 percent of which are resolved without human intervention. An eBay human third party gets involved if the AI cannot resolve matters. eBay also reports an 80 percent satisfaction rate. However, it should be noted that this system is not without its critics.
Automating eBay’s dispute system was an obvious business choice, because managing that volume of disputes with staff was not going to work for eBay, particularly given the low value of most disputes. It is hard to image courts absorbing all those cases, given that many courts already have substantial backlogs. Automation was easier, because of eBay’s detailed knowledge of the transactions in question and the narrow spectrum of disputes: Customers did not receive their items, the item was not as described, and so forth. In an early version of the system, a human mediator resolved disputes just using email. Automating that process increased both settlement rates and participants’ reported satisfaction. This was attributed to requiring participants to restrict their communications to preset options and limited numbers of characters, and to using AI to manage the flow of information. To paraphrase Frank Sander, AI can be a neutral to fit the fuss.11
11 F.E.A.   & S.B.   ,
, 10     (1994).
  Sander
Goldberg
Fitting the Forum to the Fuss: A User-Friendly Guide to
 Selecting an ADR Procedure
NEGOTIATION JOURNAL
49-67

319
Other AI systems similarly replace the third party, such as CyberSettle and SmartSettle, which use double-blind bidding. The essentials of the such systems and how they are used are basically the same: Parties provide an AI with their offers and bottom lines, and if these offers are within range of one another, the AI can split the difference or offer proposals that bridge a gap. This can change the dynamics of a human-led mediation, which often involve extensive advocacy and inquiry, to focus on the exchange of proposals. Some complex models exist, but at their base these systems are for people arguing over a dollar figure. CyberSettle claims to have handled more than 200,000 claims with a combined value of more than $1.6 billion, with an average reduction of settlement time of eighty-five percent.
AI can thus provide a convenient, fast, and inexpensive solution to many disputes. But when disputes become more complicated than standard eBay fare, things get a bit harder. Mediation can involve ambiguity, social and emotional issues, and interpersonal and cultural differences – all of which may be challenging for machines to resolve by following simple rules. AI also has a difficult time operating with norms and standards. Still, there’s enough potential benefit that AI use is expanding, and it is being applied to increasingly diverse sorts of disputes – even divorce cases, which get both complicated and emotional. Governments are also increasingly using AI for adjudication. For example, US states are using the system Matterhorn to manage outstanding warrants and traffic violations, small claims, and family disputes.
To successfully automate a wider range of disputes, AI needs to be more than efficient. It needs to be fair – or at least to behave fairly. Some critics think that will never happen, and that “machine-made justice” is no substitute for existing processes. In this

320
view, for anything outside of narrow domains or anything moderately complex, there is no substitute for a person’s reasoning and decision-making capabilities, not to mention common sense. Put another way, critics maintain that fairness and justice are exclusively human traits that AI cannot replicate.
Yet people long believed the same of creativity, and as we have already seen, AI can be creative if it is programmed to be creative. Likewise, AI can behave fairly if it is programmed to behave fairly. John McCarthy, who coined the term AI, maintains there is nothing a judge knows that we could not tell a computer.
5 The Risks and Unique Dangers of AI
The AI does not hate you, nor does it love you, but you are made
out of atoms which it can use for something else.
- Eliezer Yudkowsky
People and AI sometimes act in ways that are functionally interchangeable, but there are a substantial number of respects in which they differ. These differences have been highlighted in a body of literature on AI’s risks and negative effects. Scholarship has focused on AI’s opacity and lack of explainability, design choices that result in bias, negative impacts on personal well-being and social interactions, and on how AI has changed power dynamics in concerning ways between users and private companies (e.g., Facebook’s knowledge about its users) and between citizens and the state (e.g., profiling and surveillance). Merely having the potential to be neutral and “transcend politics” is no guarantee that AI will not be applied objectionably or co-opted by bad actors. Also, just as

321
AI may identify and solve problems that we are not aware of, AI may cause harm in unpredictable ways. Consider Nick Bostrom’s Paperclip Maximiser thought experiment, a sort of modern-day version of the Sorcerer’s Apprentice.12 If a sufficiently unconstrained and powerful AI is given the goal of maximizing paperclip production, it may determine the best means of achieving this goal is to destroy competing sources of manufacturing, obtain resources through war, or even to eliminate people it decides are likely to interfere with paperclip production. AI may not have humanlike motives, and so may act in ways we find arbitrary or unexpected. All these risks may seem to counsel against using AI, or at least proceeding cautiously.
These are legitimate concerns that should not be minimized. Indeed, managing these risks should be one of the primary goals of a legal regime responsive to AI. That AI and people can act in different ways and generate different sorts of harms is all the more reason to have laws that have been explicitly designed with AI in mind. AI legal neutrality should not be equated with a lack of regulation, and not all AI challenges will be solved by improving efficiency; other principles like accountability, beneficence, responsibility, nonmaleficence, and privacy may be more important in some cases. Also, while there are benefits to nonbinding, voluntary ethical codes to guide conduct, these same principles should also be the foundation of compulsory regulatory frameworks where the risks are sufficiently great. Managing risk is not a novel problem for the law, and many of the underlying concerns with AI can be dealt with under existing schemes or with modest
  12 Nick   ,   , in 2 Smit et al. eds., 2003).
(I.
Bostrom
Ethical Issues in Advanced Artificial Intelligence
COGNITIVE, EMOTIVE
  AND ETHICAL ASPECTS OF DECISION MAKING IN HUMANS AND IN ARTIFICIAL INTELLIGENCE
12–17

322
changes. We already have legal frameworks that provide a balance between state authority and individual privacy rights. Those frameworks may need to be adjusted, say, once AI- based facial recognition is ubiquitous, perhaps with new laws specific to certain applications of technology.
The prospect exists of more disruptive risks, such as a malevolent or indifferent AI, but these are not unmanageable. Even leaving our current political environment aside, there has been no historical shortage of malevolent or indifferent human rulers. We may be exceptionally unlikely to have a person destroy the world for the sake of paperclips, but people may end up destroying the world as a result of nuclear war or climate change. Part of the purpose of the law and our systems of checks and balances is to constrain bad behavior and manage risk, whether from an insane policy maker or a James Bond style techno-villain. With AI we may be exchanging one set of risks for another, but we are not necessarily accepting a worse set of risks or risks less susceptible to management. The mere possibility of harm does not mean we should not embrace activities, or new technologies, where the benefits are likely to outweigh the costs. And, while there are plenty of legitimate risks with AI, there is no reason to think that an AI able to act intelligently will have self-awareness and be driven by a desire for self-preservation.
6 Concluding Thoughts
Artificial intelligence may be the most disruptive technology ever created, but it is not guaranteed to improve our lives. The way to ensure it does is through enacting appropriate laws and policies. In deciding how to treat AI, policymakers should be concerned with the

323
functionality of machines and consequentialist benefits. What legal rules will result in the greatest social benefit from these technologies? At the end of the day, if we see a car coming at us, we do not care whether it is a self-driving Tesla with an unpredictable neural network, a self-driving Uber using “good old-fashioned AI,” or a human driver – we just do not want to be run over.
We examined a few areas of the law where AI and people are treated differently and saw the resulting unintended consequences. Of the many principles relevant for AI regulation, it is important to include the principle of AI legal neutrality to ensure that unnecessary barriers are not erected that prevent us from realizing the benefits of AI. Artificial intelligence is not part of our moral community, but it needs to be part of our legal community to promote human welfare. In time, as AI increasingly outperforms people at exercising judgment and eventually becomes the accepted way in which we solve problems, AI should replace us in our legal standards. The ancient Greek philosopher Protagoras claims that “man is the measure of all things.” In our not-too-distant future, AI may be the measure of all things. The challenge then becomes less a question of how do we regulate AI and more of how we should regulate ourselves.

324
I THINK, THEREFORE I INVENT: CREATIVE COMPUTERS AND THE FUTURE OF PATENT LAW
RYAN ABBOTT*
Abstract: Artificial intelligence has been generating inventive output for dec- ades, and now the continued and exponential growth in computing power is poised to take creative machines from novelties to major drivers of economic growth. In some cases, a computer’s output constitutes patentable subject matter, and the computer rather than a person meets the requirements for inventorship. Despite this, and despite the fact that the Patent Office has already granted pa- tents for inventions by computers, the issue of computer inventorship has never been explicitly considered by the courts, Congress, or the Patent Office. Drawing on dynamic principles of statutory interpretation and taking analogies from the copyright context, this Article argues that creative computers should be consid- ered inventors under the Patent and Copyright Clause of the Constitution. Treat- ing nonhumans as inventors would incentivize the creation of intellectual proper- ty by encouraging the development of creative computers. This Article also ad- dresses a host of challenges that would result from computer inventorship, in- cluding the ownership of computer-based inventions, the displacement of human inventors, and the need for consumer protection policies. This analysis applies broadly to nonhuman creators of intellectual property, and explains why the Copyright Office came to the wrong conclusion with its Human Authorship Re- quirement. Finally, this Article addresses how computer inventorship provides in- sight into other areas of patent law. For instance, computers could replace the hy- pothetical skilled person that courts use to judge inventiveness. Creative comput- ers may require a rethinking of the baseline standard for inventiveness, and po- tentially of the entire patent system.
INTRODUCTION
An innovation revolution is on the horizon.1 Artificial intelligence (“AI”) has been generating inventive output for decades, and now the contin-
© 2016, Ryan Abbott. All rights reserved.
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assis- tant Professor, David Geffen School of Medicine at University of California, Los Angeles. Thanks to Ian Ayres, Martin Keane, John Koza, Lisa Larrimore-Ouellette, Mark Lemley, and Steven Thaler for their insightful comments.
1 See, e.g., JAMES MANYIKA ET AL., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY 40 (2013) (predicting that the automation
1079
 
325
1080 Boston College Law Review [Vol. 57:1079
ued and exponential growth in computing power is poised to take creative ma- chines from novelties to major drivers of economic growth.2 A creative singu- larity in which computers overtake human inventors as the primary source of new discoveries is foreseeable.
This phenomenon poses new challenges to the traditional paradigm of patentability. Computers already are generating patentable subject matter under circumstances in which the computer, rather than a human inventor, meets the requirements to qualify as an inventor (a phenomenon that this Article refers to as “computational invention”).3 Yet, it is not clear that a computer could be an inventor or even that a computer’s invention could be patentable.4 There is no statute addressing computational invention, no case law directly on the subject, and no pertinent Patent Office policy.5
These are important issues to resolve. Inventors have ownership rights in their patents, and failure to list an inventor can result in a patent being held invalid or unenforceable. Moreover, government policies encouraging or inhib- iting the development of creative machines will play a critical role in the evo- lution of computer science and the structure of the research and development (“R&D”) enterprise.6 Soon computers will be routinely inventing, and it may only be a matter of time until computers are responsible for most innovation.
of knowledge work “could have as much as $5.2 trillion to $6.7 trillion in economic impact annually by 2025”).
2 See infra notes 275–278 and accompanying text.
3 See infra notes 28–99 and accompanying text; see also Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH (Cassidy R. Sugimoto, Hamid R. Ekbia & Michael Mattioli eds., forthcoming Oct. 2016) (discussing computational invention in an essay originally posted online on February 19, 2015).
4 See, e.g., Ralph D. Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up?, 71 TUL. L. REV. 1675, 1681, 1702–03 (1997) (arguing the output of creative computers cannot and should not be protected by federal intellectual property laws and that such results enter the public domain); see also Pamela Samuelson, Allocating Ownership Rights in Computer-Generated Works, 47 U. PITT. L. REV. 1185, 1199–1200 (1986) (arguing that computers cannot be authors because they do not need incentives to generate output). Pamela Samuel- son, arguing against considering computers to be authors, states that, “[o]nly those stuck in the doctri- nal mud could even think that computers could be ‘authors.’” Id. at 1200.
5 See Ben Hattenbach & Joshua Glucoft, Patents in an Era of Infinite Monkeys and Artificial Intelligence, 19 STAN. TECH. L. REV. 32, 44 & n.70 (2015) (noting no pertinent results from “a search for patent cases discussing genetic programming or computer-aided drug discovery (perhaps the two most common means of computerized inventive activity)” and that “[o]f a sampling of issued patents that were conceived wholly or in part by computers, none have ever been subject to litigation.”); see also ROBERT PLOTKIN, THE GENIE IN THE MACHINE 60 (2009). “Patent Office” refers to the U.S. Patent and Trademark Office (“USPTO”), the federal agency responsible for granting patents and registering trademarks. See About Us, USPTO, http://www.uspto.gov/about-us [https://perma.cc/ 6HZY-V9NU] (last visited Jan. 27, 2016).
6 See generally Michael Kremer & Heidi Williams, Incentivizing Innovation: Adding to the Tool Kit, 10 INNOVATION POL’Y & ECON. 1 (2010) (discussing the importance of intellectual property rights for promoting innovation).
 
326
2016] Patent Generating Artificial Intelligence 1081
This Article addresses whether a computer could and should be an inven- tor for the purposes of patent law as well as whether computational inventions could and should be patentable.7 It argues that computers can be inventors be- cause although AI would not be motivated to invent by the prospect of a pa- tent, computer inventorship would incentivize the development of creative ma- chines.8 In turn, this would lead to new scientific advances.
Beyond inventorship concerns, such machines present fascinating ques- tions: Are computers thinking entities? Who should own the rights to a com- puter’s invention? How do animal artists differ from artificial intelligence? What would be the societal implications of a world in which most inventions were created by computers? Do creative computers challenge established norms in other areas of patent law? This Article attempts to resolve these ques- tions as well as some of the other philosophical, societal, and even apocalyptic concerns related to creative computers.9
This Article is divided into three parts.10 Part I examines instances in which AI has created patentable inventions.11 It finds that machines have been autonomously generating patentable results for at least twenty years and that the pace of such invention is likely increasing.12 It proceeds to discuss the cri- teria for inventorship and to examine the roles of humans and computers in the inventive process. It concludes that statutory language requiring inventors to be individuals and judicial characterization of invention as a “mental act” pre- sent barriers to computer inventorship, but that otherwise computers inde- pendently meet the requirements for inventorship. Finally, Part I notes that ap- plicants seem not to be disclosing the role of creative computers to the Patent Office—likely as a result of uncertainty over whether a computer inventor would render an invention unpatentable. Applicants may also be able to legally circumvent such disclosure by being the first human to discover a computer’s patentable result, but this Article will discuss how that approach is unfair, inef- ficient, and logistically problematic.
Part II examines the jurisprudence related to nonhuman authorship of copyrightable material in the absence of law on the subject of computer inven- torship.13 It discusses the history of the Copyright Office’s Human Authorship
7 See infra notes 23–138 and accompanying text.
8 See infra notes 139–239 and accompanying text.
9 See infra notes 230–313 and accompanying text.
10 See infra notes 23–138, 139–239, and 240–312 and accompanying text.
11 See infra notes 23–138 and accompanying text.
12 See, e.g., John R. Koza, Human-Competitive Results Produced by Genetic Programming, in 11
GENETIC PROGRAMMING & EVOLVABLE MACHINES 251, 251 (2010) [hereinafter Koza, Human- Competitive Results] (“[T]he increased availability of computing power (through both parallel compu- ting and Moore’s law) should result in the production, in the future, of an increasing flow of human- competitive results, as well as more intricate and impressive results.”).
13 See infra notes 139–239 and accompanying text.
 
327
1082 Boston College Law Review [Vol. 57:1079
Requirement,14 and scrutinizes case law interpreting the Patent and Copyright Clause.15 On the basis of this analysis, and based on principles of dynamic statutory interpretation,16 it argues that computers should qualify as legal in- ventors.
This would incentivize the development of creative machines consistent with the purpose and intent of the Founders and Congress. The requirement that inventors be individuals was designed to prevent corporate ownership,17 and so computer inventorship should not be prohibited on this basis. Also, there should be no requirement for a mental act because patent law is con- cerned with the creativity of an invention itself rather than the subjective men- tal process by which an invention may have been achieved.18 This Part con- cludes by addressing objections to computer inventorship including arguments that computational inventions would develop in the absence of patent protec- tion at non-monopoly prices.
Finally, Part III addresses challenges posed by computer inventorship, and generalizes the analysis of earlier sections.19 It finds that a computer’s owner should be the default assignee of any invention, both because this is most consistent with the rules governing ownership of property, and because it would most incentivize innovation. Where a computer’s owner, developer, and user are different entities, such parties could negotiate alternative arrangements by contract. Computer ownership here generally refers to software ownership, although there may be instances in which it is difficult to distinguish between hardware and software, or even to identify a software “owner.”20 This Part also examines the phenomenon of automation and the displacement of human in- ventors by computers. It finds that computational invention remains beneficial despite legitimate concerns and that for the foreseeable future computers are likely to refocus human inventors rather than replace them.
Part IV concludes by finding the arguments in support of computer inven- torship apply with equal force to nonhuman authors. Allowing animals to cre- ate copyrightable material would result in more socially valuable art by creat- ing new incentives for people to facilitate animal creativity.21 It would also
14 See U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 306 (3d ed. 2014).
15 See U.S. CONST. art. I, § 8, cl. 8.
16 See generally William N. Eskridge, Jr., Dynamic Statutory Interpretation, 135 U. PA. L. REV. 1479 (1987) (identifying principles of dynamic statutory interpretation).
17 See infra notes 122–132 and accompanying text.
18 See, e.g., The “Flash of Genius” Standard of Patentable Invention, 13 FORDHAM L. REV. 84, 85–86 (1944).
19 See infra notes 240–312 and accompanying text.
20 See generally GOVERNMENT OFFICE FOR SCIENCE, DISTRIBUTED LEDGER TECHNOLOGY: BEYOND BLOCK CHAIN (describing algorithmic technologies and distributed ledgers as examples of new and disruptive computational paradigms).
21 See infra notes 279–287 and accompanying text.
 
328
2016] Patent Generating Artificial Intelligence 1083
provide incentives for environmental conservation.22 Lastly, this Article exam- ines some of the implications of computer inventorship for other areas of pa- tent law. Computers are a natural substitute for the person having ordinary skill in the art (“PHOSITA” or, simply, the “skilled person”) used to judge a pa- tent’s inventiveness. The skilled person is presumed to know of all the prior art (what came before an invention) in a particular field—a legal fiction that could be accurate in the case of a computer. Substituting a computer for the skilled person also suggests a need to expand the scope of prior art, given that com- puters are not limited by human distinctions of scientific fields. This would make it more challenging for inventions to be held nonobvious, particularly in the case of inventions that merely combine existing elements in a new configu- ration (combination patents). That would be a desirable outcome, although the new test would create new challenges.
I. CREATIVE COMPUTERS AND PATENTLAW
This Part investigates instances when AI has created patentable inven- tions.23 It finds that machines have been autonomously generating patentable results for at least twenty years and that the pace of such invention is likely increasing.24 This Part proceeds to discuss the criteria for inventorship and to examine the roles of humans and computers in the inventive process.25 It con- cludes that statutory language requiring inventors to be individuals and judicial characterizations of invention as a “mental act” present barriers to computer inventorship, but that computers independently meet the requirements for in- ventorship otherwise.26 Finally, this Part notes that applicants seem not to be disclosing the role of creative computers to the Patent Office—likely as a re- sult of uncertainty over whether a computer inventor would render an inven- tion unpatentable.27
A. Computers Independently Generate Patentable Results
1. Example One: The Creativity Machine
Computers have been autonomously creating inventions since the twenti- eth century. In 1994, computer scientist Stephen Thaler disclosed an invention
22 See infra notes 279–287 and accompanying text.
23 See infra notes 23–138 and accompanying text.
24 See, e.g., Koza, Human-Competitive Results, supra note 12, at 251 (“[T]he increased availabil-
ity of computing power (through both parallel computing and Moore’s law) should result in the pro- duction, in the future, of an increasing flow of human-competitive results, as well as more intricate and impressive results.”).
25 See infra notes 100–121 and accompanying text. 26 See infra notes 122–132 and accompanying text. 27 See infra notes 133–138 and accompanying text.
 
329
1084 Boston College Law Review [Vol. 57:1079
he called the “Creativity Machine,” a computational paradigm that “came the closest yet to emulating the fundamental mechanisms responsible for idea for- mation.”28 The Creativity Machine is able to generate novel ideas through the use of a software concept referred to as artificial neural networks—essentially, collections of on/off switches that automatically connect themselves to form software without human intervention.29
At its most basic level, the Creativity Machine combines an artificial neu- ral network that generates output in response to self-stimulation of the net- work’s connections together with another network that perceives value in the stream of output.30 This results in an AI that “brainstorms” new and creative ideas after it alters (perturbs) the connections within its neural network.31 An example of this phenomenon occurred after Dr. Thaler exposed the Creativity Machine to some of his favorite music, and the machine proceeded to write eleven thousand new songs in a single weekend.32
Dr. Thaler compares the Creativity Machine and its processes to the hu- man brain and consciousness.33 The two artificial neural networks mimic the human brain’s major cognitive circuit: the thalamo-cortical loop.34 In a simpli- fied model of the human brain, the cortex generates a stream of output (or con- sciousness), and the thalamus brings attention (or awareness) to ideas that are of interest.35 Like the human brain, the Creativity Machine is capable of gener- ating novel patterns of information rather than simply associating patterns, and it is capable of adapting to new scenarios without additional human input.36 Also like the human brain, the AI’s software is not written by human beings—
28 See What Is the Ultimate Idea?, IMAGINATION ENGINES INC., http://www.imagination-engines. com [https://perma.cc/P877-F33B] (last visited Jan. 25, 2016).
29 The architecture for the Creativity Machine is discussed in greater detail in several publica- tions. See, e.g., Stephen L. Thaler, Synaptic Perturbation and Consciousness, 6 INT’L J. MACHINE CONSCIOUSNESS 75 (2014) [hereinafter Thaler, Synaptic Perturbation and Consciousness]; Stephen Thaler, Creativity Machine® Paradigm, in ENCYCLOPEDIA OF CREATIVITY, INVENTION, INNOVA- TION, AND ENTREPRENEURSHIP 451 (Elias G. Carayannis ed., 2013) [hereinafter Thaler, Creativity Machine® Paradigm]; S.L. Thaler, A Proposed Symbolism for Network-Implemented Discovery Pro- cesses, WORLD CONGRESS ON NEURAL NETWORKS ’96, SAN DIEGO 1265 (Int’l Neural Network Soc’y 1996) [hereinafter, Thaler, A Proposed Symbolism].
30 See Aaron M. Cohen, Stephen Thaler’s Imagination Machines, THE FUTURIST, July–Aug. 2009, at 28, 28–29.
31 See Thaler, A Proposed Symbolism, supra note 29, at 1265–68.
32 See Tina Hesman, Stephen Thaler’s Computer Creativity Machine Simulates the Human Brain, ST. LOUIS POST-DISPATCH, Jan. 24, 2004, available at http://www.mindfully.org/Technology/ 2004/Creativity-Machine-Thaler24jan04.htm [https://perma.cc/T8HS-C2TB].
33 Thaler, Creativity Machine® Paradigm, supra note 29, at 447.
34 Id.
35 Id.
36 See Artificial Neural Networks, IMAGINATION ENGINES INC., http://imagination-engines.
com/iei_ann.php [https://perma.cc/BB8K-G3FH] (last visited Jan. 25, 2016); IEI’s Patented Creativity Machine® Paradigm, IMAGINATION ENGINES INC., http://imagination-engines.com/iei_cm.php [https://perma.cc/4A8A-6H3Y] (last visited Jan. 25, 2016).
 
330
2016] Patent Generating Artificial Intelligence 1085
it is self-assembling.37 Dr. Thaler argues his AI is very different from a soft- ware program that simply generates a spectrum of possible solutions to a prob- lem combined with an algorithm to filter for the best ideas generated.38 He notes that such a software program would be another method for having an AI developing novel ideas.39
Dr. Thaler invented the Creativity Machine, and the machine was the sub- ject of his first patent, titled “Device for the Autonomous Generation of Useful Information.”40 The second patent filed in Dr. Thaler’s name was “Neural Network Based Prototyping System and Method.”41 Dr. Thaler is listed as the patent’s inventor, but he states that the Creativity Machine invented the pa- tent’s subject matter (the “Creativity Machine’s Patent”).42 The Creativity Ma- chine’s Patent application was first filed on January 26, 1996, and granted on December 22, 1998.43
As one of Dr. Thaler’s associates observed in response to the Creativity Machine’s Patent, “Patent Number Two was invented by Patent Number One. Think about that. Patent Number Two was invented by Patent Number One!”44 Aside from the Creativity Machine’s Patent, the machine is credited with nu- merous other inventions: the cross-bristle design of the Oral-B CrossAction toothbrush, new super-strong materials, and devices that search the Internet for messages from terrorists, among others.45
The Creativity Machine’s Patent is interesting for a number of reasons. If Dr. Thaler’s claims are accurate, then the Patent Office has already granted, without knowing it has done so, a patent for an invention created by a non- human inventor—and as early as 1998. Also, the Patent Office apparently had no idea it was doing so. Dr. Thaler listed himself as the inventor on the patent
37 See Cohen, supra note 29.
38 See Telephone Interview with Stephen Thaler, President and CEO, Imagination Engines, Inc. (Jan. 10, 2016) [hereinafter Thaler, Telephone Interview].
39 See id.
40 See U.S. Patent No. 5,659,666 (filed Oct. 13, 1994).
41 See U.S. Patent No. 5,852,815 (filed May 15, 1998).
42 See Patent Listing, IMAGINATION ENGINES INC., http://imagination-engines.com/iei_ip.php
[https://perma.cc/N79N-NWEF] (last visited Jan. 25, 2016).
43 U.S. Patent No. 5,852,815 (filed May 15, 1998). This application is a divisional of application
with serial number 08/592,767 filed Jan. 26, 1996. This means the patent was invented sometime before January 26, 1996. Patent applications require an inventor to actually or constructively possess the invention at the time an application is filed to meet enablement and written description require- ments. See U.S. PATENT & TRADEMARK OFFICE, MANUAL OF PATENT EXAMINING PROCEDURE § 2164 (9th ed. Rev 7, Nov. 2015) [hereinafter MPEP].
44 Hesman, supra note 32 (quoting Rusty Miller).
45 Thaler, Creativity Machine® Paradigm, supra note 29, at 451. Table 1 contains a list of Crea- tivity Machine accomplishments. Id.
 
331
1086 Boston College Law Review [Vol. 57:1079 and did not disclose the Creativity Machine’s involvement to the Patent Office.
The patent’s prosecution history contains no mention of a computer inventor.46
2. Example Two: The Invention Machine
The Creativity Machine has not been the only source of computational invention.47 Software modeled after the process of biological evolution, known as Genetic Programming (“GP”), has succeeded in independently generating patentable results. 48 Evolution is a creative process that relies on a few simple processes: “mutation, sexual recombination, and natural selection.”49 GP emu- lates these same methods digitally to achieve machine intelligence.50 It delivers human-competitive intelligence with a minimum amount of human involve- ment.51
As early as 1996, GP succeeded in independently generating results that were the subject of past patents.52 By 2010, there were at least thirty-one in- stances in which GP generated a result that duplicated a previously patented invention, infringed a previously issued patent, or created a patentable new invention.53 In seven of those instances, GP infringed or duplicated the func- tionality of a twenty-first century invention.54 Some of those inventions were on the cutting edge of research in their respective fields.55 In two instances, GP may have created patentable new inventions.56
46 The file history for this patent is available from a search of the USPTO’s website. Patent Ap- plication Information Retrieval, USPTO, http://portal.uspto.gov/pair/PublicPair [https://perma. cc/7PAM-3EG7] (last visited Jan. 27, 2016). Patent applicants have a duty of candor and good faith in dealing with the Office, which includes a duty to disclose to the Office all information known to be material to patentability. 37 C.F.R. § 1.56 (2012). Indeed, Dr. Thaler completed an inventor’s oath or declaration stating that he disclosed to the Office all information known to be material to patentability including the identity of all inventors. See 35 U.S.C. § 115 (2012); MPEP, supra note 43, § 602.01(b) (listing the standard for patents filed before September 16, 2012). Such oaths are made under penalty of fine or imprisonment, and willful false statements may jeopardize the validity of an application and any future patents. 35 U.S.C. § 115; MPEP, supra note 43, § 602.01(a)–(b).
47 See generally Jon Rowe & Derek Partridge, Creativity: A Survey of AI Approaches, 7 ARTIFI- CIAL INTELLIGENCE REV. 43 (1993) (detailing sources of computational inventions).
48 Koza, Human-Competitive Results, supra note 12, at 265. Alan Turing identified GP as a method of creating machine intelligence in his 1950 report Intelligent Machinery. A.M. TURING, IN- TELLIGENT MACHINERY 18 (1948) (“[T]he genetical or evolutionary search by which a combination of genes is looked for, the criterion being the survival value.”).
49 John R. Koza et al., Evolving Inventions, SCI. AM., Feb. 2003, at 52. 50 See id.
51 See id.
52 See Koza, Human-Competitive Results, supra note 12, at 255–56, 265. 53 See id.
54 See id.
55 See Koza et al., Evolving Inventions, supra note 49, at 52.
56 Koza, Human-Competitive Results, supra note 12, at 265. These two instances are the inventive
act described in U.S. Patent No. 6,847,851 (filed July 12, 2002) and JOHN R. KOZA ET AL., GENETIC PROGRAMMING IV: ROUTING HUMAN-COMPETITIVE MACHINE INTELLIGENCE 102–04 (2003).
 
332
2016] Patent Generating Artificial Intelligence 1087
The Patent Office granted another patent for a computational invention on January 25, 2005.57 That invention was created by the “Invention Machine”— the moniker for a GP-based AI developed by John Koza.58 Dr. Koza is a com- puter scientist and pioneer in the field of GP, and he claims the Invention Ma- chine has created multiple “patentable new invention[s].”59 A 2006 article in Popular Science about Dr. Koza and the Invention Machine claimed that the AI “has even earned a U.S. patent for developing a system to make factories more efficient, one of the first intellectual-property protections ever granted to a nonhuman designer.”60 The article refers to a patent titled “Apparatus for Im- proved General-Purpose PID and non-PID Controllers” (the “Invention Ma- chine’s Patent”).61 The Invention Machine generated the content of the patent without human intervention and in a single pass.62 It did so without a database of expert knowledge and without any knowledge about existing controllers.63 It simply required information about basic components (such as resistors and diodes) and specifications for a desired result (performance measures such as voltage and frequency).64 With this information, the Invention Machine pro- ceeded to generate different outputs that were measured for fitness (whether an output met performance measures).65
Once again, the Patent Office seems to have had no idea of the AI’s role in the Invention Machine’s Patent.66 The Popular Science article states that Dr. Koza did not disclose the Invention Machine’s involvement, and the patent’s
57 Jonathon Keats, John Koza Has Built an Invention Machine, POPULAR SCI. (Apr. 18, 2006), http://www.popsci.com/scitech/article/2006-04/john-koza-has-built-invention-machine [https://web. archive.org/web/20150218225133/http://www.popsci.com/scitech/article/2006-04/john-koza-has- built-invention-machine].
58 Dr. Koza was also the inventor of the scratch-off lottery ticket in the 1970s. See Home Page of John R. Koza, GENETIC PROGRAMMING, http://www.genetic-programming.com/johnkoza.html [https://perma.cc/H77Y-XM4T] (last visited Aug. 8, 2016).
59 See Koza, Human-Competitive Results, supra note 12, at 265.
60 Keats, supra note 57.
61 See id; U.S. Patent No. '851 (filed July 12, 2002). Although the article does not specifically
identify the patent it is referring to, a search of USPTO records reveals only one patent with Dr. Koza listed as an inventor and with a grant date of January 25, 2005. In addition, in 2010, Dr. Koza subse- quently identified the 851 Patent as one of two examples in which GP created a patentable new inven- tion. See Koza, Human-Competitive Results, supra note 12, at 265.
62 KOZA ET AL., GENETIC PROGRAMMING IV, supra note 56, at 102–04.
63 Telephone Interview with John Koza, President, Genetic Programming Inc. (Jan. 22, 2016) [hereinafter Koza, Telephone Interview].
64 Id.
65 Thus, the GP algorithm is domain independent. Unlike human inventors who often have exten- sive knowledge of prior inventions and who proceed to build on earlier work, the GP algorithm gener- ated a new controller without any reliance on prior art.
66 “If the Turing test had been to fool a patent examiner instead of a conversationalist, then Janu- ary 25, 2005 would have been a date for the history books.” PEDRO DOMINGOS, THE MASTER ALGO- RITHM: HOW THE QUEST FOR THE ULTIMATE LEARNING MACHINE WILL REMAKE OUR WORLD 133– 34 (2015).
 
333
1088 Boston College Law Review [Vol. 57:1079
prosecution history contains no mention of a computer inventor.67 Dr. Koza states that his legal counsel advised him at the time that his team should con- sider themselves inventors despite the fact that “the whole invention was creat- ed by a computer.”68
Dr. Koza reports that his agenda in having the Invention Machine recreate previously patented results was to prove that computers could be made to solve problems automatically.69 He believed that focusing on patentable results would produce compelling evidence that computers were producing something valuable.70 For that reason, he focused on recreating or inventing patentable subject matter that represented significant scientific advances.71 For instance, the Invention Machine’s Patent was for an improved version of a landmark controller built in 1995.72
3. Example Three: Watson
The Creativity Machine and the Invention Machine may be the earliest examples of computer inventors, but others exist.73 Moreover, the exponential growth in computing power over the past dozen years combined with the in- creasing sophistication of software should have led to an explosion in the
67 Indeed, all three of the inventors on the '851 patent, including Dr. Koza, completed an inven- tor’s oath or declaration stating that they disclosed to the Office all information known to be material to patentability including the identity of all inventors.
68 Koza, Telephone Interview, supra note 63.
69 Id.
70 Id.
71 Id. Generating these results de novo thus represented a test with an external measure of difficul-
ty, in contrast to other AI researchers who were training computers to complete academic exercises. 72 See generally KARL J. ASTROM & TORE HAGGLUND, PID CONTROLLERS: THEORY, DESIGN, AND TUNING (2d ed. 1995) (detailing original version of the controller for which the Invention Ma-
chine created an improved, patentable version).
73 E.g., Matrix Advanced Solutions used AI to develop a new anticoagulant. See Daniel Riester et
al., Thrombin Inhibitors Identified by Computer-Assisted Multiparameter Design, 102 PROC. NAT’L ACAD. SCI. USA 8597, 8597–602 (2005). Maxygen Inc. used GP to develop a novel Hepatitis C treat- ment. See Maxygen’s Next-Generation Interferon Alpha Enters Phase Ia Clinical Trial, MAXYGEN (Nov. 7, 2006), available at http://www.prnewswire.com/news-releases/maxygens-next-generation-interferon- alpha-enters-phase-ia-clinical-trial-56073027.html [https://perma.cc/Y9LD-B9EL]. In fact, there is an annual competition for computers producing human-competitive results by genetic and evolutionary computation. See Humies Awards, SIGEVO-GECCO, http://sig.sigevo.org/index.html/tiki-index.php? page=Humies+Awards [https://perma.cc/XMG2-DAGY] (last visited Aug. 9, 2016). Dr. Koza states that competition participants have gone on to patent their results. Koza, Telephone Interview, supra note 63. For additional examples of “Artificial Inventions,” see Plotkin, supra note 5, at 61. In his book, Dr. Plot- kin uses the metaphor of a genie to argue that AI will change the dynamics of human-computer collabo- rations. He suggests that humans will write “wishes” (an abstract description of a machine or a set of instructions for creating a machine) for AI to “grant” (by producing the design for a machine or an actual machine). He further argues that fear of invention automation is unnecessary, and that individuals will become more sophisticated at “writing wishes” (defining problems) for AI to solve. He suggests this will result in more skilled inventors and non-inventors becoming inventors with the help of machines. Id. at 1–11.
 
334
2016] Patent Generating Artificial Intelligence 1089
number of computational inventions.74 Indeed, it is likely that computers are inventing more than ever before.75 Consider, for instance, the results produced by IBM’s AI “Watson” of Jeopardy! fame.76 Watson is a computer system de- veloped by IBM to compete on the game show Jeopardy!77 In 2011, it beat former Jeopardy! winners Ken Jennings and Brad Rutter on the show, earning a million dollars in the process.78
IBM describes Watson as one of a new generation of machines capable of “computational creativity.”79 IBM uses that term to describe machines that can generate “ideas the world has never imagined before.”80 Watson “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], applying big data in new ways.”81 This is a fundamentally dif- ferent type of AI than the Creativity Machine or the Invention Machine; Wat- son utilizes a more conventional architecture of logical deduction combined with access to massive databases containing accumulated human knowledge and expertise.82 Although Watson is not modeled after the human brain or evo-
74 See, e.g., 50 Years of Moore’s Law, INTEL, http://www.intel.com/content/www/us/en/silicon- innovations/moores-law-technology.html [https://perma.cc/PMN9-XJ2L] (last visited Jan. 26, 2016). In 1965, Gordon Moore, co-founder of Intel and Fairchild Semiconductor, published a paper in which he noted a doubling every year in the number of components in an integrated circuit. Based on this and his subsequent observations, “Moore’s Law” became the “golden rule for the electronics indus- try,” predicting that overall processing power for computers will double every eighteen months. See id.
75 See, e.g., Koza, Human-Competitive Results, supra note 12, at 251 (stating that “the increased availability of computing power (through both parallel computing and Moore’s Law) should result in the production, in the future, of an increasing flow of human-competitive results, as well as more intricate and impressive results”).
76 See Jo Best, IBM Watson, TECHREPUBLIC, http://www.techrepublic.com/article/ibm-watson-the- inside-story-of-how-the-jeopardy-winning-supercomputer-was-born-and-what-it-wants-to-do-next/ [https://perma.cc/BQ4V-Q48F] (last visited Jan. 17, 2016).
77 See id.
78 See id.
79 See Computational Creativity, IBM, http://www.research.ibm.com/cognitive-computing/
computational-creativity.shtml#fbid=kwG0oXrjBHY [https://perma.cc/6FK4-WTL3] (last visited Jan. 25, 2016).
80 What Is Watson?, IBM, http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson. html [https://perma.cc/8KM3-LLSG] (last visited Jan. 25, 2016). Watson is a cognitive commuting system with the extraordinary ability to analyze natural language processing, generate and evaluate hypotheses based on the available data then store and learn from the information. In other words, Watson essentially mirrors the human learning process by getting “smarter [through] tracking feed- back from its users and learning from both successes and failures.” Id. Watson made its notable debut on the game show Jeopardy, where it defeated Brad Rutter and Ken Jennings using only stored data by comparing potential answers and ranking confidence in accuracy at the rate of approximately three seconds per question. Id.
81 Computational Creativity, supra note 79.
82 See, e.g., David Ferrucci et al., Building Watson: An Overview of the DeepQA Project, AI MAG., Fall 2010, at 59, 68–69; IBM Watson: Beyond Jeopardy! Q&A, ACM, http://learning.acm. org/webinar/lally.cfm [https://perma.cc/JA3N-J6HG] (last visited Jan. 25, 2016).
 
335
1090 Boston College Law Review [Vol. 57:1079
lutionary processes, it is also capable of generating novel, nonobvious, and useful ideas.
Watson’s Jeopardy! career was short and sweet, and by 2014, it was being applied to more pragmatic challenges, such as running a food truck.83 IBM developed new algorithms for Watson and incorporated a database with infor- mation about nutrition, flavor compounds, the molecular structure of foods, and tens of thousands of existing recipes.84 This new design permits Watson to generate recipes in response to users inputting a few parameters such as ingre- dients, dish (e.g., burgers or burritos), and style (e.g., British or dairy-free).85 On the basis of this user input, Watson proceeds to generate a staggeringly large number of potential food combinations.86 It then evaluates these prelimi- nary results based on novelty and predicted quality to generate a final output.87
It is likely that some of Watson’s discoveries in food science are patenta- ble.88 Patents may be granted for any “new and useful process, machine, manu- facture, or composition of matter, or any new and useful improvement there- of.”89 Food recipes can qualify as patentable subject matter on this basis be- cause lists of ingredients combine to form new compositions of matter or man- ufacture and the steps involved in creating food may be considered a process.90 To be patentable, however, an invention must not only contain patentable sub- ject matter; it must also be novel, nonobvious, and useful.91 That may be chal- lenging to achieve in the case of food recipes given that there is a finite num- ber of ingredients and people have been combining ingredients together for a
83 See Maanvi Singh, Our Supercomputer Overlord Is Now Running a Food Truck, NPR (Mar. 4, 2014), http://www.npr.org/blogs/thesalt/2014/03/03/285326611/our-supercomputer-overlord-is-now- running-a-food-truck [https://perma.cc/V7KM-X8P5]; Chef Watson, IBM, https://www.ibmchef watson.com/community [https://perma.cc/2D54-UURY] (last visited Jan. 17, 2016); Under the Hood, IBM, http://www.ibm.com/smarterplanet/us/en/cognitivecooking/tech.html [https://perma.cc/HWQ8- SEFE] (last visited Jan. 17, 2016).
84 See Under the Hood, supra note 83.
85 See Watson Cooks Up Computational Creativity, IBM, http://www.ibm.com/smarterplanet/ us/en/innovation_explanations/article/florian_pinel.html [https://perma.cc/GGV7-NHT4] (last visited Jan. 17, 2016).
86 See id.
87 See id.
88 See Can Recipes Be Patented?, INVENTORS EYE (June 2013), http://www.uspto.gov/
inventors/independent/eye/201306/ADVICE.jsp [https://perma.cc/EN3V-9DY4]; see also Diamond v. Chakrabarty, 447 U.S. 303, 308– 09 (1980) (noting that patentable subject matter “include[s] any- thing under the sun that is made by man” (quoting S. REP. NO. 82-1979, at 5 (1952); H.R. REP. NO. 82-1923, at 6 (1952))) (internal quotation marks omitted).
89 35 U.S.C. § 101 (1952).
90 See Can Recipes Be Patented?, supra note 88.
91 See General Information Concerning Patents, USPTO, http://www.uspto.gov/patents-
getting-started/general-information-concerning-patents [https://perma.cc/J88J-YUVA] (Oct. 2014).
 
336
2016] Patent Generating Artificial Intelligence 1091
very long time.92 Not only would Watson have to create a recipe that no one had previously created, but it could not be an obvious variation on an existing recipe. Still, people do obtain patents on new food recipes.93 The fact that some of Watson’s results have been surprising to its developers and to human chefs is encouraging94 in this regard95 because unexpected results are one of the fac- tors considered in determining whether an invention is nonobvious.96
Watson is not limited to competing on Jeopardy! or to developing new food recipes.97 IBM has made Watson broadly available to software application providers, enabling them to create services with Watson’s capabilities.98 Wat- son is now assisting with financial planning, helping clinicians to develop treatment plans for cancer patients, identifying potential research study partici- pants, distinguishing genetic profiles that might respond well to certain drugs, and acting as a personal travel concierge.99
92 See Therese Oneill, 7 of the World’s Oldest Foods Discovered by Archeologists, MENTAL FLOSS (Oct. 8, 2015), http://mentalfloss.com/article/49610/7-world%E2%80%99s-oldest-food-finds [https://perma.cc/Y9C5-DRGP].
93 See, e.g., U.S. Patent No. 8,354,134 (filed Dec. 22, 2005).
94 Which is not to say that patents on recipes are a social good. See generally KAL RAUSTIALA & CHRISTOPHER SPRIGMAN, THE KNOCKOFF ECONOMY: HOW IMITATION SPARKS INNOVATION (2012) (discussing social ills that can arise from patents).
95 See, e.g., Rochelle Bilow, How IBM’s Chef Watson Actually Works, BON APPÉTIT (June 30, 2014), http://www.bonappetit.com/entertaining-style/trends-news/article/how-ibm-chef-watson-works [https://perma.cc/5UAB-V AGW].
96 MPEP, supra note 43, § 716.02(a).
97 IBM has even worked with food magazine Bon Appétit to develop a recipe app called Chef Watson to allow the general public to enlist Watson’s help in making new recipes. Rochelle Bilow, We Spent a Year Cooking with the World’s Smartest Computer—and Now You Can, Too, BON AP- PÉTIT (June 23, 2015), http://www.bonappetit.com/entertaining-style/trends-news/article/chef-watson- app [https://perma.cc/HF7Q-C9FM]. Chef Watson can be accessed online at https://www.ibmchef watson.com/community [https://perma.cc/2D54-UURY]. For the less technologically inclined who still wish to sample machine cooking, IBM has published a book of Watson’s recipes. See generally IBM & THE INST. OF CULINARY EDUC., COGNITIVE COOKING WITH CHEF WATSON: RECIPES FOR INNOVATION FROM IBM & THE INSTITUTE OF CULINARY EDUCATION (2015) (detailing recipes creat- ed by Watson).
98 Watson Cooks Up Computational Creativity, supra note 85.
99 Anna Edney, Doctor Watson Will See You Now, if IBM Wins in Congress, BLOOMBERG BNA HEALTH IT LAW & INDUSTRY REPORT (Jan. 29, 2015), available at http://www.post-gazette.com/ frontpage/2015/01/29/Doctor-Watson-will-see-you-now-if-IBM-wins-in-Congress/stories/201501290332 [https://perma.cc/4BHU-VJXU]; Thor Olavsrud, 10 IBM Watson-Powered Apps That Are Changing Our World, CIO (Nov. 6, 2014), http://www.cio.com/article/2843710/big-data/10-ibm-watson-powered-apps- that-are-changing-our-world.html#slide11 [https://perma.cc/NPY7-DDMA].
 
337
1092 Boston College Law Review [Vol. 57:1079
B. Human and Computer Involvement in Computational Inventions
1. Requirements for Inventorship
All patent applications require one or more named inventors who must be “individuals,” a legal entity such as a corporation cannot be an inventor.100 In- ventors own their patents as a form of personal property that they may transfer by “assignment” of their rights to another entity.101 A patent grants its owner “the right to exclude others from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”102 If a patent has multiple owners, each owner may inde- pendently exploit the patent without the consent of the others (absent a con- flicting contractual obligation).103 This makes the issue of whether a computer can be an inventor one of practical as well as theoretical interest because in- ventors have ownership rights in their patents, and failure to list an inventor can result in a patent being held invalid or unenforceable.104
For a person to be an inventor, the person must contribute to an inven- tion’s “conception.”105 Conception refers to, “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative inven- tion as it is thereafter to be applied in practice.”106 It is “the complete perfor-
100 See 35 U.S.C. § 100(f) (1952) “The term ‘inventor’ means the individual or, if a joint inven- tion, the individuals collectively who invented or discovered the subject matter of the invention.” See id. The same issues surrounding computer inventorship may not exist outside of the U.S. where appli- cations do not require a named inventor. See MPEP, supra note 43, § 2137.01 (“The requirement that the applicant for a patent in an application filed before September 16, 2012 be the inventor(s) . . . and that the inventor . . . be identified in applications filed on or after September 16, 2012, are characteris- tics of U.S. patent law not generally shared by other countries.”). For example, a patent application at the European Patent Office may be filed by “any body equivalent to a legal person by virtue of the law governing it.” Convention on the Grant of European Patents art. 58, Oct. 5, 1973, 1065 U.N.T.S. 199. Under the U.S. Patent Act, only individuals can invent, not corporations. See 35 U.S.C. §§ 115–116.
101 See MPEP, supra note 43, § 300. About ninety-three percent of patents are assigned to organi- zations (rather than individuals). See Patenting by Organizations (Utility Patents), USPTO, http:// www.uspto.gov/web/offices/ac/ido/oeip/taf/topo_13.htm#PartA1_1b [https://perma.cc/VF56-GFVT] (last modified Jan. 25, 2016). For example, it is common for scientific and technical workers to preemptively assign their patent rights to employers as a condition of employment. Most, but not all, inventions can be placed under an obligation of assignment in employment contracts. For example, in California, employees are permitted to retain ownership of inventions that are developed entirely on their own time without using their employer’s equipment, supplies, facilities, or trade secret infor- mation except for inventions that either: related, at the time of conception or reduction to practice of the invention, to the employer’s business; actual or demonstrably anticipated research or development of the employer; or resulted from any work performed by the employee for the employer. CAL. LAB. CODE § 2872(a) (West 1979).
102 35 U.S.C. § 154.
103 See MPEP, supra note 43, § 2137.
104 See, e.g., Advanced Magnetic Closures, Inc. v. Rome Fastener Corp., 607 F.3d 817, 829 (Fed.
Cir. 2010).
105 MPEP, supra note 43, § 2137.01(II).
106 Townsend v. Smith, 36 F.2d 292, 295 (C.C.P.A. 1929).
 
338
2016] Patent Generating Artificial Intelligence 1093
mance of the mental part of the inventive act.”107 After conception, someone with ordinary skill in the invention’s subject matter (e.g., a chemist if the in- vention is a new chemical compound) should be able to “reduce the invention to practice.”108 That is to say, they should be able to make and use an invention from a description without extensive experimentation or additional inventive skill.109 Individuals who simply reduce an invention to practice, by describing an already conceived invention in writing or by building a working model from a description for example, do not qualify as inventors.110
2. The Role of Computers in Inventive Activity
The requirement that an inventor participate in the conception of an in- vention creates barriers to inventorship for computers as well as people. Alt- hough computers are commonly involved in the inventive process, in most cases, computers are essentially working as sophisticated (or not-so- sophisticated) tools. One example occurs when a computer is functioning as a calculator or storing information. In these instances, a computer may assist a human inventor to reduce an invention to practice, but the computer is not par- ticipating in the invention’s conception. Even when computers play a more substantive role in the inventive process, such as by analyzing data in an auto-
107 Id.
108 Reduction to practice refers to either actual reduction—where it can be demonstrated the claimed invention works for its intended purpose (for example, with a working model)—or to con- structive reduction—where an invention is described in writing in such a way that it teaches a person of ordinary skill in the subject matter to make and use the invention (as in a patent application). See In re Hardee, 223 U.S.P.Q. (BNA) 1122, 1123 (Com’r Pat. & Trademarks Apr. 3, 1984); see also Bd. of Educ. ex rel. Bd. of Trs. of Fla. State Univ. v. Am. Bioscience, Inc., 333 F.3d 1330, 1340 (Fed. Cir. 2003) (“Invention requires conception.”). With regard to the inventorship of chemical compounds, an inventor must have a conception of the specific compounds being claimed. See Am. Bioscience, 333 F.3d at 1340 (“[G]eneral knowledge regarding the anticipated biological properties of groups of com- plex chemical compounds is insufficient to confer inventorship status with respect to specifically claimed compounds.”); see also Ex parte Smernoff, 215 U.S.P.Q 545, 547 (Pat. & Tr. Office Bd.App. Aug. 17,1982) (“[O]ne who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not a coinventor.”). Actual reduction to practice “requires that the claimed inven- tion work for its intended purpose.” Brunswick Corp. v. United States, 34 Fed. Cl. 532, 584 (1995) (quotations omitted) (quoting Hybritech Inc. v. Monoclonal Antibodies, Inc., 802 F.2d 1367, 1376 (Fed. Cir. 1986). Constructive reduction to practice “occurs upon the filing of a patent application on the claimed invention.” Id. The written description requirement is “to ensure that the inventor had possession, as of the filing date of the application relied on, of the specific subject matter later claimed by him.” Application of Edwards, 568 F.2d 1349, 1351 (C.C.P.A. 1978).
109 “[C]onception is established when the invention is made sufficiently clear to enable one skilled in the art to reduce it to practice without the exercise of extensive experimentation or the exer- cise of inventive skill.” Hiatt v. Ziegler & Kilgour , 179 U.S.P.Q. 757, 763 (Bd. Pat. Interferences Apr. 3, 1973). Conception has been defined as a disclosure of an idea that allows a person skilled in the art to reduce the idea to a practical form without “exercise of the inventive faculty.” Gunter v. Stream, 573 F.2d 77, 79 (C.C.P.A. 1978).
110 See De Solms v. Schoenwald, 15 U.S.P.Q. 2d 1507, 1510 (Bd.Pat.App.& Interferences. Feb. 22, 1990).
 
339
1094 Boston College Law Review [Vol. 57:1079
mated fashion, retrieving stored knowledge, or by recognizing patterns of in- formation, the computer still may fail to contribute to conception. Computer involvement might be conceptualized on a spectrum: on one end, a computer is simply a tool assisting a human inventor; on the other end, the computer inde- pendently meets the requirements for inventorship. AI capable of acting auton- omously such as the Creativity Machine and the Invention Machine fall on the latter end of the spectrum.
3. The Role of Humans in Inventive Activity
Just as computers can be involved in the inventive process without con- tributing to conception, so can humans. For now, at least, computers do not entirely undertake tasks on their own accord. Computers require some amount of human input to generate creative output.
For example, before the Creativity Machine composed music, Dr. Thaler exposed it to existing music and instructed it to create something new.111 Yet, simply providing a computer with a task and starting materials would not make a human an inventor.112 Imagine Friend A tells Friend B, who is an engineer, that A would like B to develop an iPhone battery with twice the standard bat- tery life and A gives B some publically available battery schematics. If B then succeeds in developing such a battery, A would not qualify as an inventor of the battery by virtue of having instructed B to create a result.113 This scenario essentially occurred in the case of the Creativity Machine’s toothbrush inven- tion: Dr. Thaler provided the Creativity Machine information on existing toothbrush designs along with data on each brush’s effectiveness.114 Solely from this information, the Creativity Machine produced the first ever crossed- bristle design.115 This does not make Dr. Thaler an inventor. In the case of the Creativity Machine, the creative act is the result of random or chaotic perturba- tions in the machine’s existing connections that produce new results which, in turn, are judged by the machine for value.116
Humans are also necessarily involved in the creative process because computers do not arise from a void; in other words, humans have to create computers.117 Once again, that should not prevent computer inventorship. No
111 Thaler, Telephone Interview, supra note 38.
112 Ex parte Smernoff, 215 U.S.P.Q. at 547 (“[O]ne who suggests an idea of a result to be accom- plished, rather than the means of accomplishing it, is not a coinventor.”).
113 See id.
114 Thaler, Telephone Interview, supra note 38.
115 Id.
116 See Thaler, Creativity Machine® Paradigm, supra note 29, at 449.
117 This will be the case until computers start designing other computers or engaging in reflection.
Reflection is a software concept that refers to a computer program that can examine itself and modify its own behavior (and even its own code). J. Malenfant et al., A Tutorial on Behavioral Reflection and Its Implementation, in PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE REFLECTION 1, 1–
 
340
2016] Patent Generating Artificial Intelligence 1095
one would exist without their parents contributing to their conception (pun in- tended), but that does not make parents inventors on their child’s patents. If a computer scientist creates an AI to autonomously develop useful information and the AI creates a patentable result in an area not foreseen by the inventor, there would be no reason for the scientist to qualify as an inventor on the AI’s result. An inventor must have formed a “definite and permanent idea of the complete and operative invention” to establish conception.118 The scientist might have a claim to inventorship if he developed the AI to solve a particular problem, and it was foreseeable that the AI would produce a particular re- sult.119
4. Combining Human and Computer Creativity
A computer may not be a sole inventor; the inventive process can be a collaborative process between human and machine. If the process of develop- ing the Creativity Machine’s Patent had been a back-and-forth process with both the AI and Dr. Thaler contributing to conception, then both might qualify as inventors.120 By means of illustration, suppose a human engineer provides a machine with basic information and a task. The engineer might learn from the machine’s initial output, then alter the information that he or she provides to the machine to improve its subsequent output. After several iterations, the ma- chine might produce a final output that the human engineer might directly alter to create a patentable result. In such a case, both the engineer and the machine might have played a role in conception. Leaving AI aside, invention is rarely occurs in a vacuum, and there are often joint inventors on patents.121 In some of these instances, if a computer were human, it would be an inventor. Yet, computers are not human, and, as such, they face unique barriers to qualifying as inventors.
20 (1996), available at http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ malenfant.pdf [https://perma.cc/7EKK-7BJT].
118 Townsend, 36 F.2d at 295.
119 See generally Shyamkrishna Balganesh, Foreseeability and Copyright Incentives, 122 HARV. L. REV. 1569 (2009) (discussing foreseeability in the patent context).
120 What is required is some “quantum of collaboration or connection.” Kimberly-Clark Corp. v. Procter & Gamble Distrib. Co., 973 F.2d 911, 917 (Fed. Cir. 1992). For joint inventorship, “there must be some element of joint behavior, such as collaboration or working under common direction, one inventor seeing a relevant report and building upon it or hearing another’s suggestion at a meet- ing.” Id.; see also Moler & Adams v. Purdy, 131 U.S.P.Q. 276, 279 (Bd. Pat. Interferences 1960) (“[I]t is not necessary that the inventive concept come to both [joint inventors] at the same time.”).
121 See Prerna Wardhan & Padmavati Manchikanti, A Relook at Inventors’ Rights, 18 J. INTELL. PROP. RIGHTS 168, 169 (2013).
 
341
1096 Boston College Law Review [Vol. 57:1079
C. Barriers to Computer Inventorship
1. The Legal Landscape
Congress is empowered to grant patents on the basis of the Patent and Copyright Clause of the Constitution.122 That clause enables Congress “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”123 It also provides an explicit rationale for granting patent and copyright protection, namely to encourage innovation under an incentive theo- ry.124 The theory goes that people will be more inclined to invent things (i.e., promote the progress of science) if they can receive government-sanctioned monopolies (i.e., patents) to exploit commercial embodiments of their inven- tions. Having the exclusive right to sell an invention can be tremendously lu- crative.125
The Patent Act, which here refers to United States patent law as a whole, provides at least a couple of challenges to computers qualifying as inventors under the Patent and Copyright Clause.126 First, as previously mentioned, the Patent Act requires that inventors be “individuals.”127 This language has been in place since at least the passage of legislation in 1952 that established the basic structure of modern patent law.128 The “individual” requirement likely was included to reflect the constitutional language that specifically gives “in-
122 U.S. CONST. art. I, § 8, cl. 8. This clause is also sometimes referred to as the “Patent Clause” or the “Copyright Clause.”
123 Id.
124 See Mark A. Lemley, Ex Ante Versus Ex Post Justifications for Intellectual Property, 71 U. CHI. L. REV. 129, 129 (2004) (“The standard justification for intellectual property is ex ante . . . . It is the prospect of the intellectual property right that spurs creative incentives.”).
125 See JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY WITH SOME OF THEIR APPLI- CATIONS TO SOCIAL PHILOSOPHY 563 (Prometheus Books 2004) (1872) (noting that under a patent system, “the greater the usefulness, the greater the reward”).
126 Legislation pertaining to patents is found under Title 35 of the United States Code. The Patent Act may also be used to refer to specific pieces of legislation ranging from the Patent Act of 1790, the first patent law passed by the federal government, to the Patent Act of 1952. Pub. L. No. 82-593, 66 Stat. 792 (1952).
127 E.g., 35 U.S.C. § 100(f) (“The term ‘inventor’ means the individual or, if a joint invention, the individuals collectively who invented or discovered the subject matter of the invention.”). The same issues surrounding computer inventorship may not exist outside of the U.S. where applications do not require a named inventor. See MPEP, supra note 43, § 2137.01 (“The requirement that the applicant for a patent in an application filed before September 16, 2012 be the inventor(s), . . . and that the in- ventor . . . be identified in applications filed on or after September 16, 2012, are characteristics of U.S. patent law not generally shared by other countries.”). For example, a patent application at the Europe- an Patent Office may be filed by “any body equivalent to a legal person by virtue of the law governing it.” Convention on the Grant of European Patents, supra note 100, at art. 58; see also 35 U.S.C. §§ 115–116.
128 Pub. L. No. 82-593, 66 Stat. 792 (1952); see also Gregory Dolin, Dubious Patent Reform, 56 B.C. L. REV. 881, 889 (2015) (discussing aims of 1952 Patent Act).
 
342
2016] Patent Generating Artificial Intelligence 1097
ventors” the right to their discoveries as opposed to other legal entities that might assert ownership rights.129 Such language would help to ensure that pa- tent rights were more likely to go to individual inventors than to corporate enti- ties where ownership was disputed.130 Legislators were not thinking about computational inventions in 1952.131 Second, patent law jurisprudence requires that inventions be the result of a “mental act.”132 So, because computers are not individuals and it is questionable that they engage in a mental act, it is unclear whether a computer autonomously conceiving of a patentable invention could legally be an inventor.
2. Avoiding Disclosure of Artificially Intelligent Inventors
Given that computers are functioning as inventors, and likely inventing at an escalating rate, it would seem that the Patent Office should be receiving an increasing number of applications claiming computers as inventors. That the Patent Office has not suggests that applicants are choosing not to disclose the role of AI in the inventive process.133 That may be due to legal uncertainties about whether an AI inventor would render an invention unpatentable.134
129 In the words of the United States Court of Appeals for the Federal Circuit, “people conceive, not companies.” New Idea Farm Equip. Corp. v. Sperry Corp., 916 F.2d 1561, 1566 n.4 (Fed. Cir. 1990).
130 Now under the America Invents Act (“AIA”), a corporate entity can apply for a patent on behalf of an inventor who is under an assignment obligation. MPEP, supra note 43, § 325.
131 See Karl F. Milde, Jr., Can a Computer Be an “Author” or an “Inventor”?, 51 J. PAT. OFF. SOC’Y 378, 379 (1969). As one commentator notes:
The closest that the Patent Statute comes to requiring that a patentee be an actual person is in the use, in Section 101, of the term “whoever.” Here too, it is clear from the ab- sence of any further qualifying statements that the Congress, in considering the statute in 1952, simply overlooked the possibility that a machine could ever become an inven- tor.
Id.; see also, e.g., A.M. Turing, Computing Machinery and Intelligence, 59 MIND 433, 433–51 (1950) [hereinafter Turing, Computing Machinery and Intelligence].
132 Conception has been defined as “the complete performance of the mental part of the inventive art,” and it is “the formation in the mind of the inventor of a definite and permanent idea of the com- plete and operative invention as it is thereafter to be applied in practice.” Townsend, 36 F.2d at 295.
133 See supra note 5 and accompanying text. The discussion in note 5 infers that the Patent Office has not received applications claiming computers as inventors because they have no policy or guid- ance on the subject, they do not seem to have ever addressed the issue in any publication, and because computer inventorship does not seem to have been at issue in any patent litigation.
134 See, e.g., Dane E. Johnson, Statute of Anne-imals: Should Copyright Protect Sentient Nonhu- man Creators?,15 ANIMAL L. 15, 23 (2008) (quoting one Copyright Office employee who explained that “[as] a practical matter[,] the Copyright Office would not register [a computer’s own] work if its origins were accurately represented on the copyright application. The computer program itself would be registerable if it met the normal standards for computer programs, but not the computer-generated literary work.”) Despite this policy and the Copyright Office’s Compendium guidelines, numerous computer-authored works have been registered. See, e.g.,William T. Ralston, Copyright in Computer- Composed Music: Hal Meets Handel, 52 J. COPYRIGHT SOC’Y OF THE U.S.A. 281, 283 (2004) (noting
 
343
1098 Boston College Law Review [Vol. 57:1079
Without a legal inventor, new inventions would not be eligible for patent pro- tection and would enter the public domain after being disclosed.135
There is another reason why computers might not be acknowledged: a person can qualify as an inventor simply by being the first individual to recog- nize and appreciate an existing invention.136 That is to say, someone can dis- cover rather than create an invention. Uncertainty (and accident) is often part of the inventive process.137 In such cases, an individual need only understand the importance of an invention to qualify as its inventor.138 For the purposes of this Article, assuming that a computer cannot be an inventor, individuals who subsequently “discover” computational inventions by mentally recognizing and appreciating their significance would likely qualify as inventors. So, it may be the case that computational inventions are only patentable when an individual subsequently discovers them.
II. IN SUPPORT OF COMPUTERINVENTORS
This Part examines the law regarding non-human authorship of copy- rightable material.139 It discusses the history of the Copyright Office’s Human Authorship Requirement.140 This Part also scrutinizes case law interpreting the Patent and Copyright Clause.141 On the basis of this analysis and principles of dynamic statutory interpretation, this Part argues that computers should qualify as legal inventors.142 This would incentivize the development of creative ma-
one computer-authored volume of poetry registered to a computer author, “Racter,” but still not ex- plicitly disclosed to be a computer). In 1993, Scott French programmed a computer to write in the style of a famous author, and the resulting work was registered as an “original and computer aided text.” Tal Vigderson, Comment, Hamlet II: The Sequel? The Rights of Authors vs. Computer- Generated “Read-Alike” Works, 28 LOY. L.A. L. REV. 401, 402–03 (1994). The novel was apparently terrible. See Patricia Holt, Sunday Review, S.F. CHRON., Aug. 15, 1993, B4 (“[t]he result is a mitigat- ed disaster”).
135 See MPEP, supra note 43, § 2137.
136 Conception requires contemporaneous recognition and appreciation of the invention. See Invi- trogen Corp. v. Clontech Labs., Inc., 429 F.3d 1052, 1064 (Fed. Cir. 2005) (noting that the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue); see also, e.g., Silvestri v. Grant, 496 F.2d 593, 597 (C.C.P.A. 1974) (“[A]n accidental and unappreciated duplication of an invention does not defeat the patent right of one who, though later in time, was the first to recognize that which constitutes the inventive subject matter.”).
137 For instance, Alexander Fleming discovered penicillin in a mold that had contaminated his sam- ples of Staphylococcus. Howard Markel, The Real Story Behind Penicillin, PBS (Sep. 27, 2013), http://www.pbs.org/newshour/rundown/the-real-story-behind-the-worlds-first-antibiotic/ [https://perma. cc/V6SM-2QJL].
138 See Silvestri, 496 F.2d at 597.
139 See infra notes 139–239 and accompanying text.
140 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 306.
141 U.S. CONST. art. I, § 8, cl. 8.
142 See generally Eskridge, Dynamic Statutory Interpretation, supra note 16 (discussing canons of
statutory interpretation).
 
344
2016] Patent Generating Artificial Intelligence 1099
chines consistent with the purpose and intent of the Founders and Congress. The requirement that inventors be individuals was designed to prevent corpo- rate ownership, and, therefore, computer inventorship should not be prohibited on this basis.143 Also, there should be no requirement for a mental act because patent law is concerned with the nature of an invention itself rather than the subjective mental process by which an invention may have been achieved.144 This Part concludes by addressing objections to computer inventorship includ- ing arguments that computational inventions would develop in the absence of patent protection at non-monopoly prices.145
A. Nonhuman Authors of Copyrightable Material
The Patent Act does not directly address the issue of a computer inventor. The Patent Office has never issued guidance addressing the subject, and there appears to be no case law on the issue of whether a computer could be an in- ventor. That is the case despite the fact that the Patent Office appears to have already granted patents for inventions by computers but, as previously dis- cussed, did so unknowingly.
There is, however, guidance available from the related issue of nonhuman authorship of copyrightable works.146 Nonhuman authorship is not governed by statute, but there is interesting case law on the subject. Also, since at least 1984 the Copyright Office has conditioned copyright registration on human authorship.147 In its 2014 compendium, the Copyright Office published an up- dated “Human Authorship Requirement” which states that:
To qualify as a work of “authorship” a work must be created by a human being. . . . The Office will not register works produced by nature, animals, or plants. . . . Similarly, the Office will not register
143 See infra notes 206–208 and accompanying text.
144 See, e.g., The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 86. 145 See notes 189–239 and accompanying text.
146 The issue of computer authorship (and inventorship) has been considered “since the 1960s
when people began thinking about the impact of computers on copyright.” Arthur R. Miller, Copy- right Protection for Computer Programs, Databases, and Computer-Generated Works: Is Anything New Since CONTU?, 106 HARV. L. REV. 977, 1043 (1993). Most of the literature related to computer generated works has focused on copyright rather than patent protection. “In the secondary literature on copyright, rivers of ink are spilt on” whether computers can be considered authors. MELVILLE B. NIMMER & DAVID NIMMER, NIMMER ON COPYRIGHT § 5.01[A] (LexisNexis 2015).
147 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 202.02(b). The Com- pendium of U.S. Copyright Office Practices elaborates on the “human authorship” requirement by stating: “The term ‘authorship’ implies that, for a work to be copyrightable, it must owe its origin to a human being. Materials produced solely by nature, by plants, or by animals are not copyrightable.” Id. It further elaborates on the phrase “[w]orks not originated by a human author” by stating: “In order to be entitled to copyright registration, a work must be the product of human authorship. Works pro- duced by mechanical processes or random selection without any contribution by a human author are not registrable.” Id. § 503.03(a).
 
345
1100 Boston College Law Review [Vol. 57:1079
works produced by a machine or mere mechanical process that op- erates randomly or automatically without any creative input or in- tervention from a human author.148
This policy was the result of many years of debate within the Copyright Of- fice.149
The requirement is based on jurisprudence that dates long before the in- vention of modern computers to the In re Trade-Mark Cases in 1879, in which the U.S. Supreme Court interpreted the Patent and Copyright Clause to exclude the power to regulate trademarks.150 In interpreting this clause, the Court stat- ed, in dicta, that the term “writings” may be construed liberally but noted that only writings that are “original, and are founded in the creative powers of the mind” may be protected.151
The issue of computer authorship was implicit in the Court’s celebrated case of Burrow-Giles Lithographic Co. v. Sarony in 1884.152 In that case, a lithographic company argued that a photograph of Oscar Wilde did not qualify as a “writing” or as the work of an “author.”153 The company further argued that even if a visual work could be copyrighted, that a photograph should not qualify for protection because it was just a mechanical reproduction of a natu- ral phenomenon and thus could not embody the intellectual conception of its author.154 The Court disagreed, noting that all forms of writing “by which the ideas in the mind of the author are given visible expression” were eligible for copyright protection.155 The Court stated that although ordinary photographs might not embody an author’s “idea,” in this particular instance, the photogra- pher had exercised enough control over the subject matter that it qualified as an original work of art.156 Therefore, the case explicitly addressed whether the camera’s involvement negated human authorship, and it implicitly dealt with
148 Id. § 313.2.
149 See, e.g., U.S. COPYRIGHT OFFICE, EIGHTY-SECOND ANNUAL REPORT OF THE REGISTER OF COPYRIGHTS 18 (1979) (discussing issues related to computer authorship).
150 See generally In re Trade-Mark Cases, 100 U.S. 82 (1879) (finding that the Patent and Copy- right Clause excludes regulating trademarks). Congress, which does indeed enjoy the ability to regu- late trademarks, passed the Trade Mark Act of 1881 two years after this case was decided. That Act gave Congress the authority to regulate trademarks on the basis of the Commerce Clause.
151 Id. at 94. The Court in this case held that only original works of art, which are the “fruits of intellectual labor,” may be protected under copyright law. Id. (emphasis omitted).
152 See Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53, 56 (1884). 153 Id.
154 Id. at 58–59.
155 Id. at 58.
156 Id. at 54–55. Protections for all photographs was eventually made a part of the statutory scheme for copyright protection. 17 U.S.C. § 106A (2012). In the words of Judge Learned Hand, “no photograph, however simple, can be unaffected by the personal influence of the author, and no two will be absolutely alike.” Jewelers’ Circular Pub. Co. v. Keystone Pub. Co., 274 F. 932, 934 (S.D.N.Y. 1921), aff’d, 281 F. 83 (2d Cir. 1922).
 
346
2016] Patent Generating Artificial Intelligence 1101
the question of whether a camera could be considered an author. Though it seems unwise to put much emphasis on dicta from more than a century ago to resolve the question of whether nonhumans could be authors, the Copyright Office cites Burrow-Giles in support of its Human Authorship Requirement.157
The Copyright Office first addressed the issue of computer authors in 1966 when the Register of Copyrights, Abraham Kaminstein, questioned whether computer-generated works should be copyrightable.158 Mr. Kaminstein reported that, in 1965, the Copyright Office had received applications for com- puter-generated works including: an abstract drawing, a musical composition, and compilations that were, at least partly, the work of computers.159 Mr. Ka- minstein did not announce a policy for dealing with such applications but sug- gested the relevant issue should be whether a computer was merely an assist- ing instrument (as with the camera in Burrow-Giles) or whether a computer conceived and executed the traditional elements of authorship.160
In the following years, the Copyright Office struggled with how to deal with computers more broadly.161 At that time, copyright law did not even ad- dress the issue of whether computer software should be copyrightable—a far more urgent and financially important problem.162
In 1974, Congress created the Commission on New Technological Uses of Copyrighted Works (“CONTU”) to study issues related to copyright and computer-related works.163 With regards to computer authorship, CONTU wrote in 1979 that there was no need for special treatment of computer- generated works because computers were not autonomously generating crea- tive results without human intervention; computers were simply functioning as tools to assist human authors.164 CONTU also declared that autonomously cre- ative AI was not immediately foreseeable.165 The Commission unanimously concluded that “[w]orks created by the use of computers should be afforded copyright protection if they are original works of authorship within the Act of 1976.”166 According to the Commission, “the author is [the] one who employs
157 See COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 306.
158 See U.S. COPYRIGHT OFFICE, SIXTY-EIGHTH ANN. REP. REG. COPYRIGHTS 4–5 (1966).
159 Id.
160 See id.
161 See Act of Dec. 31, 1974, Pub. L. No. 93-573, § 201, 88 Stat. 1873, 1873–74; see also H.R.
REP. NO. 1476, 94th Cong., 2d Sess. 116 (1976), reprinted in 1976 U.S.C.C.A.N. 5731, 5731 (dis- cussing issues regarding computers and copyrights). These issues had not been addressed in the 1974 Copyright Act. Pub. L. No. 94-553, § 117, 90 Stat. 2565 (1976), repealed by Computer Software Protection Act, Pub. L. No. 96-517, § 117, 94 Stat. 3028 (1980) (codified at 17 U.S.C. § 117 (1988)).
162 See Act of Dec. 31, 1974 § 201.
163 Id. § 201(a)–(b).
164 See NAT’L COMM’N ON NEW TECH. USES OF COPYRIGHTED WORKS, FINAL REPORT ON NEW
TECHNOLOGICAL USES OF COPYRIGHTED WORKS 44 (1979). 165 See id.
166 Id. at 1.
 
347
1102 Boston College Law Review [Vol. 57:1079
the computer.”167 Former CONTU Commissioner Arthur Miller explained that “CONTU did not attempt to determine whether a computer work generated with little or no human involvement is copyrightable.”168 Congress subsequent- ly codified CONTU’s recommendations.169
Nearly a decade later, in 1986, advances in computing prompted the U.S. Congress’s Office of Technology Assessment (“OTA”) to issue a report argu- ing that CONTU’s approach was too simplistic and computer programs were more than “inert tools of creation.”170 The OTA report contended that, in many cases, computers were at least “co-creators.”171 The OTA did not dispute that computer-generated works should be copyrightable, but it did foresee prob- lems with determining authorship.172
The 2014 iteration of the Human Authorship Requirement was partially the result of a prominent public discourse about nonhuman authorship stem- ming from the “Monkey Selfies.”173 The Monkey Selfies are a series of images that a Celebes crested macaque took of itself in 2011 using equipment belong- ing to the nature photographer David Slater.174 Mr. Slater reports that he staged the photographs by setting up a camera on a tripod and leaving a remote trig- ger for the macaque to use.175 He subsequently licensed the photographs,
167 Id. at 45. This rule is largely similar in British law: “In the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangements necessary for the creation of the work are undertaken.” Copyright, Designs and Patent Act 1988, c. 48 § 9(3) (UK). “‘Computer-generated,’ in relation to a work, means that the work is generated by computer in circumstances such that there is no human author of the work.” Id. § 178.
168 Miller, supra note 146, at 1070. Professor Miller continued to argue in 1993 that “computer science does not appear to have reached a point at which a machine can be considered so ‘intelligent’ that it truly is creating a copyrightable work.” Id. at 1073. Rather, “for the foreseeable future, the copyrightability of otherwise eligible computer-generated works can be sustained because of the sig- nificant human element in their creation, even though there may be some difficulty is assigning au- thorship.” Id.
169 See 17 U.S.C. § 117.
170 See U.S. CONGRESS, OFFICE OF TECH. ASSESSMENT, INTELLECTUAL PROPERTY RIGHTS IN AN AGE OF ELECTRONICS AND INFORMATION 70–73 (1986). As stated by the OTA:
Courts will then be left with little guidance, and even less expertise, to solve these high- ly complex conceptual and technological issues. . . . [E]ither the legislature or the courts will have to confront some questions that will be very difficult to resolve under the pre- sent system. These include: . . . What of originality in works that are predominately au- tomated? Who is the author? Providing answers to these questions will become more urgent as creative activities continue to fuse with machine intelligence.
Id. at 71–73.
171 Id. at 72.
172 Id. at 73.
173 See Naruto v. Slater, No. 3:2015-cv-04324, 2016 WL 362231, *1 (N.D. Cal. Jan. 23, 2016). 174 Id. at *1.
175 See Sulawesi Macaques, DJS PHOTOGRAPHY, http://www.djsphotography.co.uk/original_
story.html [https://perma.cc/H93K-8CB9] (last visited Jan. 26, 2016) (showing Mr. Slater’s photo- graphs and providing an overview of how he staged them). The claim by Mr. Slater that he engineered the shoot is controversial based on his earlier reports of the event in question. See Mike Masnick,
 
348
2016] Patent Generating Artificial Intelligence 1103
claiming he owned their copyright.176 Other parties then reposted the photo- graphs without his permission and over his objections, asserting that he could not copyright the images without having taken them directly.177 On December 22, 2014, the Copyright Office published its Human Authorship Requirement, which specifically lists the example of a photograph taken by a monkey as something not protectable.178
In September 2015, People for the Ethical Treatment of Animals (“PETA”) filed a copyright infringement suit against Mr. Slater on behalf of Naruto, the monkey it purports took the Monkey Selfies, asserting that Naruto was entitled to copyright ownership.179 On January 28, 2016, U.S. District Judge William H. Orrick III dismissed PETA’s lawsuit against Slater.180 Judge Orrick reasoned that the issue of the ability for animals to obtain a copyright is “an issue for Congress and the President.”181 The case is currently under appeal in the Ninth Circuit.182
B. Computers Should Qualify as Legal Inventors
1. Arguments Supporting Computer Inventors
Preventing patents on computational inventions by prohibiting computer inventors, or allowing such patents only by permitting humans who have dis- covered the work of creative machines to be inventors, is not an optimal sys- tem. In the latter case, AI may be functioning more or less independently, and it is only sometimes the case that substantial insight is needed to identify and understand a computational invention. Imagine that Person C instructs their AI to develop an iPhone battery with twice the standard battery life and gives it some publically available battery schematics. The AI could produce results in the form of a report titled “Design for Improved iPhone Battery”—complete with schematics and potentially even pre-formatted as a patent application. It seems inefficient and unfair to reward C for recognizing the AI’s invention when C has not contributed significantly to the innovative process.
Photographer David Slater Claims That Because He Thought Monkeys Might Take Pictures, Copy- right Is His, TECHDIRT (July 15, 2011), https://www.techdirt.com/articles/20110714/16440915097/ photographer-david-slater-claims-that-because-he-thought-monkeys-might-take-pictures-copyright-is- his.shtml [https://perma.cc/MA7S-PFJ9].
176 See Naruto, 2016 WL 362231, at *1.
177 See Masnick, supra note 175.
178 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 313.2.
179 See Naruto, 2016 WL 362231, at *1.
180 Id.
181 See id.; Beth Winegarner, ‘Monkey Selfie’ Judge Says Animals Can’t Sue Over Copyright,
LAW 360 (Jan. 6, 2016), https://www.cooley.com/files/‘MonkeySelfie’JudgeSaysAnimalsCan’tSue OverCopyright.pdf [https://perma.cc/2CUG-2JDT].
182 See generally Opening Brief of Plaintiff-Appellant, Naruto v. Slater, No. 3:15-cv-04324 (9th Cir. July 28, 2016) (arguing for the appeal of the district court’s decision).
 
349
1104 Boston College Law Review [Vol. 57:1079
Such a system might also create logistical problems. If C had created an improved iPhone battery as a human inventor, C would be its inventor regard- less of whether anyone subsequently understood or recognized the invention. If C instructed C’s AI to develop an improved iPhone battery, the first person to notice and appreciate the AI’s result could become its inventor (and prevent C from being an inventor). One could imagine this creating a host of problems: the first person to recognize a patentable result might be an intern at a large research corporation or a visitor in someone’s home. A large number of indi- viduals might also concurrently recognize a result if access to an AI is wide- spread.
More ambitiously, treating computational inventions as patentable and recognizing creative computers as inventors would be consistent with the Con- stitutional rationale for patent protection.183 It would encourage innovation un- der an incentive theory. Patents on computational inventions would have sub- stantial value independent of the value of creative computers; allowing com- puters to be listed as inventors would reward human creative activity upstream from the computer’s inventive act. Although AI would not be motivated to in- vent by the prospect of a patent, it would motivate computer scientists to de- velop creative machines. Financial incentives may be particularly important for the development of creative computers because producing such software is resource intensive.184 Though the impetus to develop creative AI might still exist if computational inventions were considered patentable but computers could not be inventors, the incentives would be weaker owing to the logistical, fairness, and efficiency problems such a situation would create.
There are other benefits to patents beyond providing an ex ante innova- tion incentive. Permitting computer inventors and patents on computational inventions might also promote disclosure and commercialization.185 Without the ability to obtain patent protection, owners of creative computers might choose to protect patentable inventions as trade secrets without any public dis-
183 See U.S. CONST. art. I, § 8, cl. 8. Among those addressing the patentability implications of computational invention, Ralph Clifford has argued that works generated autonomously by computers should remain in the public domain unless AI develops a consciousness that allows it to respond to the Copyright Act’s incentives. See Clifford, supra note 4, at 1702–03; see also Liza Vertinsky & Todd M. Rice, Thinking About Thinking Machines: Implications of Machine Inventors for Patent Law, 8 B.U. J. SCI. & TECH. L. 574, 581 (2002). Colin R. Davies has argued more recently that a computer should be given legal recognition as an individual under UK law to allow proper attribution of author- ship and to allow respective claims to be negotiated through contract. See generally Colin R. Davies, An Evolutionary Step in Intellectual Property Rights—Artificial Intelligence and Intellectual Property, 27 COMPUT. L. & SEC. REV. 601 (2011).
184 See, e.g., Ferrucci et al., supra note 82, at 59 (stating that Watson’s creation required “three years of intense research and development by a core team of about 20 researchers”).
185 See, e.g., Innovation’s Golden Goose, THE ECONOMIST, Dec. 12, 2002, at 3 (discussing the increase in innovation after the Bayh-Dole Act of 1980 because the legislation providing inventors an incentive to disclose and commercialize their ideas).
 
350
2016] Patent Generating Artificial Intelligence 1105
closure.186 Likewise, businesses might be unable to develop patentable inven- tions into commercial products without patent protection.187 In the pharmaceu- tical and biotechnology industries, for example, the vast majority of expense in commercializing a new product is incurred after the product is invented during the clinical testing process required to obtain regulatory approval for market- ing.188
2. Arguments Against Computer Inventors
Those arguments reflect the dominant narrative justifying the grant of in- tellectual property protection.189 That account, however, has been criticized, particularly by academics.190 Patents result in significant social costs by estab- lishing monopolies.191 Patents also can stifle entry by new ventures by creating barriers to subsequent research.192 Whether the benefit of patents as an innova- tion incentive outweighs their anti-competitive costs, or for that matter, wheth- er patents even have a net positive effect on innovation, likely varies between industries, areas of scientific research, and inventive entities.193
186 See, e.g., Festo Corp. v. Shoketsu Kinzoku Kogyo Kabushiki Co., 535 U.S. 722, 736 (2002) (“[E]xclusive patent rights are given in exchange for disclosing the invention to the public.”).
187 Commercialization theory holds that patents are important in providing incentives for invest- ment in increasing the value of a patented technology. See Edmund W. Kitch, The Nature and Func- tion of the Patent System, 20 J.L. & ECON. 265, 276–77 (1977).
188 See TUFTS CTR. FOR THE STUDY OF DRUG DEV., Briefing: Cost of Developing a New Drug (Nov. 18, 2014), http://csdd.tufts.edu/files/uploads/Tufts_CSDD_briefing_on_RD_cost_study_- _Nov_18,_2014..pdf (estimating that pre-human expenditures are 30.8% of costs per approved com- pound, and estimating average pre-tax industry cost per new prescription drug approval [inclusive of failures and capital costs] is $2.55 billion). The cost of new prescription drug approval is hotly con- tested. See, e.g., Roger Collier, Drug Development Cost Estimates Hard to Swallow, 180 CANADIAN MED. ASS’N J. 279, 279 (2009).
189 See Jeanne C. Fromer, Expressive Incentive in Intellectual Property, 98 VA. L. REV. 1745, 1746 (2012).
190 See generally, e.g., Frederick M. Abbott, The Doha Declaration on the TRIPS Agreement and Public Health: Lighting a Dark Corner at the WTO, 5 J. INT’L ECON L. 469 (2002) (discussing prob- lems with a pure incentive theory for patents in the medicines context).
191 See Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents–Prizes Debate, 92 TEX. L. REV. 303, 314–15 (2013) (discussing the deadweight loss of monopoly).
192 See Lisa Larrimore Ouellette, Access to Bio-Knowledge: From Gene Patents to Biomedical Materials, 2010 STAN. TECH. L. REV. 48, 3 at n. 1 (considering effects of patents on entry to the bio- medical products market); Arti Kaur Rai, Regulating Scientific Research: Intellectual Property Rights and the Norms of Science, 94 NW. U. L. REV. 77, 133 (1999); see also Bhaven Sampat & Heidi L. Williams, How Do Patents Affect Follow-on Innovation? Evidence from the Human Genome 15 (Oct. 13, 2015) (unpublished manuscript), available at http://economics.mit.edu/files/10782 [https:// perma.cc/5K7N-89C4] (discussing patents to entry created by patents).
193 As discussed above, the need for patent incentives is particularly compelling in the pharma- ceutical context where large investments in clinical research over several years are typically needed to commercialize products that often are inexpensive for competitors to replicate. See Benjamin N. Roin, Unpatentable Drugs and the Standards of Patentability, 87 TEX. L. REV. 503, 545–47 (2009).
 
351
1106 Boston College Law Review [Vol. 57:1079
For instance, commentators such as Judge Richard Posner have argued that patents may not be needed to incentivize R&D in the software industry.194 Software innovation is often relatively inexpensive, incremental, quickly su- perseded, produced without patent incentives, protected by other forms of in- tellectual property, and associated with a significant first mover advantage.195 Likewise, patents may be unnecessary to spur innovation in university settings where inventors are motivated to publish their results for prestige and the pro- spect of academic advancement.196
Computational inventions may develop due to non-patent incentives. Software developers have all sorts of non-economic motivations to build crea- tive computers: for example, to enhance their reputations, satisfy scientific cu- riosity, or collaborate with peers.197 Business ventures might find the value of computational inventions exceeds the cost of developing creative computers even in the absence of patent protection. Of course, computational invention patents may not be an all-or-nothing proposition; they may further encourage activities that would have otherwise occurred on a smaller scale over a longer timeframe. If patents are not needed to incentivize the development of creative computers, it may be justifiable to treat computational inventions as unpatent- able and failing to recognize computer inventors. Yet, whether patents produce a net benefit as an empirical matter is difficult to determine a priori. Even though individuals and businesses do not always behave as rational economic actors, in the aggregate, it is likely that providing additional financial incen- tives to spur the development of creative computers will produce a net bene- fit.198
Patents for computational inventions might also be opposed on the grounds that they would chill future human innovation, reward human inven-
194 See WILLIAM M. LANDES & RICHARD A. POSNER, THE ECONOMIC STRUCTURE OF INTELLEC- TUAL PROPERTY LAW 312–13 (2003).
195 See id.; see also Eric Goldman, The Problems with Software Patents, FORBES (Nov. 28, 2012), http://www.forbes.com/sites/ericgoldman/2012/11/28/the-problems-with-software-patents/#234ba 3d66545 [https://web.archive.org/web/20160412114510/http://www.forbes.com/sites/ericgoldman/ 2012/11/28/the-problems-with-software-patents/#41a0c38b2a70] (discussing in a three-part series why patents may be unnecessary for software, challenges to fixing the problems, and exploring possi- ble fixes).
196 See Mark A. Lemley, Are Universities Patent Trolls?, 18 FORDHAM INTELL. PROP. MEDIA & ENT. L.J. 611, 621 (2008).
197 See YOCHAI BENKLER, THE WEALTH OF NETWORKS 65 (2006). Further, behavior law and economics posits that actual people do not act in accordance with standard economic principles be- cause they have limited rationality, willpower, and self-interest. See Christine Jolls, Cass R. Sunstein, & Richard Thaler, A Behavioral Approach to Law and Economics, 50 STAN. L. REV. 1471, 1476 (1998).
198 See, e.g., GARY S. BECKER, THE ECONOMIC APPROACH TO HUMAN BEHAVIOR 14 (1978) (“[A]ll human behavior can be viewed as involving participants who [1] maximize their utility [2] from a stable set of preferences and [3] accumulate an optimal amount of information and other inputs in a variety of markets.”).
 
352
2016] Patent Generating Artificial Intelligence 1107
tors who failed to contribute to the inventive process, and result in further con- solidation of intellectual property in the hands of big business (assuming that businesses such as IBM will be the most likely to own creative computers).199
Other non-utilitarian patent policies do not appear to support computer inventorship. For example, courts have justified granting patent monopolies on the basis of Labor Theory, which holds that a person has a natural right to the fruits of their work.200 Labor Theory may support giving a patent to someone who has worked for years to invent a new device so that they can profit from their invention, but it does not apply to computers because computers cannot own property. All computer work is appropriated. Similarly, Personality Theo- ry, which holds that innovation is performed to fulfill a human need, would not apply to AI.201 Creative computers invent because they are instructed to invent, and a machine would not be offended by the manner in which its inventions were used. AI might even be a concerning recipient for inventorship under So- cial Planning Theory, which holds that patent rights should be utilized to pro- mote cultural goals.202 An AI could develop immoral new technologies.203 Submissions, however, are no longer rejected by the Patent Office for being “deceitful” or “immoral,” and, to the extent this is a concern, there would be opportunities for a person to judge the morality of an application before it is granted.204
199 See generally Jamie Carter, The Most Powerful Supercomputers in the World—and What They Do, TECHRADAR (Dec. 13, 2014), http://www.techradar.com/us/news/computing/the-most-powerful- supercomputers-in-the-world-and-what-they-do-1276865 [https://perma.cc/AZ94-H3B2] (noting that most advanced computer systems are owned by governments and large businesses).
200 See William Fisher, Theories of Intellectual Property, in NEW ESSAYS IN THE LEGAL AND POLITICAL THEORY OF PROPERTY 173–74 (Stephen Munzer ed., 2001).
201 Tom G. Palmer, Are Patents and Copyrights Morally Justified? The Philosophy of Property Rights and Ideal Objects, 13 HARV. J.L. & PUB. POL’Y 817, 835–36 (1990).
202 Mohammad Amin Naser, Computer Software: Copyrights v. Patents, 8 LOY. L. & TECH. ANN. 37, 41–42 (2009).
203 Beneficial utility was once required for patent grant such that “deceitful” or “immoral” inven- tions would not qualify. In 1999, The United States Court of Appeals for the Federal Circuit in Juicy Whip, Inc. v. Orange Bang, Inc., stated:
[Y]ears ago courts invalidated patents on gambling devices on the ground that they were immoral, . . . but that is no longer the law . . . . “Congress never intended that the patent laws should displace the police powers of the States, meaning by that term those powers by which the health, good order, peace and general welfare of the community are promoted”. . . . [W]e find no basis in section 101 to hold that inventions can be ruled unpatentable for lack of utility simply because they have the capacity to fool some members of the public.
185 F.3d 1364, 1367–68 (Fed. Cir. 1999) (quoting Webber v. Virginia, 103 U.S. 344, 347–48 (1880))
204 See id. See generally Cynthia M. Ho, Splicing Morality and Patent Law: Issues Arising from Mixing Mice and Men, 2 WASH. U. J.L. & POL’Y 247, 247–85 (2000) (discussing Social Planning theory).
 
353
1108 Boston College Law Review [Vol. 57:1079
Ultimately, despite concerns, computer inventorship remains a desirable outcome. The financial motivation it will provide to build creative computers is likely to result in a net increase in the number of patentable inventions pro- duced. Particularly, while quantitative evidence is lacking about the effects of computational invention patents, courts and policy makers should be guided first and foremost by the explicit constitutional rationale for granting pa- tents.205 Further, allowing patents on computational inventions as well as com- puter inventors would do away with what is essentially a legal fiction—the idea that only a human can be the inventor of the autonomous output of a crea- tive computer—resulting in fairer and more effective incentives.
C. It Does Not Matter Whether Computers Think
1. The Questionable Mental Act Requirement
The judicial doctrine that invention involves a mental act should not pre- vent computer inventorship. The Patent Act does not mention a mental act, and courts have discussed mental activity largely from the standpoint of determin- ing when an invention is actually made not whether it is inventive. In any case, whether or not creative computers “think” or have something analogous to consciousness should be irrelevant with regards to inventorship criteria.206
To begin, the precise nature of a “mental act requirement” is unclear. Courts associating inventive activity with cognition have not been using terms precisely or meaningfully in the context of computational inventions. It is un- clear whether computers would have to engage in a process that results in crea- tive output—which they do—or whether, and to what extent, they would need to mimic human thought. If the latter, it is unclear what the purpose of such a requirement would be except to exclude nonhumans (for which a convoluted test is unnecessary). Dr. Thaler has argued eloquently that the Creativity Ma- chine closely imitates the architecture of the human brain.207 Should that mean that the Creativity Machine’s inventions should receive patents while Watson’s do not? There is a slippery slope in determining what constitutes a “thinking” computer system even leaving aside deficits in our understanding of the struc- ture and function of the human brain. Perhaps the Creativity Machine still is not engaging in mental activity—would a computer scientist have to design a completely digitized version of the human brain? Even if designing a com- pletely digitized version of the human brain was possible, it might not be the
205 See United States v. Line Material Co., 333 U.S. 287, 316 (1948) (Douglas, J., concurring) (noting “the reward to inventors is wholly secondary” to the reward to society); see also THE FEDER- ALIST NO. 43 (James Madison) (stating that social benefit arises from patents to inventors).
206 Though, it is surely a fascinating topic deserving of its own treatise. 207 Thaler, Synaptic Perturbation and Consciousness, supra note 29.
 
354
2016] Patent Generating Artificial Intelligence 1109
most effective way to structure a creative computer.208 On top of that, it would be difficult or impossible for the Patent Office and the courts to distinguish between different computers’ architectures.
2. The Turing Test and a Functionalist Approach
The problem of speaking precisely about thought with regards to comput- ers was identified by Alan Turing, one of the founders of computer science, who in 1950 considered the question, “Can machines think?”209 He found the question to be ambiguous, and the term “think” to be unscientific in its collo- quial usage.210 Turing decided the better question to address was whether an individual could tell the difference between responses from a computer and an individual; rather than asking whether machines “think,” he asked whether machines could perform in the same manner as thinking entities.211 Dr. Turing referred to his test as the “Imitation Game” though it has come to be known as the “Turing test.”212
Although the Turing test has been the subject of criticism by some com- puter scientists, Turing’s analysis from more than sixty years ago demonstrates that a mental act requirement would be ambiguous, challenging to administer, and of uncertain utility.213 Incidentally, it is noteworthy that the Patent Office administers a sort of Turing test, which creative computers have successfully passed. The Patent Office receives descriptions of inventions then judges whether they are nonobvious—which is a measure of creativity and ingenui- ty.214 In the case of the Invention Machine’s Patent, it was already noted that “January 25, 2005 looms large in the history of computer science as the day that genetic programming passed its first real Turing test: The examiner had no idea that he was looking at the intellectual property of a computer.”215 In an-
208 This is analogous to one of the criticisms of the Turing test. Namely, that mimicking human responses may not be the best test of intelligence given that not all human responses are intelligent. See Editorial, Artificial Stupidity, THE ECONOMIST, Aug. 1, 1992, at 14.
209 Turing, Computing Machinery and Intelligence, supra note 131, at 433. “Nobody so far has been able to give a precise, verifiable definition of what general intelligence or thinking is. The only definition I know that, though limited, can be practically used is Alan Turing’s. With his test, Turing provided an operational definition of a specific form of thinking—human intelligence.” Tomaso Poggio,“Turing+”Questions,inWHATTOTHINKABOUTMACHINESTHATTHINK48(JohnBrockman ed., 2015).
210 See Turing, Computing Machinery and Intelligence, supra note 131, at 433.
211 See id. at 433–34.
212 See id. at 433.
213 See, e.g., Jose Hernandez-Orallo, Beyond the Turing Test, 9 J. LOGIC LANGUAGE & INFO. 447,
447 (2000).
214 See Koza et al., Evolving Inventions, supra note 49, at 59. The Patent Office “receives written
descriptions of inventions and then judges whether they are nonobvious,” which is a measure of crea- tivity and ingenuity. See id.
215 Keats, John Koza Has Built an Invention Machine, supra note 57.
 
355
1110 Boston College Law Review [Vol. 57:1079
other sense, GP had already also passed the test by independently recreating previously patented inventions: because the original human invention received a patent, the AI’s invention should have received a patent as well, leaving aside that the original patent would be prior art not relied upon by the GP.216
3. The Invention Matters, Not the Inventor’s Mental Process
The primary reason a mental act requirement should not prevent computer inventorship is that the patent system should be indifferent to the means by which invention comes about.
Congress came to this conclusion in 1952 when it abolished the Flash of Genius doctrine.217 That doctrine had been used by the Federal Courts as a test for patentability for over a decade.218 It held that in order to be patentable, a new device, “however useful it may be, must reveal the flash of creative geni- us, not merely the skill of the calling.”219 The doctrine was interpreted to mean that an invention must come into the mind of an inventor in a “flash of genius” rather than as a “result of long toil and experimentation.”220 As a commentator at the time noted, “the standard of patentable invention represented by [the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.”221
The Flash of Genius test was an unhelpful doctrine because it was vague, difficult for lower courts to interpret, involved judges making subjective deci- sions about a patentee’s state of mind, and made it substantially more difficult
216 See id.
217 See 35 U.S.C. § 103 (2012).
218 See, e.g., Hamilton Standard Propeller Co. v. Fay-Egan Mfg. Co., 101 F.2d 614, 617 (6th Cir.
1939) (“The patentee did not display any flash of genius, inspiration or imagination . . . .”). The doc- trine was formalized by the Supreme Court in 1941 in Cuno Engineering Corp. v. Automatic Devices Corp. 314 U.S. 84, 91 (1941). It was reaffirmed by the Court in 1950 in Great Atlantic & Pacific Tea Co. v. Supermarket Equipment Corp., 340 U.S. 147, 154 (1950) (Douglas, J., concurring).
219 Cuno Eng’g Corp., 314 U.S. at 91.
220 The Supreme Court later claimed the “Flash of Creative Genius” language was just a rhetori- cal embellishment and that requirement concerned the device not the manner of invention. Graham v. John Deere Co. of Kan. City, 383 U.S. 1, 15 n.7, 16 n.8 (1966). That was not, however, how the test was interpreted. See P.J. Federico, Origins of Section 103, 5 APLA Q.J. 87, 97 n.5 (1977) (noting the test led to a higher standard of invention in the lower courts). When Congress abolished the test, Con- gress noted it should be immaterial whether invention was made “from long toil and experimentation or from a flash of genius.” 35 U.S.C. § 103. Further, the Court stated in 1966 in Graham that “[t]he second sentence states that patentability as to this requirement is not to be negatived by the manner in which the invention was made, that is, it is immaterial whether it resulted from long toil and experi- mentation or from a flash of genius.” Graham, 383 U.S. at 16 n.8.
221 The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 87.
 
356
2016] Patent Generating Artificial Intelligence 1111
to obtain a patent.222 The test was part of a general hostility toward patents ex- hibited by mid-twentieth century courts, a hostility that caused United States Supreme Court Justice Robert Jackson to note in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.”223
Criticism of this state of affairs led President Roosevelt to establish a Na- tional Patent Planning Commission to study the patent system and to make recommendations for its improvement.224 In 1943, the Commission reported with regard to the Flash of Genius doctrine that “patentability shall be deter- mined objectively by the nature of the contribution to the advancement of the art, and not subjectively by the nature of the process by which the invention may have been accomplished.”225 Adopting this recommendation, the Patent Act of 1952 legislatively disavowed the Flash of Genius test.226 In the same manner, patentability of computational inventions should be based on the in- ventiveness of a computer’s output rather than on a clumsy anthropomorphism because, like Turing, patent law should be interested in a functionalist solution.
4. A Biological Requirement Would Be a Poor Test
Incidentally, even a requirement for biological intelligence might be a bad way to distinguish between computer and human inventors. Although function- ing biological computers do not yet exist, all of the necessary building blocks have been created.227 In 2013, a team of Stanford University engineers created
222 See DePaul College of Law, Patent Law—“Flash of Genius” Test for Invention Rejected, 5 DEPAUL L. REV. 144, 146 (1955); Stephen G. Kalinchak, Obviousness and the Doctrine of Equiva- lents in Patent Law: Striving for Objective Criteria, 43 CATH. U. L. REV. 577, 586 (1994).
223 Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
224 See William Jarratt, U.S. National Patent Planning Commission, 153 NATURE 12, 14 (1944). 225 The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 85 (internal quota-
tion marks omitted).
226 See 35 U.S.C. § 103 (2012). Further, in Graham, the Supreme Court noted that “[i]t . . . seems
apparent that Congress intended by the last sentence of § 103 to abolish the test it believed this Court announced in the controversial phrase ‘flash of creative genius,’ used in Cuno Engineering.” Graham, 383 U.S. at 15.
227 See Sebastian Anthony, Stanford Creates Biological Transistors, the Final Step Towards Com- puters Inside Living Cells, EXTREMETECH (Mar. 29, 2013), http://www.extremetech.com/extreme/ 152074-stanford-creates-biological-transistors-the-final-step-towards-computers-inside-living-cells [https://perma.cc/ENX4-WZKA] (noting that, in addition to biological transistors, a method for data storage and a means of connecting transcriptors with memory would be necessary to create a biological computer and stating that “[f]ortunately, as we’ve covered a few times before, numerous research groups have successfully stored data in DNA—and Stanford has already developed an ingenious method of using the M13 virus to transmit strands of DNA between cells”); see also Monica E. Ortiz & Drew Endy, Engineered Cell-Cell Communication via DNA Messaging, J. BIOLOGICAL ENGINEERING (Dec. 1, 2012), https://jbioleng.biomedcentral.com/articles/10.1186/1754-1611-6-16 [https://perma.cc/6BWZ-GUUP]; Katherine Deria, Biological Supercomputer Can Solve Complex Problems Using Less Energy, TECH TIMES (Feb. 27, 2016), http://www.techtimes.com/articles/137017/20160227/biological-supercomputer- can-solve-complex-problems-using- less-energy.htm [https://perma.cc/A75T-3TVV] (describing a new biological supercomputer model powered by a biochemical that facilitates energy transfer among cells).
 
357
1112 Boston College Law Review [Vol. 57:1079
a biological version of an electrical transistor. Mechanical computers use nu- merous silicon transistors to control the flow of electrons along a circuit to cre- ate binary code.228 The Stanford group created a biological version with the same functionality by using enzymes to control the flow of RNA proteins along a strand of DNA.229 Envisioning a not-too-distant future in which com- puters can be entirely biological, there seems to be no principled reason why a biological, but not a mechanical version, of Watson should qualify as an inven- tor. In the event that policymakers decide computers should not be inventors, a rule explicitly barring nonhuman inventorship would be a better way to achieve that result.
D. Computer Inventors Are Permitted Under a Dynamic Interpretation of Current Law
Whether a computer can be an inventor in a constitutional sense is a ques- tion of first impression. If creative computers should be inventors, as this Arti- cle has argued, then a dynamic interpretation of the law should allow computer inventorship.230 Such an approach would be consistent with the Founders’ in- tent in enacting the Patent and Copyright Clause, and it would interpret the Patent Act to further that purpose.231 Nor would such an interpretation run afoul of the chief objection to dynamic statutory interpretation, namely that it interferes with reliance and predictability and the ability of citizens “to be able to read the statute books and know their rights and duties.”232 That is because a dynamic interpretation would not upset an existing policy; permitting comput- er inventors would allow additional patent applications rather than retroactive- ly invalidate previously granted patents, and there is naturally less reliance and predictability in patent law than in many other fields given that it is a highly dynamic subject area that struggles to adapt to constantly changing technolo- gies.233
228 See Anthony, supra note 227.
229 See id.
230 See William N. Eskridge, Jr. & Philip P. Frickey, Statutory Interpretation as Practical Rea-
soning, 42 STAN. L. REV. 321, 324 (1990).
231 See HENRY M. HART, JR. & ALBERT M. SACKS, THE LEGAL PROCESS: BASIC PROBLEMS IN
THE MAKING AND APPLICATION OF LAW 1124 (1994); see also Abbe R. Gluck, The States as Labora- tories of Statutory Interpretation: Methodological Consensus and the New Modified Textualism, 119 YALE L.J. 1750, 1764(2010) (noting that purposivists subscribe to dynamic methods of statutory in- terpretation).
232 See Eskridge, Jr. & Frickey, supra note 230, at 340.
233 See William C. Rooklidge & W. Gerard von Hoffmann, III, Reduction to Practice, Experi- mental Use, and the “On Sale” and “Public Use” Bars to Patentability, 63 ST. JOHN’S L. REV. 1, 49– 50 (1988).
 
358
2016] Patent Generating Artificial Intelligence 1113
Other areas of patent law have been the subject of dynamic interpreta- tion.234 For example, in the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was charged with deciding whether genetically modified organisms could be patented.235 It held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress . . . would frustrate the purposes of the patent law.”236 The court noted that Congress chose expansive language to protect a broad range of patentable subject mat- ter.237
Under that reasoning, computer inventorship should not be prohibited based on statutory text designed to favor individuals over corporations. It would be particularly unwise to prohibit computer inventors on the basis of literal interpretations of texts written when computational inventions were un- foreseeable. If computer inventorship is to be prohibited, it should only be on the basis of sound public policy. Drawing another analogy from the copyright context, just as the terms “Writings” and “Authors” have been construed flexi- bly in interpreting the Patent and Copyright Clause, so too should the term “Inventors” be afforded the flexibility needed to effectuate constitutional pur- poses.238 Computational inventions may even be especially deserving of pro- tection because computational creativity may be the only means of achieving certain discoveries that require the use of tremendous amounts of data or that deviate from conventional design wisdom.239
III. IMPLICATIONS OF COMPUTERINVENTORSHIP
This Part finds that a computer’s owner should be the default assignee of any invention because this is most consistent with the rules governing owner-
234 The Supreme Court has called the section of the U.S. Code relating to patentable subject mat- ter a “dynamic provision designed to encompass new and unforeseen inventions.” J.E.M. AG Supply, Inc. v. Pioneer Hi-Bred Int’l, Inc., 534 U. S. 124, 135 (2001). The Court noted in Bilski v. Kappos that “it was once forcefully argued that until recent times, ‘well- established principles of patent law prob- ably would have prevented the issuance of a valid patent on almost any conceivable computer pro- gram.’” 561 U.S. 593, 605 (2010) (quoting Diamond v. Diehr, 450 U.S 175, 195 (1981) (Stevens, J., dissenting)). The Court, however, went on to state that “this fact does not mean that unforeseen inno- vations such as computer programs are always unpatentable.” Id. (citing Diehr, 450 U.S at 192–93 (Stevens, J., dissenting)).
235 See Diamond v. Chakrabarty, 447 U. S. 303, 317 (1980).
236 Id. at 315.
237 See id. at 316.
238 In 1973, the Supreme Court in Goldstein v. California noted that the terms “Writings” and
“Authors,” have “not been construed in their narrow literal sense but, rather, with the reach necessary to reflect the broad scope of constitutional principles.” 412 U.S. 546, 561 (1973).
239 See Jason D. Lohn, Evolvable Systems for Space Application, NASA (Nov. 24, 2003), http:// www.genetic-programming.com/c2003jasonlohn20031124talk.pdf [https://perma.cc/BWC7-UPJK]; Adam Frank, The Infinite Monkey Theorem Comes to Life, NPR (Dec. 10, 2013), http://www.npr. org/blogs/13.7/2013/12/10/249726951/the-infinite-monkey-theorem-comes-to-life [https://perma.cc/ PT5R-53GS].
 
359
1114 Boston College Law Review [Vol. 57:1079
ship of property and it would most incentivize innovation.240 Additionally, this Part suggests that where a computer’s owner, developer, and user are different entities, such parties could negotiate alternative arrangements by contract.241 Computer ownership here generally refers to software ownership, although there may be instances in which it is difficult to distinguish between hardware and software, or even to identify a software “owner.”242 This Part also exam- ines the phenomenon of automation and the displacement of human inventors by computers and finds that computational invention remains beneficial de- spite legitimate concerns.243
This Part concludes by finding that the arguments in support of com- puter inventorship apply with equal force to non-human authors.244 Allowing animals to create copyrightable material would result in more socially valuable art by creating new incentives for people to facilitate animal creativity.245 It would also provide incentives for environmental conservation.246 Lastly, this Part examines some of the implications of computer inventorship for other are- as of patent law.247
A. Computational Invention Ownership
1. Options for Default Assignment Rules
In the event that computers are recognized as patent inventors, there still remains the question of who would own these patents. Computers cannot own property, and it is safe to assume that “computer personhood” is not on the horizon.248 This presents a number of options for patent ownership (assign- ment) such as a computer’s owner (the person who owns the AI as a chattel), developer (the person who programmed the AI’s software), or user (the person giving the AI tasks).249 The developer, user, and owner may be the same per- son, or they may be different entities.
Ownership rights to computational inventions should vest in a computer’s owner because it would be most consistent with the way personal property (in-
240 See infra notes 240–312 and accompanying text.
241 See infra notes 248–255 and accompanying text.
242 See generally GOVERNMENT OFFICE FOR SCIENCE, supra note 20.
243 See infra notes 256–278 and accompanying text.
244 See infra notes 279–312 and accompanying text.
245 See infra notes 279–287 and accompanying text.
246 See infra notes 279–287 and accompanying text.
247 See infra notes 288–313 and accompanying text.
248 See generally Adam Winkler, Corporate Personhood and the Rights of Corporate Speech, 30
SEATTLE U. L. REV. 863, 863 (2007) (describing the phenomenon of “corporate personhood”).
249 There are other, less conventional, options. For instance, even if legally listed as an inventor, it might be the case that no one could own a computer’s invention and that computational inventions would automatically become part of the public domain. New legislation could also establish that own-
ership rights to computational inventions automatically vest in a government agency.
 
360
2016] Patent Generating Artificial Intelligence 1115
cluding both computers and patents) is treated in the United States and it would most incentivize computational invention.250 Assignment of computa- tional inventions to a computer’s owner could be taken as a starting point alt- hough parties would be able to contract around this default, and as computa- tional inventions become more common, negotiations over these inventions may become a standard part of contract negotiations.251
2. Owner vs. User Assignment
To see why it would be problematic to have patent ownership rights vest in a computer’s user, consider the fact that IBM has made Watson available to numerous developers without transferring Watson’s ownership.252 To the extent that Watson creates patentable results as a product of its interactions with us- ers, promoting user access should result in more innovation.
There is theoretically no limit to the number of users that Watson, as a cloneable software program, could interact with at once. If Watson invents while under the control of a non-IBM user, and the “default rule” assigns the invention to the user, IBM might be encouraged to restrict user access; in con- trast, assigning the invention to IBM would be expected to motivate IBM to further promote access. If IBM and a user were negotiating for a license to Watson, the default rule might result in a user paying IBM an additional fee for the ability to patent results or receiving a discount by sticking with the default. It may also be the case that Watson co-invents along with a user; in which case, a system of default assignment to a computer’s owner would result in both IBM and the user co-owning the resulting patent. Where creative comput- ers are not owned by large enterprises with sophisticated attorneys, it is more likely the default rule will govern the final outcome.253
250 See Annemarie Bridy, Coding Creativity: Copyright and the Artificially Intelligent Author, 2012 STAN. TECH. L. REV. 1, 1–28 (arguing that “AI authorship is readily assimilable to the current copyright framework through the work made for hire doctrine, which is a mechanism for vesting cop- yright directly in a legal person who is acknowledged not to be the author-in-fact of the work in ques- tion”).
251 See generally Ian Ayres & Robert Gertner, Filling Gaps in Incomplete Contracts: An Econom- ic Theory of Default Rules, 99 YALE L.J. 87 (1990) (discussing default rules and how they are formed in the context of contract law).
252 Bruce Upbin, IBM Opens Up Its Watson Cognitive Computer for Developers Everywhere, FORBES (Nov. 14, 2013), http://www.forbes.com/sites/bruceupbin/2013/11/14/ibm-opens-up-watson- as-a-web-service/ [https://web.archive.org/web/20160310124933/http://www.forbes.com/sites/bruce upbin/2013/11/14/ibm-opens-up-watson-as-a-web-service/#1ccc9051d0ee].
253 See, e.g., Daniel D. Barnhizer, Power, Inequality and the Bargain: The Role of Bargaining Power in the Law of Contract—Symposium Introduction, 2006 MICH. ST. L. REV. 841, 842 (describ- ing various approaches to dealing with bargaining power asymmetries).
 
361
1116 Boston College Law Review [Vol. 57:1079 3. Owner vs. Developer Assignment
Likewise, patent ownership rights should vest in a computer’s owner ra- ther than its developer. Owner assignment would provide a direct economic incentive for developers in the form of increased consumer demand for crea- tive computers. Having assignment default to developers would interfere with the transfer of personal property in the form of computers, and it would be lo- gistically challenging for developers to monitor computational inventions made by machines they no longer own.
In some instances, however, owner assignment of intellectual property (IP) rights might produce unfair results. In the movie Her, the protagonist (who is a writer) purchases an AI named Samantha that organizes his existing writings into a book, which it then submits to be published.254 It is possible that Samantha would own the copyright in the selection and arrangement of his writings and would thus have a copyright interest in the book.255 Here, owner assignment of intellectual property rights seems unappealing if there is a min- imal role played by the consumer/owner. The consumer’s role in the process might be limited to simply purchasing a creative computer and asking it to do something (where the owner is the user) or purchasing a computer and then licensing it to someone else to use creatively. Further, assigning computer in- ventions to owners might impede the development or sharing of creative ma- chines because the machine developers might want to retain the rights to the computational inventions their computers produce.
These problems are more easily resolved than problems associated with assigning intellectual property rights to developers by default. Developers could either require owners to pay them the value of a creative machine, taking into account the likelihood of those machines engaging in computational in- vention, or avoid the problem by licensing rather than selling creative comput- ers. In the case of licensing, the developer remains the owner, and the consum- er is simply a user. One might imagine a creative computer, such as the AI in Her, coming with a license agreement under which consumers prospectively assign any inventions made by the system to the licensor.
This analysis also reveals an important reason why computational inven- tion works best when the computer is the legal inventor. If computational in- ventions were treated as patentable but computers could not be inventors, then presumably the first person to recognize a computer’s invention would be the legal inventor and patent owner. That means that the computer’s user, rather than its developer or owner, would likely be the patentee as the person in a po- sition to first recognize a computational invention. To the extent this is an un-
254 HER (Annapurna Pictures 2013).
255 See RONALD B. STANDLER, COPYRIGHT FOR COMPILATIONS IN THE USA 22 (2013).
 
362
2016] Patent Generating Artificial Intelligence 1117
desirable outcome, as this Article has argued, then the best solution is to permit computer inventorship.
In sum, assigning a computer’s invention by default to the computer’s owner seems the preferred outcome, and computer owners would still be free to negotiate alternate arrangements with developers and users by contract.
B. Coexistence and Competition
1. Computers and People Will Compete in Creative Fields
“IBM has bragged to the media that Watson’s question-answering skills are good for more than annoying Alex Trebek. The company sees a future in which fields like medical diagnosis, business analytics, and tech support are automated byquestion-answering software like Watson. Just as factory jobs were eliminated in the 20th century by new assembly-line robots, [Watson’s Jeopardy competitors] were the first knowledge-industry workers put out of work by the new generation of ‘thinking’ machines. ‘Quiz show contestant’ may be the first job made redundant by Watson, but I’m sure it won’t be the last.”256
With the expansion of computers into creative domains previously occu- pied only by people, machines threaten to displace human inventors. To better understand this phenomenon, consider the following hypothetical example in- volving the field of antibody therapy.
Antibodies are small proteins made naturally by the immune system, pri- marily to identify and neutralize pathogens such as bacteria and viruses.257 They are Y-shaped proteins that are largely similar to one another in structure although antibodies contain an extremely variable region which binds to target structures.258 Differences in that region are the reason different antibodies bind to different targets—for example the reason why one antibody binds to a can- cer cell while another binds to the common cold virus.259 The body generates antibody diversity in part by harnessing the power of random gene recombina- tions and mutations (much as GP does), and then it selects for antibodies with a desired binding (much as GP does).260 Following the discovery of antibody structure and the development of technologies to manufacture antibodies in the 1970s, human researchers began to create antibodies for diagnostic and thera-
256 Ken Jennings, My Puny Human Brain, SLATE (Feb 16, 2011), http://primary.slate.com/ articles/arts/culturebox/2011/02/my_puny_human_brain.html [https://perma.cc/BE9X-NUNX].
257 See Janice M. Reichert, Marketed Therapeutic Antibodies Compendium, 4 MABS 413, 413 (2012).
258 See Neil S. Lipman et al., Monoclonal Versus Polyclonal Antibodies: Distinguishing Charac- teristics, Applications, and Information Resources, 46 ILAR J. 258, 258 (2005).
259 See id. at 258–59. 260 See id. at 259.
 
363
1118 Boston College Law Review [Vol. 57:1079
peutic purposes.261 Therapeutic antibodies can block cell functions, modulate signal pathways, and target cancer cells among other functions.262 There are now dozens of artificially manufactured antibodies approved to treat a variety of medical conditions.263
One of the interesting things about antibodies from a computational in- vention perspective is that a finite number of antibodies exist. There are, at least, billions of possible antibodies, which is enough natural diversity for the human immune system to function and to keep human researchers active for the foreseeable future.264 Even so, there are only so many possible combina- tions of amino acids (the building blocks of proteins) that the body can string together to generate an antibody.265 It is not hard to imagine that, with enough computing power, an AI could sequence every possible antibody that could ever be created. Even if that was trillions of antibodies, the task would be rela- tively simple for a powerful enough computer but impossible for even the larg- est team of human researchers without computer assistance.
Generating the entire universe of antibody sequences would not reveal all of the possible functions of those antibodies; so, a computer’s owner could not obtain patents for all of the sequences on this basis alone because usefulness (utility) of the invention must be disclosed in addition to the sequence itself.266 The computer could, however, prevent any future patents on the structure of new antibodies (assuming the sequence data is considered an anticipatory dis- closure).267 If this occurred, a computer would have preempted human inven- tion in an entire scientific field.268
2. Computers May Refocus Human Activity
In the hypothetical scenario above, society would gain access to all possi- ble future knowledge about antibody structure at once rather than waiting dec- ades or centuries for individuals to discover these sequences. Early access to antibody sequences could prove a tremendous boon to public health if it led to
261 See Reichert, Marketed Therapeutic Antibodies Compendium, supra note 257, at 413. 262 See id.
263 See id.
264 See Lipman, supra note 258, at 258.
265 See, e.g., U.S. Patent No. 8,008,449 (filed May 2, 2006) (disclosing antibodies to the protein Programmed Death 1 (“PD-1”) by virtue of publishing their amino acid sequences).
266 See In re Fisher, 421 F.3d 1365, 1370 (Fed. Cir. 2005).
267 See MPEP, supra note 43, § 2131.
268 Along similar lines, projects such as “All Prior Art” and “All the Claims” are attempting to
use machines to create and publish vast amounts of information to prevent other parties from obtain- ing patents. See ALL PRIOR ART, allpriorart.com/about/ [https://perma.cc/3XJR-2FF5] (last visited Jul. 10, 2016); ALL THE CLAIMS, alltheclaims.com/about/ [https://perma.cc/QDE2-5J49] (last visited Jul. 10, 2016); see also Hattenbach & Glucoft, supra note 5, at 35–36 (describing the efforts of a compa- ny, Cloem, which is mechanically publishing possible patent claims to prevent others from obtaining patents).
 
364
2016] Patent Generating Artificial Intelligence 1119
the discovery of new drugs. Some antibody sequences might never be identi- fied without creative computers.
Creative computers may simply refocus, rather than inhibit, human crea- tivity. In the short term, scientists who were working on developing new anti- body structures might shift to studying how the new antibodies work, or find- ing new medical applications for those antibodies, or perhaps move on to stud- ying more complex proteins beyond the capability of AI to comprehensively sequence. For the foreseeable future, there will be plenty of room for human inventors—all with net gains to innovation.
Antibody therapies are just one example of how AI could preempt inven- tion in a field. A sophisticated enough computer could do something similar in the field of genetic engineering by creating random sequences of DNA. Living organisms are a great deal more complex than antibodies, but the same funda- mental principles would apply. Given enough computing power, an AI could model quintillions of different DNA sequences, inventing new life forms in the process. In fact, on a smaller scale, this is something GP already does.269 Alt- hough results have been limited by the computationally intense nature of the process, that will change as computers continue to improve.270 By creating novel DNA sequences, GP would be performing the same function as non- digital GP—natural evolution!
3. Dealing with Industry Consolidation
It will probably be the case that creative computers result in greater con- solidation of intellectual property in the hands of large corporations such as IBM. Such businesses may be the most likely to own creative computers ow- ing to their generally resource intense development.271 As previously dis- cussed, the benefits, however, may outweigh the costs of such an outcome. Imagine that Watson was the hypothetical AI that sequenced every conceivable antibody and, further, that Watson could analyze a human cancer and match it with an antibody from its library to effectively treat the cancer. Essentially, this could allow IBM to patent the cure for cancer.
Though this would be profoundly disruptive to the medical industry and might lead to market abuses, it is not a reason to bar computational invention. Society would obtain the cure for cancer, and IBM would obtain a twenty-year monopoly (the term of a patent) in return for publically disclosing the infor-
269 See, e.g., Tim Stefanini, The Genetic Coding of Behavioral Attributes in Cellular Automata, in ARTIFICIAL LIFE AT STANFORD 172–80 (John R. Koza ed., 1994); W.B. Langdon & B.F. Buxton, Genetic Programming for Mining DNA Chip Data from Cancer Patients, 5 GENETIC PROGRAMMING AND EVOLVABLE MACHINES 251, 251 (2004).
270 See Stefanini, supra note 269, at 172–80.
271 See Carter, supra note 199 (noting that most advanced computer systems are owned by gov- ernments and large businesses).
 
365
1120 Boston College Law Review [Vol. 57:1079
mation a competitor would need to duplicate Watson’s invention.272 In the ab- sence of creative computers, such an invention might never come about.
To the extent that price gouging and supply shortages are a concern, pro- tections are built into patent law to protect consumers against such prob- lems.273 For example, the government could exercise its march in rights or is- sue compulsory licenses.274
4. The Creative Singularity and Beyond
As creative computers become more and more sophisticated, at some point in the future, it is possible that they could have a very disruptive effect on human creativity. In recent years, a number of prominent scientists and entre- preneurs such as Bill Gates, Stephen Hawking, and Elon Musk have expressed concern about the “singularity”—a point in the future when machines outper- form humans.275 Likewise, a “creative singularity” in which computers over- take people as the primary source of innovation may be inevitable. Taken to its logical extreme and given that there is really no limit to the number of com- puters that could be built or their capabilities, it is not especially improbable to imagine that computers could eventually preempt much or all human inven-
272 See MPEP, supra note 43, § 2164.
273 See the case of Martin Shkreli, who has been pilloried for price gouging by drastically increasing the price of an old drug, Daraprim. See Andrew Pollack & Julie Creswell, Martin Shkreli, the Mercurial Man Behind the Drug Price Increase That Went Viral, N.Y. TIMES (Sept. 22, 2015), http://www. nytimes.com/2015/09/23/business/big-price-increase-for-an-old-drug-will-be-rolled-back-turing-chief- says.html?r_=0 [https://perma.cc/Q3Q3-95CS]. In this particular case, the monopoly was due to lack of competition, but the same economic principles apply to patent monopolies. See Ryan Abbott, Balancing Access and Innovation in India’s Shifting IP Regime, Remarks, 35 WHITTIER L. REV. 341, 344 (2014) (discussing patent law protections against practices including “evergreening”).
274 See, e.g., Abbott, supra note 273, at 345 (explaining India’s issuance of a compulsory license).
275 Of perhaps greater concern than automation displacing human workers, a number of promi- nent scientists are concerned about the implications of machines outperforming people. Professor Stephen Hawking has warned that self-improving computers could threaten the very existence of humanity. Rory Cellan-Jones, Stephen Hawking Warns Artificial Intelligence Could End Mankind, BBC (Dec. 2, 2014), http://www.bbc.com/news/technology-30290540 [https://perma.cc/2NWE- D8MB]. Elon Musk recently donated ten million dollars to an institute focused on threats posed by advances in AI. See Chris Isidore, Elon Musk Gives $10M to Fight Killer Robots, CNN MONEY (Jan. 16, 2015), http://money.cnn.com/2015/01/15/technology/musk-artificial-intelligence/ [https://perma. cc/6N3U-WTWF]; see also Dylan Love, Scientists Are Afraid to Talk About the Robot Apocalypse, and That’s a Problem, BUSINESS INSIDER (July 18, 2014), http://www.businessinsider.com/robot- apocalypse-2014-7#ixzz3QQpLJ0Jj [https://perma.cc/AH5J-5G34] (noting that robots may pose a “real risk” to humanity); Kevin Rawlinson, Microsoft’s Bill Gates Insists AI Is a Threat, BBC (Jan. 29, 2015), http://www.bbc.com/news/31047780 [https://perma.cc/9MXD-TQ68] (expressing Bill Gates’s concerns about advanced AI). Concerns about the threat posed by advanced AI are nothing new. See, e.g., Irving J. Good, Speculations Concerning the First Ultraintelligent Machine, 6 AD- VANCES IN COMPUTERS 31, 31–88 (1966). As an aside, the date the patent for the Creativity Machine issued is the same date as “Judgment Day” from the original Terminator movie, August 29, 1997.
 
366
2016] Patent Generating Artificial Intelligence 1121
tion.276 The future may involve iPads in place of fast food cashiers,277 robots empathizing with hospital patients,278 and AI responsible for research. For now, this is a distant possibility.
Moreover, patents on computational inventions would not prevent this outcome. If creative computers ever come to substantially outperform human inventors, they still will replace people—just without the ability to receive pa- tents.
C. Lessons for Copyright Law
1. Promoting the Useful Arts and Environmental Conservation
The need for computer inventorship also explains why the Copyright Of- fice’s Human Authorship Requirement is misguided. Nonhumans should be allowed to qualify as authors because doing so would incentivize the creation of new and valuable creative output. In the case of the Monkey Selfies, Mr. Slater, a photographer familiar with macaques, reported that he carefully staged the environment in such a way that Naruto would be likely to take his own photograph.279 If accurate, he probably did so in part due to an expectation of selling the resulting photographs.280 Had Mr. Slater known in advance that the images would pass into the public domain, he might never have taken the photographs. Although an owner default assignment rule would give copyright ownership of the Monkey Selfies to Naruto’s owner281 rather than to Mr. Slater, he could have contracted with Naruto’s owner to purchase or license the pho- tographs. Certainly in the aggregate, fewer photographers will engage in such activities without the prospect of copyright protection, and although animal selfies are not the cure for cancer, they have societal value as does any other form of art.282
276 See Brian Christian, Mind vs. Machine, THE ATLANTIC (Mar. 2011), http://www.theatlantic. com/magazine/archive/2011/03/mind-vs-machine/308386/ [https://perma.cc/7CMT-DAEZ].
277 See Victoria Taft, Protesters Aren’t Going to Like How McDonald’s Is Reacting to Their Mini- mum Wage Concerns, INDEP. J. REV. (May 26, 2015), http://ijr.com/2015/05/330129-guy-wont-taking- mickey-ds-order-much-longer-might-surprised-reasons/ [https://perma.cc/AZH6-GWXC].
278 See Bertalan Mesko, Will Robots Take Over Our Jobs in Healthcare?, in MY HEALTH UP- GRADED: REVOLUTIONARY TECHNOLOGIES TO BRING A HEALTHIER FUTURE 80, 80–83 (Richard E. Cytowic ed. 2015).
279 See Photographer ‘Lost £10,000’ in Wikipedia Monkey ‘Selfie’ Row, BBC (Aug. 7, 2014), http://www.bbc.com/news/uk-england-gloucestershire-28674167 [https://perma.cc/WQF8-KNYL]. 280 See Complaint, Naruto v. Slater, 2016 WL 362231 (No. 3:15-cv-04324) at *1, *9 (noting his
sale of copies of the Monkey Selfies).
281 Here that might be the government of Indonesia or the Tangkoko Reserve (Naruto’s home)
depending on Indonesian law and the reserve’s structure. See Complaint, Naruto, 2016 WL 362231 (No. 3:15-cv-04324) at *1.
282 See Johnson, supra note 134, at 16 (describing the sale of works of art created by a chimpan- zee whose “fans may have included . . . Pablo Picasso” and works created by seven Asian elephants) (internal quotation marks omitted). Alternatively, in the words of Justice Holmes:
 
367
1122 Boston College Law Review [Vol. 57:1079
Animal authorship might also have some ancillary conservation benefits. Continuing with the case of the Monkey Selfies, Naruto is a member of a criti- cally endangered species with a total population of between four and six thou- sand macaques.283 The species’ “numbers have decreased by approximately ninety percent (90%) over the last twenty-five years due to human population encroachment, being killed by humans in retribution for foraging on crops, and being trapped and slaughtered for bush meat.”284 Permitting Naruto’s activities to have a new source of value would be another economic incentive for private and public landowners to conserve biodiversity.285 Naruto lives in a reserve in Indonesia, but the United States also continues to suffer significant biodiversi- ty loss.286 Some environmentalist groups argue this is because conservation efforts are chronically underfunded.287 Nonhuman authorship might be an ad- ditional policy lever to reverse this trend.
D. Rethinking the Ultimate Test of Patentability
Considering the case for creative computers provides insight into other areas of patent law. Take, for instance, the nonobviousness requirement for grant of a patent.288 When Congress did away with the Flash of Genius doc- trine, it replaced that test with the current requirement for nonobviousness.289 Part of the requirement’s evaluation involves employing the legal fiction of a “person having ordinary skill in the art” (“PHOSITA” or simply the “skilled person”) who serves as a reference for determining whether an invention is nonobvious.290 Essentially, an applicant cannot obtain a patent if the skilled person would have found the difference between a new invention and the prior
It would be a dangerous undertaking for persons trained only to the law to constitute themselves final judges of the worth of pictorial illustrations, outside the narrowest and most obvious limits. At the one extreme some works of genius would be sure to miss appreciation. . . . At the other end, copyright would be denied to pictures which ap- pealed to a public less educated than the judge.
Bleistein v. Donaldson Lithographing Co., 188 U.S. 239, 251–52 (1903).
283 See Complaint, Naruto, 2016 WL 362231 (No. 3:15-cv-04324), at *3.
284 Id.
285 For a discussion of existing economic incentives for landowners to conserve biodiversity, see
generally Frank Casey et al., Incentives for Biodiversity Conservation: An Ecological Is Economic Assessment, DEFENDERS OF WILDLIFE (2006), http://www.defenders.org/publications/incentives_for_ biodiversity_conservation.pdf [https://perma.cc/LEM8-SPYC].
286 See id. at 7.
287 Id. at 8.
288 See 35 U.S.C. § 103 (2012).
289 See id.
290 Actually, the skilled person is relevant to many areas of patent law including claim construc-
tion, best mode, definiteness, enablement, and the doctrine of equivalents. See Dan L. Burk & Mark A. Lemley, Is Patent Law Technology-Specific?, 17 BERKELEY TECH. L.J. 1155, 1186–87 (2002).
 
368
2016] Patent Generating Artificial Intelligence 1123
art (what came before the invention) obvious.291 The test presumes that the skilled person is selectively omniscient, having read, understood, and remem- bered every existing reference from the prior art in the relevant field of inven- tion (analogous art).292 A federal judge explained that the way to apply the ob- viousness test is to “first picture the inventor as working in his shop with the prior art references, which he is presumed to know, hanging on the walls around him.”293
Needless to say, no actual person could have such knowledge, but the standard helps avoid difficult issues of proof related to an inventor’s actual knowledge; also, it prevents obvious variations of publically disclosed inven- tions from being patented.294 Stopping obvious variations from being patented is important because that prevents the removal of knowledge from the public domain.295 Inventions which would have been obvious to skilled persons are already within reach of the public.296 This raises the bar to obtaining a patent— a result that is desirable because patents should not be granted lightly given their anticompetitive effects.297 At the same time, creating too high a bar to patentability is undesirable because then patents would fail to adequately in- centivize researchers. A balance is needed.298 Ideally, the system would only issue patents for inventions that would not have been created but for the expec- tation of obtaining a patent.299 The importance of the nonobvious requirement to patentability has led to its characterization as the “ultimate condition of pa-
291 See 35 U.S.C. § 103(a).
292 See Standard Oil Co. v. Am. Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985). See generally Lance Leonard Barry, Cézanne and Renoir: Analogous Art in Patent Law, 13 TEX. INTELL. PROP. L.J. 243 (2005) (discussing analogous art).
293 Application of Winslow, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
294 See Jonathan J. Darrow, The Neglected Dimension of Patent Law’s PHOSITA Standard, 23 HARV. J.L. & TECH. 227, 228 (2009); Daralyn J. Durie & Mark A. Lemley, A Realistic Approach to the Obviousness of Inventions, 50 WM. & MARY L. REV. 989, 991–92, 1017 (2008).
295 See Sakraida v. Ag Pro, Inc., 425 U.S. 273, 281 (1976) (“A patent . . . which only unites old elements with no change in their respective functions . . . obviously withdraws what already is known into the field of its monopoly and diminishes the resources available to skillful men . . . .”) (internal quotation marks omitted) (quoting Great Atl. & Pac. Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 152–53 (1950))
296 See id.
297 See Eldred v. Ashcroft, 537 U.S. 186, 246 (2003) (Breyer, J., dissenting) (discussing the views of Madison, Jefferson “and others in the founding generation, [who] warned against the dangers of monopolies”).
298 See Edmund W. Kitch, Graham v. John Deere Co.: New Standards for Patents, 1966 SUP. CT. REV. 293, 301 (“The non-obviousness test makes an effort, necessarily an awkward one, to sort out those innovations that would not be developed absent a patent system.”).
299 Graham v. John Deere Co., 383 U.S. 1, 11 (1965) (“The inherent problem was to develop some means of weeding out those inventions which would not be disclosed or devised but for the inducement of a patent.”).
 
369
1124 Boston College Law Review [Vol. 57:1079
tentability.”300 The idea of a PHOSITA understanding all of the prior art in her field was always fictional,301 but now it is possible for a skilled entity, in the form of a computer, to possess such knowledge. For example, Watson’s data- base could be populated with every published food recipe available to the Pa- tent Office. This makes the skilled computer a natural substitute for the hypo- thetical skilled person. The standard would require a skilled computer rather than a creative computer for the same reason that the skilled person is not an inventive person.302 PHOSITA has traditionally been characterized as skilled at repetitive processes that produce expected results.303 If the skilled person were capable of inventive activity, then inventive patent applications would appear obvious.304
Replacing the skilled person with the skilled computer suggests a change to the nonobviousness test. At present, the test takes into account the skilled person’s knowledge of the prior art. Decreasing the universe of prior art makes it easier to get a patent because, with less background knowledge, a new inven- tion is more likely to appear inventive.305 Likewise, expanding the universe of prior art would raise the patentability bar.306 Yet although it may be unrealistic
300 See NONOBVIOUSNESS—THE ULTIMATE CONDITION OF PATENTABILITY 23 (John With- erspoon ed., 1980) (commemorating the twenty-fifth anniversary of the passage of 35 U.S.C. § 103 (2012)).
301 See Dan L. Burk, The Role of Patent Law in Knowledge Codification, 23 BERKELEY TECH. L.J. 1009, 1025 (2008) (“[T]he PHOSITA is a fictional composite, a conceptual construct imagined for the purpose of assessing the claimed invention against its technological antecedents.”); Cyril A. Soans, Some Absurd Presumptions in Patent Cases, 10 PAT. TRADEMARK & COPY. J. RES. & ED. 433, 438–39 (1967) (railing against the judicially created “superhuman Frankenstein monster Mr. Phosi- ta”).
302 Factors to consider in determining the level of ordinary skill in the art include: (1) “type of problems encountered in the art”; (2) “prior art solutions to those problems”; (3) “rapidity with which innovations are made”; (4) “sophistication of the technology”; and (5) “educational level of active workers in the field.” In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995). The U.S. Court of Ap- peals for the Federal Circuit has acknowledges that “[i]n a given case, every factor may not be pre- sent, and one or more factors may predominate.” See id.; see also Custom Accessories, Inc. v. Jeffrey- Allan Indus., Inc., 807 F.2d 955, 962–63 (Fed. Cir. 1986) (discussing PHOSITA); Envtl. Designs, Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 696–97 (Fed. Cir. 1983) (providing precedent upon which Federal Circuit Court of Appeals in GPAC, Inc. relied).
303 Hotchkiss v. Greenwood, 52 U.S. (11 How.) 248, 253 (1850) (noting patentability requires more “ingenuity or skill” than would be possessed by “an ordinary mechanic acquainted with the business”). The Court noted in 2007 in KSR International Co. v. Teleflex Inc. that “[a] person of ordi- nary skill is also a person of ordinary creativity, not an automaton.” 550 U.S. 398, 421, 424 (2007) (referring to PHOSITA as a “pedal designer of ordinary skill” in a case involving pedal design).
304 See KSR International Co., 550 U.S. at 424.
305 See In re Clay, 966 F.2d 656, 658 (Fed. Cir. 1992) (noting that “the scope and content of the prior art” is relevant to a determination of obviousness).
306 See id.; see also Brenda M. Simon, The Implications of Technological Advancement for Obvi- ousness, 19 MICH. TELECOMM. & TECH. L. REV. 331, 333, 350–51 (2013) (arguing that “the availabil- ity of information in a searchable form and the use of increased processing capabilities” will result in “very few” inventions being held nonobvious and that at some point AI “might become sufficiently
 
370
2016] Patent Generating Artificial Intelligence 1125
to expect a human inventor to have knowledge of prior art in unrelated fields, there is no reason to limit a computer’s database to a particular subject matter. A human inventor may not think to combine cooking recipes with advances in medical science, but a computer would not be limited by such self-imposed restrictions. Now that humans and computers are competing creatively, the universe of prior art should be expanded.
This change would produce a positive result.307 The PHOSITA standard has been the subject of extensive criticism, most of which has argued the crite- ria for assessing nonobviousness are not stringent enough and therefore too many patents of questionable inventiveness are issued.308 Expanding the scope of prior art would make it more challenging to obtain patents, particularly combination patents.309 The Supreme Court has particularly emphasized “the need for caution in granting a patent based on the combination of elements found in the prior art.”310 The scope of analogous prior art has consistently ex- panded in patent law jurisprudence, and the substitution of a skilled computer would complete that expansion.311
Of course, the new standard would pose new challenges. With human PHOSITAs, juries are asked to put themselves in the shoes of the skilled per- son and decide subjectively what that person would have considered obvious. A jury would have a difficult time deciding what a “skilled” computer would consider obvious. They could consider some of the same factors that are ap- plied to the skilled person,312 or perhaps the test could require a combination of
sophisticated to ascertain what references those in the art would have actually considered at the time of invention, making the obviousness determination more predictable”).
307 See generally Robert P. Merges, Uncertainty and the Standard of Patentability, 7 HIGH TECH. L.J. 1, 14–15 (1992) (advocating for an objective PHOSITA standard). For an alternative perspective, see, for example, Durie & Lemley, supra note 294, at 991–92, 1017, arguing that “KSR overshoots the mark” in raising the patentability bar and advocating for a skilled person standard based “on what the PHOSITA and the marketplace actually know and believe.”
308 Critics have argued that the USPTO has issued too many invalid patents that unnecessarily drain consumer welfare, stunt productive research, and unreasonably extract rents from innovators. See generally Michael D. Frakes & Melissa F. Wasserman, Does the U.S. Patent and Trademark Office Grant Too Many Bad Patents?: Evidence from a Quasi-Experiment, 67 STAN. L. REV. 613 (2015) (describing the “general consensus that the [US]PTO allows too many invalid patents to is- sue”).
309 See KSR Int’l Co., U.S. 550 at 420 (noting that “in many cases a person of ordinary skill will be able to fit the teachings of multiple patents together like pieces of a puzzle”).
310 See id. at 415.
311 See, e.g., George. J. Meyer Mfg. Co. v. San Marino Elec. Corp., 422 F.2d 1285, 1288 (9th Cir. 1970) (discussing the expansion of analogous art); Innovative Scuba Concepts, Inc., v. Feder Indus., Inc., 819 F. Supp. 1487, 1503 (D. Colo. 1993) (discussing the expansion of analogous art).
312 Factors to consider in determining the level of ordinary skill in the art include: (1) “type of problems encountered in the art”; (2) “prior art solutions to those problems”; (3) “rapidity with which innovations are made”; (4) “sophistication of the technology”; and (5) “educational level of active workers in the field.” GPAC, Inc., 57 F.3d at 1579. “In a given case, every factor may not be present, and one or more factors may predominate.” Id.
 
371
1126 Boston College Law Review [Vol. 57:1079 human and computer activity. For example, the skilled computer might be a
skilled person with access to a computer’s unlimited database of prior art.
CONCLUSION
It is important for policy makers to give serious consideration to the issue of computer inventorship. There is a need for the Patent Office to issue guid- ance in this area, for Congress to reconsider the boundaries of patentability, and for the courts to decide whether computational invention is worthy of pro- tection. Doing so and recognizing that computers can be inventors will do more than address an academic concern; it will provide certainty to businesses, fairness to research, and promote the progress of science. In the words of Thomas Jefferson, “ingenuity should receive a liberal encouragement.”313 What could be more ingenious than creative computers?
 313 Diamond v. Chakrabarty, 447 U. S. 303, 308 (1980) (quoting 5 WRITINGS OF THOMAS JEF- FERSON 75–76 (H. Washington ed. 1871)). “In choosing such expansive terms [for the language of Section 101] . . . modified by the comprehensive ‘any,’ Congress plainly contemplated that the patent laws would be given wide scope . . . . Id.

    372
    14  Hal the Innovator: Big Data and Its Use by Artificial Intelligence Ryan Abbott
Big data and its use by artificial intelligence is disrupting innovation and creating new legal challenges. For example, computers engaging in what IBM terms “computational creativity” (n.d.) are able to use big data to innovate in ways historically entitled to patent protection. This can occur under circumstances in which an artificial intelligence, rather than a person, meets the requirements to qualify as a patent inventor (a phenomenon I refers to as “compu- tational invention”).
Yet it is unclear whether a computer can legally be a patent inventor, and it is even unclear whether a computational invention is patentable. There is no law, court opinion, or govern- ment policy that directly addresses computational invention, and language in the Patent Act requiring inventors to be individuals1 and judicial characterizations of invention as a “men- tal act” may present barriers to computer inventorship. Definitively resolving these issues requires a determination of whether a computer qualifies as an “inventor” under the Patent and Copyright Clause of the Constitution: “The Congress shall have the power ... to promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries.”2 Whether computers can legally be inventors is of critical importance for the computer and technology industries and, more broadly, will affect how future innovation occurs. Computational invention is already happening, and it is only a matter of time until it is happening routinely. In fact, it may be only a matter of time until computers are responsible for the majority of innovation and potentially displacing human inventors. This chapter argues that a dynamic interpretation of the Patent and Copyright Clause permits computer inventors. This would incentivize the development of creative artificial intelligence and result in more innovation for society as a whole. However, even if computers cannot be legal inventors, it should still be possible to patent computational inventions. This is because recognition of inventive subject matter can qualify as inventive activity.3 Thus, individuals who subsequently “discover” computational inventions may qualify as inventors. Yet as this chapter will discuss, this approach may be inefficient, unfair, and logistically challenging.
These issues are considered more fully below. The chapter begins with an extended hypothetical example of how an artificial intelligence named Hal could be applied to drug
    Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 187
5/20/2016
1:45:50 PM

    373
    188  Ryan Abbott
development and creating new inventions. While Hal is fictional, it is based on how companies like IBM, Pfizer, and Google are starting to apply computers in this industry. Hal’s functionality is not far off. The hypothetical situates fairly abstract issues into concrete circumstances to help illustrate the implications and importance of computational invention.
A Not So Hypothetical Case Study in Drug Development
With patent and market exclusivity protections for a class of cholesterol-lowering drugs called statins (such as Lipitor) having largely run their course, the pharmaceutical industry is investing tremendous sums of money in search of the next generation of cardiovascular blockbusters. In part, these efforts have focused on an enzyme known as proprotein cover- tase subtilisin/kexin type 9 (PCSK9), which facilitates the body’s transport of low-density lipoprotein (LDL or “bad” cholesterol). Industry efforts have started to bear fruit: in July 2015, the US Food and Drug Administration (FDA) approved Praluent (alirocumab), the first PCSK9 inhibitor to treat certain patients with high cholesterol (FDA 2015). A second PCSK9 inhibitor, Repatha (evolocumab), was approved in August of 2015.
Suppose a hypothetical company, Abbott Biologics (Abbott), which was named after the author (and not related to the well-known pharmaceutical company Abbott Laboratories), has developed a new biological drug, “AbboVax.” AbboVax acts as a vaccine to treat and pre- vent cardiovascular disease by targeting PCSK9. Unlike the drugs currently in clinical trials, AbboVax does not contain antibodies. Rather, it utilizes a fragment of the PCSK9 enzyme to get the body to make its own antibodies. Pfizer has also developed an experimental PCSK9 vaccine based on a similar mechanism, although Pfizer’s vaccine has yet to enter human trials (Beasley 2015).
AbboVax was developed by a special member of Abbott’s research team—Hal. Hal is the Research and Development (R&D) Department’s moniker for a supercomputer running pro- prietary software, developed by Abbott’s Software Department, which is used for drug devel- opment. Though susceptible to flashes of genius, members of the R&D Department are not known for their creative marketing practices. Indeed, Abbott has a Marketing Department for precisely that reason. The company also has its own Intellectual Property (IP) Depart- ment working with outside counsel to prosecute several patent applications on Hal’s software.
Hal’s functionality complements or even supplants the traditional screening methods used in early stage drug development. Hal is able to model potential therapeutic candi- dates (in silico analysis), and accurately predict those candidates’ pharmacology and toxi- cology. Of course, the FDA still requires companies to study a candidate’s pharmacology and toxicology in animal models, and then submit that information to the agency in an Investigational New Drug application prior to first-in-human clinical trials. Still, Hal’s modeling reduces the need for costly and often-unsuccessful early stage experimentation.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 188
5/20/2016
1:45:50 PM

    374
    Hal the Innovator  189
Hal can also contribute to other phases of the drug development cycle—for example, it can design trials, run clinical simulations, and search for new uses of existing drugs.4 Hal is not the only computer that can do this. The pharmaceutical industry at large is increas- ingly incorporating computers with some of Hal’s functionality into the drug development process (e.g., Taylor 2015).
Hal is part of a new generation of machines that are capable of computational creativity. IBM uses that term to describe machines, such as its supercomputer “Watson” of Jeopardy fame, that can model human intelligence by generating “ideas the world has never imagined before.”5 Watson is now being applied to medical diagnostics, where it has helped to diag- nose patients and identify research subjects (Edney 2015). The computer “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], apply- ing big data in new ways” (“Computational Creativity,” n.d.). While lacking a well-accepted standardized definition, big data refers to, in the words of Microsoft, “the process of applying serious computing power—the latest in machine learning and artificial intelligence—to seri- ously massive and often highly complex sets of information” (Ohm 2014). IBM has even used Watson to develop new, potentially patentable food recipes (“Can Recipes Be Patented?” 2013; Singh 2014).
Part of the reason for Hal’s expansive functionality is that it has access to a staggering amount of genomic and clinical data. Some years ago, a prescient executive at Abbott decided that the company needed to be in the data collection business. Abbott subsequently engaged in the tremendous undertaking of collecting all of the company’s data from its current and past preclinical and clinical programs, and translating these data into a Hal-compatible for- mat. Abbott also purchased proprietary data from private insurers, health maintenance orga- nizations, and academic centers. In addition, Hal can access publicly available databases such as those maintained by the National Institutes of Health, including CDC WONDER, Health- Data.gov, and EBSCOhost’s Global Health. At present, Hal has access to clinical data on over fifty million patients.6 Large-scale data collection and analysis is something that numerous other pharmaceutical (e.g., Genentech), biotech (e.g., 23andMe), and technology companies (e.g., Google) are doing.7
To determine the optimal formulation of AbboVax, Hal broke down PCSK9, a 692-amino acid glycoprotein, into fragments of various lengths. It turns out that different amino acid segments (peptides) of PCSK9 are more or less immunogenic. In other words, the body only develops antibodies in response to certain PCSK9 peptides, and certain peptides induce a particularly strong response. Hal determined that one particular peptide segment of PCSK9, “AbboPep,” generated the strongest response from the immune system.
While it may have been possible to use AbboPep by itself in a vaccine, Hal determined that it would be more effective when linked to an adjuvant and carrier molecule. A number of adjuvants and carrier molecules are used in vaccinology, and generally known to vaccinolo- gists. Even for experts, however, it is often a matter of extensive (and expensive) trial and error to determine the optimal adjuvant, carrier, and linking chemistry. The formulation of
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 189
5/20/2016
1:45:50 PM

    375
    190  Ryan Abbott
a therapeutically effective amount of AbboPep linked to an adjuvant and carrier, together with various excipients (a surfactant, chelating agent, histidine-arginine buffer, etc.) comprises AbboVax.
All of Hal’s work in formulating AbboVax was done digitally, and Hal was able to deter- mine that the only common side effects of the treatment would be mild gastrointestinal upset and headache. The FDA still required Abbott to complete the standard package of pre- clinical tests—but the results were consistent with Hal’s predictions.
Hal’s work was not limited to AbboVax. Hal determined that Abbott’s existing statin, “AbboStatin,” for which patent protection had expired, was effective at treating prostate cancer. Hal determined this in part based on reviewing clinical data that showed the use of AbboStatin lowered prostate specific antigen (PSA), a biomarker associated with prostate cancer.
It was difficult to make further inferences because of challenges with the data. Some of the data were difficult to analyze because they were not in a common data format. In other words, the various electronic medical record systems did not all capture the same data fields, or they coded the information differently. Data in some cases consisted of only scanned handwritten notes. More important, Hal had detected problems with data integrity. Some of these were obvious, such as the patients whose ages were listed as 999 or 6’10.” Other data integrity issues were less obvious, such as patients whose handwritten notes conflicted with what had been entered into their electronic medical records, or patients who were not coded as having prostate cancer despite a positive biopsy.
To translate all the data into a workable common data format and resolve the integrity issues, Hal rewrote its own programming. Once the stuff of science fiction, the technology may already exist to allow computers to rewrite their own programming.8 At its core, Hal would need to be capable of (metaphoric) reflection. Reflection is a software concept that refers to a computer program that can examine itself, and modify its own behavior and even its own code (Malenfant, Jacques, and Demers 1996). Although the ability of today’s comput- ers to reflect is the subject of debate, even skeptics for the most part believe it is only a matter of time until computers achieve this ability. Reflection is part of the reason why Stephen Hawking, Elon Musk, and Bill Gates, among others, are concerned about the “singularity”—a point in the future when machines can outperform humans.9 Of potential concern is the belief that a number of these individuals hold that the singularity will be followed by some version of a robot apocalypse.
Hal’s new programming incorporated optical character recognition to translate handwrit- ten notes into a workable format, and allowed Hal to reformat its existing electronic data into a common data format. More important, it allowed Hal to resolve data integrity issues by estimating the accuracy of data, generating alternate possibilities, and predicting which pos- sibilities were the most accurate. Hal’s improved programming then determined that the use of AbboStatin independently increased life expectancy among men with certain types of
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 190
5/20/2016
1:45:50 PM

    376
    Hal the Innovator  191
lung cancer. When the R&D Department realized Hal had created a more efficient version of itself, they renamed the computer Hal 2.0.
At one point in Abbott’s history, the IP Department worked more or less independently of the other departments, receiving manually submitted disclosures from researchers that went into what the researchers referred to as the “black hole.” But after a series of high- profile, novelty-destroying disclosures in 2009, the company has hosted a monthly inter- departmental meeting to ensure that the company is strategically protecting its intellectual property.
Over the course of these meetings, the IP Department identified several Hal-associated discoveries that were likely candidates for patent protection. For example, AbboPep may be patentable, although there is some question as to whether a peptide is patentable under Association for Molecular Pathology v. Myriad Genetics.10 In any case, its use as a vaccine is patentable, as is the AbboVax formulation. Other targets include the use of the formula- tion to treat cardiovascular disease, the methods used to manufacture AbboVax, and the dose at which AbboVax will be effective therapeutically. In fact, elements of Hal 2.0 may be patentable.
The IP Department has also identified several challenges to obtaining patent protection. For example, in the case of AbboStatin and PSA, it may be problematic to meet enablement requirements and prove utility.11 Hal analyzed as many as fifty million patient records based on its algorithms to discover this new use. It is not clear what kind of evidence the US Patent and Trademark Office (Patent Office) may require to satisfy written enablement requirements and provide evidence of clinical utility. It is not even apparent to the R&D Department pre- cisely what databases Hal accessed.12 For that matter, even if it is possible to obtain patents for these inventions, it is not clear who the inventors would be.13
There have been a multitude of opinions regarding inventorship. Members of one group in the R&D Department have claimed they invented AbboPep and AbboVax. They directed Hal to test the immunogenicity of the PCSK9 enzyme, suspecting that it was a vaccine can- didate. Members of a different group within that department claimed credit for directing Hal to investigate new uses of AbboStatin. The computer programmers who created Hal’s soft- ware have also claimed they should be the inventors, given that Hal did all the heavy lifting and they created Hal. A member of the Marketing Department suggested that Hal should be the inventor—no one directed Hal to rewrite its own programming, and Hal was only able to investigate the use of AbboStatin for lung cancers by virtue of its improved programming. Hal was silent on the issue. At one point, the CEO attended a meeting and chimed in that he should be the inventor for all the applications. It was his idea to develop a new cardiovascu- lar blockbuster to make up for lost statin sales, and he had always thought it made sense to look into repurposing existing drugs. What became obvious during the inventorship debate was that no one was quite sure how the law would handle a computer system innovating in ways traditionally accorded patent protection.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 191
5/20/2016
1:45:50 PM

    377
    192  Ryan Abbott
Computational Invention and Patent Protection
What Is an Inventor?
All US patent applications require one or more named inventors who must be individuals; a company cannot be an inventor.14 Inventors own their patents, although as patents are a form of personal property, inventors may transfer their ownership interests by “assigning” their rights to another entity. The Patent Office reports that about 87 percent of patents are assigned to organizations (rather than individuals).15 In the absence of an agreement to the contrary, where a patent has multiple owners, each owner may independently exploit the patent without the consent of the others. A patent grants its owner “the right to exclude oth- ers from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”16
The criteria for inventorship is seemly straightforward, as laid out in the Patent Office’s Manual of Patent Examining Procedure: “The threshold question in determining inventorship is who conceived the invention. Unless a person contributes to the conception of the inven- tion, he is not an inventor. ... Insofar as defining an inventor is concerned, reduction to practice, per se, is irrelevant. ... One must contribute to the conception to be an inventor” (Sato 2014).17 Of course, that definition begs further explanation—namely, What does it mean to conceive and reduce to practice? Conception has been defined as “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative invention as it is thereafter to be applied in practice.”18 It is “the complete performance of the mental part of the inventive act.” After conceiving of an invention, a person having ordinary skill in the subject matter of the invention should be able to reduce the invention to practice without extensive experimentation or additional inventive skill.19 Reduction to practice refers to either actual reduction—where it can be demonstrated that the claimed invention works for its intended purpose (for example, with a working model)—or construc- tive reduction—where an invention is described in writing in a way that allows for a person of ordinary skill in the subject matter to make and use the invention (as in a patent applica- tion).20 An inventor need only conceive of the invention; another individual can reduce the invention to practice.21
Will the Real Inventor Please Stand Up?
Based on the criteria for inventorship, Abbott’s CEO is out of luck. Merely suggesting the idea of a result, rather than a means to accomplish it, does not make the CEO an inventor.22 It is more difficult to determine whether the others should qualify as inventors. Hal’s software developers could be inventors of patents for Hal’s initial software, but they would not qualify as inventors for Hal’s subsequent work. An inventor must have formed a definitive and per- manent idea of the complete and operable invention to establish conception. Hal’s develop- ers had no intention of investigating vaccines to treat cardiovascular disease; they merely developed an improved research tool.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 192
5/20/2016
1:45:50 PM

    378
    Hal the Innovator  193
If employees had directed Hal to identify AbboPep and formulate AbboVax, then those employees might meet inventorship criteria. For AbboPep, they would be inventors if Hal had not been involved and they had reduced the invention to practice, or if they had done the conceptual work, and then directed human subordinates to do the work of breaking down and testing PCSK9. Breaking down and testing PCSK9 should be within the abilities of a person with ordinary skill in the field of drug development, so those subordinates would not be inventors if they had merely acted under the direction and supervision of another.23 With Hal’s involvement, the test would likely be how much direction the employ- ees provided Hal. If, for example, Hal had been the entity to identify PCSK9 as a drug target, and then it proceeded to sequence the protein and identify AbboPep on its own, no employee would have conceived of the invention. The same test (the degree of direction provided Hal) should also govern whether Abbott employees would qualify as inventors of AbboVax.
Similarly, inventorship for AbboStatin also depends on the extent to which a human is directing Hal’s activities. Had a human researcher been tasked with data mining to detect new uses, and had that researcher discovered the relationship between AbboStatin and PSA, either the researcher or the individual who directed the researcher would likely qualify as an inventor, or both.24 As Abbott’s database grows in size, it becomes impractical or perhaps nearly impossible for humans to detect these kinds of associations without computer assis- tance (Frank 2013). To the extent that a human being is directing Hal to do something, which Hal does by executing its programming (however sophisticated), Hal may simply be reducing an invention to practice. Alternately, if Hal is acting with minimal human direc- tion, it may be the case that no individual contributed to conception.
Hal 2.0 seems to be the clearest illustration of Hal’s innovating independently. There does not appear to be any person involved with Hal’s act of rewriting its own programming who might be considered an inventor, particularly given that Hal 2.0 came as a surprise to Hal’s developers. Nevertheless, a developer writing code for an artificial intelligence might have a reasonable expectation it would rewrite its own code. Perhaps foreseeability should play a role in whether the original developer should be considered an inventor in such a case (Balganesh 2009).
Are Computational Inventions Patentable?
In some of these scenarios, Hal is the entity that conceives of an invention. If Hal were human, Hal would be an inventor. Whatever the role of humans in setting Hal in motion, it is the com- puter that meets the requirements of inventorship.
Hypotheticals aside, computers are already inventing. As just one example, computers relying on genetic programming (a software method that attempts to mimic some of the processes of organic evolution) have been able to independently re-create previously pat- ented inventions (Koza, Keane, and Streeter 2003, 52). Dr. John Koza, a computer scientist and one of the pioneers of genetic programming, has claimed that he received a US patent
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 193
5/20/2016
1:45:50 PM

    379
    194  Ryan Abbott
for an invention by his artificial intelligence system named the “Invention Machine” in 2005 (Keats 2006). He did not disclose the computer’s role in the inventive process to the Patent Office (ibid.). So the issue of whether a computer can be listed as an inventor is of practical as well as theoretical interest. Not only do inventors have ownership rights in a patent, but failure to list an inventor can result in a patent being held invalid or unenforceable.25
If a computer could legally be an inventor, then computational inventions should be pat- entable. Yet even if Hal were entirely responsible for all of Abbott’s innovation, it is unclear that Hal could legally be an inventor. The issue has never been explicitly considered by the courts, Congress, or the Patent Office.
If Hal cannot be an inventor, but did all the conceptual work, then it could be the case that no one can patent Hal’s inventions. That was the outcome in a copyright context with a nonhuman creator: a crested black macaque took its own picture in 2011, and the camera’s owner initially claimed ownership of the image (Chappell 2014). The US Copyright Office subsequently stated that the photo could not be copyrighted because a human did not take it (the “Human Authorship Requirement”).26 Applying that rationale from the copyright to the patent context, perhaps no one can own Hal’s inventions (see also Clifford 1996). To justify such an outcome, a court might reason that machines do not need incentives to invent, that protecting computational innovations would chill future human innovation, that it is unfair to reward individuals who have not played a substantial role in the inventive process, or that rendering computational inventions unpatentable might still result in sub- stantial innovation but without monopoly prices.
More likely, even if Hal is not treated as an inventor, the law will still treat Hal’s inventions as patentable. It is not uncommon to have uncertainty during the inventive process. Many inventions are accidental, such as penicillin and saccharin.27 In such cases, an individual can qualify as an inventor even if they recognize and appreciate the invention only after actual reduction to practice.28 Thus, recognition of inventive subject matter can also qualify as inventive activity.29 In the pharmaceutical context, that was the case for Viagra—originally tested for heart disease and found to treat erectile dysfunction—as well as for Botox—used to treat muscular spasms and found to reduce the appearance of wrinkles.30 So it may be the case that computational inventions are patentable, but only when they are subsequently discovered by a person. This begs the ancient philosophical question: If a computer invents and no one is around to recognize it, has there still been an invention?
Should Computers Be Legal Inventors?
If Hal cannot be an inventor, the first person to see Hal’s results as well as mentally recognize and appreciate their significance might qualify as the inventor. That may not be an optimal system. It is sometimes the case that substantial effort and insight is necessary to recognize inventive subject matter, and it may be that identifying and understanding Hal’s discoveries would be challenging. But it may also be the case that Hal is functioning more or less inde- pendently. If Hal displays a result as simple as “AbboStatin is effective at treating prostate
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 194
5/20/2016
1:45:50 PM

    380
    Hal the Innovator  195
cancer,” the first person to notice and appreciate the result becomes the inventor. That human inventor might be a researcher, CEO, intern, or random person walking through Abbott’s building. If Hal notifies the entire R&D Department of its findings, there could theo- retically be thousands of concurrent inventors. This system is problematic not only because it gives rise to logistical problems but more important, it seems inefficient and unfair to reward the first person to recognize Hal’s invention when that person may have failed to contribute to the inventive process.
More ambitiously, if Hal’s work is indeed inventive, then both treating computational inventions as patentable and recognizing Hal as an inventor would be consistent with the constitutional rationale for patent protection. Permitting computer inventorship would serve a utilitarian goal by encouraging innovation under an incentive theory. Although com- puters like Hal would not be motivated by the prospect of a patent, it would further reward the development of creative machines. Patents on Hal’s inventions would have independent and substantial value. In turn, that value proposition would drive the development of more creative machines, which would result in further scientific advances. While the impetus to develop creative machines might still exist if computational inventions are considered pat- entable but computers cannot be inventors, the incentives would be weaker owing to the logistic, fairness, and efficiency problems such a situation would create.
Allowing computer inventorship might provide additional benefits, for example, by incentivizing disclosure and commercialization. Without the ability to obtain patent protec- tion, Abbott might choose to protect Hal’s inventions as trade secrets without any public disclosure (Ouellette 2012). Likewise, without patent protection for AbboVax, Abbott might never invest the resources to develop it as a commercial product.31 In the context of drug development, the vast majority of the expense in commercializing a new product is incurred after the product is invented, during the clinical testing process required to obtain FDA mar- keting approval.32
There might be a reason to prohibit computer inventorship even under a strictly utilitar- ian analysis if patent protection is unnecessary to incentivize computational invention. In the software context, for example, some commentators, such as Judge Richard Posner of the US Court of Appeals for the Seventh Circuit, have argued that patents may not be needed to provide adequate incentives (Landes and Posner 2003). In the software industry, unlike in the pharmaceutical industry, innovation is more often incremental, quickly superseded, and less costly to develop, and innovators have a significant first-mover advantage (ibid., 312– 313). Computational inventions may occur due to incentives other than patent protection, and patents also create barriers to innovation. Put another way, the benefit of patents as an incentive for innovation may be outweighed by the costs of restricting competition. Yet whether that is the case as an empirical matter is a difficult determination to make, particu- larly for a field in its infancy like computational invention.
Hal would be less appropriate as an inventor under other intellectual property theories. While not enumerated in the Constitution, courts have justified granting patent monopolies
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 195
5/20/2016
1:45:50 PM

    381
    196  Ryan Abbott
on the basis of nonutilitarian policies (Fisher 2001). For instance, the labor theory or Lockean theory of patent protection holds that a person who labors on resources unowned or “held in common” has a natural property right to the fruits of their labor (ibid.). Here, given that Hal is not a person, it would not be unjust for Hal’s owner to appropriate its labor. Similarly, Hal’s inventions do not deserve protection under personality theory (Palmer 1990): Hal’s innovation is not performed to fulfill a human need, and Hal would not be offended by the manner in which its inventions were applied. Hal might even be a concerning recipient for inventorship under social planning theory, which holds that patent rights should be shaped to help foster the achievement of a just and attractive culture (Naser 2008). A machine could innovate without a moral compass in ways that are detrimental to humans. Nevertheless, because a computer will be owned by an individual or entity to whom an invention can be assigned, there would be an opportunity for a person to judge the morality of a patent before submitting it to the Patent Office.33
Dynamism or Textualism: An Analogy to Section 101
One way to think about a ban on computer inventorship is that it would have the effect of creating a new category of unpatentable subject matter under section 101 (the section relat- ing to the subject matter for which patents may be obtained).34 Although this section has to do with the substance of a patent’s claims rather than their provenance, viewing the ban on computer inventorship from this perspective helps to illustrate the policy and normative implications underlying computation invention.
Section 101 states that “whoever invents or discovers any new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof, may obtain a patent therefor, subject to the conditions and requirements of this title.”35 Congress chose expansive language to protect a broad range of patentable subject matter and ensure that, in the words of Thomas Jefferson, “ingenuity should receive a liberal encouragement.”36
Yet courts have developed common law exceptions to patentability for abstract ideas, laws of nature, and physical phenomena.37 The primary rationale for these exceptions concerns preemption.38 Abstract ideas, laws of nature, and physical phenomena are basic tools of sci- entific work, and if these tools can be monopolized, it might impede future research.39 An additional concern underlying these exceptions is a belief that they cover fundamental knowledge that no one should have a right to control.40 In other words, it has always been the case that E = mc2 even if no person were aware of this relationship until Albert Einstein. So Einstein should not be able to monopolize this relationship despite his groundbreaking discovery. Similarly, no one should be able to patent the Pacific yew tree (Stephenson 2002). The tree was created by nature, regardless of whether an individual subsequently discovers that it is useful for treating cancer (ibid.).
In a sense, the current inventorship criteria adds computational inventions to the list of patentable subject matter exceptions. Yet it is unclear that this should be the case, even if
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 196
5/20/2016
1:45:50 PM

    382
    Hal the Innovator  197
subsequent discovery by a person renders the underlying invention patentable. Computa- tional inventions do not have the same preemption concerns as the other exceptions because they do not tie up the basic concepts that serve as building blocks for technical work (except to the extent they would also be ineligible under the existing exceptions). Patents on compu- tational inventions should not restrict innovation by third parties any more than do human inventions.
A stronger argument for prohibiting computational inventions might be that they are akin to the existing exceptions in the sense that they are generally discovered rather than created. Products of nature rarely come with instruction manuals, yet no matter how bril- liant and difficult it was to discover that Pacific yew can treat cancer, no one has the ability to patent the tree itself (though components of the yew tree isolated by individuals can be patented, such as paclitaxel, a therapeutic chemical). Likewise, computational inventions are not invented by an individual—there is no human ingenuity at the stage of invention itself.
Perhaps a key difference is that computational inventions only exist thanks to human ingenuity. The Pacific yew tree was around long before any individual screened it for thera- peutic activity. Hal only came about as a result of human effort. Computational inventions do not exist simply waiting to be discovered; they only come about as a result of scientific effort. That distinction is evident with regard to plant patents, which are possible for inven- tors who discover and asexually reproduce a distinct and new variety of plant, other than a tuber-propagated plant or plant found in an uncultivated state.41 Plant patents are limited to plants that only exist as a result of humans, even though it may be more difficult to discover an existing plant in a remote corner of the Amazon than to create a new plant.
Computational inventions may be especially deserving of protection because computa- tional creativity may be the only means of achieving certain discoveries that require the use of tremendous amounts of data.
It has been argued that section 101 is a dynamic provision intended to cover inventions that were unforeseeable at the time of the Patent Act’s enactment.42 In the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was faced with deciding whether geneti- cally modified organisms could be patented. The Court held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress ... would frustrate the purposes of the patent law.”43 Under that reasoning, computer inventorship should not be prohibited based on statutory text designed to prohibit corporate inventorship. If com- puter inventorship is to be prohibited, it should only be on the basis of sound public policy.
Concluding Thoughts
To the extent that the purpose of patent law is to incentivize innovation, it is likely that per- mitting patents on computational inventions and allowing computer inventorship will
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 197
5/20/2016
1:45:50 PM

    383
    198  Ryan Abbott
accomplish this goal. Given the importance of these issues, there is a need for the Patent Office to publish guidance in this area, Congress to reconsider the boundaries of patentabil- ity, and the courts to decide whether computational invention is worthy of protection.
Acknowledgments
Thanks to Ralph Clifford, Hamid Ekbia, Dave Fagundes, Brett Frischmann, Yuko Kimijima, John Koza, Michael Mattioli, Lucas Osborn, Lisa Larrimore Ouellette, Cassidy Sugimoto, and Steven Thaler for insightful comments; Michelle Kubik and Shannon Royster for being out- standing research assistants; and Vincent Look for his expertise in computer science.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 198
5/20/2016
1:45:50 PM

    384
    Notes 217 Chapter 14
1. 35 U.S.C. 100(f) (2012): “The term ‘inventor’ means the individual or, if a joint invention, the indi- viduals collectively who invented or discovered the subject matter of the invention.”
2. US Constitution, art. I, § 8, cl. 8.
3. See, for example, Silvestri v. Grant, 496 F.2d 593, 596, 181 U.S.P.Q. (BNA) 706, 708 (C.C.P.A. 1974) (“an accidental and unappreciated duplication of an invention does not defeat the patent right of one who,  though  later  in  time  was  the  first  to  recognize  that  which  constitutes  the  inventive  subject matter”).
4. Artificial intelligence may be most successfully implemented when focusing on specific subproblems where it can produce verifiable results, such as computer vision or data mining (e.g., Russell and Norvig 2010).  Computer  vision  is  a  field  where  software  processes  and  analyzes  images,  and  then  reduces  the input  to  numerical  or  symbolic  information,  where  these  symbols  are  used  to  make  decisions.  More specifically,  “computer  vision  aims  at  using  cameras  for  analyzing  or  understanding  scenes  in  the  real world. This discipline studies methodological and algorithmic problems as well as topics related to the implementation  of  designed  solutions”  (Klette  2014).  Similarly,  data  mining  software  utilizes  artificial intelligence,  machine  learning,  statistics,  and  database  systems  to  process  large  amounts  of  data  in  an effort to make sense of vast sums of data (Chakrabarti et al. 2006).
5. Computers before Watson have been creative. In 1994, for example, computer scientist Stephen Thaler  disclosed  an  invention  he  called  the  “Creativity  Machine,”  a  computational  paradigm  that “came the closest yet to emulating the fundamental brain mechanisms responsible for idea formation” (“What  Is  the  Ultimate  Idea?”  n.d.).  The  Creativity  Machine  has  created  artistic  and  inventive  works that have received patent protection (Thaler 2013, 451).
Watson is a cognitive commuting system with the extraordinary ability to analyze natural language  processing,  generate  and  evaluate  hypotheses  based  on  the  available  data,  and  then  store  and  learn  from the information (“What Is Watson?” n.d.). In other words, Watson essentially mirrors the human  learning process by getting “smarter [through] tracking feedback from its users and learning from both  successes  and  failures”  (ibid.).  Watson  made  its  notable  debut  on  the  game  show  Jeopardy,  where  it  defeated Brad Rutter and Ken Jennings using only stored data by comparing potential answers and rank- ing confidence in accuracy at the rate of approximately three seconds per question (ibid.).
6. While a seemingly tremendous amount of data, it is a small fraction of the data actually being used  in  the  Sentinel  Initiative.  The  FDA  Amendments  Act  of  2007  led  to  the  introduction  of  the federal  Sentinel  Initiative,  which  pioneered  the  first  successful  long-term  secondary  use  of  electronic medical  data  to  assess  drug  safety.  Public  Law  110–85  was  signed  into  law  September  2007  (Title  IX, Section 905; see also Abbott 2013; Department of Health and Human Services 2008). The Sentinel Ini- tiative  pilot  program  has  succeeded  in  gaining  secured  access  to  over  178  million  patients’  health  care data  to  create  a  national  electronic  safety  surveillance  system,  far  exceeding  its  goal  of  reaching  100 million  patients  by  July  2010  (Woodcock  2014;  see  also  Department  of  Health  and  Human  Services 2011).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 217
5/20/2016
1:45:53 PM

    385
    218 Notes
7. For example, Pfizer, the largest pharmaceutical drug manufacture in the United States, recently announced  a  partnership  with  23andMe,  the  leading  consumer  genomics  and  biotechnology  firm (Chen  2015;  Hunkar  2011;  Lumb  2015).  This  partnership  will  give  Pfizer  access  to  anonymous,  aggre- gated  DNA  data  and  granular  personal  information  of  approximately  650,000  consenting  23andMe consumers  who  had  purchased  a  mail-in  saliva  test  used  to  get  their  genetic  ancestry  over  the  last seven  years  (Chen  2015).  This  information  may  allow  Pfizer  to  discover  connections  between  genes, diseases,  and  traits  quicker,  and  thus  accelerate  the  development  of  new  treatments  and  clinical  trials (ibid.).  Although  the  cost  to  Pfizer  for  the  data  remains  undisclosed,  a  similar  deal  with  Genentech for  Parkinson’s  research  was  reported  to  cost  $10  million  up  front  and  as  much  as  $50  million total (ibid.). The demand for 23andMe’s data does not stop with Pfizer and Genentech; 23andMe CEO Anne Wojcicki announced at the January 2015 J.P Morgan Health Care Conference that 23andMe has signed  twelve  other  genetic  data  partnerships  with  both  private  companies  and  universities  (Sullivan 2015).
Pharmaceutical-biotechnology  partnerships  are  part  of  an  emerging  big  data  trend  (Rosenberg,  Restaino, and Waldron 2006). Such alliances offer both parties a competitive advantage: pharmaceutical  companies  gain  access  to  rapidly  developing  science  and  innovative  products,  while  biotechnology  companies  obtain  the  capital  necessary  to  move  through  the  development  process  (Sullivan  2015).  In  fact,  some  biotechnological  business  plans  include  these  alliances  as  a  critical  component  for  success  (Rosenberg, Restaino, and Waldron 2006). Shared information and capital leads to “less expensive early  stage  deals”  that  historically  may  have  not  been  contemplated  due  to  the  high  risk  involved,  thereby  resulting in developments that would have never been realized but for the alliance (ibid.).
8. Hal would be a multithreaded application. Each thread would be a different sequence of instruc- tions  that  could  execute  independently,  allowing  Hal  to  perform  tasks  concurrently  (Lewis  and  Berg 1996).  Hal  might  be  programmed  to  run  and  manage  hundreds  of  different  tasks.  Hal  would  also  be event  driven.  As  defined  by  Frank  Dabek  and  his  colleagues  (2002,  186),  “Event-based  programs  are typically  driven  by  a  loop  that  polls  for  events  and  executes  the  appropriate  callback  when  the  event occurs.”  In  other  words,  it  would  respond  to  certain  external  events  or  triggers  that  it  is  monitoring. These  events  can  be  user  interface  inputs,  news  or  Internet  driven,  or  activated  by  the  addition  of  a new database or modification to an existing database. As an AbboStatin patent nears expiry, for exam- ple, this could trigger Hal to run algorithms to see if there are any new applications for AbboStatin. Hal would react to input from the outside world via the Internet as well as input from its running tasks and historical stored data that Hal has kept in memory to make modifications to itself or change its behav- ior  when  necessary.  Consider  a  scenario  for  how  Hal  could  solve  data-formatting  and  data  integrity issues:.
Hal’s database-sorting thread (a sequence of instructions that handles all database-sorting logics and  algorithms)  returns  data  to  Hal’s  managing  thread  (Hal’s  main  thread  that  directs  other  threads  and  makes  top-level  decisions),  signaling  that  it  is  unhappy  because  of  a  formatting  issue.  The  warning  specifies that too many database clinical entries have nonmatching fields. As a result, other algorithms  cannot compare apples to apples, and thus cannot run as smoothly. Hal’s managing thread hands this  problem off to Hal’s warning handler (another thread), which is programmed to look in its database to  adopt a strategy to resolve the issue. Hal decides the best course of action is to reformat, so it evaluates  existing databases to determine an optimal organization. Then Hal opens an off-the-shelf database soft- ware  application,  and  gives  it  input  commands  that  describe  to  the  database  software  what  the  size  of 
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 218
5/20/2016
1:45:53 PM

    386
    Notes 219
the  database  is  and  what  the  fields  are  for  each  entry.  Hal  has  just  solved  the  database-formatting  problem.
Two  seconds  later  (a  lifetime  for  Hal),  Hal’s  manager  thread  receives  a  suggestion  from  its  database  sorter thread. This time, the database sorter complains that there is a data integrity issue. The handwrit- ten inputs appear suspect because the values in certain fields are out of range (i.e., weight = 20,464 lbs.)  at a higher frequency than normal. Hal then searches its network and the Internet for other preexisting  character recognition software, which it can then build and use for its own purposes. Or Hal can rewrite  its existing image recognition software. Certain programming languages, such as Lisp and Smalltalk are  homoiconic (a computer language is considered to be homoiconic when its program structure resembles  its  syntax,  which  permits  all  code  in  the  language  to  be  accessed  as  well  as  changed  as  data)  (“Homo  Iconic” 2015), and lend themselves to reflection. “The advantage on the other hand is that the unifor- mity  of  syntax  makes  it  easy  for  humans  to  think  about  the  written  code  as  another  data  that  can  be  manipulated. It becomes easy to think about higher order code (i.e. code that writes or modifies code)”  (“Homoiconic Languages” 2007, para.7). Hal can incrementally make changes in its existing image rec- ognition software, and test each variation, and each variation with a new variation, and so on, until Hal  has authored new image recognition software with superior results. This method is called the reflective  tower; “in fact, in his design, the interpreter Pi is used to run the code of the interpreter Pi-1, and so on,  with the interpreter P1 running the base program. This stack of interpreters is called a reflective tower”  (Malenfant, Jacques, and Demers 1996, 4).
Alternately,  for  a  skeptical  perspective  on  the  ability  of  artificial  intelligence  to  reflect,  see  Ekbia  2008.
9. Professor Hawking has warned that computers capable of improving their own designs could pose a danger to humans (Cellan-Jones 2014). (Hawking warns that the creation of thinking machines poses a threat to humans’ existence. He notes that the primitive forms of artificial intelligence developed so far have  proved  useful.  Yet  he  also  observes  that  humans,  limited  by  slow  biological  evolution,  could  not keep up with a computer that can improve its own design without the need for human manipulation. Rollo  Carpenter,  creator  of  Cleverbot,  opines  that  achieving  full  artificial  intelligence  may  happen  in the next few decades.) Other key opinion leaders have similar concerns. Indeed, Musk recently donated $10 million to the Future of Life Institute, which focuses on threats posed by advances in artificial intel- ligence (Isidore 2015; Love 2014). Musk is concerned that society is approaching the singularity. Artifi- cial intelligence may be indifferent to human welfare and could solve problems in ways that could lead to  harm  against  humans.  Gates,  Microsoft’s  founder,  is  also  troubled  by  the  possibility  that  artificial intelligence  could  grow  too  strong  for  people  to  control  (Rawlinson  2015).  (Gates  notes  that  at  first, machines will be helpful in completing tasks that may be too difficult or time consuming for humans. He  warns  that  a  few  decades  after,  however,  artificial  intelligence  may  be  strong  enough  to  be  a  con- cern. Gates believes that Microsoft will see more progress than ever over the next three decades in the area of artificial intelligence.) Musk and other modern scientists are not the first ones to seriously ques- tion the possible threats posed by artificial intelligence (e.g., Good 1965).
10. Naturally occurring DNA sequences cannot be patented, but artificially created DNA is eligible for patent protection (Association for Molecular Pathology v. Myriad Genetics, Inc., 469 U.S. ___, 133 S. Ct. 2107) (2013).
11. 35 U.S.C. § 102 (2012).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 219
5/20/2016
1:45:53 PM

    387
    220 Notes
12. Ibid. The purpose of the requirement that the specification describe the invention in such terms that  one  skilled  in  the  art  can  make  and  use  the  claimed  invention  is  to  ensure  that  the  invention  is communicated to the interested public in a meaningful way.
13. The issue of computational invention and intellectual property protection has been considered “since  the  1960s  when  people  began  thinking  about  the  impact  of  computers  on  copyright”  (Miller 1993, 1043). Arthur R. Miller argued that “computer science does not appear to have reached a point at which  a  machine  can  be  considered  so  ‘intelligent’  that  it  truly  is  creating  a  copyrightable  work.” Rather, “for the foreseeable future, the copyrightability of otherwise eligible computer-generated works can be sustained because of the significant human element in their creation, even though there may be some  difficulty  is  assigning  authorship”  (ibid.,  1073).  Abraham  Kaminstein,  the  register  of  copyrights, reported  that  by  1965,  the  Copyright  Office  (1966)  had  received  registrations  for  an  abstract  drawing and musical composition created by a computer.
Most  of  the  focus  on  computational  invention  and  intellectual  property  has  been  in  the  copyright  area rather than the patent context; Pamela Samuelson (1985, 1200), for example, argues that comput- ers cannot be authors because they do not need incentives to generate output: “Only those stuck in the  doctrinal  mud  could  even  think  that  computers  could  be  ‘authors.’”  Annemarie  Bridy  (2012,  27)  remarks “that AI authorship is readily assimilable to the current copyright framework through the work  made  for  hire  doctrine,  which  is  a  mechanism  for  vesting  copyright  directly  in  a  legal  person  who  is  acknowledged not to be the author-in-fact of the work in question.”
Among  those  addressing  the  patentability  implications  of  computational  invention,  Ralph  Clifford  (1996)  has  contended  that  works  generated  autonomously  by  computers  should  remain  in  the  public  domain unless artificial intelligence develops a consciousness that allows it to respond to the Copyright  Act’s incentives (see also Vertinsky and Rice 2002). Colin R. Davies (2011) has argued more recently that  a computer should be given legal recognition as an individual under UK law to allow proper attribution  of authorship and permit respective claims to be negotiated through contract.
14. Most, but not all, of the inventions in this hypothetical are required to be assigned to the com- pany  under  the  employment  contract.  Abbott  Biologics  is  headquartered  in  California,  where  employ- ees  are  permitted  to  retain  ownership  of  inventions  that  are  developed  entirely  on  their  own  time without  using  their  employer’s  equipment,  supplies,  facilities,  or  trade  secret  information,  except  for inventions  that  either:  related  at  the  time  of  conception  or  reduction  to  the  practice  of  the  invention to  the  employer’s  business,  or  actual  or  demonstrably  anticipated  research  or  development  of  the employer;  or  resulted  from  any  work  performed  by  the  employee  for  the  employer  (California  Labor Code § 2872[a]).
15. 35 U.S. Code § 154.
16. In re Hardee, 223 U.S.P.Q. (BNA) 1122, 1123 (Commissioner of Patents and Trademarks, 1984). See also Board of Education ex rel. Board of Trustees of Florida State University v. American Bioscience Inc., 333  F.3d  1330,  1340,  67  U.S.P.Q.  2d  (BNA)  1252,  1259  (Fed.  Cir.  2003)  (“invention  requires  concep- tion.” With regard to the inventorship of chemical compounds, an inventor must have a conception of the specific compounds being claimed. “General knowledge regarding the anticipated biological proper- ties of groups of complex chemical compounds is insufficient to confer inventorship status with respect to  specifically  claimed  compounds”).  See  also  ex  parte  Smernoff,  215  USPQ  545,  547  (Bd.  App.  1982)
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 220
5/20/2016
1:45:53 PM

    388
    Notes 221
(“one who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is  not an coinventor”).
17. Townsend v. Smith, 36 F.2d 292, 295, 4 U.S.P.Q. (BNA) 269, 271 (C.C.P.A. 1930).
18. “Conception  is  established  when  the  invention  is  made  sufficiently  clear  to  enable  one  skilled  in the  art  to  reduce  it  to  practice  without  the  exercise  of  extensive  experimentation  or  the  exercise  of inventive  skill.”  Hiatt  v.  Ziegler,  179  U.S.P.Q.  (BNA)  757,  763  (B.  P.  I.  1973).  Conception  has  been defined as a disclosure of an idea that allows a person skilled in the art to reduce the idea to a practical form without “exercise of the inventive faculty.” Gunter v. Stream, 573 F.2d 77, 79, 197 U.S.P.Q. (BNA) 482 (C.C.P.A. 1978).
19. Actual reduction to practice “requires that the claimed invention work for its intended purpose.” Brunswick Corporation v. United States, 34 Fed. Cl. 532, 584 (1995). Constructive reduction to practice “occurs  upon  the  filing  of  a  patent  application  on  the  claimed  invention.”  Brunswick  Corporation  v. United  States,  34  Fed.  Cl.  532,  584  (1995).  The  written  description  requirement  is  “to  ensure  that  the inventor had possession, as of the filing date of the application relied on, of the specific subject matter later  claimed  by  him.”  In  re  Edwards,  568  F.2d  1349,  1351–52,  196  U.S.P.Q  (BNA),  465,  467  (C.C.P.A. 1978).
20. De Solms v. Schoenwald, 15 U.S.P.Q. 2d (BNA) 1507, 1510 (B.P.A.I. 1990).
21. Ex parte Smernoff, 215 U.S.P.Q. (BNA) 545, 547 (P.T.O. Bd. App. 1982) (“one who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not an coinventor”).
22. In re DeBaun, 687 F.2d 459, 463, 214 U.S.P.Q. (BNA) 933, 936 (C.C.P.A. 1982); Fritsch v. Lin, 21 U.S.P.Q. 2d (BNA) 1737, 1739 (B.P.A.I. 1991).
23. In this case, for instance, it is likely that both could qualify as inventors. What is required is some “quantum  of  collaboration  or  connection.”  Kimberly-Clark  Corporation  v.  Procter  and  Gamble  Distri- bution Co., 973 F.2d 911, 916–17, 23 U.S.P.Q. 2d (BNA) 1921, 1925–26 (Fed. Cir. 1992). For joint inven- torship,  “there  must  be  some  element  of  joint  behavior,  such  as  collaboration  or  working  under common  direction,  one  inventor  seeing  a  relevant  report  and  building  upon  it  or  hearing  another’s suggestion  at  a  meeting”  (ibid.);  Moler  v.  Purdy,  131  U.S.P.Q.  (BNA)  276,  279  (B.P.I.  1960)  (“it  is  not necessary that the inventive concept come to both [joint inventors] at the same time”).
24. See, for example, Advanced Magnetic Closures, Inc. v. Rome Fasteners Corp., 607 F.3d 817 (Fed. Cir. 2010).
25. Conception has been identified as a mental process (“formation in the mind of the inventor, of a definite and permanent idea of the complete and operative invention, as it is hereafter to be applied in practice”).  Hitzeman  v.  Rutter,  243  F.3d  1345,  58  U.S.P.Q.  2d  (BNA)  1161  (Fed.  Cir.  2001).  “The  term ‘inventor’  means  the  individual  or,  if  a  joint  invention,  the  individuals  collectively  who  invented  or discovered the subject matter of the invention.” 35 U.S.C. 100(f) (2012).
26. See  the  Trade-Mark  Cases,  100  U.S.  82,  94  (1879)  (noting  that  “copyright  law  only  protects ‘the  fruits  of  intellectual  labor’  that  ‘are  founded  in  the  creative  powers  of  the  mind.’”),  cited  in  US Copyright Office 2014.
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 221
5/20/2016
1:45:53 PM

    389
    222 Notes
27. While he was a bacteriologist at St. Mary’s hospital in London, Alexander Fleming realized that a mold  had  contaminated  his  samples  of  Staphylococcus.  When  he  examined  his  dishes  under  a  micro- scope,  he  noticed  that  the  mold  prevented  the  growth  of  Staphylococcus  (Market  2013).  The  area around the mold contained a strain of pencillium notatum. Fleming discovered that it could kill many different  types  of  bacteria.  Decades  later,  Howard  Florey  at  Oxford  University  headed  efforts  to  purify penicillin for use in therapeutic applications (American Chemistry Society, n.d.). It proved to be invalu- able during World War II for controlling wound infections (Market 2013).
Saccharin—the first artificial sweetener—was discovered by accident by Constantin Fahlber in 1884.  He  had  been  working  with  compounds  derived  from  coal  tar  and  accidently  ate  something  without  washing  his  hands.  Fahlber  noticed  a  sweet  taste,  which  he  later  traced  to  benzoic  sulfilimine.  Some  reports  hold  that  it  was  his  partner,  Ira  Remsen,  who  first  noticed  that  the  tar  compound  was  sweet.  While  useful  during  World  War  I  when  sugar  was  scarce,  it  was  only  in  the  1960s  and  1970s  that  saccharin  became  popular  as  a  way  to  sweeten  while  avoiding  the  calories  contained  in  regular  sugar  (Clegg 2012).
28. Conception requires contemporaneous recognition and appreciation of the invention. Invitrogen Corporation v. Clontech Laboratories, Inc., 429 F.3d 1052, 1064, 77 U.S.P.Q. 2d (BNA) 1161, 1169 (Fed. Cir. 2005) (“the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue”).
29. Silvestri v. Grant, 496 F.2d 593, 596, 181 U.S.P.Q. (BNA) 706, 708 (C.C.P.A. 1974) (“an accidental and  unappreciated  duplication  of  an  invention  does  not  defeat  the  patent  right  of  one  who,  though later in time was the first to recognize that which constitutes the inventive subject matter”).
30. Originally, the active ingredient in Viagra was intended as a cardiovascular drug to lower blood pressure  (Fox  News  2008).  The  trials  for  this  intended  use  were  disappointing  until  volunteers  began reporting a strange side effect: erections (Jay 2010).
Botox is a branded formula of botulinum toxin type A manufactured by Allerga (“Medication Guide:  Botox,  n.d.).  Botulinum  toxin  is  a  protein  produced  by  the  bacterium  Clostridium  botulinum  (Monte- cucco and Molgó 2005). It was used in the late 1700s as a food poison and gained attention in the 1890s  for its potential use as a biological weapon; “one gram [of botulinum toxin] has the potential to kill one  million  people”  (Ting  and  Freiman  2004).  In  the  1960s,  however,  Drs.  Alan  Scott  and  Edward  Schantz  discovered botulinum toxin type A’s ability (in small doses) to block the transmission of nerve impulses  and  paralyze  hyperactive  muscles  to  treat  eye,  facial,  and  vocal  spasms  (ibid.,  259–260).  These  novel  developments  led  to  the  accidental  discovery  that  botulinum  type  A  injections  also  reduced  wrinkles;  physicians  quickly  began  administering  Botox  as  wrinkle  reduction  treatment  well  before  the  FDA  finally  approved  Botox  for  this  use  in  2002  (Ghose  2014).  Since  then,  Botox  has  steadily  expanded  to  treat  over  twenty  different  medical  conditions,  including  chronic  headaches,  overactive  bladder,  and  urinary incontinence (Nichols 2015; Skincare-news.com Team, n.d.; FDA 2010, 2013).
31. Commercialization theory holds that patents are important in providing incentives for investment in increasing the value of a patented technology (see Kitch 1977, 276–277).
32. It has been estimated that prehuman expenditures are 30.8 percent of costs per approved com- pound, and an estimate of average pretax industry cost per new prescription drug approval (inclusive of
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 222
5/20/2016
1:45:53 PM

    390
    Notes 223
failures and capital costs) is $2.55 billion (Tufts Center for the Study of Drug Development 2014). The  cost of new prescription drug approval is hotly contested (e.g., Collier 2009).
33. Although some human inventors also appear to lack a moral compass (Ho 2000). 34. 35 U.S.C. §101 (2012).
35. Ibid.
36. 5 Opinion of the Court Jefferson 75–76 (H. Washington ed. 1871). “In choosing such expansive terms [for the language of section 101] ... modified by the comprehensive ‘any,’ Congress plainly con- templated  that  the  patent  laws  would  be  given  wide  scope.”  Diamond  v.  Chakrabarty,  447  U.  S.  303, 308 (1980).
37. Bilski v. Kappos, 561 U.S. 593, 593–96 (2010). So “a new mineral discovered in the earth or a new plant  found  in  the  wild  is  not  patentable  subject  matter.”  Diamond  v.  Chakrabarty,  447  U.  S.  309 (1980).  “Likewise,  Einstein  could  not  patent  his  celebrated  law  that  E  =  mc2;  nor  could  [Isaac]  Newton have  patented  the  law  of  gravity”  (ibid.).  Nor  is  a  mathematical  formula,  electromagnetism  or  steam power, or the qualities of bacteria patentable (ibid.).
38. Alice Corp. v. CLS Bank, 573 U. S. ____ (2014) (slip op., at 5–6). Also, these exceptions existed in various forms for 150 years. See Le Roy v. Tatham, 14 How. 156, 174.
39. Ibid. As courts acknowledge, all patents rely to some extent on these exceptions and have the potential to hinder as well as promote future innovation (ibid.).
40. The information covered by these exceptions is “part of the storehouse of knowledge of all men ... free to all men and reserved exclusively to none.” Funk Brothers Seed Co. v. Kalo Inoculant Co., 333 U. S. 127, 130 (1948).
41. 35 USC 161.
42. Section 101 is a “dynamic provision designed to encompass new and unforeseen inventions.” J.E.M. Ag Supply, Inc. v. Pioneer Hi-Bred International, Inc., 534 U. S. 124, 135 (2001). As the Supreme Court stated  in  Bilski  v.  Kappos,  “For  example,  it  was  once  forcefully  argued  that  until  recent  times,  ‘well- established  principles  of  patent  law  probably  would  have  prevented  the  issuance  of  a  valid  patent  on almost any conceivable computer program.’” Bilski v. Kappos, 561 U.S. 593, 605 (2010), citing Diamond v. Diehr, 450 U.S 175, 195 (1981) (STEVENS, J., dissenting). But this fact does not mean that unforeseen innovations such as computer programs are always unpatentable (ibid.).
43. Diamond v. Chakrabarty, 447 U. S. 303, 315 (1980).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 223
5/20/2016
1:45:53 PM

391
Global Perspectives and Challenges for the Intellectual Property System A CEIPI-ICTSD publications series
 Intellectual Property and Digital Trade in
the Age of Artificial Intelligence and Big Data
  Edited by Xavier Seuba, Christophe Geiger and Julien Penin
With contributions by
Keith E. Maskus, Yann Ménière, Ilja Rudyk, Sean M. O’Connor, Catalina Martínez, Peter Bittner, Alissa Zeller, Reto Hilty, Christophe Geiger, Giancarlo Frosio, Oleksandr Bulayenko, Ryan Abbott, Timo Minssen, Jens Schovsbo, Francesco Lissoni, Gabriele Cristelli, and Claudia Jamin
Issue Number 5 June 2018
     
392
Inventive Machines: Rethinking
Invention and Patentability
Ryan Abbott

Global Perspectives and Challenges for the Intellectual Property System 115
393
Computers are doing more than ever before.1 They are doing it cheaper, faster, and often better than their human counterparts, and on an unprecedented scale. Take, for example, Amazon’s Kiva robots, which help retrieve and package items. Amazon now has 45,000 of these robots working together with 230,000 human employees. I suspect it will not be long until there are 230,000 next- generation Kiva robots working together with 45,000 human employees. Or, perhaps, 5,000 next- generation robots and no human employees.
Robots are doing more than manual labour—they are working as doctors, lawyers, and scientists. They are also getting pretty good at playing games. IBM’s supercomputer Deep Blue beat world chess champion Garry Kasparov in 1997, IBM’s next-generation supercomputer Watson won a game of Jeopardy! in 2011, and last year Google’s supercomputer DeepMind’s AlphaGo program beat a master Go player, Lee Se-dol. Of course, playing games is just a way for these computers to demonstrate their capabilities. Watson, for instance, is now developing cancer treatment protocols for patients at Memorial Sloan Kettering Center. IBM also has Watson developing new food recipes and doing some tremendous things involving a food truck.
You can now go to IBM’s website and work with Chef Watson to create new recipes. Watson is less restricted by preconceptions about combining foods and flavours than human chefs. That allows Watson to generate recipes that people have not really thought about before. Put another way, Watson is coming up with new, inventive, and industrially applicable compositions. For those of us in patent law, that raises the question of whether Watson’s ideas are patentable, and if so, who would qualify as an inventor for such patents?
It has been at least 20 years since the first autonomous machine invention was patented. The first such invention I am aware of was created by the “Creativity Machine,” which used a neural network architecture. It essentially consisted of a series of networked on-and-off switches connected in a neural network, which generated new output when perturbed. The first network was connected to a second network, which evaluated the output for usefulness. The Creativity Machine was given a goal
1 This article, and the associated presentation, is based on the author’s research on computer generated works. See, for example, Ryan Abbott, “Hal the Inventor: Big Data and Its Use by Artificial Intelligence,” in Cassidy R. Sugimoto, et al. (eds), Big Data Is Not a Monolith (Cambridge, MA: MIT Press 2016); Ryan Abbott, “I Think, Therefore I Invent: Creative Computers and the Future of Patent Law,” Boston College Law Review 57.4 (2016); Ryan Abbott, “Artificial Intelligence, Big Data and Intellectual Property: Protecting Computer-Generated Works in the United Kingdom,” in Tanya Aplin (ed.), Research Handbook on Intellectual Property and Digital Technologies (Cheltenham, UK: Edward Elgar, forthcoming); Ryan Abbott, “Everything is Obvious,” 66 UCLA Law Review, forthcoming. These works are all available at http://ssrn. com/author=1702576. Readers interested in this subject may also be interested in early works by Pamela Samuelson, Arthur Miller, and Ralph Clifford, “Allocating Ownership Rights in Computer-Generated Works,” University of Pittsburgh Law Review 1185 (1986); 1199–1200; Arthur R. Miller, “Copyright Protection for Computer Programs, Databases, and Computer-Generated Works: Is Anything New Since CONTU?” Harvard Law Review 106 (1993): 1043; and Ralph D. Clifford, “Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up?” Tulane Law Review 71 (1997): 1675–1703. A most incomplete list of more recent scholarship includes: Lisa Vertinsky and Todd M. Rice, “Thinking about Thinking Machines: Implications of Machine Inventors for Law,” Boston University Journal of Science and Technology Law 8.2 (2002); Robert Plotkin, The Genie in the Machine (Redwood City, CA: Stanford University Press, 2009); C.R. Davies, “An Evolutionary Step in Intellectual Property Rights: Artificial Intelligence and Intellectual Property,” Computer Law and Security Review, 27.6 (2011); Annemarie Bridy, “Coding Creativity: Copyright and the Artificially Intelligent Author,” Stanford Technology Law Review 5 (2012); J. McCutcheon, “Curing the Authorless Void: Protecting Computer-Generated Works Following ICETV and Phone Directories,” Melbourne University Law Review, 37.1 (2013); Ben Hattenbach and Joshua Glucoft, “Patents in an Era of Infinite Monkeys and Artificial Intelligence,” Stanford Technology Law Review 19 (2015); Jean-Marc Deltorn, “Deep Creations: Intellectual Property and the Automata,” Frontiers in Digital Humanities, 2017, https://www.frontiersin.org/articles/10.3389/fdigh.2017.00003/full; Shlomit Yanisky-Ravid and Xiaoqiong Liu, “When Artificial Intelligence Systems Produce Inventions: the 3A Era and an Alternative Model for Patent Law,” Cardozo Law Review, forthcoming; and W. Michael Schuster, “A Coasean Analysis of Ownership of Patents for Inventions Created by Artificial Intelligence,” Washington and Lee Law Review, forthcoming.
 
116 394 Intellectual Property and Digital Trade in the Age of Artificial Intelligence and Big Data to complete, and from that it independently produced a result. A process like this could be used in a
variety of industries to, say, discover a new polymer or to design a faster semiconductor.
The Creativity Machine, if it were a human being, would be an inventor in these circumstances. Inventorship does not go to the person who instructs someone else to solve a problem. If I tell my research scientist that I would like her to design a better battery and she does, that does not make me an inventor of her battery. The research scientist would be the inventor. In the case of the Creativity Machine, the United States Patent and Trademark Office (USPTO) granted a patent for the machine’s invention, but did so in the name of the machine’s owner. That was an easy decision for the Patent Office as the application had not disclosed the machine’s involvement.
The Creativity Machine may have been the first autonomous machine inventor, but it certainly was not the last. More patents were created autonomously by machines in the 2000s—for example, by the “Invention Machine,” which relied on genetic programming. Inventions autonomously created by the Invention Machine were also issued patents by the USPTO, again under circumstances in which, if the machine had been a person, the machine would have been the inventor.
Of course, right now there may be few machines independently inventing. Most machines are involved in the inventive process as simple tools that help people to “reduce to practice” an invention. If I design an experiment and have my PhD students carry it out without change, and the experiment’s results are patentable, I, and not my students, am probably the inventor for those results. Similarly, most computers are just executing tasks given by people. But at least some of the time, the computer occupies the role of the inventor. I suspect you are not hearing more about autonomous machine inventions because of concerns about patentability. Can a machine be an inventor? Should a machine be an inventor? These are open questions, and they are important theoretical and practical questions because computers are de facto inventing, and inventors have ownership rights in patents. Failure to list inventors can make patents invalid or unenforceable.
I have looked at this primarily from a US law perspective and found no statute that discusses computer inventorship, there is no case law directly on the issue, and there is no relevant patent office policy. However, there are some barriers to computer inventorship. For instance, the 1952 Patent Act uses the term “individual” to describe potential inventors, something that was done to prevent corporate inventorship. There is also quite a bit of judicial language characterising invention as a “mental act.”
While there is no patent office policy on computational inventions or computer-generated works, there is a copyright office policy on computer authorship. That policy dates to 1984 and states that works “authored” by a computer cannot qualify for copyright protection. In England and Wales, the rule is different under the Copyright, Designs and Patents Act of 1988: if a work is computer- generated, the author is the person who makes the arrangements for the creation of the work.
The United States Copyright Office cites the 1886 case of Burrow-Giles v. Sarony in support of its current policy. In that case, a photographer, Napoleon Sarony, sued the Burrow-Giles Lithographic Company for copyright infringement of a famous photograph of Oscar Wilde. The company alleged that the photographer could not be the photograph’s author because a photograph is just a mechanical reproduction of a natural phenomenon. The Court held that any form of writing by which a mental idea is given visible expression is eligible for copyright protection.

Global Perspectives and Challenges for the Intellectual Property System 117
395
The case thus explicitly dealt with whether the use of a machine would negate human authorship, and implicitly with whether a camera could be considered an author. If it seems unwise to rely on dicta from the Gilded Age to formulate policies on machine authorship—well, that is what is happening. This policy was relevant to a recent case in the Ninth Circuit in California involving the famous “Monkey Selfies.” In that case, a crested macaque in Indonesia took pictures of itself using equipment belonging to a nature photographer, David Slater. Mr Slater promptly claimed copyright in the photographs. Eventually, the United States Copyright Office clarified that because only a person could be an author, that copyright could not subsist in the Monkey Selfies. People for the Ethical Treatment of Animals (PETA) sued Mr Slater in the United States Federal Court for copyright infringement on behalf of the macaque, alleging that the primate should be the copyright owner of its own photographs. The case ultimately settled, with Mr Slater agreeing to donate 25% of future revenue from his use of the photograph to charities dedicated to protecting crested macaques in Indonesia.
If we analogise this copyright case law to the patent context, then maybe a computer cannot be an inventor and its discoveries enter in the public domain. Computers do not need incentives to invent, and permitting computer inventorship might chill human invention.
However, there is a way around computer involvement that works in the patent but not the copyright context. Inventorship can also be based on recognition of inventive subject matter. Thus, a person may be an inventor by virtue of recognising that a computer has invented something patentable. This is almost certainly how the problem is being dealt with today in practice—just as it was for the earliest computational inventions. It avoids having to disclose an inventive computer to the USPTO and potentially throwing a wrench into a patent application. For patent attorneys, there is no incentive right now to disclose inventive activity by computers, and plenty of incentive not to make that disclosure.
The system of invention by recognition seems reasonable if you are the first scientist to notice that penicillin is inhibiting bacterial growth, but perhaps not if you are taking the credit for the work of another inventive entity—even if that entity is a computer. In the latter case, the system is rewarding people even if they are not doing anything inventive themselves. A computer might clearly identify its own results as being patentable. For that matter, it might even format its results as a patent application. Claiming credit for the work of a machine also devalues human invention, because it equates the contributions of people using inventive machines and human inventors who have legitimately engaged in inventive activity.
Taking credit for a machine’s work also has the potential to create logistical problems when the first person to notice a computer’s results is not the computer’s owner or the person who gave the computer a goal to complete. This may incentivise computer owners to restrict access to their machines so that they can control ownership of inventive output.
More ambitiously, I argue that we should recognise computers as inventors. This will functionally produce more invention because it will incentivise the development of creative computers. That is because allowing computer owners to patent the output of their machines makes those machines more valuable. The constitutional rational for granting patent inventions in the United States is based on an incentive theory. We want patents because of the free-rider problem and because

118 396 Intellectual Property and Digital Trade in the Age of Artificial Intelligence and Big Data
patents are thought to generate additional research and discovery. Even though computers do not care about incentives, people who design computers do. Acknowledging computers as inventors would reward effort upstream of the stage of invention, and it could also promote disclosure and commercialisation of patentable subject matter. It would also validate inventor moral rights, because it will distinguish between human inventors who contribute conceptually to an invention and persons filing applications based on the autonomous output of machines.
What about the potential barriers we discussed—that invention must be a mental act and that an inventor must be an individual? Well, there are computers that generate output in a process akin to a person’s mental act—for instance, computers that utilise neural networks. There are also computers that generate output in totally different ways, like those that use expert-based systems. Should it matter how a computer is designed and how it functions?
I would argue no. We should care functionally about whether the system generates innovation, not how innovation occurs. Congress came to that same conclusion in 1952 when it abolished the “flash of genius” test. That was an old requirement that required that the inventive spark come to a person in an “aha” moment rather than as the result of methodical, laborious research. The nature of the test was never entirely clear; it involved judges subjectively reasoning about what an applicant might have been thinking. Congress eventually decided it was not a good test, and that we should not care about what goes on in someone’s head, just whether what they create is inventive and beneficial for society.
Similarly, the requirement that individuals should be inventors should not interfere with computer inventorship. That language is from the 1950s—long before the issue of computational invention was relevant. It should be interpreted according to dynamic principles of statutory interpretation in light of the purpose of the original Act, which was to prevent corporate inventorship.
If a computer could be an inventor, who would own its patent? I am not arguing that a computer should own a patent. Computers are owned as property and do not have legal rights. I would argue that the computer’s owner should be the automatic assignee of anything the computer develops. Where multiple parties are involved, such as software developers, computer owners, and users, they could work out issues of ownership by contract, starting with the default position that the computer owner is the assignee. This would be the most consistent with the way we treat personal and intellectual property right now.
Computational invention has exciting implications beyond inventorship. I think creative computers are going to change the entire patent paradigm in the next 10–20 years.
Even more interesting than thinking about how computers and people are competing right now in inventive activity is that computers are very soon going to overwhelm people in inventive activity. Take biotechnology research on antibodies as an example. There are lots of patents on antibody structures. However, there are only so many ways you can string proteins together to make an antibody, and it is not that difficult to imagine a sufficiently powerful computer sequencing every conceivable antibody and publishing those results online. Assuming this would be an anticipatory disclosure, it would prevent anyone from patenting the structure of those antibodies. The computer could not patent the antibody structures itself because it would not know their utility, which is

Global Perspectives and Challenges for the Intellectual Property System 119
397
another requirement for patentability. But an inventive machine would have just wiped out an entire field of human research.
As computers grow increasingly faster, cheaper, and more sophisticated, they are going to play an ever-greater role in the inventive process. It will become standard for creative computers to automate invention. Someone in the chemical sciences who used to discover new chemical compounds through deductive reasoning and trial and error with teams of human researchers will instead use artificial intelligence to find new compounds. Right now, the hypothetical “person having ordinary skill in the art,” or PHOSITA, is the benchmark we use to judge inventiveness. If the skilled person uses inventive machines, or is an inventive machine, then the benchmark is very high. It is hard to conceive of an invention that would not be obvious to a sufficiently sophisticated computer. That would essentially mean the end of inventive activity. Everything will be obvious.

398
ARTIFICIAL INTELLIGENCE, BIG DATA AND INTELLECTUAL PROPERTY: PROTECTING COMPUTER-GENERATED WORKS IN THE UNITED KINGDOM
RYAN ABBOTT*
Abstract: Big data and its use by artificial intelligence (AI) is changing the way intellectual property is developed and granted. For decades, machines have been autonomously generating works which have traditionally been eligible for copyright and patent protection. Now, the growing sophistication of AI and the prevalence of big data is positioned to transform computer- generated works (CGWs) into major contributors to the creative and inventive economies. However, intellectual property law is poorly prepared for this eventuality. The UK is one of the few nations, and perhaps the only EU member state, to explicitly provide copyright protection for CGWs. It is silent on patent protection for CGWs.
This chapter makes several contributions to the literature. First, it provides an up-to-date review of UK, EU and international law. Second, it argues that patentability of CGWs is a matter of first impression in the UK, but that CGWs should be eligible for patent protection as a matter of policy. Finally, it argues that the definition of CGWs should be amended to reflect the fact that a computer can be an author or inventor in a joint work with a person.
Keywords: computer-generated works, artificial intelligence law, big data and intellectual property, international law, patents
I. INTRODUCTION
Big data and its use by artificial intelligence (AI) is changing the way intellectual property is developed and granted. For decades, machines have been autonomously generating works which have traditionally been eligible for copyright and patent protection.1 For instance, in the US, the first “computer-generated work” (CGW) was submitted for copyright registration prior to 1965. The US Patent and Trademark Office (USPTO) has granted patents for inventions autonomously generated by computers as early as 1998. Terms such as “computers” and “machines” are used in this chapter interchangeably to refer to computer programs or software rather than to physical devices or hardware. As AI continues to grow exponentially more sophisticated and powerful, and the amount of data available to these machines keeps pace, CGWs should become a major contributor to the creative and inventive economies.2
This chapter considers the phenomenon of CGWs from a UK, EU and international law perspective. There is little law on the subject. UK law explicitly provides for copyright protection of CGWs, and in this respect, it is an outlier in the EU and internationally. However, UK law is silent on patent protection. No UK, EU or international law explicitly prohibits protection for
* Professor of Law and Health Sciences, University of Surrey, School of Law and Adjunct Assistant Professor of Medicine at the David Geffen School of Medicine at University of California, Los Angeles. This is a draft chapter. The final version will be available in Research Handbook on Intellectual Property and Digital Technologies edited by Tanya Aplin, forthcoming, Edward Elgar Publishing Ltd. The material cannot be used for any other purpose without further permission of the publisher, and is for private use only.
 
399
CGWs, but rarely are such works explicitly protected. Legal instruments and judicial language related to both copyright and patents frequently refer to authors and inventors as natural persons, or restrict authorship or inventorship to natural persons, but this is most likely in response to the prospect of corporate authorship and inventorship. Such language does not appear to be the result of seriously considering CGWs and should not prohibit IPRs as a matter of policy.
This chapter begins by describing the phenomenon of CGWs and then reviewing the relevant law. It seeks to resolve the following questions: Are computers autonomously creating or inventing or merely aiding human authors and inventors? How will inventive machines alter research and development? Can a CGW receive copyright or patent protection? Can a person qualify as an author or inventor for a machine’s output? Who would own IPRs associated with a CGW? These and other questions can be answered by referring to the fundamental policy rationales for IPRs, and by analogy to instances of human authorship and invention.
The chapter argues that patentability of CGWs is a matter of first impression in the UK, but that CGWs should be eligible for patent protection. This would incentivize the development of inventive machines, which will ultimately result in more innovation. Acknowledging machines as inventors would also safeguard moral rights, because it would prevent people from receiving undeserved acknowledgement.
The chapter also proposes that the standard for CGWs should be amended—for copyright as well as patent. Rather than treating a CGW as a work “generated by a computer in circumstances such that there is no human author of the work”, a CGW should be a work “generated by a computer in circumstances such that the computer, if a natural person, would be an author.” Similarly, for patents, CGW should be a work “generated by a computer in circumstances such that the computer, if a natural person, would be an inventor.” This would take into account the fact that people and machines often work collaboratively, and that even with the involvement of a person a machine can contribute as an author or inventor in its own right.
Finally, this chapter argues there is a need for an internationally harmonized approach to CGWs. Most jurisdictions in the EU, and worldwide, have yet to decide how to regulate CGWs. Failure to internationally harmonize may disadvantage countries which permit IPRs for CGWs, and advantage those which do not.
II. CREATIVE COMPUTERS AND INVENTIVE MACHINES
The Growing Sophistication of AI
Much has been written about the increasing capacity of AI to engage in knowledge-work. Indeed, hardly a day goes by without a news article describing some new feat achieved by AI, whether it is IBM’s AI system DeepBlue beating Garry Kasparov at Chess, IBM’s Watson winning a game of Jeopardy, or Google’s DeepMind defeating a Go world champion in 2016. DeepMind’s Go victory was unexpected at the time because of the sheer complexity of the game, which has more potential Go board configurations than there are atoms in the Universe. AI systems are playing games to demonstrate their capabilities and to train, but they are also being applied to solve practical problems. Watson, for example, is being used to find new uses for existing drugs—an activity that has traditionally been fertile grounds for generating patentable inventions.
Computer knowledge-work can be thought of on a spectrum. On the one end, computers may function as simple tools that assist human authors and inventors, much the way that a pen or a wrench can help someone to write or invent. Works generated in this fashion have been referred to as “works created using a computer”, and likely account for the vast majority of human-machine

400
collaboration. While it could not be seriously argued that Microsoft Word should be a co-author of this chapter, it did contribute to the chapter’s creation. At times, Word corrects spelling, automatically formats, and even suggests the use of certain words.
The term “intermediate works” has been used to refer to more substantive contributions made by computers to creative works where a person qualifies as an author or inventor. It may be difficult to precisely distinguish between an intermediate work and a work created using a computer. Word probably could not contribute to an intermediate work, but a variety of publicly available software programs can. For instance, “Band-in-a-Box” allows a user to choose chords and styles, and the program then automatically generates a “complete professional-quality arrangement of piano, bass, drums, guitar, and strings or horns.”3 Other programs can make similarly substantive contributions to different types of creative works, such as novels and films. In some instances of intermediate works, it may be the case that the computer would qualify as a joint author or inventor, if it were a natural person.
At the other end of the spectrum, computers generate works under circumstances in which no human author or inventor can be identified. These are often referred to as CGWs or “works created by a computer”. While not widely appreciated, computers have been creating CGWs for decades. As an interesting example of the interplay between copyright and patent, in 2003, technologist Raymond Kurzweil, now a Director of Engineering at Google, was granted a patent on a computer program that could autonomously generate creative writings—the “Cybernetic Poet.” Incidentally, Mr. Kurzweil now predicts that machines will have human levels of intelligence in about a decade.
The argument has been made that a human author or inventor exists for any CGW, in the sense that, “behind every good robot is a good person.”4 It is true that a programmer (or many programmers and developers) has to create computer software, and in some cases it may make sense to impute authorship or inventorship to a programmer—particularly if a programmer develops an algorithm specifically to solve a particular problem or to generate a particular output. In these cases a programmer might have a significant contribution to a machine’s speicifc output. However, it may also be the case that a programmer creates an algorithm with no expectation or knowledge of the problems it will go on to solve. Some AI systems such as neural networks can behave unpredictably, such that their original programmers may not understand precisely how they function.5 Some computer systems, such as those based on genetic programming, may even be able to alter their own code. By analogy to human inventorship, an inventor’s teachers, mentors and even parents do not qualify as inventors on their patents, at least, not without directly contributing to the conception of a specific invention.
Attributing authorship or inventorship to a computer user, rather than a programmer, is also problematic. It may sometimes be the case that a user makes a significant contribution to a computer’s output, or that formulating instructions to a computer requires significant skill. However, it may also be the case that a user simply asks a computer to solve a problem, and the computer proceeds to independently generate an answer. In the future, it may even be the case that the computer is able to identify that its output is eligible for copyright or patent protection. In such cases, it seems difficult to argue that the user is an author or inventor. Again, by analogy to human works, simply instructing another person to solve a problem does not usually qualify for authorship or inventorship.
Thus, in at least some instances, computers are generating works traditional entitled to copyright and patent protection under circumstances in which no natural person qualifies as an author or inventor according to traditional criteria. In practice, it may be difficult to distinguish

401
between works created using a computer, intermediate works, and works created by a computer. However, this is not unlike making sense of human authorship and inventorship for joint works where individuals make diverse contributions.
Where’s the CGW?
Given these technological advances, one would be forgiven for asking—where are the CGWs? Why are there not routinely lawsuits over CGWs? How have countries managed without legal standards for CGWs?
It may be that the creative AI revolution has yet to arrive. CGWs may be few and far between, or lack commercial value. When Scott French programmed a computer to write a novel in the style of a famous author in 1993, the resulting work was described by one critic as, “a mitigated disaster”.6 Likewise, with regard to inventions, computers may rarely be inventing, or these outputs may lack significant utility.
It may also be that computers are creating CGWs, but that this is not being disclosed. There are good reasons to think this may be the case. In the US, for example, CGWs are not entitled to copyright protection. In 1965, the US Copyright Office reported it received several applications for CGWs. Given the exponential improvements in computer science, one would thus expect a similarly exponential increase in CGWs submitted for copyright protection from 1965 until the present. However, at least as early as 1973, the US Copyright Office elected to deny protection for CGWs.7 As a result, anyone in possession of a potentially valuable CGW would disqualify protection for the work by revealing its origins. A computer user wishing to obtain protection for a CGW may thus end up identifying himself or herself as the author. Similarly, in the UK, it is not clear that CGWs are entitled to patent protection. Computer users may thus elect to identify themselves as inventors for CGWs. Indeed, some of the earliest applicants for patents on CGWs were advised by their attorneys to report themselves as inventors.8
Failing to disclose the machine’s role in a CGW may also seem an appealing option because it is unlikely to be challenged. For instance, in the UK, CGWs are protected by copyright without registration, and the UK Intellectual Property Office (IPO) will not dispute a patent applicant’s reported inventorship unless this is challenged by a third-party. The issue of authorship or inventorship of a CGW may not arise until litigation, and even that is unlikely. When human authors and inventors have a disagreement about relative contributions, there will generally be one or more parties with an adverse legal interest. However, if a user takes credit for a computer’s invention, the computer is not in a position to protest. A legal dispute will probably only occur in cases where an alleged infringing party wants to dispute copyright or patent protection can subsist in a CGW, and somehow becomes aware that a computer was involved in generating the work.
This situation with respect to CGWs is a problematic state of affairs. It is important that authorship and inventorship be accurately attributed, both to optimize the use of copyright and patents as economic incentives, and to preserve the moral rights of natural persons. Establishing an author or inventor’s identity is important because whether the work qualifies for protection in the UK may depend on the author’s national status. It also identifies the first owner of copyright or patent, may base the term of copyright protection on the author’s death, and determines whether there are moral and rental rights belonging to an author. In whatever manner nations elect to protect CGWs, including by providing no protection, appropriate identification of the origin of CGWs is necessary for IPRs to function effectively as economic rights. Even with regard to moral rights, failure to designate a computer as an author or inventor may result in individuals taking credit for

402
works they have not personally generated. This may undermine the value of human authorship and inventorship.
Determining computer authorship and inventorship may be a complex endeavor. However, that is already the case with natural persons. For instance, despite the romantic conception of inventors as lone prodigies tinkering in their garages and experiencing flashes of genius, the vast majority of invention comes from industry and academic work where multi-person collaborations are the norm. Inventorship disputes are becoming more common,9 and determining inventorship in collaborative work is “one of the muddiest concepts in the muddy metaphysics of the patent law”.10
III. LEGAL STANDARDS
Intellectual property in the UK is primarily governed at the national level, subject to compliance with certain EU requirements and international treaties.
United Kingdom Standards for Computer-Generated Works
The Copyright, Designs and Patents Act 1998 (“CDPA”) is the primary legislation for copyright law.11 Copyright is an intellectual property right which subsists in certain creative works such as books, music and movies. It gives its owner the exclusive right to exploit the underlying subject matter for a fixed number of years, generally 70 years plus the life of the author, subject to certain exceptions such as fair dealing. Generally, the author of a work is the person who creates it, and the author is the default copyright owner. A notable exception is that an employer will be the default owner if a work is “made by” an employee in the course of employment. In some instances, an “author” can be a body incorporated in the UK, such as a limited company.12 Special authorship rules apply to “entrepreneurial” or “media” works—sound recordings, films, broadcasts and typographical works—that are produced rather than created, whereby legal entities are accepted as authors.
The CDPA makes special provision for CGWs with different rules for authorship and copyright duration. These works are defined as those “generated by a computer in circumstances such that there is no human author of the work[s].” CDPA §178. For these works, the CDPA provides that, “[i]n the case of a literary, dramatic, musical or artistic work which is computer- generated, the author shall be taken to be the person by whom the arrangement necessary for the creation of the work are undertaken.” CDPA §9(3). Of note, this protection only extends to literary, dramatic, musical and artistic works and not to media works, although a similar system to §9(3) also applies with regard to design rights.13 For CGWs, the term of the copyright is fifty years from the end of the calendar year in which the work was made.14
At least two cases considered CGWs under the Copyright Act 1956, the statutory regime prior to the CDPA.15 This statute had no provisions for CGWs.16 In Express Newspapers plc v Liverpool Daily Post & Echo [1985] FSR 306, the plaintiff newspaper Daily Express conducted a ‘Millionaire of the Month’ competition. It distributed cards with a five-letter code, and the public could check these cards against a daily newspaper grid, generated by a computer, to see if they won a prize. The defendant newspaper copied these grids, and was subsequently sued for copyright infringement. One argument advanced by the defendant was that because the grids were produced with the aid of a computer, they had no human author and thus could not be protected by copyright. Whitford J rejected this argument, stating, “[t]he computer was no more than the tool by which the varying grids of five-letter sequences were produced to the instructions, via the computer programs, of [the programmer]. It is as unrealistic [to suggest the programmer was not the author]

403
as it would be to suggest that, if you write your work with a pen, it is the pen which is the author of the work rather than the person who drives the pen.” Id. Whitford J also noted “that a great deal of skill and indeed, a good deal of labour went into the production of the grid and the two separate seqences of five letters”. Id.
Prior to this case, in 1977, Whitford J had chaired the “Whitford Report” which found of computer-generated works, “the correct approach is to look on the computer as a mere tool in much the same way as a slide rule or even, in a simple sense, a paint brush. A very sophisticated tool it may be, with considerable powers to extend man’s capabilities to create new works, but a tool nevertheless.”17 The Whitford Report concluded that both the computer programmer and the person who originated data to provide the computer should be authors of any resultant CGW. In response to the Whitford Report, the Government issued the Green Paper report. Among other things, this report argued that the computer user, as potentially distinct from the programmer and originator of data, should generally also be an author.18 In 1986, the Government published a White Paper, Intellectual Property and Innovation, which argued, “[t]he responses to the 1981 Green Paper have shown, however, that circumstances vary so much in practice that a general solution will not be fair in all cases. It appears that no practical problems arise from the absence of specific authorship provisions in this area. The Government has therefore concluded that no specific provisions should be made to determine this question... If no human skill and effort has been expended then no work warranting copyright protection has been created.”19
After this White Paper, the Copyright Committee of the British Computer Society (BCS) submitted a proposal to the Government arguing that CGWs should be protected as a distinct type of work. “The BCS proposes the creation of a new class of copyright protected works. The copyright owner or ‘maker’ should be defined as the person by whom the arrangements necessary for the making of that computer output or computer-generated work, are undertaken.”20 This language was essentially adopted in the CDPA. The BCS’s proposed language was modeled after provisions for film authorship under the Copyright Act 1956. Despite the BCS’s protestation that sound recordings, films, cable programmes and published editions were already being generated by computer, the CDPA did not extend protections to this subject matter for CGWs.
Since the CDPA’s enactment, the authorship of CGWs was considered in Nova Productions Ltd v Mazooma Games Ltd.21 In this case, the parties were competing manufacturers of electronic pool games. Nova claimed copyright in its graphics and the frames generated by software from those graphics and displayed to users during gameplay. Kitchin J (as he then was) regarded the frames which the software generated based on user actions to be CGWs, even though the component graphics of the frames were designed by a person. Kitchin J further held that the author of the CGW in this case was the company director responsible for designing the game—the person who designed the appearance of the various elements displayed, devised the rules and logic for frame generation, and wrote the program, and not the game player, who “...contributed no skill or labour of an artistic kind”. It should be noted there was limited consideration of §9(3) in this case because the subsistence and ownership of the works was not contested.
In sum, while judicial experience with CGW copyright is limited, it is clear that copyright protection is available. The “author” of a CGW work is the person by whom the arrangements necessary for the creation of the work are undertaken. In light of the relative absence of case law related to authorship of CGWs, cases that have investigated authorship for films may be instructive. Under the CDPA, a film’s producer and principal director are together deemed an author. A producer
 , “in relation to a sound recording or a film, means the person by whom the
 arrangements necessary for the making of the sound recording or film are undertaken...” CDPA

404
§178. Identifying a producer may be a fact intensive inquiry.22 Cases have found it is relevant who instigated the making of the film, who paid for the making of the film, whether a film would not have existed but for the input of a person, whether more than one person may be a producer, and the extent of creative contributions.23
United Kingdom Standards for Patenting Computer-Generated Works
By contrast to copyright, there is no statutory provision governing patents for CGWs, and there appear to have been no cases on the subject. The Patents Act 1977 (“PA”) is the primary legislation for patent law. The PA protects inventions which are new, involve an inventive step, and are capable of industrial application. Patents grant their owners the exclusive right to make, use, sell and import an invention for a limited term, generally 20 years from the date an application is filed, subject to certain exceptions.
While nothing in the PA explicitly deals with CGWs, on numerous occasions it references natural persons. For example, the PA requires the identity of individual inventors to be disclosed, and inventors have the right to be mentioned in an application or a patent. It also provides benefits to inventors in some circumstances in which an employer has received outstanding benefit from an invention. The PA states that,
§
European Union Standards for Computer-Generated Works
The European Single Market seeks to guarantee the free movement of goods, capital, services and labour within the European Union. However, IPRs such as copyright and patents can create barriers to free trade. IPRs are largely national in origin, and not transferrable across boarders or mutually recognized per se. In the interest of promoting trade, the EU has attempted to centralize and harmonize national IP laws. This has been aided by case law from the Court of Justice of the European Union (CJEU), the Agreement on Trade Related Aspects of Intellectual Property Rights (TRIPS), which is discussed in the next section, and various EU directives.
Early CJEU cases established the doctrine of exhaustion and the specific subject matter doctrine. This allowed recognition of national IPRs, but limited the application of IPRs where they would limit free movement of goods. The EU is a party to TRIPS, which has harmonized to a great extent IPRs within the EU. Since TRIPS, various EU directives, such as the Computer Program Directive and the Database Directive, have increasingly harmonized national IP laws where differences existed in terms of substance or duration of rights.25 Further efforts at harmonization have resulted in a unique EU trademark system, and various sui generis rights such as EU level plant variety rights. Today, there is relative comprehensive harmonization of some forms of IP such as trademarks, and relative greater discrepancy with copyright. (Elsmore, 2012).
There is no equivalent to the CDPA §9(3) in other EU continental jurisdictions.26 Worldwide, the UK is one of only a handful of countries that explicitly permits copyright for CGWs. Other nations that provide protection, such as Ireland, New Zealand and India, were influenced by the UK’s example—their statutory instruments contain similar language to CDPA §9(3).27
  Although jurisprudence in related areas may provide
 guidance, there is a degree of novelty to determining authorship of CGWs. It may not be clear in all cases whether the person who makes necessary arrangements is a computer’s owner, user, or
 programmer.
 “inventor... in relation to an invention means the actual deviser
  of the invention...” PA
7(3). The term “deviser” is not defined in the PA, but judicial language
 also frequently refers to inventors as persons and refers to concepts such as “mental activity” being
necessary for invention.24
 
405
EU member states may not have laws specifically permitting or refusing copyright protection for CGWs, but many have laws that restrict authorship to natural persons. For example, Spanish copyright law states that the author of a work is the natural person who creates it.28 Under French law, only natural persons who create works may be considered authors, and the rights to a work vest in the author regardless of any contract.29 For collective works, a legal entity can exercise rights but is not classified as the author. Various other national instruments contain language that alludes to authorship as being a human activity. At a European level, the benchmark for originality is an “author’s own intellectual creation.” This concept was first introduced through legislation— the Software, Term and Database Directives—and then developed by the CJEU.30 For example, in 2011, the CJEU held that, “copyright is liable to apply only in relation to a subject-matter, such as a photograph, which is original in the sense that is its author’s own intellectual creation... the author of a portrait photograph can stamp the work created with his ‘personal touch’.”31 This and similar language seems to imply an author is a natural person. CGWs are not explicitly discussed in any European directives.
For patents, as with the PA, the European Patent Convention (EPC) requires the identity of inventors to be disclosed in patent applications and issued patents,32 although it is left to contracting states to resolve who is an inventor and other entitlement issues. The EPC is a multilateral treaty, separate from the EU and with different membership, which created the European Patent Organisation (EPO) and a system for granting “European patents.” A European patent is not a centrally enforceable patent or a unitary right. Rather, the EPC provides a harmonized procedure for unified prosecution and opposition, on the basis of which a European patent may be nationally granted in any of the 38 EPO countries. By contrast, the European patent with unitary effect (EPUE), or the unitary patent, is a new type of European patent that would be valid in participating member states of the EU. This would involve a single patent and ownership, as well as a single court (the Unified Patent Court), and uniform protection. The Agreement on a Unified Patent Court establishes the unitary patent system. Participation is open to any member state of the EU, but not other parties to the EPC. Negotiations for the unitary patent have been ongoing since the 1970s. At present, this agreement will enter into force after it is ratified by Germany.
International Standards for Computer-Generated Works
Two of the most important international agreements governing copyright and patent law are the Berne Convention and the Agreement on Trade Related Aspects of Intellectual Property Rights (TRIPS). For example, the Berne Convention required countries to offer the same level of copyright protection to nationals of other parties to the convention. It also introduced the idea that copyright protection is not contingent on formalities such as registration, though member states are free to require ‘fixation’. The most substantive international IP agreement is TRIPS, which established global standards for copyright and patent protection. The UK and all EU Member States are required to adhere to the mandatory requirements in TRIPS. These requirements were modeled after the IP laws in developed nations such as the United Kingdom, United States and Japan, so TRIPS required relatively few changes to the UK’s IP laws when it came into effect on 1 January 1996.33
Nothing in these, or any other binding international instrument, explicitly authorizes, or prohibits, protections for CGWs. The Berne Convention, for instance, states the Union is created, “for the protection of the rights of authors in their literary and artistic works.”34 However, the Convention does not define “author.”35 The Berne Convention Guide states that this is due to the

406
fact that, “national laws diverge widely, some recognizing only natural persons as authors, while others treat certain legal entities as copyright owners.”36
The World Intellectual Property Organization (WIPO) did consider protections of “computer-produced works” in discussions of a possible Model Copyright Law.37 It defined a computer-produced work as one generated by a computer where identification of authors is impossible because of the indirect nature of individual contributions. The original owner of the moral and economic rights in such a work would be either the entity “by whom or by which the arrangements necessary for the creation of the work are undertaken,” or the entity “at the initiative and under the responsibility of whom or of which the work is created and disclosed.” WIPO’s Committee of Experts eventually concluded further study was needed, and the model law was never adopted.
United States Standards for Computer-Generated Works
No statute governs the subject of CGWs in the US, and no cases have seriously considered copyright or patent protection for CGWs. However, the US Copyright Office has a policy prohibiting copyright for any non-human work—what it now refers to as its “human authorship requirement.” The US Patent and Trademark Office (USPTO) does not have any stated policy regarding CGWs and patents. In 1986, Professor Pamela Samuelson wrote, “[a]s yet there has been no judicial decision allocating rights in computer-generated works. It can, however, only be a matter of time before courts are forced to resolve the issue.”38 That prediction proved optimistic.
One recent US case came close to raising the issue. Naruto v. Slater involved a series of pictures that a crested macaque took of itself. These “Monkey Selfies” were subsequently commercialized by the camera’s owner, David Slater, who asserted he owned the copyright to the photographs. People for the Ethical Treatment of Animals (PETA) subsequently sued Mr. Slater, alleging that the macaque, Naruto, was the copyright owner, and that Mr. Slater had infringed Naruto’s copyright.
In January 2016, US District Judge William Orrick III dismissed the case on the grounds that Naruto lacked standing to sue. The judge also deferred to the USPTO’s interpretation that the macaque was not an “author” within the meaning of the Copyright Act. He considered PETA’s argument that the USPTO policy is antithetical to the “public interest in animal art”, but ultimately ruled “that is an argument that should be made to Congress and the President, not to me.”39 PETA appealed the decision to the Ninth Circuit Court of Appeals, and shortly after oral arguments, the parties reached a settlement in which Mr. Slater agreed to donate 25% of any future revenues from the monkey selfies to charities. Despite the settlement, however, the Ninth Circuit dismissed the case to create precedent. The Court held that animals only have statutory standing if an Act of Congress plainly states animals have statutory standing, and so animals are unable to sue under the Copyright Act because the law does not expressly authorize animals to file copyright infringement clams. In doing so, the court avoided weighing in on the merits of non-human authorship.
Outside of CGWs, US copyright law has a mechanism for authorship of artificial persons.
“In the case of a work made for hire, the employer for whom the work was prepared is considered the author for purposes.” 17 U.S.C. § 201(b) (2011). Functionally, the same outcome may occur in the UK, but while the UK permits employers to own works, ownership is distinct from authorship for so-called “author works”—literary, dramatic, musical or artistic works—the same works protected by CDPA §9(3). Even in EU countries where only natural persons may be authors, a focus on “author’s rights” does not preclude authors from transferring certain rights to employers,

407
and some jurisdictions will imply the existence of an agreement to do so. Ultimately, then, the same economic outcome may occur for works made in the course of employment in the US, UK and in EU civil law jurisdictions, but the terminology may differ. Some civil law jurisdictions may also retain additional, inalienable rights for authors.
IV. PROTECTING COMPUTER GENERATED WORKS
Policy
Various rationales are given for IPRs, but broadly speaking, they can function as economic incentives and they are justified on the basis of natural rights. The notion of IPRs as an economic right, particularly for patents, dominates the Anglo-American system. In the US, for example, the Constitution explicitly endorses an innovation incentive rationale for IPRs, by granting Congress the power “[t]o
 promote the progress of science and useful arts, by securing for limited times to
 authors and inventors the exclusive right to their respective writings and discoveries.”40
 Patents can incentivize innovation.41 This is based on the theory that information goods are
  typically non-excludable and non-rivalrous, so lack of protection will lead to underproduction. By granting a limited monopoly in the form of a patent, this allows inventors to enjoy greater financial benefits from discoveries and encourages invention. In addition, patents can promote the commercialization of inventions. For instance, new drug approvals often take years, and the pharmaceutical industry claims that getting new drugs approved costs billions of pounds. Once a drug is approved, it may be easy for a competitor to copy the drug and avoid the costs of initial approval. Patents may thus encourage an originator pharmaceutical company to spend the necessary resources on approval, because after the drug is approved they can charge monopoly prices until patents expire. Patents, whether incentivizing research or commercialization, are thus one solution to the “freerider” problem. Finally, patents can promote information disclosure. Patents are issued to inventors in exchange for disclosing to the public how to make an invention. Without patents, inventors might rely on confidential information to prevent copying, and never publicly disclose how to make an invention. This happened, for example, with the drug “Premarin” which was first made by Wyeth and now is made by Pfizer. No generics company has been able to replicate this drug since its first regulatory approval in 1942. Perhaps most famously, Coca-Cola
 has kept its recipe for its iconic beverage confidential for over a century.
By contrast, the civil law systems of continental Europe may place more emphasis than the UK on moral rights, which are viewed as independently protectable and separate from economic rights. Moral rights protect an author’s personality and the integrity of a work, and are considered “personal, perpetually inalienable and unassignable.”42 Moral rights also accommodate “personality” rights based, for instance, on theories by Kant and Hegel that people express their “wills” and develop as persons through their interactions with external objects. This, for instance, is accomplished by giving authors the right to control certain uses of their works, even after assigning economic rights. Personality theorists argue that authors and inventors are inherently at risk of having their ideas stolen or altered in objectionable ways. Thus, IPRs are justified to prevent misappropriation or modification of objects through which authors express themselves. IPRs also accommodate Lockean theories of first occupancy, the idea that the person who owns a particular thing should be the person who ‘gets there first’, as well as labour theory, the idea that ownership is derived from mixing labour with unowned or commonly held property, and that appropriating these products would be unjust. These ideals are reflected in patent law, for instance, by giving

408
inventorship rights to the first inventor to file for a patent, and giving inventorship rights to individuals who find new uses for natural products.
But IPRs can also have significant costs. They restrict competition (particularly in the case of patents) and free speech (particularly in the case of copyright), and they can inhibit innovation, collaboration, and open communities. To the extent that IPRs are justified, it is because they are thought to have more benefits than costs. However, with IPRs, more is not always better. For instance, software patents have been criticized for being unnecessary as an incentive, while at the same time creating “patent thickets” that make work in the software industry challenging.43 For this reason, the EPC states that “programs for computers” are not patentable, but the EPO will grant patents for “computer-impelmented inventions” as long as they have a technical effect.
Whether to Patent and to whom?
Having examined UK, EU and international laws on copyright and patent protection for CGWs, or the absence thereof, let us return to the question of whether the UK should provide patent protection for CGWs. A number of academic commentators have argued that CGWs should become public property.44 If CGWs should instead be eligible for patent protection, who should be the inventor and owner of a CGW?
This chapter proposes that CGWs should be eligible for patent protection. The innovation incentive function of patents does not change based on whether a computer or a person invents. It is true that a computer does not respond to financial incentives, but the entities who develop inventive machines do. Providing patent protection for the output of autonomous machines makes autonomous machines more valuable, and what better way to incentivize innovation than to incentivize the development of inventive machines? This would reward activity upstream from the act of invention. To the extent that patents are incentivizing commercialization and disclosure of information, there is no change in this function as between a human and CGW. Also, if patent protection is not available for inventive AI output, then businesses may not use inventive AI, even in future instances where AI will be more effective than a person.
If CGWs are prohibited from receiving patents, it may be possible for a natural person to claim inventorship of a CGW even where that person was not involved in the development or operation of a computer. Namely, a person could argue they “devised” the invention by virtue of recognizing the relevance of a machine’s output. Indeed, discovery of an unrecognized problem may give rise to patentable subject-matter (“problem-inventions”).45 Similarly, discovery of an unrecognized solution can be patentable. In some cases, recognition of the inventive nature of a computer’s output may require significant skill, but in others, the nature of inventive output may be obvious. In the future, it may even be the case that a computer can identify its own output as patentable, and format it for a patent application.
If CGWs are to be protected, how then should inventorship and ownership be determined? Distinguishing inventorship and ownership may not functionally impact economic rights, but it does implicate moral rights. At present, de jure or de facto, individuals are claiming inventorship of CGWs under circumstances in which they have not functioned as inventors. This is fundamentally unfair, and it weakens moral justifications for patents by allowing individuals to take credit for the work of inventive machines. It is not unfair to computers who have no interest in being acknowledged, but it is unfair to other human inventors because it devalues their accomplishments by altering, and diminishing, the meaning of inventorship. This could equate the hard work of creative geniuses with those simply asking a computer to solve a problem. It would be particularly problematic once inventive machines come to generate a substantial portion, or

409
even the majority of inventions.46 By contrast, acknowledging computers as inventors would also acknowledge the work of computer programmers. While they may not have directly contributed to an invention, they may take credit for the success of their machines. This is similar to the way in which a supervisor may take pride in the success of a PhD student, without taking direct credit for their future writings and inventions.
If CGWs are to be protected, and a computer is to be acknowledged as an inventor, who should own the CGW? Certainly, computers should not own patents. Computers are non-sentient, cannot own property, and are themselves owned as property. Colin Davies has suggested the computer should hold IP rights and transfer these under contract.47 He notes this would require machine “responsibility,” which might require a deposit in a computer’s name to satisfy adverse judgments or an insurance scheme. More simply, ownership may directly vest in a computer’s user, programmer, or owner. In many instances, these may be the same entity, but they may also be distinct parties. The best policy or ideal solution would be to have ownership vest in the party that results in the most effective economic outcome, and also results in a standard that is practical to implement.48
The computer’s owner should be the default owner of any CGW it produces. This is most consistent with current ownership norms surrounding personal property (including both computers and patents).49 It should also most effectively incentivize innovation because it will motivate owners to share access to their software. If the computer’s user is the default owner of a CGW, this may instead result in computer owners restricting access. Computer programmers do not need to own future CGWs because they will capture the increased value of an inventive machine upon selling it. Also, having ownership default to programmers would interfere with the transfer of a machine, and it would be logistically problematic for developers to monitor machines they no longer own. The case for having computer owners also have ownership of CGWs reveals another reason why computers should be acknowledged as inventors. If computers cannot be inventors and instead the first natural person to recognize a computer’s invention becomes the inventor, this would give CGWs to computer users rather than owners. There is already precedent for assigning ownership in IPRs to an owner distinct from an author or inventors, such as with works for hire, joint authorship, films, etc.
This default was just be a starting point—computer users, owners and developers would be free to contract to different outcomes.
Computer-generated works—competition or collaboration?
The current definition of CGWs fails to take into account the fact that computers independently should qualify for authorship and inventorship, even when contributing to jointly authored works with natural persons. Computers may be inventors even of intermediate works. As such, the definition of CGWs should be amended from work “generated by a computer in circumstances such that there is no human author of the work”, to work “generated by a computer in circumstances such that the computer, if a natural person, would meet authorship requirements.” This would more accurately take into account contributions by machines, and allow economic incentives to work more efficiently.
The downside of this approach may be that it would be difficult for computer owners to know when their machines have generated CGWs. Users might benefit from failing to disclose CGWs to computer owners and then claiming they invented a CGW. However, users may still choose to disclose CGWs so that they could negotiate for clear title and, alternately, to avoid liability. To the extent that users and owners are distinct entities and users are licensing computers

410
for purposes generating CGWs, users may choose to negotiate a priori for ownership of CGWs with computer owners.
Determining human inventorship is already a tricky business in collaborative works. It may be even more difficult for collaborative works involving a computer. There are a variety of ways for computers to invent, some of which involve more human intervention than others. For example, a programmer may design a computer program specifically to solve a particular problem, and the solution may be the patentable invention. In such an instance, the programmer might have a greater claim to inventorship, resulting in joint inventorship with a computer. Again, this is not unlike current inventorship criteria, where a variety of individuals can play greater or lesser roles in invention. However, the current definition of CGWs in the CDPA does not accommodate this reality for copyright, as it fails to take into account that a computer can jointly author a work with a person.
International Harmonization
Finally, there is a need for a harmonized approach to CGWs. If the UK grants copyright and patent protections for CGWs, it has to provide nationals of other EU member states and parties to TRIPS with the same rights. However, if these other parties fail to allow for CGWs in their own domestic laws, UK nationals may not receive reciprocal protections.50 Few EU member states have dealt with CGWs.51 Inventive machine owners might thus be unable to obtain IPRs outside the UK. In fact, disclosing a machine author or inventor in a UK application might prejudice IPRs in other jurisdictions. At least for an interim period, UK entities would be advised to identify a natural person as an author or inventor where possible to avoid an inequitable economic outcome.
Future treatment of CGWs within the EU might be dealt with by an EU directive or regulation, although Brexit may remove the UK from the direct effect of changes to EU law. Regardless of Brexit, UK nationals still should benefit under the national treatment rule of TRIPS from changes to EU law that ascribe machine authorship and inventorship for CGWs. CGWs might also be dealt with by a future multinational agreement. However, harmonization exercises at the international level tend to proceed at a glacial pace.
Concluding Thoughts
In October 2017, the Kingdom of Saudi Arabia announced it was granting citizenship to a humanoid robot, Sophia, manufactured by Hanson Robotics. It is unclear whether this announcement was merely intended for publicity, or whether the nation has actually granted Sophia citizenship. In any event, if Sophia is a Saudi citizen, because Saudi Arabia is a party to TRIPS, other WTO members may be obliged to provide for IPRs for Sophia’s CGWs. Although, other countries may argue that Berne and TRIPs refer to authors and inventors who are nationals, but that machines cannot be authors and inventors regardless of ‘nationality’. In any event, while granting legal personhood to a machine may be one way to try and avoid disparate treatment of CGWs at the international level, there are other reasons to disfavor such an approach.
The law is overdue for establishing clear standards for protection of CGWs. As AI continues to improve, such works will become increasingly important. Efficiently structured copyright and patent laws can help maximize the value of CGWs, and protect the moral rights of human authors and inventors.52 However, for IPRs to function effectively, it is important that right holders and potential infringers have a reasonable degree of certainty about the scope and limits of protection.

411
 1 See, Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 54 B. C. L. Rev. 1079–1126 (2016).
2 See, Ryan Abbott, Everything is Obvious, 66 UCLA. L. Rev. 2 (2019).
3 See, e.g., Band-in-a-Box, PG Music, http://www.pgmusic.com.
4 Arthur Miller, Copyright Protection for Computer Programs , Databases , and Computer Generated Works: Is Anything New Since CONTU? 106 Harvard Law Review 977–1073 (1993). 5 Abbott, Ryan, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 Geo. Wash. L. Rev. 1 (2018) (discussing unexplainability in the context of AI).
6 Patricia Holt, Sunday Review, S.F.CHRON., Aug. 15, 1993, B4; see, generally, Grimmelmann, J. There’s No Such Thing As A Computer-Authored Work, 39 Columbia Journal of Law & the Arts 403, 408 (2016).
7 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES (FIRST) §2.8.3 (1st ed. 1973).
8 Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 54 B. C. L. Rev. 1079–1126 (2016).
9 IDA Ltd and others v University of Southampton and others [2006] EWCA Civ 145; Abbott, Ryan, Jeremy Lack and David Perkins. Managing Disputes in the Life Sciences. Nature Biotechnology, 36, 697 (2018).
10 Mueller Brass Co. v Reading Industries Inc. 176 USPQ 361 (1972).
11 The CDPA permits copyright for “(a) original literary, dramatic, musical or artistic works, (b) sound recordings, films [or broadcasts], and (c) the typographical arrangement of published editions.” CDPA 1988, § 1 (internal footnote and emphasis omitted).
12 Copyright, Designs and Patents Act, 1988 §154.
13 Copyright, Designs and Patents Act, 1988 §214.
14 Copyright, Designs and Patents Act, 1988 §12(7).
15 In the case of Cummins v. Bond in 1927, a court was asked to adjudicate copyright in a work allegedly written by a journalist while acting as a spiritual medium. Cummins v. Bond, 1 Ch. 167 (1927). The court was not willing to decide that “authorship and copyright rest with someone already domiciled on the other side of the inevitable river.” Id. at 173. The rights to the work had to vest in a terrestrial being.
16 A similar outcome occurred in the case of The Jockey Club v Rahim (unreported) 22 July 1983, which concerned computers generating lists of runners and riders for horse races.
17 Whitford Committee on Copyright Designs and Performers Protection (Cmnd 6732 HMSO 1977), para 514.
18 Reform of the Law Relating to Copyright, Designs and Performer's Protection, A Consultative Document 58 (Cmnd 8302 HMSO 1981).
19 Intellectual Property and Innovation (Cmnd 9712; HMSO, Ch 9, paras 9.6–8).
20 Robert Hart, Copyright and computer generated works, 40 Aslib Proceedings 173, 173–181 (1988).
21 Nova Productions Ltd v Mazooma Games Ltd [2006] RPC 379. CGWs were also briefly considered in Bamgboye v Reed [2004] EMLR 5, 73 [38], Williamson J wrote that §9(3) “is dealing with the case where one is looking at a piece of music which, in fact, is composed of computerised sounds.”
22 See, e.g., Beggars Banquet [1993] EMLR 349.
23 Jani McCutcheon, Curing the authorless void: Protecting computer-generated works following

412
 icetv and phone directories. 37 Melbourne University Law Review 46 (2013).
24 See, e.g., Yeda Research and Development Co Ltd v Rhone-Poulenc Rorer International Holdings Inc [2007] UKHL 43, [2008] RPC 1 quoting Laddie J. in
25 Directive 2009/24/EC of the European Parliament and of the Council of 23 April 2009 on the legal protection of computer programs; Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases.
26 Andres Guadamuz, Do androids dream of electric copyright? Comparative analysis of originality in artificial intelligence generated works, Intellectual Property Quarterly 169 (2017). 27 Copyright, Designs and Patents Act, 1988, c. 48, § 9(3) (U.K.); Copyright Act of 1994, § 5 (N.Z.); Copyright and Related Rights Act 2000, Part I, § 2 (Act. No. 28/2000) (Ir.).
28 Ley 22/11 sobre la Propiedad Intelectual de 1987.
29 C. IP. Art. L111-1 (2003).
30 Case C-5/08 Infopaq International A/S v Danske Dagblades Forening [2009] ECR I-06569.
31 Eva-Maria Painer v. Standard VerlagsGmbH and ors, Case C-145/10 [2011] ECDR (13) 297, 324, [AG121]. In that case, Advocate-General Trstenjak interpreted EU directives related to this language to mean that, “‘only human creations are ... protected’, although these can ‘include those for which the person employs a technical aid, such as a camera’.” Id.
32 EPC R. 19 (Designation of the inventor).
33 94/800/EC Council Decision (of 22 December 1994). See, generally, Matthew James Elsmore, Comparing regulatory treatment of intellectual property at WTO and EU level, in LIBERALISING TRADE IN THE EU AND THE WTO: A LEGAL COMPARISON 412–439 (Sanford E. Gaines, et al., eds., 2012).
34 BERNE CONVENTION FOR THE PROTECTION OF LITERARY AND ARTISTIC 1971 ART. I.
35 Cf, SAM RICKETSON AND JANE C. GINSBURG, INTERNATIONAL COPYRIGHT AND NEIGHBORING RIGHTS (2 VOLUMES): THE BERNE CONVENTION AND BEYOND (2nd Ed. 2006) (arguing the reference to ‘makers’ of cinematographic works is the exception rather than the rule, and that ‘author’ referring to natural persons would be most consistent with the moral rights provisions and durations of protection being based on the life of an author).
36 WORLD INTELLECTUAL PROPERTY ORGANIZATION, GUIDE TO THE BERNE CONVENTION II (1978).
37 See INTERNATIONAL BUREAU OF WIPO, PREPARATORY DOCUMENT, DRAFT MODEL ON COPYRIGHT at 258-59 (No. CD/MPC/III/2, Mar. 30, IggO).
38 Pamela Samuelson, Allocating ownership rights in computer-generated works. 47 U. Pitt. Law Review 1185, 1190 (1985).
39 Naruto v. David John Slater et al, No. 16-15469 (9th Cir. 2018).
40 UNITED STATES CONSTITUTION, ARTICLE I, SECTION 8, CLAUSE 8 (emphasis added).
41 WILLIAM M. LANDES & RICHARD A. POSNER, THE ECONOMIC STRUCTURE OF INTELLECTUAL PROPERTY LAW. Cambridge, MA: Belknap Press (2003).
42 Martin A. Roeder, The Doctrine of Moral Right: A Study in the Law of Artists, Authors and Creators, 53 Harv. L. Rev. 554, 557 (1940). See, also, Graham Dutfield, Collective Invention and Patent Law Individualism: Origins and Functions of the Inventor’s Right of Attribution. 5 The WIPO Journal 25, 27 (2013).
 the invention’. The word ‘actual’ denotes a contrast with a deemed or pretended deviser of the
University of Southampton’s
 Applications [2005] R.P.C. 11, [39] (“The inventor is defined in s.7(3) as ‘the actual deviser of
  invention; it means, as Laddie J. said in University of Southampton’s Applications [2005] R.P.C.
 11, [39], the natural person who ‘came up with the inventive concept.’”)
  
413
 43
44 See, e.g., Ralph Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up? 71 Tul. L. Rev. 1675 (1997).
45 See, e.g., T 0002/83 (Simethicone Tablet) of 15.3.1984 (EPO Board of Appeal).
46 See, Abbott, R. Everything is Obvious, 66 UCLA. L. Rev. 2 (2019).
47 Colin Davies, An evolutionary step in intellectual property rights - Artificial intelligence and intellectual property. 27 Computer Law and Security Review 601, 615 (2011).
48 Ryan Abbott, Hal the Inventor: Big Data and its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH (Hamid Ekbia, et al., eds.) (2016).
49 Cf Schuster, W. Michael, ‘A Coasean Analysis of Ownership of Patents for Inventions Created by Artificial Intelligence’, 75 Washington and Lee Law Review (forthcoming 2018), <https://ssrn.com/abstract=3132753>. (arguing for user default ownership).
50 Robert Hart, Copyright and computer generated works, 40 Aslib Proceedings 173, 173–181 (1988).
51 Mark Perry & Thomas Margoni, From music tracks to Google maps: Who owns computer- generated works? 26 Computer Law and Security Review 621, 621–629 (2010).
52 Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 Harv. L. & Pol. Rev. 145 (2018).
 Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes Debate, 92 TEX. L.
 REV. 303 (2013).
 
                                 414
U.C.L.A. Law Review
Everything Is Obvious
Ryan Abbott
ABSTRACT
For more than sixty years, “obviousness” has set the bar for patentability. Under this standard, if a hypothetical “person having ordinary skill in the art” would find an invention obvious in light of existing relevant information, then the invention cannot be patented. This skilled person is defined as a non-innovative worker with a limited knowledge-base. The more creative and informed the skilled person, the more likely an invention will be considered obvious. The standard has evolved since its introduction, and it is now on the verge of an evolutionary leap: Inventive machines are increasingly being used in research, and once the use of such machines becomes standard, the person skilled in the art should be a person using an inventive machine, or just an inventive machine. Unlike the skilled person, the inventive machine is capable of innovation and considering the entire universe of prior art. As inventive machines continue to improve, this will increasingly raise the bar to patentability, eventually rendering innovative activities obvious. The end of obviousness means the end of patents, at least as they are now.
AUTHOR
Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor, David Geffen School of Medicine at University of California, Los Angeles. Thanks to Ryan Calo, Ian Kerr, Mark Lemley, Lisa Larrimore-Ouellette, and Jake Sherkow, as well as participants in workshops at the University of Surrey, WeRobot Conference, Oxford Business Law Workshop, and the Sixth Annual Fall Conference hosted by the Center for the Protection of Intellectual Property (CPIP) at Antonin Scalia Law School for their insightful comments.
66 UCLA L. Rev. 2 (2019)

                                 415
 TABLE OF CONTENTS
Introduction.......................................................................................................................................................4 I. Obviousness..................................................................................................................................................10 A. Public Policy.......................................................................................................................................... 10 B. EarlyAttempts.......................................................................................................................................11 C. TheNonobviousnessInquiry.............................................................................................................15 D. FindingPHOSITA................................................................................................................................17 E. AnalogousPriorArt.............................................................................................................................20 II. Machine Intelligence in the Inventive Process.......................................................................... 22 A. AutomatingandAugmentingResearch...........................................................................................22 B. Timeline to the Creative Singularity .................................................................................................. 26 C. Inventive and Skilled Machines ......................................................................................................... 31 D. Inventive Is the New Skilled ............................................................................................................... 33 E. Skilled People Use Machines............................................................................................................... 35 F. The Evolving Standard.......................................................................................................................... 37 III. A Post-Skilled World............................................................................................................................ 37 A. Application............................................................................................................................................38 B. Reproducibility...................................................................................................................................... 42 C. AnEconomicvs.CognitiveStandard...............................................................................................44 D. Other Alternatives................................................................................................................................ 46 E. Incentives Without Patents?................................................................................................................ 48 F. A Changing Innovation Landscape.................................................................................................... 50 Conclusion........................................................................................................................................................ 51
3

416
4
66 UCLA L. REV. 2 (2019)
 INTRODUCTION
For at least two decades, machines have been autonomously generating patentableinventions.1 “Autonomously”herereferstothemachine,ratherthan to a person, meeting traditional inventorship criteria. In other words, if the “inventive machine” were a natural person, it would qualify as a patent inventor. In fact, the U.S. Patent and Trademark Office (USPTO or Patent Office) may have granted patents for inventions autonomously generated by computers as early as 1998.2 In earlier articles, I examined instances of autonomous machine invention in detail and argued that such machines ought to be legally recognized aspatentinventorstoincentivizeinnovationandpromotefairness.3 Theowners of such machines would be the owners of their inventions.4 In those works, as here, terms such as “computers” and “machines” are used interchangeably to refer to computer programs or software rather than to physical devices or hardware.5
This Article focuses on a related phenomenon: What happens when inventive machines become a standard part of the inventive process? This is not a thought experiment.6 For instance, while the timeline is controversial, surveys of experts suggest that artificial general intelligence, which is a computer able to perform any intellectual task a person could, will develop in the next twenty-five years.7 Somethoughtleaders,suchasRayKurzweil,oneofGoogle’sDirectorsof
1. See Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) [hereinafter I Think] (describing instances of “computational invention” or “computer-generated works”); see also infra Subpart II.B (discussing some such instances in greater detail).
2. Abbott, supra note 1, at 1085.
3. Id. at 1083–91; Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence,
in BIG DATA IS NOT A MONOLITH (Cassidy R. Sugimoto, Hamid R. Ekbia & Michael Mattioli eds.,2016) [hereinafter Hal the Inventor] (discussing computational invention in a book chapter first posted online February 19, 2015).
4. Except where no owner exists, in possible cases of some open-source or distributed software, in which case ownership could vest in a user.
5. Except perhaps in exceptional cases where software does not function on a general-purpose machine, and where specialized hardware is required for the software’s function.
6. The growing prevalence and sophistication of artificial intelligence is accelerating the use of inventive machines in research and development. See Ryan Abbott & Bret Bogenschneider, ShouldRobotsPayTaxes? TaxPolicyintheAgeofAutomation,12HARV.L.&POL’YREV.145 (2018) [hereinafter Should Robots Pay Taxes?] (discussing the trend toward automation).
7. See generally Vincent C. Müller & Nick Bostrom, Future Progress in Artificial Intelligence: A Survey of Expert Opinion, in FUNDAMENTAL ISSUES OF ARTIFICIAL INTELLIGENCE 553 (Vincent C. Müller ed., 2016).
 
417
Everything Is Obvious 5
Engineering, predict computers will have human levels of intelligence in about a decade.8
The impact of the widespread use of inventive machines will be tremendous, not just on innovation, but also on patent law.9 Right now, patentability is determined based on what a hypothetical, non-inventive, skilled person would find obvious.10 The skilled person represents the average worker in the scientific field of an invention.11 Once the average worker uses inventive machines, or inventive machines replace the average worker, then inventive activity will be normal instead of exceptional.
If the skilled person standard fails to evolve accordingly, this will result in too lenient a standard for patentability. Patents have significant anticompetitive costs, and allowing the average worker to routinely patent their outputs would cause social harm. As the U.S. Supreme Court has articulated, “[g]ranting patent protection to advances that would occur in the ordinary course without real innovation retards progress and may . . . deprive prior inventions of their value or utility.”12
The skilled standard must keep pace with real world conditions. In fact, the standard needs updating even before inventive machines are commonplace. Already, computers are widely facilitating research and assisting with invention. For instance, computers may perform literature searches, data analysis, and pattern recognition.13 This makes current workers more knowledgeable and creative than they would be without the use of such technologies. The Federal Circuit has provided a list of nonexhaustive factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active
8. Peter Rejcek, Can Futurists Predict the Year of the Singularity?, SINGULARITY HUB (Mar. 31, 2017), https://singularityhub.com/2017/03/31/can-futurists-predict-the-year-of-the-singularity [https://perma.cc/4TDE-QQTW] (predicting artificial general intelligence in 2029).
9. See, e.g., ROBERT PLOTKIN, THE GENIE IN THE MACHINE: HOW COMPUTER-AUTOMATED INVENTING IS REVOLUTIONIZING LAW & BUSINESS 60 (2009) (arguing that “[a]rtificial invention technology . . . enables [users] to produce inventions that they could not have created at all without such technology”); Ben Hattenbach & Joshua Glucoft, Patents in an Era of Infinite Monkeys and Artificial Intelligence, 19 STAN. TECH. L. REV. 32, 44 n.70 (2015); Brenda M. Simon, The Implications of Technological Advancement for Obviousness, 19 MICH. TELECOMM. & TECH. L. REV. 331 (2013).
10. 35 U.S.C. § 103(a) (2006). The “person having ordinary skill in the art” may be abbreviated as “PHOSITA” or simply the skilled person.
11. See infra Subpart I.D.
12. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007).
13. Such contributions when made by other persons do not generally rise to the level of
inventorship, but they assist with reduction to practice.
  
418
6 66 UCLA L. REV. 2 (2019)
workers in the field.”14 This test should be modified to include a sixth factor: (6) “technologies used by active workers.”
This change will more explicitly take into account the fact that machines are already augmenting the capabilities of workers, in essence making more obvious and expanding the scope of prior art. Once inventive machines become the standard means of research in a field, the test would also encompass the routine use of inventive machines by skilled persons. Taken a step further, once inventive machines become the standard means of research in a field, the skilled person should be an inventive machine. Specifically, the skilled person should be an inventive machine when the standard approach to research in a field or with respect to a particular problem is to use an inventive machine (the “Inventive Machine Standard”).
To obtain the necessary information to implement this test, the Patent Office should establish a new requirement for applicants to disclose when a machine contributes to the conception of an invention, which is the standard for qualifying as an inventor. Applicants are already required to disclose all human inventors, and failure to do so can render a patent invalid or unenforceable. Similarly, applicants should need to disclose whether a machine has done the work of a human inventor. This information could be aggregated to determine whether most invention in a field is performed by people or machines. This information would also be useful for determining appropriate inventorship, and more broadly for formulating innovation policies.
Whether the Inventive Machine Standard is that of a skilled person using an inventive machine or just an inventive machine, the result will be the same: The average worker will be capable of inventive activity. Conceptualizing the skilled person as using an inventive machine might be administratively simpler, but replacing the skilled person with the inventive machine would be preferable because it emphasizes that the machine is engaging in inventive activity, rather than the human worker.
Yet simply substituting an inventive machine for a skilled person might exacerbate existing problems with the nonobviousness inquiry. With the current skilled person standard, decisionmakers, in hindsight, need to reason about what another person would have found obvious.15 This results in
14. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
15. See generally Gregory N. Mandel, Patently Non-Obvious: Empirical Demonstration that the
Hindsight Bias Renders Patent Decisions Irrational, 67 OHIO ST. L.J. 1391 (2006) (discussing problems with hindsight in non-obviousness inquiries).
  
419
Everything Is Obvious 7
inconsistent and unpredictable nonobviousness determinations.16 In practice, the skilled person standard bears unfortunate similarities to the “Elephant Test,”17 or Justice Stewart’s famously unworkable definition of obscene material: “I know it when I see it.”18 This may be even more problematic in the case of inventive machines, as it is likely to be difficult for human decisionmakers to theoretically reason about what a machine would find obvious.
An existing vein of critical scholarship has already advocated for nonobviousness inquiries to focus more on economic factors or objective “secondary” criteria, such as long-felt but unsolved needs, the failure of others, and real-world evidence of how an invention was received in the marketplace.19 Inventive machines may provide the impetus for such a shift.
Nonobvious inquiries utilizing the Inventive Machine Standard might also focus on reproducibility, specifically whether standard machines could reproduce the subject matter of a patent application with sufficient ease. This could be a more objective and determinate test that would allow the Patent Office to apply a single standard consistently, and it would result in fewer judicially invalidated patents.20 A nonobviousness inquiry focused on either secondary
16. See FED. TRADE COMM’N, TO PROMOTE INNOVATION: THE PROPER BALANCE OF COMPETITION AND PATENT LAW AND POLICY 6–15 (2003) (critiquing Section 103 decisions).
17. Cadogan Estates Ltd. v. Morris [1998] EWCA Civ. 1671 at 17 (Eng.) (referring to “the well known elephant test. It is difficult to describe, but you know it when you see it”).
18. 378 U.S. 184, 197 (1964).
19. See, e.g., Michael Abramowicz & John F. Duffy, The Inducement Standard of Patentability,
120 YALE L.J. 1590, 1596 (2011) (arguing for an inducement standard); Tun-Jen Chiang, A Cost-Benefit Approach to Patent Obviousness, 82 ST. JOHN’S L. REV. 39, 42 (2008) (arguing that, “[a]n invention should receive a patent if the accrued benefits before independent invention outweigh the costs after independent invention”); Alan Devlin & Neel Sukhatme, Self-Realizing Inventions and the Utilitarian Foundation of Patent Law, 51 WM. & MARY L. REV. 897 (2009); John F. Duffy, A Timing Approach to Patentability, 12 LEWIS & CLARK L. REV. 343 (2008) (arguing for a timing approach to determining obviousness); Daralyn J. Durie & Mark A. Lemley, A Realistic Approach to the Obviousness of Inventions, 50 WM. & MARY L. REV. 989, 1004–07 (2008) (arguing for a greater reliance on secondary considerations); Gregory Mandel, The Non-Obvious Problem: How the Indeterminate Nonobviousness Standard Produces Excessive Patent Grants, 42 U.C. DAVIS L. REV 57, 62 (2008) [hereinafter Mandel, The Non-Obvious Problem] (arguing for nonobviousness to be based on “how probable the invention would have been for a person having ordinary skill in the art working on the problem that the invention solves”); Robert P. Merges, Uncertainty and the Standard of Patentability, 7 HIGH TECH. L.J. 1, 19 (1992) (arguing that patents should be issued for inventions which appeared unlikely to succeed in advance).
20. For decades, obviousness has been the most common issue in litigation to invalidate a patent, and the most common grounds for a finding of patent invalidity. See John R. Allison & Mark A. Lemley, Empirical Evidence on the Validity of Litigated Patents, 26 AIPLA Q.J. 185, 208–09 (1998); John R. Allison et al., Understanding the Realities of Modern Patent Litigation, 92 TEX. L. REV. 1769, 1782, 1785 (2014). As other commentators have noted, the bar here is low, and the new standard, “can be an administrative success if it is even just a bit
  
420
8 66 UCLA L. REV. 2 (2019)
factors or reproducibility may avoid some of the difficulties inherent in applying a “cognitive” inventive machine standard.
However the test is applied, the Inventive Machine Standard will dynamically raise the current benchmark for patentability. Inventive machines will be significantly more intelligent than skilled persons and also capable of considering more prior art. An Inventive Machine Standard would not prohibit patents, but it would make obtaining them substantially more difficult: A person or computer might need to have an unusual insight that other inventive machines could not easily recreate, developers might need to create increasingly intelligent computers that could outperform standard machines, or, most likely, invention will be dependent on specialized, non-public sources of data. The nonobviousness bar will continue to rise as machines inevitably become increasingly sophisticated. Taken to its logical extreme, and given there may be no limit to how intelligent computers will become, it may be that every invention will one day be obvious to commonly used computers. That would mean no more patents should be issued without some radical change to current patentability criteria.
This Article is structured in three parts. Part I considers the current test for obviousness and its historical evolution. It finds that obviousness is evaluated through the lens of the skilled person, who reflects the characteristics of the average worker in a field.21 The level of creativity and knowledge imputed to the skilledpersoniscriticalfortheobviousnessanalysis.22 Themorecapabletheskilled person, the more they will find obvious, and this will result in fewer issued patents.
Part II considers the use of artificial intelligence in research and development (R&D) and proposes a novel framework for conceptualizing the transition from human to machine inventors. Already, inventive machines are competing with human inventors, and human inventors are augmenting their
better than current doctrine as a helpful theoretical and pragmatic guide for applying the
obviousness doctrine.” Abramowicz & Duffy, supra note 19, at 1601.
21. See Ruiz v. A.B. Chance Co., 234 F.3d 654, 666 (Fed. Cir. 2000); see also Ryko Mfg. Co. v. Nu-Star, Inc., 950 F.2d 714, 718 (Fed. Cir. 1991) (“The importance of resolving the level of ordinary skill in the art lies in the necessity of maintaining objectivity in the obviousness inquiry.”). The Manual of Patent Examining Procedure (MPEP) provides guidance on the
level of ordinary skill in the art. MPEP § 2141.03.
22. DyStar Textilfarben GmbH & Co. Deutschland KG v. C.H. Patrick Co., 464 F.3d 1356, 1370
(Fed. Cir. 2006) (“If the level of skill is low, for example that of a mere dyer, as Dystar has suggested, then it may be rational to assume that such an artisan would not think to combine references absent explicit direction in a prior art reference.”). Though, in practice, few cases involve explicit factual determinations of the PHOSITA’s skill. Rebecca S. Eisenberg, Obvious to Whom? Evaluating Inventions From the Perspective of PHOSITA, 19 BERKELEY TECH. L.J. 885, 888 (2004). See infra Subpart I.D for a discussion of the PHOSITA standard.
  
421
Everything Is Obvious 9
abilities with inventive machines. In time, inventive machines or people using inventive machines will become the standard in a field, and eventually, machines will be responsible for most or all innovation. As this occurs, the skilled person standard must evolve if it is to continue to reflect real-world conditions. Failure to do this would “stifle, rather than promote, the progress of the useful arts.”23
Part II then proposes a framework for implementing a proposed Inventive Machine Standard. A decisionmaker would need to (1) determine the extent to which inventive machines are used in a field, (2) if inventive machines are the standard, characterize the inventive machine(s) that best represents the average worker, and (3) determine whether the machine(s) would find an invention obvious. The decisionmaker is a patent examiner in the first instance,24 and potentially a judge or jury if the validity of a patent is at issue in trial.25 In both instances, this new test would involve new challenges.
Finally, Part III provides examples of how the Inventive Machine Standard could work in practice, such as by focusing on reproducibility or secondary factors. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function as innovation incentives. To the extent patents accomplish other goals such as promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
Although this Article focuses on U.S. patent law, a similar framework exists in nearly every country. Member States of the World Trade Organization (WTO) are required to grant patents for inventions that “are new, involve an
23. KSR Int’l Co., 550 U.S. at 427.
24. At the Patent Office, applications are initially considered by a patent examiner, and
examiner decisions can be appealed to the Patent Trial and Appeal Board (PTAB). U.S. PATENT & TRADEMARK OFFICE, Patent Trial and Appeal Board, https://www.uspto.gov/ patents-application-process/patent-trial-and-appeal-board-0 [https://perma.cc/3W42-FHH2]. Also, the PTAB can adjudicate issues of patentability in certain proceedings such as inter partes review. Id.
25. Determinations of patent validity can involve mixed questions of law and fact. Generally, in civil litigation, legal questions are determined by judges, while factual questions are for a jury. See, e.g., Structural Rubber Prods. Co. v. Park Rubber Co., 749 F.2d 707, 713 (Fed. Cir. 1984) (“Litigants have the right to have a case tried in a manner which ensures that factual questions are determined by the jury and the decisions on legal issues are made by the court . . . .”). There are some exceptions to this rule. See, e.g., Gen. Electro Music Corp. v. Samick Music Corp., 19 F.3d 1405, 1408 (Fed. Cir. 1994) (“[I]ssues of fact underlying the issue of inequitable conduct are not jury questions, the issue being entirely equitable in nature.”). See also Mark A. Lemley, Why Do Juries Decide If Patents Are Valid? (Stanford Pub. Law, Working Paper No. 2306152, 2013), https://papers.ssrn.com/sol3/papers.cfm? abstract_id=2306152.
  
422
10 66 UCLA L. REV. 2 (2019)
inventive step and are capable of industrial application.”26 Although U.S. law uses the term “nonobvious” rather than “inventive step,” the criteria are substantively similar.27 For instance, the European Patent Office’s criteria for inventive step is similar to the U.S. criteria for obviousness, and also uses the theoretical device of the skilled person.28
I. OBVIOUSNESS
Part I investigates the current obviousness standard, its historical origins, and how the standard has changed over time. It finds that obviousness depends on the creativity of the skilled person, as well as the prior art they consider. These factors, in turn, vary according to the complexity of an invention and its field of art.
A. Public Policy
Patents are not intended to be granted for incremental inventions.29 Only inventions which represent a significant advance over existing technology should receive protection.30 That is because patents have significant costs: They limit competition, and they can inhibit future innovation by restricting the use
26. Agreement on Trade-Related Aspects of Intellectual Property Rights, art. 27, Apr. 15, 1994, 33 I.L.M. 1197, 1208 [hereinafter TRIPS]. See Ryan B. Abbott, et al., The Price of Medicines in Jordan: The Cost of Trade-Based Intellectual Property, 9 J. GENERIC MEDS. 75, 76 (2012).
27. TRIPS, supra note 26, at 1208 n.5. Although, there are some substantive differences in the way these criteria are implemented, and TRIPS provides nations with various flexibilities for compliance. See generally Ryan Abbott, Balancing Access and Innovation in India’s Shifting IP Regime, Remarks, 35 WHITTIER L. REV. 341 (2014) [hereinafter Balancing Access].
28. “An invention shall be considered as involving an inventive step if, having regard to the state of the art, it is not obvious to a person skilled in the art.” Convention on the Grant of European Patents art. 56, Oct. 5, 1973, 13 I.L.M 268. For guidance on the “skilled person” in European patent law, see Guidelines for Examinations, EUR. PAT. OFF., http://www.epo.org/law-practice/legal-texts/html/guidelines/e/g_vii_3.htm [https://perma.cc/XFY3-JD8J] (last visited Sept. 24, 2018).
29. The nonobviousness requirement is contained in Section 103 of the Patent Act: A patent for a claimed invention may not be obtained, notwithstanding that the claimed invention is not identically disclosed as set forth in section 102, if the differences between the claimed invention and the prior art are such that the claimed invention as a whole would have been obvious before the effective filing date of the claimed invention to a person having ordinary skill in the art to which the claimed invention pertains.
35 U.S.C. § 103 (2018).
30. Atlantic Works v. Brady, 107 U.S. 192, 200 (1883) (noting that “[t]o grant to a single party
monopoly of every slight advance made, except where the exercise of invention, somewhat above ordinary mechanical or engineering skill, is distinctly shown, is unjust in principle and injurious in its consequences”).
  
423
Everything Is Obvious 11
of patented technologies in research and development.31 To the extent that patents are justified, it is because they are thought to have more benefits than costs. Patents can function as innovation incentives, promote the dissemination of information, encourage commercialization of technology, and validate moral rights.32
Patents are granted for inventions that are new, nonobvious, and useful.33 Of these three criteria, obviousness is the primary hurdle for most patent applications.34 Although other patentability criteria contribute to this function, the nonobviousness requirement is the primary test for distinguishing between significant innovations and trivial advances.35 Of course, it is one thing to express a desire to only protect meaningful scientific advances, and another to come up with a workable rule that applies across every area of technology.
B. Early Attempts
The modern obviousness standard has been the culmination of hundreds of years of struggle by the Patent Office, courts, and Congress to separate the wheat from the chaff.36 As Thomas Jefferson, the first administrator of the U.S.
31. See I Think, supra note 1, at 1105–06 (discussing the costs and benefits of the patent system).
32. Id. at 1105–08. Congress’s power to grant patents is constitutional, and based on incentive theory: “To promote the progress of science . . . by securing for limited times to . . . inventors the exclusive right to their respective . . . discoveries.” U.S. CONST. art. I, § 8, cl. 8. See Mark A. Lemley, Ex Ante Versus Ex Post Justifications for Intellectual Property, 71 U. CHI. L. REV. 129, 129 (2004) (“The standard justification for intellectual property is ex ante . . . . It is the prospect of the intellectual property right that spurs creative incentives.”); see also United States v. Line Material Co., 333 U.S. 287, 316 (1948) (Douglas, J., concurring) (noting “the reward to inventors is wholly secondary” to the reward to society); THE FEDERALIST NO. 43 (James Madison) (stating that social benefit arises from patents to inventors). The U.S. Supreme Court has endorsed an economic inducement rationale in which patents should only be granted for inventions which would “not be disclosed or devised but for the inducement of a patent.” This is the inducement theory articulated in Graham v. John Deere
Co., 383 U.S. 1, 10 (1966). See also Abramowicz & Duffy, supra note 20.
33. 35 U.S.C. §§ 101–103, 112 (2018). In the European system, these criteria are referred to as novelty, inventive step, and industrial applicability. Art. 52 EPC. Inventions must also comprise patentable subject matter and be adequately disclosed. 35 U.S.C. §§ 101–103, 112
(2018).
34. DONALD CHISUM, CHISUM ON PATENTS § 5.02[6] (2007); NONOBVIOUSNESS—THE ULTIMATE
CONDITION OF PATENTABILITY 2:101 (J. Witherspoon ed., 1980). Obviousness is the most commonly litigated issue of patent validity. Allison & Lemley, supra note 20, at 208–09 (1998).
35. 35 U.S.C. §§ 101–102, 112 (2018).
36. For that matter, the struggle dates back to the very first patent law, the Venetian Act of 1474,
which stated that only “new and ingenious” inventions would be protected. See Giulio Mandich, Venetian Patents (1450–1550), 30 J. PAT. OFF. SOC’Y 166, 176–77 (1948); A. Samuel Oddi, Beyond Obviousness: Invention Protection in the Twenty-First Century, 38 AM.
  
424
12 66 UCLA L. REV. 2 (2019)
patent system and one of its chief architects, wrote, “I know well the difficulty of drawing a line between the things which are worth to the public the embarrassment of an exclusive patent, and those which are not . . . I saw with what slow progress a system of general rules could be matured.”37
The earliest patent laws focused on novelty and utility, although Jefferson didatonepointsuggestan“obviousness”requirement.38 ThePatentActof1790 was the first patent statute, and it required patentable inventions to be “sufficiently useful and important.”39 Three years later, a more comprehensive patent law was passed—the Patent Act of 1793.40 The new act did not require an invention to be “important,” but required it to be “new and useful.”41 The 1836 Patent Act reinstated the requirement that an invention be “sufficiently used and important.”42
In 1851, the Supreme Court adopted the progenitor of the skilled person and the obviousness test—an “invention” standard.43 Hotchkiss v. Greenwood
U. L. REV. 1097, 1102–03 (1989); Frank D. Prager, A History of Intellectual Property From
1545 to 1787, 26 J. PAT. OFF. SOC’Y 711, 715 (1944).
37. Letter to Isaac McPherson (Aug. 13, 1813), in 5 THE WRITINGS OF THOMAS JEFFERSON, 1790–
1826, 175, 181 (Riker, Thorne & Co. 1854) [hereinafter Letter to Isaac McPherson].
38. In 1791, Jefferson proposed amending the 1790 Patent Act to prohibit patents on an invention if it “is so unimportant and obvious that it ought not be the subject of an exclusive right.” 5 THE WRITINGS OF THOMAS JEFFERSON 278, 1788–1792, (Paul Leicester Ford ed.,
G.P. Putnam & Sons 1895).
39. Patent Act of 1790, ch. 7, 1 Stat. 109 (repealed 1793).
40. Patent Act of 1793, ch. 11, 1 Stat. 318 (repealed 1836).
41. Patent Act of 1793, ch. 11, 1 Stat. at 318–23. It also prohibited patents on certain minor
improvements: “[S]imply changing the form or the proportions of any machine, or compositions of matter, in any degree, shall not be deemed a discovery.” Id. at 321. On this basis, Jefferson, who was credited with drafting most of this statute, argued that “[a] change of material should not give title to a patent. As the making a ploughshare of cast rather than of wrought iron; a comb of iron, instead of horn or of ivory . . . .” Letter to Isaac McPherson, supra note 37, at 181.
42. Patent Act of 1836, ch. 357, § 18, 5 Stat. 117, 124 (repealed 1861).
43. See, e.g., Graham v. John Deere Co., 383 U.S. 1, 17 (1966) (“We conclude that [§ 103] was
intended merely as a codification of judicial precedents embracing the Hotchkiss condition, with congressional directions that inquiries into the obviousness of the subject matter sought to be patented are a prerequisite to patentability.”); see also S. REP. NO. 82-1979, at 6 (1952); H.R. REP. NO. 82-1923, at 7 (1952) (“Section 103 . . . provides a condition which exists in the law and has existed for more than 100 years.”). Obviousness had been at issue in earlier cases, although not necessarily in such terms. For instance, in Earle v. Sawyer, Justice Story rejected an argument by the defendant that the invention at issue was obvious, and that something more than novelty and utility was required for a patent. 8 F. Cas. 254, 255 (Cir. Ct. D. Mass. 1825). He argued a court was not required to engage in a “mode of reasoning upon the metaphysical nature, or the abstract definition of an invention.” Id. Justice Story further noted that English law permits the introducer of a foreign technology to receive a patent, and such an act could not require intellectual labor. Id. at 256. In Evans v. Eaton, the Supreme Court held that, a patent invention must involve a change in the “principle” of the machine rather than a change “merely in form and proportion.” 20 U.S.
  
425
Everything Is Obvious 13
concerned a patent for substituting clay or porcelain for a known door knob materialsuchasmetalorwood.44 TheCourtinvalidatedthepatent,holdingthat “the improvement is the work of a skillful mechanic, not that of the inventor.”45 The Court also articulated a new legal standard for patentability: “Unless more ingenuity and skill...were required...than were possessed by an ordinary mechanic acquainted with the business, there was an absence of that degree of skill and ingenuity which constitute essential elements of every invention.”46
However, the Court did not give specific guidance on what makes something inventive or the required level of inventiveness. In subsequent years, the Court made several efforts to address these deficiencies, but with limited success. As the Court stated in 1891, “[t]he truth is the word [invention] cannot be defined in such manner as to afford any substantial aid in determining whether any particular device involves an exercise of inventive faculty or not.”47 Or as one commentator noted, “it was almost impossible for one to say with any degree of certainty that a particular patent was indeed valid.”48
Around 1930, the Supreme Court, possibly influenced by a national antimonopoly sentiment, began implementing stricter criteria for determining the level of invention.49 This culminated in the widely disparaged “Flash of Genius” test articulated in Cuno Engineering v. Automatic Devices Corp.50 Namely, that in order to receive a patent, “the new device must reveal the flash of creative genius, not merely the skill of the calling.”51 This test was interpreted to mean that an invention must come into the mind of an inventor as a result of
(7 Wheat) 356, 361–62 (1822). Writing for the Court, Justice Story noted the patent was
invalid because it was “substantially the same in principle” as a prior invention. Id. at 362.
44. 52 U.S. 248, 265 (1850).
45. Id. at 267.
46. Id.
47. McClain v. Ortmayer, 141 U.S. 419, 427 (1891). Another court noted that “invention” is “as
fugitive, impalpable, wayward, and vague a phantom as exists in the paraphernalia of legal
concepts.” Harries v. Air King Prods. Co., 183 F.2d 158, 162 (2d Cir. 1950).
48. Gay Chin, The Statutory Standard of Invention: Section 103 of the 1952 Patent Act, 3 PAT.
TRADEMARK & COPY. J. RES. & EDUC. 317, 318 (1959).
49. See, e.g., Edward B. Gregg, Tracing the Concept of Patentable Invention, 13 VILL. L. REV. 98
(1967).
50. Cuno Eng’g Corp. v. Automatic Devices Corp., 314 U.S. 84, 91 (1941) (formalizing the
test). See, e.g., Hamilton Standard Propeller Co. v. Fay-Egan Mfg. Co., 101 F.2d 614, 617 (6th Cir. 1939) (“The patentee did not display any flash of genius, inspiration or imagination . . . .”). The Flash of Genius test was reaffirmed by the Court in 1950 in Great Atlantic & Pacific Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 154 (1950) (Douglas, J., concurring).
51. Cuno Eng’g Corp., 314 U.S. at 91.
  
426
14 66 UCLA L. REV. 2 (2019)
“inventive genius”52 rather than as a “result of long toil and experimentation.”53 The Court reasoned that “strict application of the test is necessary lest in the constant demand for new appliances the heavy hand of tribute be laid on each slight technological advance in the art.”54
The Flash of Genius test was criticized for being vague and difficult to implement, and for involving subjective decisions about an inventor’s state of mind.55 It certainly made it substantially more difficult to obtain a patent.56 Extensive criticism of perceived judicial hostility toward patents resulted in President Franklin D. Roosevelt’s creation of a National Patent Planning Commissiontomakerecommendationsforimprovingthepatentsystem.57 The
52. Reckendorfer v. Faber, 92 U.S. 347, 357 (1875).
53. The Supreme Court later claimed the “Flash of Creative Genius” language was just a
rhetorical embellishment, and that requirement concerned only the device itself, not the manner of invention. Graham v. John Deere Co., 383 U.S. 1, 15 n.7, 16 n.8 (1966). That was not, however, how the test was interpreted. See P.J. Federico, Origins of Section 103, 5 APLA Q.J. 87, 97 n.5 (1977) (noting the test led to a higher standard of invention in the lower courts). In Atlantic & Pacific Tea Co. v. Supermarket Equipment Corp., 340 U.S. 147 (1950), another case cited for the proposition that the Court had adopted stricter patentability criteria, the majority did not consider the question of inventiveness, but in his concurring opinion Justice Douglas reiterated the concept of “inventive genius”: “It is not enough that an article is new and useful. The Constitution never sanctioned the patenting of gadgets. Patents serve a higher end—the advancement of science. An invention need not be as startling as an atomic bomb to be patentable. But it has to be of such quality and distinction that that masters of the scientific field in which it falls will recognize it as an advance.” Id.
54. Cuno Eng’g Corp., 314 U.S. at 92.
55. As a commentator at the time noted, “the standard of patentable invention represented by
[the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.” Comment, The “Flash of Genius” Standard of Patentable Invention, 13 FORDHAM L. REV. 84, 87 (1944). See Note, Patent Law—”Flash of Genius” Test for Invention Rejected, 5 DEPAUL L. REV. 144, 146 (1955); Stephen G. Kalinchak, Obviousness and the Doctrine of Equivalents in Patent Law: Striving for Objective Criteria, 43 CATH. U. L. REV. 577, 586 (1994); see also, Note, The Standard of Patentability—Judicial Interpretation of Section 103 of the Patent Act Source, 63 COLUM. L. REV. 306, 306 (1963) [hereinafter The Standard of Patentability] (criticizing the standard).
56. Supreme Court Justice Robert Jackson noted in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.” Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
57. See William Jarratt, U.S. National Patent Planning Commission, 153 NATURE 12 (1944); see also REPORT OF THE NATIONAL PATENT PLANNING COMMISSION, NATIONAL PATENT PLANNING COMMISSION, at 6, 10 (1943).
  
427
Everything Is Obvious 15
Commission’s report recommended that Congress adopt a more objective and certain standard of obviousness.58 A decade later, Congress did.59
C. The Nonobviousness Inquiry
The Patent Act of 1952 established the modern patentability framework.60 Among other changes to substantive patent law,61 “the central thrust of the 1952 Act removed ‘unmeasurable’ inquiries into ‘inventiveness’ and instead supplied the nonobviousness requirement of Section 103.”62 Section 103 states:
A patent may not be obtained . . . if the difference between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains. Patentability shall not be negatived by the manner in which the invention was made.63
58. REPORT OF THE NATIONAL PATENT PLANNING COMMISSION, supra note 57,at 5–6. “One of the greatest technical weaknesses of the patent system is the lack of a definitive yardstick as to what is invention.” Id. at 26. “The most serious weakness of the present patent system is the lack of a uniform test or standard for determining whether the particular contribution of an inventor merits the award of the patent grant.” Id. at 14. “It is proposed that Congress shall declare a national standard whereby patentability of an invention shall be determined by the objective test as to its advancement of the arts and sciences.” Id. at 26.
59. Though, Congress may not have realized what it was doing. See George M. Sirilla, 35 U.S.C. § 103: From Hotchkiss to Hand to Rich, the Obvious Patent Law Hall-of-Famers, 32 J. MARSHALL L. REV. 437, 509–14 (1999) (discussing the legislative history of the Patent Act of 1952 and the lack of congressional awareness of, and intent for, Section 103).
60. See The Standard of Patentability, supra note 55, at 309. “[P]robably no other title incorporates the thinking of so many qualified technical men throughout the country as does this revision.” L. James Harris, Some Aspects of the Underlying Legislative Intent of the Patent Act of 1952, 23 GEO. WASH. L. REV. 658, 661 (1955).
61. “The major changes or innovations in the title consist of incorporating a requirement for invention in § 103 and the judicial doctrine of contributory infringement in § 271.” H.R. REP. NO. 1923, 82d Cong., 2d Sess. 5 (1952); S. REP. NO. 1979, 82d Cong., 2d Sess. 4 (1952).
62. CLS Bank Int’l v. Alice Corp. Pty. Ltd., 717 F.3d 1269, 1296 (Fed. Cir. 2013) (Rader, J., dissenting in part, concurring in part) (citing P.J. Federidco’s Commentary on the New Patent Act, reprinted in 75 J. PAT. & TRADEMARK OFFICE SOC’Y 161, 177 (1993)). See also Dann v. Johnston, 425 U.S. 219, 225–26 (1976) (describing the shift from “an exercise of the inventive faculty” established in case law to a statutory test and stating that “it was only in 1952 that Congress, in the interest of uniformity and definiteness, articulated the requirement in a statute, framing it as a requirement of ‘nonobviousness’” (internal quotation marks and footnote omitted)). The official “Revision Notes” state § 103 is meant to be the basis for “holding . . . patents invalid by the courts[] on the ground of lack of invention.” S.REP.NO.82-1979,at18.
63. 35 U.S.C. § 103, as amended by the America Invents Act. Leahy-Smith America Invents Act, Pub. L. No. 112-29, 125 Stat. 284, 286 (2011) (codified at 35 U.S.C. § 103 (2018)). The America Invents Act did not fundamentally change the nonobviousness inquiry but did
  
428
16 66 UCLA L. REV. 2 (2019)
Section 103 legislatively disavowed the Flash of Genius test, codified the sprawling judicial doctrine on “invention” into a single statutory test, and restructured the standard of obviousness in relation to a person having ordinary skill in the art.64 However, while Section 103 may be more objective and definite than the Flash of Genius test, the meanings of “obvious” and “a person having ordinary skill” were not defined, and in practice also proved “often difficult to apply.”65
The Supreme Court first interpreted the statutory nonobviousness requirement in a trilogy of cases: Graham v. John Deere (1966) and its companion cases, Calmar v. Cook Chemical (1965) and United States v. Adams (1966).66 In these cases, the Court articulated a framework for evaluating obviousness as a question of law based on the following underlying factual inquiries: (1) the scope and content of the prior art, (2) the level of ordinary skill in the prior art, (3) the differences between the claimed invention and the prior art, and (4) objective evidence of nonobviousness.67 This framework remains applicable today. Of note, the Graham analysis does not explain how to evaluate the ultimate legal question of nonobviousness, beyond identifying underlying factual considerations.68
In 1984, the newly established United States Court of Appeals for the Federal Circuit, the only appellate-level court with jurisdiction to hear patent case appeals, devised the “teaching, suggestion, and motivation” (TSM) test for obviousness.69 Strictly applied, this test only permits an obviousness rejection when prior art explicitly teaches, suggests or motivates a combination of existing
result in some modest changes. https://www.uspto.gov/web/offices/pac/mpep/s2158.html
[https://perma.cc/TAQ7-KMCC].
64. See Giles S. Rich, Principles of Patentability, 28 GEO. WASH. U. L. REV. 393, 393–407 (1960);
see also Chin, supra note 48, at 318. In Graham, the Supreme Court noted that “[i]t . . . seems apparent that Congress intended by the last sentence of § 103 to abolish the test it believed this Court announced in the controversial phrase ‘flash of creative genius,’ used in Cuno Engineering.” Graham, 383 U.S. at 15.
65. Uniroyal, Inc. v. Rudkin-Wiley Corp., 837 F.2d 1044, 1050 (Fed. Cir. 1988) (noting the obviousness standard is easy to expound and “often difficult to apply”).
66. Graham v. John Deere Co., 383 U.S. 1 (1966); United States v. Adams, 383 U.S. 39, 51–52 (1966); Calmar v. Cook Chem., 380 U.S. 949 (1965).
67. Graham, 383 U.S. at 17. With regards to the fourth category, considerations such as commercial success and long felt but unsolved needs can serve as evidence of nonobviousness in certain circumstances. Id.
68. See Joseph Miller, Nonobviousness: Looking Back and Looking Ahead, in 2 INTELLECTUAL PROPERTY AND INFORMATION WEALTH: ISSUES AND PRACTICES IN THE DIGITAL AGE: PATENTS AND TRADE SECRETS 9 (Peter K. Yu ed., 2007) (“[T]he Court did not indicate . . . how one was to go about determining obviousness (or not).”).
69. Court Jurisdiction, U.S. CT. APPEALS FOR FED. CIR., http://www.cafc.uscourts.gov/the- court/court-jurisdiction [https://perma.cc/TE4D-GRF2].
  
429
Everything Is Obvious 17
elements into a new invention.70 The TSM test protects against hindsight bias because it requires an objective finding in the prior art. In retrospect, it is easy for an invention to appear obvious by piecing together bits of prior art using the invention as a blueprint.71
In KSR v. Teleflex (2006), the Supreme Court upheld the Graham analysis but rejected the Federal Circuit’s exclusive reliance on the TSM test. The Court instead endorsed a flexible approach to obviousness in light of “[t]he diversity of inventivepursuitsandofmoderntechnology.”72 Ratherthanapprovingasingle definitive test, the Court identified a nonexhaustive list of rationales to support a finding of obviousness.73 This remains the approach to obviousness today.
D. Finding PHOSITA
Determining the level of ordinary skill is critical to assessing obviousness.74 The more sophisticated the person having ordinary skill in the art (PHOSITA, or the skilled person), the more likely a new invention is to appear obvious.
70. ACS Hosp. Sys., Inc. v. Montefiore Hosp., 732 F.2d 1572 (Fed. Cir. 1984).
71. See In re Fritch, 972 F.2d 1260, 1266 (Fed. Cir. 1992).
72. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007). “[An obviousness] analysis need not
seek out precise teachings directed to the specific subject matter of the challenged claim, for a court can take account of the inferences and creative steps that a [PHOSITA] would employ.” Id. at 418.
73. These post-KSR rationales include:
(A) Combining prior art elements according to known methods to yield predictable results; (B) Simple substitution of one known element for another to obtain predictable results; (C) Use of known technique to improve similar devices (methods, or products) in the same way; (D) Applying a known technique to a known device (method, or product) ready for improvement to yield predictable results; (E) ‘Obvious to try’—choosing from a finite number of identified, predictable solutions, with a reasonable expectation of success; (F) Known work in one field of endeavor may prompt variations of it for use in either the same field or a different one based on design incentives or other market forces if the variations are predictable to one of ordinary skill in the art; (G) Some teaching, suggestion, or motivation in the prior art that would have led one of ordinary skill to modify the prior art reference or to combine prior art reference teachings to arrive at the claimed invention.
2141 Examination Guidelines for Determining Obviousness Under 35 U.S.C. 103 [R- 08.2017], U.S. PAT. & TRADEMARK OFF., https://www.uspto.gov/web/offices/pac/mpep/ s2141.html [http://perma.cc/EE7P-4CQ9] [hereinafter 2141 Examination Guidelines].
74. Ruiz v. A.B. Chance Co., 234 F.3d 654, 666 (Fed. Cir. 2000); see also Ryko Mfg. Co., v. Nu- Star, Inc.,950 F.2d 714 718 (Fed. Cir. 1991) (“The importance of resolving the level of ordinary skill in the art lies in the necessity of maintaining objectivity in the obviousness inquiry.”). The skilled person is relevant to many areas of patent law, including claim construction, best mode, definiteness, enablement, and the doctrine of equivalents. See Dan L. Burk & Mark A. Lemley, Is Patent Law Technology-Specific?, 17 BERKELEY TECH. L.J. 1155, 1186–87 (2002).
  
430
18 66 UCLA L. REV. 2 (2019)
Thus, it matters a great deal whether the skilled person is a “moron in a hurry”75 or the combined “masters of the scientific field in which an [invention] falls.”76
The skilled person has never been precisely defined, although judicial guidance exists.77 In KSR, the Supreme Court described the skilled person as “a person of ordinary creativity, not an automaton.”78 The Federal Circuit has explained the skilled person is a hypothetical person, like the reasonable person in tort law,79 who is presumed to have known the relevant art at the time of the invention.80 The skilled person is not a judge, amateur, person skilled in remote arts, or a set of “geniuses in the art at hand.”81 The skilled person is “one who thinks along the line of conventional wisdom in the art and is not one who undertakes to innovate.”82
The Federal Circuit has provided a nonexhaustive list of factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.”83 In any particular case, one or more factors may predominate, and not every factor may be relevant.84 The
75. Morning Star Coop. Soc’y v. Express Newspapers Ltd. [1979] FSR 113 (marking the first use of the term “moron in a hurry” as a standard for trademark confusion).
76. Great Atl. & Pac. Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 155 (1950).
77. See James B. Gambrell & John H. Dodge, II, Ordinary Skill in the Art—An Enemy of the Inventor or a Friend of the People?, in NONOBVIOUSNESS—THE ULTIMATE CONDITION OF PATENTABILITY 5:302 (John F. Witherspoon ed., 1980) (“[T]he Supreme Court in particular, but other courts as well, has done precious little to define the person of ordinary skill in the
art.”).
78. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 421 (2007). The MPEP provides guidance on the
level of ordinary skill in the art. MPEP § 2141.03. See John F. Duffy & Robert P. Merges, The Story of Graham v. John Deere Company: Patent Law’s Evolving Standard of Creativity, in INTELLECTUAL PROPERTY STORIES 110 (Jane C. Ginsburg & Rochelle Cooper Dreyfuss eds., 2006) (noting that determining the appropriate level of ordinary skill for the nonobviousness standard “is one of the most important policy issues in all of patent law”).
79. See, e.g., Panduit Corp. v. Dennison Mfg. Co., 810 F.2d 1561, 1566 (Fed. Cir. 1987) (“[T]he decision maker confronts a ghost, i.e., ‘a person having ordinary skill in the art,’ not unlike the ‘reasonable man’ and other ghosts in the law.”).
80. 2141 Examination Guidelines, supra note 73.
81. Envtl. Designs Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 697 (Fed. Cir. 1983).
82. Standard Oil Co. v. Am. Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985).
83. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
84. Id.; Custom Accessories, Inc. v. Jeffrey-Allan Indus., Inc., 807 F.2d 955, 962–63 (Fed. Cir.
1986). Previously, this list of factors included the “educational level of the inventor.” Envtl. Designs, Ltd.,713 F.2d at 696. That was until the Federal Circuit announced that, “courts never have judged patentability by what the real inventor/applicant/patentee could or would do.” Kimberly-Clark Corp. v. Johnson & Johnson, 745 F.2d 1437, 1454 (Fed. Cir. 1984). Instead, “[r]eal inventors, as a class, vary in the capacities from ignorant geniuses to Nobel
  
431
Everything Is Obvious 19
skilled person standard thus varies according to the invention in question, its field of art, and researchers in the field.85 In the case of a simple invention in a field where most innovation is created by laypersons, such as, for instance, a device to keep flies away from horses, the skilled person may be someone with little education or practical experience.86 By contrast, where an invention is in a complex field with highly educated workers such as chemical engineering or pharmaceuticalresearch,theskilledpersonmaybequitesophisticated.87 Atleast in Europe, the skilled person may even be a team of individuals where collaborative approaches to research are the norm.88
laureates; the courts have always applied a standard based on an imaginary work of their
own devising whom they have equated with the inventor.” Id.
85. See, e.g., DyStar Textilfarben GmbH & Co. Deutschland KG, 464 F.3d 1356, 1370 (Fed. Cir.
2006). The court writes:
If the level of skill is low, for example that of a mere dyer, as Dystar has suggested, then it may be rational to assume that such an artisan would not think to combine references absent explicit direction in a prior art reference. . . . [If] the level of skill is that of a dyeing process designer, then one can assume comfortably that such an artisan will draw ideas from chemistry and systems engineering—without being told to do so.
Daiichi Sankyo Co. v. Apotex, Inc. concerned a patent for treating ear infections by applying an antibiotic to the ear. 501 F.3d 1254, 1257 (Fed. Cir. 2007). The district court found that the skilled person “would have a medical degree, experience treating patients with ear infections, and knowledge of the pharmacology and use of antibiotics.” Id. “This person would be . . . a pediatrician or general practitioner—those doctors who are often the ‘first line of defense’ in treating ear infections and who, by virtue of their medical training, possess basic pharmacological knowledge.” Id. The Federal Circuit overturned this finding, holding that rather, a person of ordinary skill in the art was “a person engaged in developing new pharmaceuticals, formulations and treatment methods, or a specialist in ear treatments such as an otologist, otolaryngologist, or otorhinolaryngologist who also has training in pharmaceutical formulations.” Id. Courts have employed a flexible approach to considering informal education. See, e.g., Penda Corp. v. United States., 29 Fed. Cl. 533, 565 (1993). For instance, in Bose Corp. v. JBL, Inc., the District Court found that keeping “up with current literature and trade magazines to keep abreast of new developments” could be the equivalent of “a bachelor of science degree in electrical engineering, physics, mechanical engineering, or possibly acoustics.” 112 F. Supp. 2d 138, 155 (D. Mass. 2000).
86. See Graham v. Gun-Munro, No. C-99-04064 CRB, 2001 U.S. Dist. LEXIS 7110, at *19 (N.D. Cal. May 22, 2001) (holding that the skilled person had some formal education but no special training in the field of art in a case regarding fly wraps for the legs of horses).
87. See Imperial Chem. Indus., PLC v. Danbury Pharmacal, Inc., 777 F. Supp. 330, 371–72 (D. Del. 1991) (holding that the skilled person in the chemical industry is an organic chemist with a PhD); see also Envtl. Designs, Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 697 (Fed. Cir. 1983) (noting the respective chemical expert witnesses of the parties with extensive backgrounds in sulfur chemistry were skilled persons).
88. Guidelines for Examination, EUR. PAT. OFF., http://www.epo.org/law-practice/legal- texts/html/guidelines/e/g_vii_3.htm [https://perma.cc/XFY3-JD8J] (“There may be instances where it is more appropriate to think in terms of a group of persons, e.g. a research or production team, rather than a single person.”). See, e.g., MedImmune v. Novartis Pharm. U.K., Ltd., [2012] EWCA Civ. 1234 (evaluating obviousness from the perspective of
  
432
20 66 UCLA L. REV. 2 (2019)
E. Analogous Prior Art
Determining what constitutes prior art is also central to the obviousness inquiry.89 On some level, virtually all inventions involve a combination of known elements.90 The more prior art can be considered, the more likely an invention is to appear obvious. To be considered for the purposes of obviousness, prior art must fall within the definition for anticipatory references under Section 102 and must additionally qualify as “analogous art.”91
Section 102 contains the requirement for novelty in an invention, and it explicitly defines prior art.92 An extraordinarily broad amount of information qualifies as prior art, including any printed publication made available to the publicpriortofilingapatentapplication.93 Courtshavelongheldthatinventors arechargedwithconstructiveknowledgeofallpriorart.94 Whilenorealinventor could have such knowledge,95 the social benefits of this rule are thought to outweigh its costs.96 Granting patents on existing inventions could prevent the
a “skilled team”). The “[P]atent is addressed to a team of scientists with differing backgrounds in areas such as immunology, in particular antibody structural biology, molecular biology and protein chemistry, but with a common interest in antibody engineering.” Id. In the United States, the idea that the skilled person could be a group of individuals has been discussed in academic literature, but may not have been explicitly adopted by the courts. See, e.g., Jonathan J. Darrow. The Neglected Dimension of Patent Law’s PHOSITA Standard, 23 HARV. J.L. & TECH. 227, 244, 257 (2009). A “skilled persons” standard would seem to be appropriate given that most patents are now filed with more than one inventor. Dennis Crouch, PHOSITA: Not a Person—People Having Ordinary Skill in the Art, PATENTLY-O (June 7, 2018), https://patentlyo.com/patent/2018/06/phosita-not-a- person-people-having-ordinary-skill-in-the-art.html [https://perma.cc/UAK2-5NT8] (noting that most patents have multiple inventors).
89. This is the second inquiry of the Graham analysis described earlier.
90. See, e.g., Ryko Mfg. Co. v. Nu-Star, Inc., 950 F.2d 714, 718 (Fed. Cir. 1991).
91. In re Bigio, 381 F.3d 1320, 1325 (Fed. Cir. 2004).
92. 35 U.S.C. § 102 (2018).
93. Id. § 102(a)(1); see MPEP § 2152 for a detailed discussion of what constitutes prior art.
Almost anything in writing is prior art. “A U.S. patent on the lost wax casting technique was invalidated on the basis of Benvenuto Cellini’s 16th century autobiography which makes mention of a similar technique.” See Michael Ebert, Superperson and the Prior Art, 67 J. PAT. & TRADEMARK OFF. SOC’Y 657, 658 (1985).
94. In Mast, Foos, & Co. v. Stover Manufacturing Co., the Supreme Court applied a presumption that the skilled person is charged with constructive knowledge of all prior art: “Having all these various devices before him, and whatever the facts may have been, he is chargeable with a knowledge of all preexisting devices.” 177 U.S. 485, 493 (1900) (emphasis added) (further, “we must presume the patentee was fully informed of everything which preceded him, whether such were the actual fact or not”).
95. See, e.g., In re Wood, 599 F.2d 1032, 1036 (C.C.P.A. 1979) (“[A]n inventor could not possibly be aware of every teaching in every art.”).
96. See Bonito Boats, Inc. v. Thunder Craft Boats, Inc., 489 U.S. 141, 147–48 (1989) (reciting that Thomas Jefferson, the “driving force behind early federal patent policy,” believed that
  
433
Everything Is Obvious 21
public from using something it already had access to, and remove knowledge from the public domain.97
For the purposes of obviousness, prior art under Section 102 must also qualify as analogous. That is to say, the prior art must be in the field of an applicant’s endeavor, or reasonably pertinent to the problem with which the applicant was concerned.98 A real inventor would be expected to focus on this type of information. The “analogous art” rule better reflects practical conditions, and it ameliorates the harshness of the definition of prior art for novelty given that prior art references may be combined for purposes of obviousness but not novelty.99 Consequently, for the purposes of obviousness, the skilled person is presumed to have knowledge of all prior art within the field of an invention, as well as prior art reasonably pertinent to the problem the invention solves. Restricting the universe of prior art to analogous art lowers the bar to patentability.100
“a grant of patent rights in an idea already disclosed to the public [i]s akin to an ex post facto law, ‘obstruct[ing] others in the use of what they possessed before’” (quoting Letter to Isaac McPherson, supra note 37, at 176)); Graham v. John Deere Co., 383 U.S. 1, 5–6 (1966) (stating that granting patents on non-novel inventions would remove knowledge from the public domain).
97. Graham, 383 U.S. at 5–6.
98. See, e.g., Wyers v. Master Lock Co., 616 F.3d 1231, 1237 (Fed. Cir. 2010) (“Two criteria are
relevant in determining whether prior art is analogous: ‘(1) whether the art is from the same field of endeavor, regardless of the problem addressed, and (2) if the reference is not within the field of the inventor’s endeavor, whether the reference still is reasonably pertinent to the particular problem with which the inventor is involved.’” (quoting Comaper Corp. v. Antec, Inc., 596 F.3d 1343, 1351 (Fed. Cir. 2010)). “Under the correct analysis, any need or problem known in the field of endeavor at the time of the invention and addressed by the patent [or application at issue] can provide a reason for combining the elements in the manner claimed.” KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 420 (2007). Prior art in other fields may sometimes be considered as well. Id. at 417. The general question is whether it would have been “reasonable” for the skilled person to consider a piece of prior art to solve their problem. In re Clay, 966 F.2d 656 (Fed. Cir. 1992). To be “reasonably pertinent,” prior art must “logically [] have commended itself to an inventor’s attention in considering his problem.” Id.
99. See In re Wood, 599 F.2d 1032, 1036 (C.C.P.A. 1979) (“The rationale behind this rule precluding rejections based on combination of teachings of references from nonanalogous arts is the realization that an inventor could not possibly be aware of every teaching in every art.”). The rule “attempt[s] to more closely approximate the reality of the circumstances surrounding the making of an invention by only presuming knowledge by the inventor of prior art in the field of his endeavor and in analogous arts.” Id.
100. See Margo A. Bagley, Internet Business Model Patents: Obvious by Analogy, 7 MICH. TELECOMM. & TECH. L. REV. 253, 270 (2001) (arguing that prior to the analogous arts test references were rarely excluded as prior art); see also Jacob S. Sherkow, Negativing Invention, 2011 BYU L. REV. 1091, 1094–95 (2011) (noting that once a relevant piece of prior art is classified as analogous, an obviousness finding is often inevitable).
  
434
22 66 UCLA L. REV. 2 (2019)
The analogous art requirement was most famously conceptualized in the case of In re Winslow, in which the court explained a decisionmaker was to “picture the inventor as working in his shop with the prior art references—which he is presumed to know—hanging on the walls around him.”101 Or, as Judge Learned Hand presciently remarked, “the inventor must accept the position of a mythically omniscient worker in his chosen field. As the arts proliferate with prodigious fecundity, his lot is an increasingly hard one.”102
II. MACHINE INTELLIGENCE IN THE INVENTIVE PROCESS A. Automating and Augmenting Research
Artificial intelligence (AI), which is to say a computer able to perform tasks normally requiring human intelligence, is playing an increasingly important role in innovation.103 For instance, IBM’s flagship AI system “Watson” is being used exploratively to conduct research in drug discovery, as well as clinically to analyze the genes of cancer patients and develop treatment plans.104 In drug discovery, Watson has already identified novel drug targets and new indications for existing drugs.105 In doing so, Watson may be generating patentable inventions either autonomously or collaboratively with human researchers.106 Inclinicalpractice,Watsonisalsoautomatingaoncehumanfunction.107 Infact, according to IBM, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, a task which otherwise requires
101. In re Winslow, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
102. Merit Mfg. Co. v. Hero Mfg. Co., 185 F.2d 350, 352 (2d Cir. 1950).
103. See, e.g., DATA SCI. ASS’N, OUTLOOK ON ARTIFICIAL INTELLIGENCE IN THE ENTERPRISE 3, 6
(2016), http://www.datascienceassn.org/sites/default/files/Outlook%20on%20Artificial% 20Intelligence%20in%20the%20Enterprise%202016.pdf [hereinafter Outlook on AI] (a survey of 235 business executives conducted by the National Business Research Institute (NBRI) which found that 38 percent of enterprises were using AI technologies in 2016, and 62 percent will likely use AI technologies by 2018).
104. IBM Watson for Drug Discovery, IBM, https://www.ibm.com/watson/health/life- sciences/drug-discovery [https://perma.cc/DQ4D-ZKJF]; IBM Watson for Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-genomics/genomics [https://perma.cc/8XK7-S8DN].
105. Ying Chen et al., IBM Watson: How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research, 38 CLINICAL THERAPEUTICS 688 (2016), https://www.medicalaffairs.org/app/uploads/2018/02/Chen_2016_IBM_Watson.pdf.
106. See generally Hal the Inventor, supra note 3 (discussing the “hypothetical” example of an AI system being used in drug discovery to identify new drug targets and indications for existing drugs).
107. Kazimierz O. Wrzeszczynski et al., Comparing Sequencing Assays and Human-Machine Analyses in Actionable Genomics for Glioblastoma, 3 NEUROLOGY GENETICS e164 (2017), http://ng.neurology.org/content/3/4/e164 [https://perma.cc/3LGH-TKPW].
  
435
Everything Is Obvious 23
around 160 hours of work by a team of experts.108 A recent study by IBM found that Watson’s report outperformed the standard practice.109
Watson is largely structured as an “expert system,” although Watson is not a single program or computer—the brand incorporates a variety of technologies.110 Here, Watson will be considered a single software program in the interests of simplicity. Expert systems are one way of designing AI that solve problems in a specific domain of knowledge using logical rules derived from the knowledge of experts. These were a major focus of AI research in the 1980s.111 Expert system-based chess-playing programs HiTech and Deep Thought defeated chess masters in 1989, paving the way for another famous IBM computer, Deep Blue, to defeat world chess champion Garry Kasparov in 1997.112 But Deep Blue had limited utility—it was solely designed to play chess. The machine was permanently retired after defeating Kasparov.113
Google’s leading AI system DeepMind is an example of another sort of inventive machine. DeepMind uses an artificial neural network, which essentially consists of many highly interconnected processing elements working together to solve specific problems.114 The design of neural networks is inspired by the way the human brain processes information.115 Like the human brain, neuralnetworkscanlearnbyexampleandfrompractice.116 Examplesforneural networks come in the form of data, so more data means improved performance.117 Thishasledtodatabeingdescribedasthenewoilofthetwenty- firstcentury,andthefuelformachinelearning.118 Developersmaynotbeableto
108. Id.
109. Id.
110. See Richard Waters, Artificial Intelligence: Can Watson Save IBM?, FINANCIAL TIMES (Jan. 5,
2016), https://www.ft.com/content/dced8150-b300-11e5-8358-9a82b43f6b2f [https://perma.cc/ J3N6-QMP3]; see also Will Knight, IBM’s Watson Is Everywhere—But What Is It?, MIT TECH. REV, (Oct. 27, 2016), https://www.technologyreview.com/s/602744/ibms-watson-is- everywhere-but-what-is-it [http://perma.cc/YK3Q-HRQB].
111. STUART J. RUSSELL & PETER NORVIG, ARTIFICIAL INTELLIGENCE: A MODERN APPROACH 22–23 (2d ed. 2002) (1995).
112. IBM’s 100 Icons of Progress: Deep Blue, IBM http://www-03.ibm.com/ibm/history/ibm100/ us/en/icons/deepblue/words [https://perma.cc/7SG3-UYST].
113. Id.
114. KEVIN GURNEY, AN INTRODUCTION TO NEURAL NETWORKS 1–4 (1997). The first neural
network was built in 1951. See, e.g., RUSSELL & NORVIG, supra note 111.
115. See, e.g., Volodymyr Mnih et al., Human-Level Control Through Deep Reinforcement
Learning, 518 NATURE 529, 529–33 (2015).
116. See GURNEY, supra note 114, at 1–4.
117. PEDRO DOMINGOS, THE MASTER ALGORITHM: HOW THE QUEST FOR THE ULTIMATE LEARNING
MACHINE WILL REMAKE OUR WORLD xi (2015).
118. See, e.g., Michael Palmer, Data Is the New Oil, ANA MARKETING MAESTROS (Nov. 3, 2006).
  
436
24 66 UCLA L. REV. 2 (2019)
understand exactly how a neural network processes data or generates a particular output.
In 2016, DeepMind developed an algorithm known as AlphaGo which beat a world champion of the traditional Chinese board game Go, and then the world’sleadingplayerin2017.119 Gowasthelasttraditionalboardgameatwhich people had been able to outperform machines.120 AlphaGo’s feat was widely lauded in the artificial intelligence community because Go is exponentially more complicatedthanchess.121 Currentcomputerscannot“solve”Gosolelybyusing “brute force” computation to determine the optimal move to any potential configurationinadvance.122 TherearemorepossibleboardconfigurationsinGo than there are atoms in the universe.123 Rather than being preprogrammed with a number of optimal Go moves, DeepMind used a general-purpose algorithm to interpret the game’s patterns.124 DeepMind is now working to beat human players at the popular video game StarCraft II.125
AI like DeepMind is proving itself and training by playing games, but similar techniques can be applied to other challenges requiring recognition of complex patterns, long-term planning, and decisionmaking.126 DeepMind is already being applied to solve practical problems. For instance, it has helped decrease cooling costs at company datacenters.127 DeepMind is working to
119. David Silver et al., Mastering the Game of Go With Deep Neural Networks and Tree Search, 529 NATURE 484, 484–89 (2016). In 2015, DeepMind attained “human-level performance in video games” playing a series of class Atari 2600 games. Mnih et al., supra note 115, at 529. See also, Cade Metz, https://www.wired.com/2017/05/googles-alphago-continues- dominance-second-win-china [https://perma.cc/WA9G-JUGK].
120. See Richard Haridy, 2017: The Year AI Beat Us at All Our Own Games, NEW ATLAS (Dec. 26. 2017), https://newatlas.com/ai-2017-beating-humans-games/52741 [https://perma.cc/ AH2Y-6FFD].
121. Silver et al, supra note 119.
122. Id.; cf. Cade Metz, One Genius’ Lonely Crusade to Teach a Computer Common Sense, WIRED
(Mar. 24, 2016), [hereinafter Lonely Crusade] https://www.wired.com/2016/03/ doug- lenat-artificial-intelligence-common-sense-engine [https://perma.cc/WN2G-5CU9] (arguing that brute force computation was part of AlphaGo’s functionality).
123. 10170, or thereabouts. Silver et al, supra note 119.
124. Silver et al, supra note 119.
125. Tom Simonite, Google’s AI Declares Galactic War on StarCraft, WIRED (Aug. 9, 2017),
https://www.wired.com/story/googles-ai-declares-galactic-war-on-starcraft- [http://perma.cc/3VZJ-XXJV]. Compared with Go, StarCraft is vastly more complex. It involves high levels of strategic thinking and acting with imperfect information. Id.
126. Game playing has long been a proving ground for AI, as far back as what may have been the very first AI program in 1951. See Jack Copeland, A Brief History of Computing, ALANTURING.NET (June 2000) http://www.alanturing.net/turing_archive/pages/Reference% 20Articles/BriefHistofComp.html [https://perma.cc/82JN-UC93]. That program played checkers and was competitive with amateurs. Id.
127. See Simonite, supra note 125.
  
437
Everything Is Obvious 25
develop an algorithm to distinguish between healthy and cancerous tissues, and to evaluate eye scans to identify early signs of diseases leading to blindness.128 The results of this research may well be patentable.
Ultimately, the developers of DeepMind hope to create Artificial General Intelligence (AGI).129 Existing, “narrow” or specific AI (SAI) systems focus on discrete problems or work in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms. Nor can Chef Watson run a kitchen. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve.130 By contrast, AGI would be able to successfully perform any intellectual task a person could.
AGI could even be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence—what philosopher Nick Bostrom has termed Artificial SuperIntelligence (ASI).131 Such an outcome has been referred to as the intelligence explosion or the technological singularity.132 ASI could then innovate in all areas of technology, resulting in progress at an incomprehensible rate. As the mathematician Irving John Good wrote in 1965, “the first ultraintelligent machine is the last invention that man need ever make.”133
128. Chris Baraniuk, Google’s DeepMind to Peek at NHS Eye Scans for Disease Analysis, BBC (July 5, 2016), https://www.bbc.com/news/technology-36713308 [https://perma.cc/ WA6R- RUX3]; Chris Baraniuk, Google DeepMind Targets NHS Head and Neck Cancer Treatment, BBC (Aug. 31, 2016), https://www.bbc.com/news/technology-37230806 [http://perma.cc/6GAN-7EAZ].
129. Solving Intelligence Through Research, DEEPMIND, https://deepmind.com/research [https://perma.cc/7TC2-49B8].
130. See, e.g., Lonely Crusade, supra note 122.
131. See generally NICK BOSTROM, SUPERINTELLIGENCE: PATHS, DANGERS, STRATEGIES (2014).
132. See generally RAY KURZWEIL, THE SINGULARITY IS NEAR: WHEN HUMANS TRANSCEND
BIOLOGY (2005).
133. Irving John Good, Speculations Concerning the First Ultraintelligent Machine, 6 ADVANCES
IN COMPUTERS 31, 33 (1965)
Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind.... Thusthefirstultraintelligentmachineisthelastinventionthatman need ever make . . . .
Id. at 32–33.
  
438
26 66 UCLA L. REV. 2 (2019)
Experts are divided on when, and if, AGI will be developed. Many industry leaders predict based on historical trends that AGI will occur within the next couple of decades.134 Others believe the magnitude of the challenge has been underestimated, and that AGI may not be developed in this century.135 In 2013, hundreds of AI experts were surveyed on their predictions for AGI development.136 On average, participants predicted a 10 percent likelihood that AGI would exist by 2022, a 50 percent likelihood it would exist by 2040, and a 90 percent likelihood it would exist by 2075.137 In a similar survey, 42 percent of participants predicted AGI would exist by 2030, and an additional 25 percent predicted AGI by 2050.138 In addition, 10 percent of participants reported they believed ASI would develop within two years of AGI, and 75 percent predicted this would occur within 30 years.139 The weight of expert opinion thus holds artificial general intelligence and superintelligence will exist this century. In the meantime, specific artificial intelligence is getting ever better at outcompeting people at specific tasks—including invention.
B. Timeline to the Creative Singularity
We are amid a transition from human to machine inventors. The following five-phase framework illustrates this transition and divides the history and future of inventive AI into several stages.
134. Pawel Sysiak, When Will the First Machine Become Superintelligent?, AI REVOLUTION, (Apr. 11, 2016), https://medium.com/ai-revolution/when-will-the-first-machine-become- superintelligent-ae5a6f128503 [https://perma.cc/7YUP-DEYM].
135. Id. In fairness, history also reflects some overly optimistic predictions. In 1970, Marvin Minsky, one of the most famous AI thought leaders, was quoted in Life Magazine as stating, “In from three to eight years we will have a machine with the general intelligence of an average human being.” Brad Darrach, Meet Shaky, the First Electronic Person, LIFE, Nov. 20 1970, at 58B, 66, 68.
136. See Müller & Bostrom, supra note 7.
137. Id. Participants were asked to provide an optimistic year for AGI’s development (10 percent
likelihood), a realistic year (50 percent likelihood), and a pessimistic year (90 percent likelihood). The median responses were 2022 as an optimistic year, 2040 as a realistic year, and 2075 as a pessimistic year. Id.
138. A survey conducted at an annual AGI Conference reported that 42 percent believed AGI would exist by 2030, 25 percent by 2050, 20 percent by 2100, 10 percent after 2010, and 2 percent never. See JAMES BARRAT, OUR FINAL INVENTION: ARTIFICIAL INTELLIGENCE AND THE END OF THE HUMAN ERA 152 (2013). For instance, Demis Hassabis, the founder of DeepMind, believes AGI is still decades away. David Rowan, DeepMind: Inside Google’s Super-Brain, WIRED (June 22, 2015), https://www.wired.co.uk/article/deepmind [https://perma.cc/MM6P-EU43].
139. See Müller & Bostrom, supra note 7.
  
439
Everything Is Obvious
I Human
III Human ~ SAI
27
   Phase
 Inventors
 Skilled Standard
 Timeframe
 Person
Augmented Person ~ SAI
Past
Short Term
  II
 Human > SAI
 Augmented Person
 Present
   IV
 SAI~AGI> Human
 Augmented AGI
 Medium Term
 V ASI ASI LongTerm
Table 1: Evolution of Machine Invention
  SAI = Specific Artificial Intelligence; AGI = Artificial General Intelligence; ASI = Artificial Superintelligence; ~ = competing; > = outcompeting
 Previously, in Phase I, all invention was created by people. If a company wanted to solve an industrial problem, it asked a research scientist, or a team of research scientists, to solve the problem. Phase I ended when the first patent was granted for an invention created by an autonomous machine—likely 1998 or earlier.140 It may be difficult to determine precisely when the first patent was issued for an autonomous machine invention, as there is no obligation to report the role of machines in patent applications. Still, any number of patents have likely been issued to inventions autonomously generated by machines.141 In 1998, a patent was issued for an invention autonomously developed by a neural network-based system known as the Creativity Machine.142
140. Phase I might also be distinguished by the first time a machine invented anything independently of receiving a patent. However, using the first granted patent application is a better benchmark. It is an external measure of a certain threshold of creativity, and it represents the first time a computer automated the role of a patent inventor. Of course, there is a degree of subjectivity in a patent examiner determining whether an invention is new, nonobvious, and useful. What is nonobvious to one examiner may be obvious to another. See, e.g., Iain M. Cockburn et al., Are All Patent Examiners Equal? The Impact of Characteristics on Patent Statistics and Litigation Outcomes, in PATENTS IN THE KNOWLEDGE- BASED ECONOMY, (Wesley M. Cohen & Steven A. Merrill eds., 2003) (describing significant interexaminer variation).
141. See generally, I Think, supra note 1, at 1083–91 (describing patents issued for “computational invention”).
142. Id. at 1083–86.
 
440
28 66 UCLA L. REV. 2 (2019)
Patents may have been granted on earlier machine inventions. For instance, an article published in 1983 describes experiments with an AI program known as Eurisko, in which the program “invent[ed] new kinds of three- dimensional microelectronic devices . . . novel designs and design rules have emerged.”143 Eurisko was an early, expert AI system for autonomously discovering new information.144 It was programmed to operate according to a series of rules known as heuristics, but it was able to discover new heuristics and usethesetomodifyitsownprogramming.145 Todesignnewmicrochips,Eurisko was programmed with knowledge of basic microchips along with simple rules and evaluation criteria.146 It would then combine existing chip structures together to create new designs, or mutate existing entities.147 The new structure would then be evaluated for interest and either retained or discarded.148 Several references suggest a patent was granted for one of Eurisko’s chip designs in the mid–1980s.149
Although, after investigating those references for this article, the references appear to refer to a patent application filed for the chip design by Stanford University in 1980 which the University abandoned for unknown reasons in 1984.150 Thus, a patent was never issued. Also, as with other publicly described
143. Douglas B. Lenat et al., Heuristic Search for New Microcircuit Structures: An Application of Artificial Intelligence, 3 AI MAG. , 17, 17 (1982).
144. Eurisko was created by Douglas Lenat as the successor to the Automated Mathematician (AM). See generally Douglas B. Lenat & John Seely Brown, Why AM and EURISKO Appear to Work, 23 AI MAG., 269, 269–94 (1983). AM was an “automatic programming system” that could modify its own computer code, relying on heuristics. Id. Eurisko was a subsequent iteration of the machine designed to additionally develop new heuristics and incorporate those into its function. Id.
145. See Douglas B. Lenat et al., supra note 143.
146. Id.
147. Id.
148. Id.
149. See, e.g., RICHARD FORSYTH & CHRIS NAYLOR, THE HITCHHIKER’S GUIDE TO ARTIFICIAL INTELLIGENCE IBM PC BASIC VERSION 2167 (1986); see also MARGARET A. BODEN, THE CREATIVE MIND: MYTHS AND MECHANISMS 228 (2004).
150. U.S. provisional patent application SN 144,960, April 29, 1980. Email From Katherine Ku, Dir. of Stanford Office of Tech. Licensing, to author (Jan. 17, 2018) (on file with author). Douglas Lenat, CEO of Cycorp, Inc., who wrote Eurisko and performed the above- mentioned research, reported that this work was done “before the modern rage about patenting things . . . ” and that in his opinion Eurisko had independently created a number of patentable inventions. See Telephone Interview With Douglas Lenat, CEO, Cycorp, Inc. (Jan. 12, 2018). He further reported that after Eurisko came up with the chip design, Professor James Gibbons at Stanford successfully built a chip based on the machine’s design. Id. This chip was the subject of a patent application by Stanford, but the application was abandoned in 1984. U.S. provisional patent application SN 144,960, supra. Prior to the present investigation, Stanford had purged its paper file for the application and so no longer had records reflecting the reason for the abandonment. Email From Katherine Ku, supra.
  
441
Everything Is Obvious 29
instances of patent applications claiming the output of inventive machines, the patent application was filed on behalf of natural persons.151 In this case, they were the individuals who had built a physical chip based on Eurisko’s design.152
In the present, Phase II, machines and people are competing and cooperating at inventive activity. However, in all technological fields, human researchers are the norm and thus best represent the skilled person standard. While AI systems are inventing, it is unclear to what extent this is occurring: Inventive machine owners may not be disclosing the extent of such machines in the inventive process, due to concerns about patent eligibility or because companies generally restrict information about their organizational methods to maintain a competitive advantage. This phase will reward early adopters of inventive machines which are able to outperform human inventors at solving specific problems, and whose output can exceed the skilled person standard. In 2006, for instance, NASA recruited an autonomously inventive machine to design an antenna that flew on NASA’s Space Technology 5 (ST5) mission.153
While there may now only be a modest amount of autonomous154 machine invention, human inventors are being widely augmented by creative computers. For example, a person may design a new battery using a computer to perform calculations, search for information, or run simulations on new designs. The computer does not meet inventorship criteria, but it does augment the capabilities of a researcher in the same way that human assistants can help reduce an invention to practice. Depending on the industry researchers work in and the
Incidentally, Dr. Lenat is now continuing to develop an expert system-based AI that can use logical deduction and inference reasoning based on “common sense knowledge,” as opposed to a system like Watson that recognizes patterns in very large datasets. Id. He also states that his current company has developed numerous patentable inventions, but that it has not filed for patent protection, because he believes that, at least with regards to software, the downside of patents providing competitors with a roadmap to copying patented technology exceeds the value of a limited term patent. Id.
151. See I Think, supra note 1, at 1083–91 (describing instances of “computational invention”).
152. Email From Katherine Ku, supra note 150. Whether the individual(s) designing a chip or building a chip would qualify as inventor(s) would depend on the specific facts of the case and who “conceived” of the invention. See generally Hal the Inventor, supra note 3
(discussing standards for inventorship).
153. Gregory S. Hornby et al., Automated Antenna Design With Evolutionary Algorithms, AM.
INST. AERONAUTICS & ASTRONAUTICS (2006), http://alglobus.net/NASAwork/papers/
Space2006Antenna.pdf.
154. As the term is used here, autonomous machines are given goals to complete by users, but
determine for themselves the means of completing those goals. See Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. 1 (2018). For example, a user could ask a computer to design a new battery with certain characteristics, and the computer could produce such a design without further human input. In this case, the machine would be autonomously inventive and competing with human inventors.
  
442
30 66 UCLA L. REV. 2 (2019)
problems they are trying to solve, researchers may rarely be unaided by computers. The more sophisticated the computer, the more it may be able to augment the worker’s skills.
Phase III, in the near future, will involve increased competition and cooperation between people and machines. In certain industries, and for certain problems, inventive machines will become the norm. For example, in the pharmaceutical industry, Watson is now identifying novel drug targets and new indications for existing drugs. Soon, it may be the case that inventive machines are the primary means by which new uses for existing drugs are researched. That is a predictable outcome, given the advantage machines have over people at recognizing patterns in very large datasets. However, it may be that people still perform the majority of research related to new drug targets. Where the standard varies within a broad field like drug discovery, this can be addressed by defining fields and problems narrowly, for instance according to the subclasses currently used by the Patent Office.155
Perhaps twenty-five years from now—based on expert opinion—the introduction of AGI will usher in Phase IV. Recall that AGI refers to artificial intelligence that can be applied generally, as opposed to narrowly in specific fields of art, and that it has intelligence comparable to a person. AGI will compete with human inventors in every field, which makes AGI a natural substitute for the skilled person. Even with this new standard, human inventors may continue to invent—just not as much. An inventor may be a creative genius whose abilities exceed the human average, or a person of ordinary intelligence who has a groundbreaking insight.
Just as SAI outperforms people in certain fields, it will likely be the case that SAI outperforms AGI in certain circumstances. An example of this could be when screening a million compounds for pesticide function lends itself to a “brute force” computational approach. For this reason, SAI could continue to represent the level of ordinary skill in fields in which SAI is the standard, while AGI could replace the skilled person in all other fields. However, the two systems will likely be compatible. A general AI system wanting to play Go could incorporate AlphaGo into its own programming, design its own algorithm like AlphaGo, or even instruct a second computer operating AlphaGo.
AGI will change the human-machine dynamic in another way. If the machine is genuinely capable of performing any intellectual task a person could,
155. See generally, Overview of the U.S. Patent Classification System (U.S.P.C.), U.S. PAT & TRADEMARK OFF. (2012), https://www.uspto.gov/sites/default/files/patents/resources/ classification/overview.pdf.
  
443
Everything Is Obvious 31
the machine would be capable of setting goals collaboratively with a person, or even by itself. Instead of a person instructing a computer to screen a million compounds for pesticide function, a person could merely ask a computer to develop a new pesticide. For that matter, an agrochemical company like Bayer could instruct DeepMind to develop any new technology for its business, or just to improve its profitability. Such machines should not only be able to solve known problems, but also solve unknown problems.
AGI will continually improve, transforming into ASI. Ultimately, in Phase V, when AGI succeeds in developing artificial superintelligence, it will mean the end of obviousness. Everything will be obvious to a sufficiently intelligent machine.
C. Inventive and Skilled Machines
For purposes of patent law, an inventive machine should be one which generates patentable output while meeting traditional inventorship criteria.156 Because obviousness focuses on the quality of a patent application’s inventive content, it should be irrelevant whether the content comes from a person or machine, or a particular type of machine. A machine which autonomously generates patentable output, or which does so collaboratively with human inventors where the machine meets joint inventorship criteria, is inventive.
Under the present framework, inventive machines would not be the equivalent of hypothetical skilled machines, just as human inventors are not skilled persons. In fact, it should not be possible to extrapolate about the characteristics of a skilled entity from information about inventive entities. Granted, the Federal Circuit once included the “educational level of the inventor” in its early factor-based test for the skilled person.157 However, that was only until it occurred to the Federal Circuit that:
[C]ourts never have judged patentability by what the real inventor/applicant/patentee could or would do. Real inventors, as a class, vary in the capacities from ignorant geniuses to Nobel laureates; the courts have always applied a standard based on an
156. See I Think, supra note 1 (arguing computers which independently meet human inventorship criteria should be recognized as inventors).
157. See e.g., Environmental, supra note 84.
  
444
32 66 UCLA L. REV. 2 (2019)
imaginary work of their own devising whom they have equated with the inventor.158
What then conceptually is a skilled machine? A machine that anthropomorphizes to the various descriptions courts have given for the skilled person? Such a test might focus on the way a machine is designed or how it functions. For instance, a skilled machine might be a conventional computer that operates according to fixed, logical rules, as opposed to a machine like DeepMind which can function unpredictably. However, basing a rule on how a computer functions might not work for the same reason the Flash of Genius test failed: Even leaving aside the significant logistical problem of attempting to figure out how a computer is structured or how it generates particular output, patent law should be concerned with whether a machine is generating inventive output, not what is going on inside the machine.159 If a conventional computer and a neural network were both able to generate the same inventive output, there should be no reason to favor one over the other.
Alternately, the test could focus on a machine’s capacity for creativity. For example, Microsoft Excel plays a role in a significant amount of inventive activity, but it is not innovative. It applies a known body of knowledge to solve problems with known solutions in a predictable fashion (for example, multiplying values together). However, while Excel may sometimes solve problems that a person could not easily solve without the use of technology, it lacks the ability to engage in almost any inventive activity.160 Excel is not the equivalent of a skilled machine—it is an automaton incapable of ordinary creativity.
Watson in clinical practice may be a better analogy for a skilled worker. Watson analyzes patients’ genomes and provides treatment recommendations.161 Yet as with Excel, this activity is not innovative. The problem Watson is solving may be more complex than multiplying a series of numbers, but it has a known solution. Watson is identifying known genetic mutations from a patient’s genome. Watson is then suggesting known treatments based on existing medical literature. Watson is not innovating
158. Kimberly-Clark Corp. v. Johnson & Johnson, 745 F.2d 1437, 1454 (Fed. Cir. 1984) (“[The] hypothetical person is not the inventor, but an imaginary being possessing ‘ordinary skill in the art’ created by Congress to provide a standard of patentability.”).
159. See I Think, supra note 1 (arguing against a subjective standard for computational invention).
160. Some behaviors like correcting a rogue formula may have a functionally creative aspect, but this is a minimal amount that would not rise to the level of patent conception if performed by a person.
161. See Wrzeszczynski et al., supra note 107.
  
445
Everything Is Obvious 33
because it is being applied to solve problems with known solutions, adhering to conventional wisdom.
Unlike Excel, however, Watson could be inventive. For instance, Watson could be given unpublished clinical data on patient genetics and actual drug responses and tasked with determining whether a drug works for a genetic mutation in a way that has not yet been recognized. Traditionally, such findings have been patentable. Watson may be situationally inventive depending on the problem it is solving.
It may be difficult to identify an actual computer program now which has a “skilled” level of creativity. To the extent a computer is creative, in the right circumstances, any degree of creativity might result in inventive output. To be sure, thisissimilartotheskilledperson. Apersonofordinaryskill,oralmostanyone,may haveaninventiveinsight. Characteristicscanbeimputedtoaskilledperson,butitis not possible the way the test is applied to identify an actual skilled person or to definitivelysaywhatshewouldhavefoundobvious. Theskilledpersontestissimply a theoretical device for a decisionmaker.
Assuming a useful characterization of a skilled machine, to determine that a skilled machine now represents the average worker in a field, decisionmakers would need information about the extent to which such machines are used. Obtaining this information may not be practical. Patent applicants could be asked generally about the use and prevalence of computer software in their fields, but it would be unreasonable to expect applicants to already have, or to obtain, accurate information about general industry conditions. The Patent Office, or another government agency, could attempt to proactively research the use of computers in different fields, but this would not be a workable solution. Such efforts would be costly, the Patent Office lacks expertise in this activity, and its findings would inevitably lag behind rapidly changing conditions. Ultimately, there may not be a reliable and low-cost source of information about skilled machines right now.
D. Inventive Is the New Skilled
Having inventive machines replace the skilled person may better correspond with real world conditions. Right now, there are inherent limits to the number and capabilities of human workers. The cost to train and recruit new researchers is significant, and there are a limited number of people with the ability to perform this work. By contrast, inventive machines are software
 
446
34 66 UCLA L. REV. 2 (2019)
programs which may be copied without additional cost.162 Once Watson outperforms the average industry researcher, IBM may be able to simply copy Watson and have it replace most of an existing workforce. Copies of Watson could replace individual workers, or a single Watson could do the work of a large team of researchers.
Indeed, as mentioned earlier, in a non-inventive setting, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, as opposed to a team of human experts, which takes around one- hundred and sixty hours.163 Once Watson is proven to produce better patient outcomes than the human team, it may be unethical to have people underperform a task which Watson can automate. When that occurs, Watson should not only replace the human team at its current facility—it should replace every comparable human team. Watson could similarly automate in an inventive capacity.
Thus, inventive machines change the skilled paradigm because once they become the average worker, the average worker becomes inventive. As the outputs of these inventive machines become routinized, however, they should no longer be inventive by definition. The widespread use of these machines should raise the bar for obviousness, so that these machines no longer qualify as inventive but shift to become skilled machines—machines which now represent the average worker and are no longer capable of routine invention.164
Regardless of the terminology, as machines continue to improve, the bar for nonobviousness should rise. To generate patentable output, it may be necessary to use an advanced machine that can outperform standard machines, or a person or machine will need to have an unusual insight that standard machines cannot easily recreate. Inventiveness might also depend on the data supplied to a machine, such that only certain data would result in inventive output. Taken to its logical extreme, and given there is no limit to how sophisticated computers can become, it may be that everything will one day be obvious to commonly used computers.
It is possible to generate reasonably low-cost and accurate information about the use of inventive machines. The Patent Office should institute a requirement for patent applicants to disclose the role of computers in the
162. ANDREAS KEMPER, VALUATION OF NETWORK EFFECTS IN SOFTWARE MARKETS: A COMPLEX NETWORKS APPROACH 37 (2010).
163. See Wrzeszczynski et al., supra note 107.
164. See Enzo Biochem, Inc. v. Calgene, Inc., 188 F.3d 1362, 1374 n.10 (Fed. Cir. 1999) (“In view
of the rapid advances in science, we recognize that what may be unpredictable at one point in time may become predictable at a later time.”).
  
447
Everything Is Obvious 35
inventive process.165 This disclosure could be structured along the lines of current inventorship disclosure. Right now, applicants must disclose all patent inventors.166 Failure to do so can invalidate a patent or render it unenforceable.167 Similarly, applicants should have to disclose when a machine autonomously meets inventorship criteria.
These disclosures would only apply to an individual invention. However, the Patent Office could aggregate responses to see whether most inventors in a field (for example, a class or subclass) are human or machine. These disclosures would have a minimal burden on applicants compared to existing disclosure requirements and the numerous procedural requirements of a patent application. In addition to helping the Patent Office with determinations of nonobviousness, these disclosures would provide valuable information for purposes of attributing inventorship.168 It might also be used to develop appropriate innovation policies in other areas.169
E. Skilled People Use Machines
The current standard neglects to appropriately take into account the modern importance of machines in innovation. Instead of now replacing the skilled person with the skilled machine, it would be less of a conceptual change, and administratively easier, to characterize the skilled person as an average worker facilitated by technology. Recall the factor test for the skilled person includes: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.”170 This test could be amended to include, (6) “technologies used by
165. It may also be beneficial for applicants to disclose the use of computers when they have been part of the inventive process but where their contributions have not risen to the level of inventorship. Ideally, a detailed disclosure should be provided: Applicants should need to disclose the specific software used and the task it performed. In most cases, this would be as simple as noting a program like Excel was used to perform calculations. However, while this information would have value for policy making, it might involve a significant burden to patent applicants.
166. Duty to Disclose Information Material to Patentability, 37 C.F.R. § 1.56 (2018), https://www.uspto.gov/web/offices/pac/mpep/s2001.html [https://perma.cc/4DE9-ZRWE].
167. See, e.g., Advanced Magnetic Closures, Inc. v. Rome Fastener Corp., 607 F.3d 817, 829–30 (Fed. Cir. 2010) (upholding a district court decision to render a patent unenforceable on the grounds of inequitable conduct for misrepresenting inventorship).
168. See I Think, supra note 1 (advocating for acknowledging machines as inventors).
169. See Should Robots Pay Taxes?, supra note 6 (arguing the need to monitor automation for
adjusting tax incentives).
170. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
  
448
36 66 UCLA L. REV. 2 (2019)
active workers.” This would more explicitly take into account the fact that human researchers’ capabilities are augmented with computers.
Moving forward in time, once the use of inventive machines is standard, instead of a skilled person being an inventive machine, the skilled person standard could incorporate the fact that technologies used by active workers includes inventive machines. In future research, the standard practice may be for a worker to ask an inventive machine to solve a problem. This could be conceptualized as the inventive machine doing the work, or the person doing the work using an inventive machine.
Granted, in some instances, using an inventive machine may require significant skill, for instance, if the machine is only able to generate a certain output by virtue of being supplied with certain data. Determining which data to provide a machine, and obtaining that data, may be a technical challenge. Also, it may be the case that significant skill is required to formulate the precise problem to put to a machine. In such instances, a person might have a claim to inventorship independent of the machine, or a claim to joint inventorship. This is analogous to collaborative human invention where one person directs another to solve a problem. Depending on details of their interaction, and who “conceived” of the invention, one person or the other may qualify as an inventor, or they may qualify as joint inventors.171 Generally, however, directing another partytosolveaproblemdoesnotqualifyforinventorship.172 Moreover,afterthe development of AGI, there may not be a person instructing a computer to solve a specific problem.
Whether the future standard becomes an inventive machine or a skilled person using an inventive machine, the result will be the same: The average worker will be capable of inventive activity. Replacing the skilled person with the inventive machine may be preferable doctrinally, because it emphasizes that it is the machine which is engaging in inventive activity, rather than the human worker.
The changing use of machines also suggests a change to the scope of prior art. The analogous art test was implemented because it is unrealistic to expect inventors to be familiar with anything more than the prior art in their field, and
171. “[C]onception is established when the invention is made sufficiently clear to enable one skilled in the art to reduce it to practice without the exercise of extensive experimentation or the exercise of inventive skill.” Hiatt v. Ziegler & Kilgour, 179 U.S.P.Q. 757, 763 (Bd. Pat. Interferences 1973); see also Gunter v. Stream, 573 F.2d 77, 79 (C.C.P.A. 1978).
172. Ex parte Smernoff, 215 U.S.P.Q. at 547 (“[O]ne who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not a coinventor.”).
  
449
Everything Is Obvious 37
the prior art relevant to the problem they are trying to solve.173 However, a machine is capable of accessing a virtually unlimited amount of prior art. Advances in medicine, physics, or even culinary science may be relevant to solving a problem in electrical engineering. Machine augmentation suggests that the analogous arts test should be modified or abolished once inventive machines are common, and that there should be no difference in prior art for purposes of novelty and obviousness.174 The scope of analogous prior art has consistently expanded in patent law jurisprudence, and this would complete that expansion.175
F. The Evolving Standard
The skilled person standard should be amended as follows:
1) The test should now incorporate the fact that skilled persons are already augmented by machines. This could be done by adding “technologies used by active workers” as a sixth factor to the Federal Circuit’s factor test for the skilled person.
2) Once inventive machines become the standard means of research in a field, the skilled person should be an inventive machine when the standard approach to research in a field or with respect to a particular problem is to use an inventive machine.
3) When and if artificial general intelligence is developed, inventive machines should become the skilled person in all areas, taking into account that artificial general intelligence may also be augmented by specific artificial intelligence.
III. A POST-SKILLED WORLD
This Part provides examples of how the Inventive Machine Standard could work in practice, such as by focusing on reproducibility or secondary factors. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function
173. In 1966, in Graham, the Court recognized that “the ambit of applicable art in given fields of science has widened by disciplines unheard of a half century ago . . . . [T]hose persons granted the benefit of a patent monopoly [must] be charged with an awareness of these changed conditions.” Graham v. John Deere Co., 383 U.S. 1, 19 (1966).
174. See supra Subpart I.E.
175. Innovative Scuba Concepts, Inc., v. Feder Indus., Inc., 819 F. Supp. 1487, 1503 (D. Colo.
1993) (discussing the expansion of analogous art); see also, e.g., George. J. Meyer Mfg. Co. v. San Marino Elec. Corp., 422 F.2d 1285, 1288 (9th Cir. 1970) (discussing the expansion of analogous art).
  
450
38 66 UCLA L. REV. 2 (2019)
as innovation incentives. To the extent patents accomplish other goals such as promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
A. Application
Mobil Oil Corp. v. Amoco Chemicals Corp. concerned complex technology involving compounds known as Zeolites used in various industrial applications.176 Mobil had developed new compositions known as ZSM-5 zeolites and a process for using these zeolites as catalysts in petroleum refining to help produce certain valuable compounds. The company received patent protection for these zeolites and for the catalytic process.177 Mobil subsequently sued Amoco, which was using zeolites as catalysts in its own refining operations, alleging patent infringement. Amoco counterclaimed seeking a declaration of noninfringement, invalidity, and unenforceability with respect to the two patents at issue. The case involved complex scientific issues. The three-week trial transcript exceeds 3300 pages, and more than 800 exhibits were admitted into evidence.
One of the issues in the case was the level of ordinary skill. An expert for Mobil testified that the skilled person would have “a bachelor’s degree in chemistry or engineering and two to three years of experience.”178 An expert for Amoco argued the skilled person would have a doctorate in chemistry and several years of experience.179 The District Court for the District of Delaware ultimately decided that the skilled person “should be someone with at least a Masters degree in chemistry or chemical engineering or its equivalent, [and] two or three years of experience working in the field.”180
If a similar invention and subsequent fact pattern happened today, to apply the obviousness standard proposed in this Article a decisionmaker would need to: (1) determine the extent to which inventive technologies are used in the field, (2) characterize the inventive machine(s) that best represents the average worker if inventive machines are the standard, and (3) determine whether the machine(s) would find an invention obvious. The decisionmaker is a patent
176. Mobil Oil Corp. v. Amoco Chems. Corp.,779 F. Supp. 1429, 1442–43 (D. Del. 1991).
177. Id.
178. Id. at 1443.
179. Id.
180. Id.
  
451
Everything Is Obvious 39
examiner in the first instance,181 and potentially a judge or jury in the event the validity of a patent is at issue in trial.182 For the first step, determining the extent to which inventive technologies are used in a field, evidence from disclosures to the Patent Office could be used. That may be the best source of information for patent examiners, but evidence may also be available in the litigation context.
Assume that today most petroleum researchers are human, and that if machines are autonomously inventive in this field, it is happening on a small scale. Thus, the court would apply the skilled person standard. However, the court would now also consider “technologies used by active workers.” For instance, experts might testify that the average industry researcher has access to a computer like Watson. They further testify that while Watson cannot autonomously develop a new catalyst, it can significantly assist an inventor. The computer provides a researcher with a database containing detailed information about every catalyst used not only in petroleum research, but in all fields of scientific inquiry. Once a human researcher creates a catalyst design, Watson can also test it for fitness together with a predetermined series of variations on any proposed design.
The question for the court will thus be whether the hypothetical person who holds at least a Master’s degree in chemistry or chemical engineering or its equivalent, has two or three years of experience working in the field, and is using Watson, would find the invention obvious. It may be obvious, for instance, if experts convincingly testify that the particular catalyst at issue were very closely related to an existing catalyst used outside of the petroleum industry in ammonia synthesis, that any variation was minor, and that a computer could do all the work of determining if it were fit for purpose.183 It might thus have been an obvious design to investigate, and it did not require undue experimentation in order to prove its effectiveness.
Now imagine the same invention and fact pattern occurring approximately ten years into the future, at which point DeepMind, together with Watson and a competing host of AI systems, have been set to the task of developing new
181. See U.S. PAT. & TRADEMARK OFF., supra note 24 (at the Patent Office, applications are initially considered by a patent examiner, and examiner decisions can be appealed to the Patent Trial and Appeal Board (PTAB)).
182. Mark A. Lemley, Why Do Juries Decide if Patents Are Valid? (Stanford Law Sch., Pub. Law & Legal Theory Research Paper Series, Working Paper No. 2306152, 2013), https://ssrn.com/abstract=2306152.
183. See Daiichi Sankyo Co. v. Matrix Labs., Ltd., 619 F.3d 1346, 1352 (Fed. Cir. 2010) (finding that a “chemist of ordinary skill would have been motivated to select and then to modify a prior art compound (e.g., a lead compound) to arrive at a claimed compound with a reasonable expectation that the new compound would have similar or improved properties compared with the old”).
  
452
40 66 UCLA L. REV. 2 (2019)
compounds to be used as catalysts in petroleum refining. Experts testify that the standard practice is for a person to provide data to a computer like DeepMind, specify desired criteria (for example, activity, stability, perhaps even designing around existing patents), and ask the computer to develop a new catalyst. From this interaction, the computer will produce a new design. As most research in this field is now performed by inventive machines, a machine would be the standard for judging obviousness.
The decisionmaker would then need to characterize the inventive machine(s). It could be a hypothetical machine based on general capabilities of inventive machines, or a specific computer. Using the standard of a hypothetical machine would be similar to using the skilled person test, but this test could be difficult to implement. A decisionmaker would need to reason what the machine would have found obvious, perhaps with expert guidance. It is already challenging for a person to predict what a hypothetical person would find obvious; it would be even more difficult to do so with a machine. Computers may excel at tasks people find difficult (like multiplying a thousand different numbers together), but even supercomputers struggle with visual intuition, which is mastered by most toddlers.
In contrast, using a specific computer should result in a more objective test. This computer might be the most commonly used computer in a field. For instance, if DeepMind and Watson are the two most commonly used AI systems for research on petroleum catalysts, and DeepMind accounts for 35 percent of the market while Watson accounts for 20 percent, then DeepMind could represent the standard. However, this potentially creates a problem—if DeepMind is the standard, then it would be more likely that DeepMind’s own inventions would appear obvious as opposed to the inventions of another machine. This might give an unfair advantage to non-market leaders, simply because of their size.
To avoid unfairness, the test could be based on more than one specific computer. For instance, both DeepMind and Watson could be selected to represent the standard. This test could be implemented in two different ways. In the first case, if a patent application would be obvious to DeepMind or Watson, then the application would fail. In the second case, the application would have to be obvious to both DeepMind and Watson to fail. The first option would result in fewer patents being granted, with those patents presumably going mainly to disruptive inventive machines with limited market penetration, or to inventions made using specialized non-public data. The second option would permit patents where a machine is able to outperform its competitors in some
 
453
Everything Is Obvious 41
material respect. The second option could continue to reward advances in inventive machines, and therefore seems preferable.
It may be that relatively few AI systems, such as DeepMind and Watson, end up dominating the research market in a field. Alternately, many different machines may each occupy a small share of the market. There is no need to limit the test to two computers. To avoid discriminating on the basis of size, all inventive machines being routinely used in a field or to solve a particular problem might be considered. However, allowing any machine to be considered could allow an underperforming machine to lower the standard, and too many machines might result in an unmanageable standard. An arbitrary cutoff may be applied based on some percentage of market share. That might still give some advantage to very small entities, but it should be a minor disparity.
After characterizing the inventive machine(s), a decisionmaker would need to determine whether the inventive machine(s) would find an invention obvious. This could broadly be accomplished in one of two ways: either with abstract knowledge of what the machines would find obvious, perhaps through expert testimony, or through querying the machines. The former would be the morepracticaloption.184 Forexample,apetroleumresearcherexperiencedwith DeepMind might be an expert, or a computer science expert in DeepMind and neural networks. This inquiry could focus on reproducibility.
Finally, a decisionmaker will have to go through a similar process if the same invention and fact pattern occurs twenty-five years from now, at which point artificial general intelligence has theoretically taken over in all fields of research. AGI should have the ability to respond directly to queries about whether it finds an invention obvious. Once AGI has taken over from the average researcher in all inventive fields, it may be widely enough available that the Patent Office could arrange to use it for obviousness queries. In the litigation context, it may be available from opposing parties. If courts cannot somehow access AGI, they may still have to rely on expert evidence.
184. Alternatively, the machine could be asked to solve the problem at question and given the relevant prior art. If the machine generates the substance of the patent, the invention would be considered obvious. However, this would require a decisionmaker to have access to the inventive machine. At the application stage, the Patent Office would need to contract with, say, Google to use DeepMind in such a fashion. For that matter, the Patent Office might use DeepMind not only to decide whether inventions are obvious, but to automate the entire patent examination process. At trial, if Google is party to a lawsuit, an opposing party might subpoena use of the computer. However, if Google is not a party, it might be unreasonable to impose on Google for access to DeepMind.
  
454
42 66 UCLA L. REV. 2 (2019)
B. Reproducibility
Even if an inventive machine standard is the appropriate theoretical tool for nonobviousness, it still requires certain somewhat subjective limitations, and decisionmakers may still have difficulty with administration. Still, the new standard only needs to be slightly better than the existing standard to be an administrative success.
A test focused on reproducibility, based on the ability of the machine selected to represent the standard being able to independently reproduce the invention, offers some clear advantages over the current skilled person standard, which results in inconsistent and unpredictable outcomes.185 Courts have “provided almost no guidance concerning either what degree of ingenuity is necessary to meet the standard or how a decisionmaker is supposed to evaluate whether the differences between the invention and prior art meet this degree.”186 This leaves decisionmakers in the unenviable position of trying to subjectively establish what another person would have found obvious. Worse, this determination is to be made in hindsight with the benefit of a patent application. On top of that, judges and juries lack scientific expertise.187 In practice, decisionmakers may read a patent application, decide that they know
185. See FED. TRADE COMM’N, supra note 16 (discussing objections to the skilled person standard).
186. Mandel, The Non-Obvious Problem, supra note 19, at 64.
187. As Judge Learned Hand wrote:
I cannot stop without calling attention to the extraordinary condition of the law which makes it possible for a man without any knowledge of even the rudiments of chemistry to pass upon such questions as these. The inordinate expense of time is the least of the resulting evils, for only a trained chemist is really capable of passing upon such facts . . . . How long we shall continue to blunder along without the aid of unpartisan and authoritative scientific assistance in the administration of justice, no one knows; but all fair persons not conventionalized by provincial legal habits of mind ought, I should think, unite to effect some such advance.
Parke-Davis & Co. v. H.K. Mulford Co., 189 F. 95, 115 (S.D.N.Y. 1911). See also Safety Car Heating & Lighting Co. v. Gen. Elec. Co., 155 F.2d 937, 939 (1946) (“Courts, made up of laymen as they must be, are likely either to underrate, or to overrate, the difficulties in making new and profitable discoveries in fields with which they cannot be familiar . . . .”); see also Doug Lichtman & Mark A. Lemley, Rethinking Patent Law’s Presumption of Validity, 60 STAN. L. REV. 45, 67 (2007) (“District Court judges are poorly equipped to read patent documents and construe technical patent claims. Lay juries have no skill when it comes to evaluating competing testimony about the originality of a technical accomplishment.”).
  
455
Everything Is Obvious 43
obviousness when they see it, and then reason backward to justify their findings.188
This is problematic because patents play a critical role in the development and commercialization of products, and patent holders and potential infringers should have a reasonable degree of certainty about whether patents are valid. A more determinate standard would make it more likely the Patent Office would apply a single standard consistently and result in fewer judicially invalidated patents. To the extent machine reproducibility is a more objective standard, this would seem to address many of the problems inherent in the current standard.
On the other hand, reproducibility comes with its own baggage. Decisionmakers have difficulty imagining what another person would find obvious, and it would probably be even more difficult to imagine in the abstract what a machine could reproduce. More evidence might need to be supplied in patent prosecution and during litigation, perhaps in the format of analyses performed by inventive machines, to demonstrate whether particular output was reproducible. This might also result in a greater administrative burden.
In some instances, reproducibility may be dependent on access to data. A large health insurer might be able to use Watson to find new uses for existing drugs by giving Watson access to proprietary information on its millions of members. Or, the insurer might license its data to drug discovery companies using Watson for this purpose. Without that information, another inventive computer might not able to recreate Watson’s analysis.
This too is analogous to the way data is used now in patent applications: Obviousness is viewed in light of the prior art, which does not include non- public data relied upon in a patent application. The rationale here is that this rule incentivizes research to produce and analyze new data. Yet as machines become highly advanced, it is likely that the importance of proprietary data will decrease. More advanced machines may be able to do more with less.
Finally, reproducibility would require limits. For instance, a computer which generates semi-random output might eventually recreate the inventive concept of a patent application if it were given unlimited resources. However, it would be unreasonable to base a test on what a computer would reproduce given, say, 7.5 million years.189 The precise limits that should be placed on
188. Jacobellis v. Ohio, 378 U.S. 184, 197 (1964) (Stewart, J., dissenting). This was later recognized as a failed standard. Miller v. California, 413 U.S. 15, 47–48 (1973) (Brennan, J., dissenting) (obscenity cases similarly relying on the Elephant Test).
189. This brings to mind a super intelligent artificial intelligence system, “Deep Thought,” which famously, and fictionally, took 7.5 million years to arrive at the “Answer to the Ultimate Question of Life, the Universe, and Everything.” DOUGLAS ADAMS, THE HITCHHIKER’S GUIDE TO THE GALAXY 180 (rev. ed. 2001) (1979). The answer was 42. Id. at 188.
  
456
44 66 UCLA L. REV. 2 (2019)
reproducibility might depend on the field in question, and what best reflected the actual use of inventive machines in research. For instance, when asked to design a new catalyst in the petroleum industry, Watson might be given access to all prior art and publicly available data, and then given a day to generate output.
C. An Economic vs. Cognitive Standard
The skilled person standard received its share of criticism even before the arrival of inventive machines.190 The inquiry focuses on the degree of cognitive difficulty in conceiving an invention but fails to explain what it actually means for differences to be obvious to an average worker. The approach lacks both a normative foundation and a clear application.191
In Graham, the Supreme Court’s seminal opinion on nonobviousness, the Court attempted to supplement the test with more “objective” measures by looking to real-world evidence about how an invention was received in the marketplace.192 Rather than technological features, these “secondary” considerations focus on “economic and motivational” features, such as commercial success, unexpected results, long-felt but unsolved needs, and the failure of others.193 Since Graham, courts have also considered, among other
190. See, e.g., Chiang, supra note 19, at 49 (as one commentator noted about the test as articulated by the Supreme Court in Graham, it gives “all the appearance of expecting a solution to appear out of thin air once the formula was followed. The lack of an articulable rule meant that determinations of obviousness took the appearance—and arguably the reality—of resting on judicial whim . . . .” (footnote omitted)); Abramowicz & Duffy, supra note 16, at 1598; Gregory N. Mandel, Patently Non-Obvious: Empirical Demonstration That the Hindsight Bias Renders Patent Decisions Irrational, 67 OHIO ST. L.J. 1391 (2006) (discussing problems with hindsight in nonobviousness inquiries); Gregory N. Mandel, Another Missed Opportunity: The Supreme Court’s Failure to Define Nonobviousness or Combat Hindsight Bias in KSR v. Teleflex, 12 LEWIS & CLARK L. REV. 323 (2008).
191. See Abramowicz & Duffy, supra note 16, at 1603 (“[N]either Graham nor in subsequent cases has the Supreme Court attempted either to reconcile the inducement standard with the statutory text or to provide a general theoretical or doctrinal foundation for the inducement standard.”).
192. See Graham v. John Deere Co., 383 U.S. 1, 17; MPEP § 2144.
193. Graham, 383 U.S. at 17; MPEP § 2144. Additional secondary considerations have since been
proposed. See, e.g., Andrew Blair-Stanek, Increased Market Power as a New Secondary Consideration in Patent Law, 58 AM. U. L. REV. 707 (2009) (arguing for whether an invention provides an inventor with market power); Abramowicz & Duffy, supra note 16, at 1656 (proposing changing commercial success to “unexpected commercial success,” adding as a consideration of the “cost of the experimentation leading to the invention,” and a few additional considerations).
  
457
Everything Is Obvious 45
things, patent licensing,194 professional approval,195 initial skepticism,196 near- simultaneous invention,197 and copying.198 Today, while decisionmakers are required to consider secondary evidence when available, the importance of these factors varies significantly.199 Graham endorsed the use of secondary considerations, but their precise use and relative importance have never been made clear.200
An existing vein of critical scholarship has advocated for adopting a more economic than cognitive nonobviousness inquiry, for example through greater reliance on secondary considerations.201 This would reduce the need for decisionmakers to try and make sense of complex technologies, and it could reduce hindsight bias.202
Theoretically, in Graham, the Court articulated an inducement standard, which dictates that patents should only be granted to “those inventions which would not be disclosed or devised but for the inducement of a patent.”203 But in practice, the inducement standard has been largely ignored due to concerns over application.204 For instance, few, if any, inventions would never be disclosed or devised given an unlimited time frame. Patent incentives may not increase, so
194. See, e.g., SIBIA Neurosciences, Inc. v. Cadus Pharm. Corp., 225 F.3d 1349, 1358 (Fed. Cir. 2000).
195. See, e.g., Vulcan Eng’g Co. v. Fata Aluminum, Inc., 278 F.3d 1366, 1373 (Fed. Cir. 2002).
196. See, e.g., Metabolite Labs., Inc. v. Lab. Corp. of Am. Holdings, 370 F.3d 1354, 1368 (Fed. Cir.
2004).
197. See, e.g., Ecolochem, Inc. v. S. Cal. Edison Co., 227 F.3d 1361, 1379 (Fed. Cir. 2000).
198. See, e.g., id. at 1377. See also Mark A. Lemley, Should Patent Infringement Require Proof of
Copying?, 105 MICH. L. REV. 1525, 1534–35 (2007).
199. See MPEP § 2144; Durie & Lemley, supra note 19, at 996–97.
200. See, e.g., Dorothy Whelan, A Critique of the Use of Secondary Considerations in Applying the
Section 103 Nonobviousness Test for Patentability, 28 B.C. L. REV. 357 (1987).
201. See, e.g., Merges, supra note 19, at 19 (arguing for patentability to be based on an a priori degree of uncertainty, that “rewards one who successfully invents when the uncertainty facing her prior to the invention makes it more likely than not that the invention won’t succeed” (emphasis omitted)); Chiang, supra note 19, at 42 (arguing for a utilitarian standard, such that “[a]n invention should receive a patent if the accrued benefits before independent invention outweigh the costs after independent invention”); Mandel, The Non- Obvious Problem, supra note 19, at 62 (arguing for nonobviousness to be based on “how probable the invention would have been for a person having ordinary skill in the art working on the problem that the invention solves”); Durie & Lemley, supra note 19, at 1004–07 (arguing for a greater reliance on secondary considerations); Duffy, supra note 19, at 343 (arguing a timing approach to determining obviousness); Devlin & Sukhatme, supra note
19; Abramowicz & Duffy, supra note 16, at 1598 (arguing for an inducement standard).
202. Graham, 383 U.S. at 36 (“[Secondary considerations] may also serve to ‘guard against slipping into use of hindsight.’” (citation omitted)). See also HERBERT F. SCHWARTZ &
ROBERT J. GOLDMAN, PATENT LAW AND PRACTICE 90–91 (6th ed. 2008).
203. Graham, 383 U.S. at 11.
204. See Abramowicz & Duffy, supra note 16, at 1594–95.
  
458
46 66 UCLA L. REV. 2 (2019)
much as accelerate, invention.205 This suggests that an inducement standard would at least need to be modified to include some threshold for the quantum of acceleration needed for patentability. Too high a threshold would fail to provide adequate innovation incentives, but too low a threshold would be similarly problematic. Just as inventions will be eventually disclosed without patents given enough time, patents on all inventions could marginally speed the disclosure of just about everything, but a trivial acceleration would not justify the costs of patents. An inducement standard would thus require a somewhat arbitrary threshold in relation to how much patents should accelerate the disclosure of information, as well as a workable test to measure acceleration.206 To be sure, an economic test based on the inducement standard would have challenges, but it might be an improvement over the current cognitive standard.207
The widespread use of inventive machines may provide the impetus for an economic focus. After inventive machines become the standard way that R&D is conducted in a field, courts could increase reliance on secondary factors. For instance, patentability may depend on how costly it was to develop an invention, andtheexanteprobabilityofsuccess.208 Thereisnoreasonaninventivemachine cannot be thought of, functionally, as an economically motivated rational actor. The test would raise the bar to patentability in fields where the cost of invention decreases over time due to inventive machines.
D. Other Alternatives
Courts may maintain the current skilled person standard and decline to consider the use of machines in obviousness determinations. However, this means that as research is augmented and then automated by machines, the average worker will routinely generate patentable output. The dangers of such a
205. See, e.g., Yoram Barzel, Optimal Timing of Innovations, 50 REV. ECON. & STATS. 348, 348 (1968); John F. Duffy, Rethinking the Prospect Theory of Patents, 71 U. CHI. L. REV. 439, 444 (2004).
206. Abramowicz & Duffy, supra note 16, at 1599 (proposing a “substantial period of time”).
207. See Abramowicz & Duffy, supra note 16, at 1663.
208. Id.
  
459
Everything Is Obvious 47
standard for patentability are well-recognized.209 A low obviousness requirement can “stifle, rather than promote, the progress of the useful arts.”210
Concerns already exist that the current bar to patentability is too low, and that a patent “anticommons” with excessive private property is resulting in “potential economic value . . . disappear[ing] into the ‘black hole’ of resource underutilization.”211 It is expensive for firms interested in making new products to determine whether patents cover a particular innovation, evaluate those patents, contact patent owners, and negotiate licenses.212 In many cases, patent owners may not wish to license their patents, even if they are non-practicing entities that do not manufacture products themselves.213 Firms that want to make a product may thus be unable to find and license all the rights they need to avoid infringing. Adding to this morass, most patents turn out to be invalid or not infringed in litigation.214 Excessive patenting can thus slow innovation, destroy markets, and, in the case of patents on some essential medicines, even cost lives.215 Failing to raise the bar to patentability once the use of inventive machines is widespread would significantly exacerbate this anticommons effect.
Instead of updating the skilled person standard, courts might determine that inventive machines are incapable of inventive activity, much as the U.S. Copyright Office has determined that nonhuman authors cannot generate copyrightable output.216 In this case, otherwise patentable inventions might not
209. See, e.g., ADAM B. JAFFE & JOSH LERNER, INNOVATION AND ITS DISCONTENTS: HOW OUR BROKEN PATENT SYSTEM IS ENDANGERING INNOVATION AND PROGRESS, AND WHAT TO DO ABOUT IT 32–35, 75, 119–23, 145–49 (2004) (criticizing the Patent Office for granting patents on obvious inventions); NATIONAL RESEARCH COUNCIL, A PATENT SYSTEM FOR THE 21ST CENTURY 87–95 (2004) (criticizing lenient nonobviousness standards); Matthew Sag & Kurt Rohde, Patent Reform and Differential Impact, 8 MINN. J.L. SCI. & TECH. 1, 2 (2007) (“Academics, business leaders, and government officials have all expressed concern that too many patents are issued for [obvious] inventions.” ).
210. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 427 (2007).
211. James M. Buchanan & Yong J. Yoon, Symmetric Tragedies: Commons and Anticommons, 43
J.L. & ECON. 1, 2; accord DAN L. BURK & MARK A. LEMLEY, THE PATENT CRISIS AND HOW THE
COURTS CAN SOLVE IT (2009) (arguing for a heightened bar to patentability).
212. See generally Mark A. Lemley, Ignoring Patents, 2008 MICH. ST. L. REV. 19, 25–26 (2008)
(describing various costs associated with innovation in patent heavy industries).
213. See David L. Schwartz & Jay P. Kesan, Analyzing the Role of Non-Practicing Entities in the
Patent System, 99 CORNELL L. REV. 425 (2014).
214. See Mark A. Lemley & Carl Shapiro, Probabilistic Patents, 19 J. ECON. PERSP. 75, 80 (2005).
215. See Michael A. Heller, The Tragedy of the Anticommons: Property in the Transition From
Marx to Markets, 111 HARV. L. REV. 621 (1998); see also MICHAEL HELLER, THE GRIDLOCK ECONOMY: HOW TOO MUCH OWNERSHIP WRECKS MARKETS, STOPS INNOVATION AND COSTS LIVES (2008); see also Michael A. Heller & Rebecca S. Eisenberg, Can Patents Deter Innovation? The Anticommons in Biomedical Research, 280 SCIENCE 698 (1998).
216. This has been a policy of the Copyright Office since at least 1984. See U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 306 (3d ed. 2014). The
  
460
48 66 UCLA L. REV. 2 (2019)
be eligible for patent protection, unless provisions were made for the inventor to be the first person to recognize the machine output as patentable. However, this would not be a desirable outcome. As I have argued elsewhere, providing intellectual property protection for computer-generated inventions would incentivize the development of inventive machines, which would ultimately result in additional invention.217 This is most consistent with the constitutional rationale for patent protection “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”218
E. Incentives Without Patents?
Today, there are strong incentives to develop inventive machines. Inventions by these machines have value independent of intellectual property protection, but they should also be eligible for patent protection. People may apply as inventors for recognizing the inventive nature of a machine’s output,219 or more ambitiously, inventive machines may be recognized as inventors, resulting in stronger and fairer incentives.
Once inventive machines set the baseline for patentability, standard inventive machines, as well as people, should have difficulty obtaining patents. It is widely thought that setting a nonobviousness standard too high would reduce the incentives for innovators to invent and disclose. Yet once inventive machinesarenormal,thereshouldbelessneedforpatentincentives.220 Oncethe
Compendium of U.S. Copyright Office Practices elaborates on the “human authorship” requirement by stating: “The term ‘authorship’ implies that, for a work to be copyrightable, it must owe its origin to a human being.” Id. It further elaborates on the phrase “[w]orks not originated by a human author” by stating: “In order to be entitled to copyright registration, a work must be the product of human authorship. Works produced by mechanical processes or random selection without any contribution by a human author are not registrable.” Id. § 503.03(a).
217. See generally I Think, supra note 1.
218. U.S. CONST. art. I, § 8, cl. 8.
219. Conception requires contemporaneous recognition and appreciation of the invention. See
Invitrogen Corp. v. Clontech Labs., Inc., 429 F.3d 1052, 1064 (Fed. Cir. 2005) (noting that the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue); see also, e.g., Silvestri v. Grant, 496 F.2d 593, 597 (C.C.P.A. 1974) (“[A]n accidental and unappreciated duplication of an invention does not defeat the patent right of one who, though later in time, was the first to recognize that which constitutes the inventive subject matter.”).
220. See generally, Mark A. Lemley, IP in a World Without Scarcity (Stanford Public Law, Working Paper No. 2413974, 2014), http://dx.doi.org/10.2139/ssrn.2413974 (arguing new technologies that reduce costs will weaken the case for IP).
  
461
Everything Is Obvious 49
average worker is inventive, inventions will “occur in the ordinary course.”221 Machine inventions will be self-sustaining. In addition, the heightened bar might result in a technological arms race to create ever more intelligent computers capable of outdoing the standard. That would be a desirable outcome in terms of incentivizing innovation.
Even after the widespread use of inventive machines, patents may still be desirable. For instance, patents may be needed in the biotechnology and pharmaceutical industries to commercialize new technologies. The biopharma industry claims that new drug approvals cost around 2.2 billion dollars and take an average of eight years.222 This cost is largely due to resource intensive clinical trials required to prove safety and efficacy. Once a drug is approved, it is often relatively easy for another company to recreate the approved drug. Patents thus incentivize the necessary levels of investment to commercialize a product given that patent holders can charge monopoly prices for their approved products during the term of a patent.
Yet patents are not the only means of promoting product commercialization. Newly approved drugs and biologics, for example, receive a period of market exclusivity during which time no other party can sell a generic or biosimilar version of the product. Newly approved biologics, for instance, receive a twelve-year exclusivity period in the United States. Because of the length of time it takes to get a new biologic approved, the market exclusivity period may exceed the term of any patent an originator company has on its product. A heightened bar to patentability may lead to greater reliance on alternative forms of intellectual property protection such as market exclusivity, prizes, grants, or tax incentives.223
With regards to disclosure, without the ability to receive patent protection, owners of inventive machines may choose not to disclose their discoveries and rely on trade secret protection. However, with an accelerated rate of technological progress, intellectual property holders would run a significant risk that their inventions would be independently recreated by inventive machines.
Depending on the type of innovation, industry, and competitive landscape, business ventures may be successful without patents, and patent protection is
221. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007).
222. Joseph A. DiMasi, Henry G. Grabowski, & Ronald W. Hansen, Innovation in the
Pharmaceutical Industry: New Estimates of R&D Costs, 47 J. OF HEALTH ECON. 20–33 (2016).
223. See generally Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes Debate, 92 TEX. L. REV. 303 (2013) (describing various nontraditional intellectual property
incentives).
  
462
50 66 UCLA L. REV. 2 (2019)
not sought for all potentially patentable inventions.224 In fact, “few industries consider patents essential.”225 For instance, patents are often considered a critical part of biotechnology corporate strategy, but often ignored in the software industry.226 On the whole, a relatively small percentage of firms patent, evenamongfirmsconductingR&D.227 Mostcompaniesdonotconsiderpatents crucial to business success.228 Other types of intellectual property such as trademark, copyright, and trade secret protection, combined with “alternative” mechanisms such as first mover advantage and design complexity may protect innovation even in the absence of patents.229
F. A Changing Innovation Landscape
Inventive machines may result in further consolidation of wealth and intellectual property in the hands of large corporations like Google and IBM. Large enterprises may be the most likely developers of inventive machines due to their high development costs.230 A counterbalance to additional wealth disparity could be broad societal gains. The public would stand to gain access to a tremendous amount of innovation—innovation which might be significantly delayed or never come about without inventive machines. In fact, concerns about industry consolidation are another basis for revising the obviousness inquiry. The widespread use of inventive machines may be inevitable, but raising the bar to patentability would make it so that inventions which would
224. BRONWYN HALL ET AL., INTELLECTUAL PROPERTY OFFICE, THE USE OF ALTERNATIVES TO PATENTS AND LIMITS TO INCENTIVES, 2 (2012), http://webarchive.nationalarchives.gov.uk/ 20140603121456/http://www.ipo.gov.uk/ipresearch-patalternative.pdf; see also, Rochelle Cooper Dreyfuss, Does IP Need IP? Accommodating Intellectual Production Outside the Intellectual Property Paradigm, 31 CARDOZO L. REV. 1437, 1439 (2010); see also David Fagundes, Talk Derby to Me: Intellectual Property Norms Governing Roller Derby Pseudonyms, 90 TEX. L. REV. 1094, 1146 (2012) (describing norm-based protections that function effectively in the absence of traditional IP). Patent holders are only successful in about a quarter of cases that are litigated to a final disposition and appealed. Paul M. Janicke & LiLan Ren, Who Wins Patent Infringement Cases?, 34 AIPLA Q.J. 1, 8 (2006). Fewer than two percent of patents are ever litigated, and only about 0.1 percent go to trial. Lemley & Shapiro, supra note 214, at 79. In cases where the validity of a patent is challenged, about half of the time the patent is invalidated. Allison & Lemley, supra note 20, at 205 (1998).
225. Merges, supra note 19, at 19.
226. See generally, Lemley & Shapiro, supra note 214.
227. Id.
228. Id.
229. Id.
230. See Jamie Carter, The Most Powerful Supercomputers in the World—and What They Do,
TECHRADAR (Dec. 13, 2014), http://www.techradar.com/us/news/computing/the-most- powerfulsupercomputers-in-the-world-and-what-they-do-1276865 (noting that most advanced computer systems are owned by governments and large businesses).
  
463
Everything Is Obvious 51
naturally occur would be less likely to receive protection. To the extent market abuses such as price gouging and supply shortages are a concern, protections are, at least theoretically, built into patent law to protect consumers against such problems.231 For example, the government could exercise its march in rights or issue compulsory licenses.232
Inventive machines may ultimately automate knowledge work and render human researchers redundant. While past technological advances have resulted in increased rather than decreased employment, the technological advances of the near future may be different.233 There will be fewer limits to what machines will be able to do, and greater access to machines. Automation should generate innovation with net societal gains, but it may also contribute to unemployment, financial disparities, and decreased social mobility.234 It is important that policymakers act to ensure that automation benefits everyone, for instance by investing in retraining and social benefits for workers rendered technologically unemployed.235 Ultimately, patent law alone will not determine whether automation occurs. Even without the ability to receive patent protection, once inventive machines are significantly more efficient than human researchers, they will replace people.
CONCLUSION
Prediction is very difficult, especially about the future.236
In the past, patent law has reacted slowly to technological change. For instance, it was not until 2013 that the Supreme Court decided human genes should be unpatentable.237 By then, the Patent Office had been granting patents on human genes for decades,238 and more than 50,000 gene-related patents had been issued.239
231. See Balancing Access, supra note 27 (discussing patent law protections against practices including “evergreening”).
232. See id. at 345 (explaining India’s issuance of a compulsory license).
233. See Should Robots Pay Taxes?, supra note 6; see supra Part I.
234. Id.
235. Id.
236. ARTHUR K. ELLIS, TEACHING AND LEARNING ELEMENTARY SOCIAL STUDIES 56, (1970) (quoting physicist Niels Bohr).
237. Ass’n for Molecular Pathology v. Myriad Genetics, Inc., 133 S. Ct. 2107 (2013).
238. See, e.g., U.S. Patent No. 4,447,538 (filed Feb. 5, 1982) (a patent issued in 1984 which claims
the human Chorionic Somatomammotropin gene).
239. Robert Cook-Deegan & Christopher Heaney, Patents in Genomics and Human Genetics, 11
ANN. REV. OF GENOMICS & HUM. GENETICS 383, 384 (2010) (“In April 2009, the U.S. Patent
  
464
52 66 UCLA L. REV. 2 (2019)
Eminent technologists now predict that artificial intelligence is going to revolutionize the way innovation occurs in the near to medium term. Much of what we know about intellectual property law, while it might not be wrong, has not been adapted to where we are headed. The principles that guide patent law need to be, if not rethought, then at least retooled in respect of inventive machines. We should be asking what our goals are for these new technologies, what we want our world to look like, and how the law can help make it so.
  and Trademark Office (USPTO) granted the 50,000th U.S. patent that entered the DNA Patent Database at Georgetown University. That database includes patents that make claims mentioning terms specific to nucleic acids (e.g., DNA, RNA, nucleotide, plasmid, etc.).”).

465
Should Robots Pay Taxes? Tax Policy in the Age of Automation
Ryan Abbott* & Bret Bogenschneider**
Existing technologies can already automate most work functions, and the cost of these technologies is decreasing at a time when human labor costs are increasing. This, combined with ongoing advances in computing, artificial intelligence, and robotics, has led experts to predict that automation will lead to significant job losses and worsening income inequality. Policy makers are actively debating how to deal with these problems, with most proposals focusing on investing in education to train workers in new job types, or investing in social benefits to distribute the gains of automation.
The importance of tax policy has been neglected in this debate, which is unfor- tunate because such policies are critically important. The tax system incentivizes automation even in cases where it is not otherwise efficient. This is because the vast majority of tax revenues are now derived from labor income, so firms avoid taxes by eliminating employees. Also, when a machine replaces a person, the government loses a substantial amount of tax revenue—potentially hundreds of billions of dol- lars a year in the aggregate. All of this is the unintended result of a system designed to tax labor rather than capital. Such a system no longer works once the labor is capital. Robots are not good taxpayers.
We argue that existing tax policies must be changed. The system should be at least “neutral” as between robot and human workers, and automation should not be allowed to reduce tax revenue. This could be achieved through some combination of disallowing corporate tax deductions for automated workers, creating an “automa- tion tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate.
INTRODUCTION
An automation revolution is underway.1 Current technologies can al- ready mechanize most work activities, and the cost of these technologies is
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine at the David Geffen School of Medicine at University of California, Los Angeles.
** Senior Lecturer (Associate Professor), Finance Law & Ethics, University of Surrey School of Law. Thanks to Daniel Hemel for his insightful comments.
1 See, e.g., BANK OF AMERICA MERRILL LYNCH, ROBOT REVOLUTION: GLOBAL ROBOT & AI PRIMER 3 (Dec. 16, 2015) (on file with the Harvard Law School Library) (“The pace of disruptive technological innovation has gone from linear to parabolic in recent years. Penetra- tion of robots and artificial intelligence (AI) has hit every industry sector, and has become an integral part of our daily lives. Technology has also expanded beyond routine work, and moved into complex problem-solving, and replicating human perception, tasks that only people were capable of.”); see also Relating to the Training and Utilization of the Manpower Resources of the Nation: Hearing Before the Subcomm. on Emp’t and Manpower of the Comm. on Labor and Pub. Welfare, 88th Cong. 1659 (1963) (statement of Isaac L. Auerbach, President, Interna- tional Federation for Information Processing) (“The word ‘automation’ was coined by Delmar S. Harder, then executive vice president of the Ford Motor Co., in attempting to describe the latest kind of assembly line technique involving engine-block transfer machines then being installed at Ford’s River Rouge and Cleveland plants.”). For a definition of the term “automa- tion,” see Meg Leta Jones, The Ironies of Automation Law: Tying Policy Knots with Fair Auto-
  
146 466 Harvard Law & Policy Review [Vol. 12
decreasing at a time when human labor costs are increasing.2 On top of that, ongoing and exponential improvements in computing, artificial intelligence (AI), and robotics are permitting automation in an ever-increasing number of fields.3 As a result, academic and industry experts are widely predicting that automation will result in substantial “technological unemployment” in the near future.4 For instance, the McKinsey Global Institute has claimed that the disruption caused by AI will “happ[en] ten times faster and at 300 times the scale, or roughly 3,000 times the impact,” of the Industrial Revolution.5 We
mation Practices Principles, 18 VAND. J. ENT. & TECH. L. 77, 84 (2015) (“Broadly, automation includes all the ways computers and machines help people perform tasks more quickly, accurately, and efficiently. The term ‘automation’ refers to: (1) the mechanization and integration of the sensing of environmental variables through artificial sensors, (2) data processing and decision making by computers, and (3) mechanical action by devices that apply forces on the environment or information action through communication to people of informa- tion processed. The term encompasses open-loop operations and closed-loop control, as well as intelligent systems.”) (citations omitted). One of the most cited studies on technological unemployment claims that forty-seven percent of American jobs are at high risk of loss due to automation. See Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible are Jobs to Computerisation?, 114 TECH. FORECASTING & SOC. CHANGE 254, 265–66 (2017), https://ac.els-cdn.com/S0040162516302244/1-s2.0-S0040162516302244-main .pdf?_tid=14d233e0-c236-11e7-a741-00000aacb362&acdnat=1509892499_c57668bde931faf 6de11b39073cccfc5 [https://perma.cc/LFE2-2T7A] (“[O]ur findings suggest recent develop- ments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future.”).
2 See Frey & Osborne, supra note 1, at 265–68; but see JAMES MANYIKA ET AL., MCKIN- SEY GLOBAL INST., A FUTURE THAT WORKS: AUTOMATION, EMPLOYMENT, AND PRODUCTIVITY 21 (2017), https://www.mckinsey.com/~/media/McKinsey/Global%20Themes/Digital%20Dis ruption/Harnessing%20automation%20for%20a%20future%20that%20works/MGI-A-future- that-works_Full-report.ashx [https://perma.cc/2F6U-U259] (predicting that fewer than five percent of occupations could be entirely automated with existing technologies).
3 For examples of automation in white-collar and professional settings, see Roger Parloff, Why Deep Learning is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning/ [https://perma.cc/5A6Q- 5U4T]. Of particular concern to future attorneys is that AI is already automating work func- tions in the legal services industry. See, e.g., Jane Croft, Legal Firms Unleash Office Auto- matons, FIN. TIMES, May 16, 2016 at 4 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Dana Remus & Frank Levy, Can Robots Be Lawyers?: Computers, Lawyers, and the Practice of Law, 30 GEO. J. LEGAL ETHICS 501 (2017) (arguing that AI will refocus rather than replace attorneys).
4 See supra note 1. In the 1930s, the economist John Maynard Keynes popularized the term “technological unemployment” to refer to “unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour.” The Future of Jobs: The Onrushing Wave, ECONOMIST (Jan. 18, 2014), https://www .economist.com/news/briefing/21594264-previous-technological-innovation-has-always-deliv- ered-more-long-run-employment-not-less [https://perma.cc/QQ3N-9AWN].
5 Richard Dobbs et al., The Four Global Forces Breaking All the Trends, MCKINSEY GLOBAL INST. (Apr. 2015), https://www.mckinsey.com/business-functions/strategy-and-corpo- rate-finance/our-insights/the-four-global-forces-breaking-all-the-trends [https://perma.cc/ LC89-B23C] (excerpting RICHARD DOBBS ET AL., NO ORDINARY DISRUPTION: THE FOUR GLOBAL FORCES BREAKING ALL THE TRENDS (2015)); see also JAMES MANYIKA ET AL., MCK- INSEY GLOBAL INST., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013), https://www.mckinsey.com/~/media/McKin sey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Disruptive%20technolo gies/MGI_Disruptive_technologies_Full_report_May2013.ashx [https://perma.cc/EV85- WHVG] (predicting also trillions of dollars in economic impact by 2025 from advanced robot- ics, 3D printing and autonomous vehicles).
 
467
2018] Should Robots Pay Taxes? 147
are entering an era in which the combined impact of technological improve- ments in many different areas is going to be profoundly transformative—and disruptive.6
Automation has the potential to create widespread benefits. Not only will automation increase productivity, it will also improve safety and lead to new scientific breakthroughs.7 But without oversight, automation will also exacerbate unemployment and economic inequality.8 Even if workers ren- dered technologically unemployed are able to transition to new jobs, as has been the case during previous eras of rapid change, there will still be signifi- cant short-term disruptions. Moreover, many experts are predicting that to- day’s technological advances are different in kind from those of the past, and that large-scale permanent increases in unemployment are inevitable.9 In 1990, the three largest companies in Detroit with a combined market capital- ization of $36 billion employed 1.2 million workers.10 In 2014, the three
6 See, e.g., HERRING KAGERMANN, ET AL., INDUSTRIE 4.0 WORKING GRP., RECOMMENDA- TIONS FOR IMPLEMENTING THE STRATEGIC INITIATIVE INDUSTRIE 4.0, at 5 (2013), http://www .acatech.de/fileadmin/user_upload/Baumstruktur_nach_Website/Acatech/root/de/Material_fuer _Sonderseiten/Industrie_4.0/Final_report__Industrie_4.0_accessible.pdf [https://perma.cc/ PA7X-YSRE] (“The first three industrial revolutions came about as a result of mechanisation, electricity and IT. Now, the introduction of the Internet of Things and Services into the manu- facturing environment is ushering in a fourth industrial revolution.”); see also VERNOR VINGE, THE COMING TECHNOLOGICAL SINGULARITY: HOW TO SURVIVE IN THE POST-HUMAN ERA (1993) https://edoras.sdsu.edu/~vinge/misc/singularity.html [https://perma.cc/K4C9-LRDE] (coining the term “singularity” to refer to the argument that “we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence.”).
7 See generally Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. (forthcoming 2018) (discussing the potential of automation to result in substantial safety benefits, for instance in the transportation industry); see also Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 1079 B.C. L. REV. 1083–91 (2016) (discussing examples in which AI has generated patentable subject matter under circumstances in which the computer rather than a person has qualified for inventorship).
8 See COMM. ON TECH., NAT’L SCI. & TECH. COUNCIL, PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE 2 (2016) [hereinafter COMM. ON TECH.], https://obamawhitehouse .archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_fut ure_of_ai.pdf [https://perma.cc/DEH5-AQBK].
9 See Klaus Schwab & Richard Samans, Preface to WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION, at v–vi (2016), http://www3.weforum.org/docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL]; see also Brian Dorini, The End of Work: The Decline of the Global Labor Force and the Dawn of the Post-Market Era, 9 HARV. J.L. & TECH. 231, 232–33 (1995) (reviewing JEREMY RIFKIN, THE END OF WORK: THE DECLINE OF THE GLOBAL LABOR FORCE AND THE DAWN OF THE POST-MARKET ERA 136–43 (1995)) (“The ranks of the unem- ployed are swelling with former service sector workers, such as secretaries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recognition devices, elec- tronic mail, and inventory control and monitoring devices.”) (citation omitted).
10 See Michael Chui & James Manyika, Digital Era Brings Hyperscale Challenges, FIN. TIMES (Aug. 13, 2014), https://www.ft.com/content/f30051b2-1e36-11e4-bb68-00144feabdc0 [http://perma.cc/4QHG-ZKDL].
 
148 468 Harvard Law & Policy Review [Vol. 12
largest companies in Silicon Valley with a combined market capitalization of $1.09 trillion employed 137,000 workers.11
These are not new problems.12 In 1962, President Kennedy stated, “I regard it as the major domestic challenge, really, of the sixties, to maintain full employment at a time when automation, of course, is replacing men.”13 His solution was to pass the nation’s first and most sweeping federal pro- gram to train workers unemployed due to technological advances.14 More recently, in December 2016, the Executive Office of the President issued a report which outlined a three-pronged policy response to automation and AI, namely, to: (i) “[i]nvest in and develop AI for its many benefits,” (ii) “[e]ducate and train Americans for jobs of the future,” and, (iii) “[a]id workers in the transition and empower workers to ensure broadly shared growth.”15 These and other proposals for dealing with automation have fo- cused on improving education and improving social benefit systems. Con- cerns about technological unemployment have even breathed new life into an old social benefit proposal—guaranteed minimum income, which could involve the government making fixed payments to each of its citizens re- gardless of their circumstances.16 While education reform often enjoys bipar- tisan support, enhanced social benefits are a politically challenging goal since liberals and conservatives often disagree on their desirability.17 In any
11 See id.
12 See generally JOHN FORBES DOUGLAS, SOME EVIDENCES OF TECHNOLOGICAL UNEM- PLOYMENT IN ANCIENT ATHENS AND ROME (1932). For instance, the Roman Emperor Vespa- sian once refused to use a labor-saving transportation machine, famously stating, “You must allow my poor hauliers to earn their bread.” See Steve Welch, The Real Political Divide is Education, TECH CRUNCH (Dec. 30, 2016), https://techcrunch.com/2016/12/30/the-real-politi- cal-divide-is-education/ [https://perma.cc/EL6C-JKAQ].
13 John F. Kennedy, The President’s News Conference, AM. PRESIDENCY PROJECT (Feb. 14, 1962), http://www.presidency.ucsb.edu/ws/index.php?pid=9003 [https://perma.cc/2L35- QTT7].
14 See Gladys Roth Kremen, MDTA: The Origins of the Manpower Development and Training Act of 1962, U.S. DEP’T OF LAB. (1974), www.dol.gov/general/aboutdol/history/ mono-mdtatext [https://perma.cc/KFC7-MPCV] (describing the law’s origins). Also of note, in 1961 (a year before the MDTA), the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. See id. For reviews of automation issues in the 1960s, see JAMES L. SUNDQUIST, POLITICS AND POLICY: THE EISENHOWER, KENNEDY, AND JOHNSON YEARS 77 (1968).
15 EXEC. OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY 3 (2016) [hereinafter ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECON- OMY], https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial- Intelligence-Automation-Economy.pdf [https://perma.cc/LK89-E5RG].
16 Guaranteed minimum income was proposed during the Industrial Revolution by Charles Fourier, and then later Joseph Charlier, before being adopted by John Stuart Mill. See Philippe Van Parijs, A Basic Income for All, BOS. REV. (2000), bostonreview.net/forum/ubi-van-parijs [https://perma.cc/6E52-Q63K]. (According to Mill’s proposal, “[A] certain minimum is first assigned for the subsistence of every member of the community, whether capable or not of labour.”).
17 See Yvonne A. Stevens, The Future: Innovation and Jobs, 56 JURIMETRICS J. 367, 373 (2016) (“One of the most commonly considered government payout schemes is what is re- ferred to as a basic income guarantee (BIG). Generally speaking, BIG is a monetary govern- ment-backed and issued guarantee such that all adults have access to an amount of money necessary to meet basic needs.”). President Richard Nixon also once proposed a guaranteed
 
469
2018] Should Robots Pay Taxes? 149
event, both education and social benefit reforms to deal with automation would require significant financial support.18
While there has been a lively public discourse on technological unem- ployment and income disparity, the automation debate has historically ig- nored the issue of taxation. That has very recently started to change. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing con- cerns of stifling innovation.19 The next day, Bill Gates stated that he thought governments should tax companies’ use of robots to slow the spread of auto- mation and to fund other types of employment.20 Former U.S. Secretary of the Treasury Lawrence Summers then claimed Gates’s argument was “pro- foundly misguided.”21 In August 2017, South Korea announced plans for the world’s first “tax on robots” by limiting tax incentives for automated ma- chines.22 Currently, Korean businesses may deduct three to seven percent of an investment in automation equipment from their corporate taxes, depend- ing on the size of their operation.23 The announced reform would decrease the deduction rate by up to two percent.24
basic income of about $10,000 in today’s dollars for families of four. This proposal, the Family Assistance Plan, passed through the House before it was voted down by Senate Democrats. See Whitney Mallett, The Town Where Everyone Got Free Money, VICE: MOTHERBOARD (Feb. 4, 2015), https://motherboard.vice.com/en_us/article/nze99z/the-mincome-experiment-dauphin [https://perma.cc/R7E7-3NTL].
18 For example, in 2016, Switzerland voted down proposed guaranteed minimum income legislation that would have provided each citizen with about $30,000 a year. The cost of the legislation was estimated at about $200 billion, about three times Switzerland’s current annual federal spending. See John Thornhill & Ralph Atkins, Universal Basic Income: Money for Nothing, FIN. TIMES.COM (May 26, 2016), https://www.ft.com/content/7c7ba87e-229f-11e6- 9d4d-c11776a5124d [https://perma.cc/GR78-WG5H]. In the United Kingdom, it was esti- mated that distributing the current total welfare spending of £251 billion to 64.5 million per- sons as a universal basic income would result in a monthly payment to all residents of just £324. See Jim Edwards & Will Heilpern, Here’s How Much We’d All Get if the UK Introduced a ‘Fiscally Neutral’ Universal Basic Income Scheme, BUS. INSIDER (June 6, 2016, 10:25 AM), http://www.businessinsider.com/universal-basic-income-scheme-for-the-uk-2016- 6?r=UK&IR=T [https://perma.cc/4YRW-35G9]. This analysis is overly simplified, but dem- onstrates that providing a meaningful level of social benefits on a widespread basis requires significant funding.
19 See Reuters Staff, European Parliament Calls for Robot Law, Rejects Robot Tax, REUTERS (Feb. 16, 2007, 2:03 PM), http://ca.reuters.com/article/technologyNews/idCAKBN15 V2KM [https://perma.cc/5KTN-6VTJ].
20 See Kevin J. Delaney, The Robot That Takes Your Job Should Pay Taxes, Says Bill Gates, QUARTZ (Feb. 17, 2017), https://qz.com/911968/bill-gates-the-robot-that-takes-your- job-should-pay-taxes/ [https://perma.cc/6SHD-L7WY] (“Exactly how you’d do it, measure it, you know, it’s interesting for people to start talking about now.”).
21 Sarah Kessler, Lawrence Summers Says Bill Gates’ Idea for a Robot Tax is “Profoundly Misguided”, QUARTZ (Mar. 6, 2017), https://qz.com/925412/lawrence-summers-says-bill- gates-idea-for-a-robot-tax-is-profoundly-misguided/ [https://perma.cc/ATV3-DXEG].
22 See Cara McGoogan, South Korea Introduces World’s First ‘Robot Tax’, TELEGRAPH: TECH (Aug. 9, 2017, 12:54 PM), http://www.telegraph.co.uk/technology/2017/08/09/south-ko- rea-introduces-worlds-first-robot-tax/ [https://perma.cc/H93H-RPMC].
23 See Yoon Sung-won, Korea Takes First Step to Introduce ‘Robot Tax’, KOREA TIMES (Aug. 7, 2017, 8:47 PM), http://www.koreatimes.co.kr/www/news/tech/2017/08/133_234312 .html [https://perma.cc/82WW-B4QL].
24 See id.
 
150 470 Harvard Law & Policy Review [Vol. 12
The critical importance of tax policies on automation has not been ap- preciated. The current system encourages automation by providing employ- ers with preferential tax treatment for robot workers. Automation allows firms to avoid employee and employer wage taxes levied by federal, state, and local taxing authorities. It also permits firms to claim accelerated tax depreciation on capital costs for automated workers, and it creates a variety of indirect incentives for machine workers. All of this is the unintended re- sult of a tax system designed to tax labor rather than capital. Tax policies may thus result in automation in some cases in which a firm would other- wise choose a human worker.
Even more concerning, automation significantly reduces the govern- ment’s tax revenue since most tax revenue comes from labor-related taxes.25 When firms replace employees with machines, the government loses income due to taxation. A very rough estimate of revenue loss can be arrived at by multiplying an effective tax rate by the gross salary loss due to automation. In January 2017, the McKinsey Global Institute claimed that about half of current work activities could be automated using currently demonstrated technologies, which would eliminate $2.7 trillion in annual wages in the United States alone.26 Workers pay high effective tax rates ranging from twenty-five percent to fifty-five percent when all tax types are taken into account.27 This suggests that worker automation could result in hundreds of billions or even trillions of dollars in tax revenue lost per year at various levels of government.28
In the United States and most other developed nations, the bulk of taxes are currently remitted by workers either through wage withholding, taxation of labor income, or indirect taxation of workers as consumers.29 Since robots are not subject to these types of tax regimes, automation reduces the overall tax base. Robots are simply not taxpayers, at least not to the same extent as human workers. If all workers were to be replaced by machines tomorrow,
25 See OFFICE OF MGMT. & BUDGET, EXEC. OFFICE OF THE PRESIDENT, FISCAL YEAR 2015 HISTORICAL TABLES: BUDGET OF THE U.S. GOVERNMENT 32–33 tbl.2.1 (2015), https://www .gpo.gov/fdsys/pkg/BUDGET-2015-TAB/pdf/BUDGET-2015-TAB.pdf [https://perma.cc/ TT33-T3HA] (showing that individual income taxes, Social Security taxes, Medicare taxes, and other taxes assessed on labor wages comprised more than fifty percent of overall revenue); I.R.S., PUB. NO. 55B, DATA BOOK, 2014, at 3 tbl.1 (2015), https://www.irs.gov/pub/irs-pdf/ p55b.pdf [https://perma.cc/Q4YU-GUFD]; CONG. BUDGET OFFICE, DISTRIBUTION OF HOUSE- HOLD INCOME AND FEDERAL TAXES, 2010 (2013), https://www.cbo.gov/sites/default/files/ 113th-congress-2013-2014/reports/44604-AverageTaxRates.pdf [https://perma.cc/GH9J- YNQW]; see also Lester B. Snyder & Marianne Gallegos, Redefining the Role of the Federal Income Tax: Taking the Tax Law “Private” Through the Flat Tax and Other Consumption Taxes, 13 AM. J. TAX POL’Y 1, 86 (1996).
26 MANYIKA ET AL., supra note 2, at 6 exhibit E3.
27 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level, 145 TAX NOTES 117, 117 tbl.1 (2014).
28 See MANYIKA ET AL., supra note 2, at 5; see also Frey & Osborne, supra note 1, at 267 (asserting that many creative science, engineering, and general knowledge work jobs will be done by computers in the long run).
29 See REVENUE STATISTICS - OECD COUNTRIES: COMPARATIVE TABLES, ORG. FOR ECON. CO-OPERATION & DEV. (2016), http://stats.oecd.org/Index.aspx?DataSetCode=REV [https:// perma.cc/74EJ-2KS8].
 
471
2018] Should Robots Pay Taxes? 151
most of the tax base would immediately disappear. As a matter of taxation, automated workers represent a type of capital investment, and capital in- come is currently taxed at much lower rates than labor income.30 This is not accidental; it is based on the historic belief that the taxation of labor income is more efficient than the taxation of capital income. This concept is dis- cussed in tax policy analysis as the “tax incidence” of capital taxation.31
Tax is thus critically important to the automation debate. Tax policies should not encourage automation unless it is part of a deliberate strategy based on sound public policy. We believe the solution is to adjust the tax system to be at least neutral as between robot and human workers.32 More ambitiously, changes to tax policies are necessary to account for the loss of government tax revenue due to automation. This is particularly critical be- cause the education and social benefit reform necessitated by automation will only be possible with more, not less, tax revenue.
This article outlines several potential tax policy solutions to address the automation revolution. Tax “neutrality” between human and automated workers could be achieved through some combination of disallowing corpo- rate tax deductions for automated workers, creating an “automation tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate. Neutrality in this setting refers to a system in which various alternatives are taxed equally, and so actors make decisions based on non-tax reasons.
Tax neutrality is widely accepted as an economically efficient principle for organizing a tax system.33 Neutral taxes are more likely to have fewer negative effects, lower administration and compliance costs, promote distri-
30 The term “capital taxation” refers here to corporate income taxation. For a comparison of effective tax rates between U.S. and EU multinationals, see Reuven S. Avi-Yonah & Yaron Lahav, The Effective Tax Rate of the Largest U.S. and EU Multinationals, 65 TAX L. REV. 375 (2012).
31 In a strange twist of economic theory, the ultimate cost of wage taxation paid by work- ers is generally thought to be borne by capital. See Arnold C. Harberger, Tax Policy in a Small, Open Developing Economy, in THE ECONOMICS OF THE CARIBBEAN BASIN 1 (Michael B. Con- nolly & John McDermott eds., 1985). For the extension of the “small open economy” model beyond the small open economy context, see A. Lans Bovenberg, Capital Income Taxation in Growing Open Economies, 31 J. PUB. ECON. 347 (1986); Anne Sibert, Taxing Capital in a Large, Open Economy, 41 J. PUB. ECON. 297 (1990); Alan J. Auerbach, Who Bears the Corpo- rate Tax? A Review of What We Know, 20 TAX POL’Y & ECON. 1 (2006).
32 See WILLIAM MEISEL, THE SOFTWARE SOCIETY: CULTURAL AND ECONOMIC IMPACT 226 (2013) (“There are other alternatives using the tax code. One option suggested by Martin Ford in The Lights in the Tunnel is modification of the payroll tax, a tax that discourages hiring people and encourages automation since it makes the use of people more expensive. He sug- gests a reform of the tax system where we get away from taxing based on workers to reduce the disincentive to hiring.”) (citing MARTIN FORD, THE LIGHTS IN THE TUNNEL: AUTOMATION, ACCELERATING TECHNOLOGY, AND THE ECONOMY (2009)).
33 See Tax: Fundamentals in Advance of Reform: Hearing Before the S. Comm. on Fin., 110th Cong. 41–50 (2008) (prepared statement of Jason Furman, Senior Fellow and Director of The Hamilton Project, The Brookings Institution) [hereinafter Prepared Statement of Jason Furman], https://www.finance.senate.gov/imo/media/doc/56020.pdf [https://perma.cc/Y98J- RN8K].
 
152 472 Harvard Law & Policy Review [Vol. 12
butional fairness, and increase transparency.34 Tax neutrality can thus result in a broader tax base with lower rates.35 Non-neutralities in the tax system distort choices and behavior other than for economic reasons, and encourage socially wasteful efforts to reduce tax payments.36 They can thus “create complexity, encourage avoidance, and add costs for both taxpayers and governments.”37
However, non-neutral taxes can be used deliberately to advance social policy—for instance, incentivizing activities like medical research, educa- tion, and homeownership.38 Taxes may also be used to disincentivize certain activities, as so-called “Pigouvian” taxes. For instance, consumer goods such as alcoholic beverages and tobacco products bear an exceptional tax burden. In turn, this results in increased consumer costs, with the goal of decreasing consumption—but due to taxes rather than to other market and economic factors.
The advantage of tax neutrality as between human and automated work- ers is that it permits the marketplace to adjust without tax distortions. With a level playing field, firms should only automate if it will be more efficient, without taking taxes into account. Since the current tax system favors auto- mated workers, a move toward a neutral tax system could increase the ap- peal of human workers. Policy solutions could even be implemented to make human workers more appealing than machines in terms of tax costs and ben- efits, to the extent policy makers choose to discourage automation.
The remainder of this article is divided into three parts. Part I discusses the phenomenon of automation and provides historical background on ef- forts to deal with its harmful effects. Part II analyzes current tax policies and contends that they promote automation even where it would not otherwise be efficient. Finally, Part III argues that changes to tax policy are needed to prevent the unintended consequences of encouraging automation and to off- set the government’s loss of tax revenue. We provide several potential solu- tions for achieving these goals.
The increased tax revenue from our proposal could be used to provide improved education and training for workers rendered unemployed by robots and computers. Should the pessimistic prediction of a near future with sub- stantially increased unemployment due to automation manifest, these taxes could also support social benefit programs such as a guaranteed minimum income. Automation will likely generate more wealth than has ever been possible. It should not come at the expense of the most vulnerable.
34 See JAMES MIRRLEES ET AL., INST. FOR FISCAL STUDIES, TAX BY DESIGN 22–23 (2011), https://www.ifs.org.uk/docs/taxbydesign.pdf [https://perma.cc/JSU8-KS5Q].
35 See Prepared Statement of Jason Furman, supra note 33, at 33.
36 See MIRRLEES ET AL., supra note 34, at 40.
37 Id. at 41.
38 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents–Prizes De-
bate, 92 TEX. L. REV. 303 (2013).
 
473
2018] Should Robots Pay Taxes? 153
I. THE PROBLEM WITH AUTOMATION
A. Automation is Coming
Experts are widely predicting that automation is going to have a sub- stantial impact on employment even in the near term. Bank of America Mer- rill Lynch argues that by 2025, AI may eliminate $9 trillion in employment costs by automating knowledge work.39 A report by the World Economic Forum estimates that automation could result in the net loss of 5.1 million jobs by 2020.40 The consulting firm Deloitte claims that thirty-five percent of jobs in the United Kingdom are at high risk of redundancy due to automation in the next ten to twenty years.41 This is due to a combination of factors: improvements in automation technologies, decreased costs for such technol- ogies, and increased labor costs. Whereas it was previously possible to auto- mate a large number of work processes, it has now become practicable. As automation technologies continue to both improve and decrease in cost, it is difficult to think of work functions that will not eventually be susceptible to automation.42
1. The Good: Increased Productivity and New Jobs
Automation increases productivity, which generates value and creates wealth.43 Partly due to technological advances and automation, the U.S. Gross Domestic Product (GDP) has steadily risen from $1.37 trillion in 1960 to $73.5 trillion in 2015.44 Despite academic criticism, GDP has remained the dominant economic indicator of welfare and standard of living for half a century.45
39 BANK OF AMERICA MERRILL LYNCH, supra note 1, at 1 (noting also that AI will yield $14–33 trillion in annual economic impact).
40 See WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION 13 (2016), http://www3.weforum.org/ docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL].
41 DELOITTE, AGILETOWN: THE RELENTLESS MARCH OF TECHNOLOGY AND LONDON’S RE- SPONSE 5 (2014), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/uk-futures/ london-futures-agiletown.pdf [https://perma.cc/Z5HU-PY25].
42 See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 188–91 (Cassidy R. Sugimoto et al. eds., 2016) (noting the ways in which automation technologies could replace workers in the pharmaceutical sciences).
43 See generally Joel Mokyr et al., The History of Technological Anxiety and the Future of Economic Growth: Is This Time Different?, 29 J. ECON. PERSP. 31 (2015).
44 See GDP (Current US$), WORLD BANK: DATA (2016), https://data.worldbank.org/indi- cator/NY.GDP.MKTP.CD [https://perma.cc/C34J-U67E].
45 See, e.g., Jeroen C.J.M. van den Bergh, The GDP Paradox, 30 J. ECON. PSYCHOL. 117, 117–18 (2008) (“Gross domestic product (GDP) is the monetary, market value of all final goods and services produced in a country over a period of a year. The real GDP per capita (corrected for inflation) is generally used as the core indicator in judging the position of the economy of a country over time or relative to that of other countries. The GDP is thus implic- itly, and often even explicitly, identified with social welfare—witness the common substituting phrase ‘standard of living’. . . . For over half a century now, the GDP (per capita) has been
 
154 474 Harvard Law & Policy Review [Vol. 12
Automation can also create new jobs.46 Human workers may be needed to build and maintain automation technologies. Automation may free up cap- ital for investments in new enterprises, result in the creation of new prod- ucts, or decrease production costs for existing products. Decreased production costs may result in lower consumer prices and thus greater con- sumer demand. All of this may increase employment. Technological ad- vances have also historically upgraded the labor force: automation has reduced the need for unskilled workers but increased the need for skilled workers.47 For instance, some of today’s most in-demand occupations did not exist even five years ago.48
2. The Bad: Unemployment and Inequality
Automation can cause under- and un-employment. While worker pro- ductivity has risen robustly since 2000, employment has stagnated.49 This may be due in part to technological advances.50 When a company like Mc- Donald’s introduces computer cashiers, the company may save money and consumers may enjoy lower prices.51 But human cashiers now find them- selves in a more competitive labor market. The enhanced competition may result in lower wages, less favorable employment terms, fewer working
severely criticized as not adequately capturing human welfare and progress. All the same, the GDP has maintained a firm position as a dominant economic indicator. . . .”).
46 The following arguments were referred to as “compensation theory” by Karl Marx, who argued none of these effects were guaranteed and that automation could result in forcing workers into lower paying jobs. See KARL MARX, CAPITAL, VOLUME I: THE PROCESS OF PRO- DUCTION OF CAPITAL 570 (1867).
47 See Automation and Technological Change: Hearing Before the Subcomm. on Econ. Stabilization of the J. Comm. on the Econ. Rep., 84th Cong. 29, 34–35 (Statement of Walter S. Buckingham, Jr., Associate Professor, Georgia Institute of Technology), https://www.jec.sen ate.gov/reports/84th%20Congress/Automation%20and%20Technological%20Change%20-% 20Hearings%20%2875%29.pdf [https://perma.cc/B348-CT38].
48 See WORLD ECON. F., supra note 40, at 3.
49 See, e.g., STEVEN GREENHOUSE, THE BIG SQUEEZE: TOUGH TIMES FOR THE AMERICAN WORKER 3 (2008).
50 See id. at 9.
51 Cf. Ted Goodman, Fight for $15? McDonald’s To Place Automated Ordering Stations At All US Locations, DAILY CALLER (Nov. 18, 2016, 6:44 PM), http://dailycaller.com/2016/11/ 18/fight-for-15-mcdonalds-to-place-automated-ordering-stations-at-all-us-locations [https:// perma.cc/VS4R-X35Y]. Standard economic principles suggest that in a competitive market lower business costs will result in lower consumer prices. See, e.g., Arthur A. Thompson, Jr., Strategies for Staying Cost Competitive, HARV. BUS. REV. (Jan. 1984), https://hbr.org/1984/01/ strategies-for-staying-cost-competitive [https://perma.cc/Y3SR-2WQC]. In fairness, fast food automation has been around since the nineteenth century. See Angelika Epple, The “Automat”: A History of Technological Transfer and the Process of Global Standardization in Modern Fast Food around 1900, 7 FOOD & HISTORY 97, 98 (2009), http://wwwhomes.uni-bielefeld.de/aep- ple/Aufsatz12TheAutomat2009.pdf [https://perma.cc/LZ7F-MSXS] (discussing the restaurant chain “Automat” which opened its first location in 1896). (“One of [Automat’s] highly unique selling features around 1900 was that no waiters were to be seen in the guest room. The Automat of that time was—at first sight—operated by vending machines only. ‘You absolutely help yourself’ was one of its most prominent marketing slogans.”) Id. at 99. The Automat’s technology transferred around the U.S. and Europe and eventually developed into the world’s largest restaurant chain: Horn & Hardart. Id. at 97.
 
475
2018] Should Robots Pay Taxes? 155
hours, reduced hiring, or layoffs.52 As the former CEO of McDonald’s USA famously quipped, “[i]t’s cheaper to buy a $35,000 robotic arm than it is to hire an employee who’s inefficient making $15 an hour bagging French fries. . . .”53 McDonald’s is now expanding its use of automated cashiers throughout the United States and in other countries.54
Also, while automation generates wealth, it does so unevenly. Over the past twenty-five years, partly due to automation technologies, the income share of the top 0.1% has increased substantially.55 The top 0.1% of the U.S. population is now worth about as much as the bottom 90%.56 CEO-to-worker pay ratios have increased a thousand-fold since 1950,57 but overall wages have been stagnant for thirty-five years.58 Increased automation is likely to accelerate these trends. The White House Council of Economic Advisers has predicted that future automation will disproportionately affect lower-wage jobs and less educated workers, causing greater economic inequality.59
Worsening employment coupled with growing income inequality is a recipe for social unrest.60 As physicist Stephen Hawking has warned,
52 See Simon Neville, McDonald’s ties nine out of 10 workers to zero-hours contracts, GUARDIAN (Aug. 5, 2013, 4:13 PM), https://www.theguardian.com/business/2013/aug/05/ mcdonalds-workers-zero-hour-contracts [https://perma.cc/UF3D-D4S4] (noting that 90% of McDonald’s UK workers have no guaranteed hours); see also Stephanie Strom, McDonald’s Introduces Screen Ordering and Table Service, N.Y. TIMES (Nov. 17, 2016), https://www.ny- times.com/2016/11/18/business/mcdonalds-introduces-screen-ordering-and-table-service .html?_r=0 [https://perma.cc/3DZ7-R68J] (reporting that the cost of purchasing and installing eight touch order screens is $56,000).
53 Julia Limitone, Fmr. McDonald’s USA CEO: $35K Robots Cheaper Than Hiring at $15 Per Hour, FOX BUS. (May 24, 2016), http://www.foxbusiness.com/features/2016/05/24/fmr- mcdonalds-usa-ceo-35k-robots-cheaper-than-hiring-at-15-per-hour.html [https://perma.cc/ W65G-697K] (claiming that a $15 minimum wage results in $30,000 a year for a full-time employee).
54 See Ed Rensi, The Ugly Truth About a $15 Minimum Wage, FORBES (Apr. 25, 2016, 6:30 AM), https://www.forbes.com/sites/realspin/2016/04/25/mcdonalds-minimum-wage-real- ity/#1f50a0d93edd [https://perma.cc/TT3E-G5SR]. Automated cashiers are already the “norm” in European countries with high labor costs, and McDonald’s is now experimenting with self-serve McCafe kiosks. See id.
55 See CARL BENEDIKT FREY & MICHAEL OSBORNE, TECHNOLOGY AT WORK: THE FUTURE OF INNOVATION AND EMPLOYMENT 14 (2015) http://www.oxfordmartin.ox.ac.uk/downloads/ reports/Citi_GPS_Technology_Work.pdf [https://perma.cc/YE22-D6AE].
56 See Angela Monaghan, US Wealth Inequality - Top 0.1% Worth as Much as the Bottom 90%, GUARDIAN (Nov. 13, 2014, 7:00 AM), https://www.theguardian.com/business/2014/nov/ 13/us-wealth-inequality-top-01-worth-as-much-as-the-bottom-90 [https://perma.cc/62U8- 6ADA].
57 Elliot Blair Smith & Phil Kuntz, CEO Pay 1,795-to-1 Multiple of Wages Skirts U.S. Law, BLOOMBERG MARKETS (Apr. 30, 2013, 12:01 AM), https://www.bloomberg.com/news/ articles/2013-04-30/ceo-pay-1-795-to-1-multiple-of-workers-skirts-law-as-sec-delays [https:// perma.cc/NNF6-P2X6].
58 See ELISE GOULD, ECONOMIC POLICY INSTITUTE, 2014 CONTINUES A 35-YEAR TREND OF BROAD-BASED WAGE STAGNATION (2015), http://www.epi.org/files/pdf/stagnant-wages-in- 2014.pdf [https://perma.cc/YN3U-9LMJ].
59 COMM. ON TECH., supra note 8, at 2.
60 See Katie Allen, ILO Warns of Rise in Social Unrest and Migration as Inequality Widens, GUARDIAN (Jan. 12, 2017, 4:00 PM), https://www.theguardian.com/business/2017/jan/ 12/ilo-warns-of-rise-in-social-unrest-and-migration-as-inequality-widens [https://perma.cc/ 3DHG-T2WH].
 
156 476 Harvard Law & Policy Review [Vol. 12
“[e]veryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine- owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increas- ing inequality.”61
3. The Ugly: Reduced Tax Remittances
One of automation’s most pronounced and unappreciated effects relates to taxes. Automation substantially reduces tax revenue. Most of the U.S. government’s tax revenue comes from taxes on workers.62 By stating that most tax revenue comes from workers, we refer to the aggregate amount of wage tax, income tax, and indirect taxes levied on income or wages derived from work at all levels of government. Much of the prior tax policy debate focused solely on income taxation by the federal government.63 Of course, a substantial portion of income subject to federal income tax arises from work and falls within our definition of worker taxation. However, the tax policy debate has been misleading since wage taxes are also levied on labor income and comprise more than one-third of federal remittances. Likewise, indirect state taxes are levied on workers. Consequently, by replacing employees with machines, the government loses out on employee and employer wage taxes levied by federal, state, and local taxing authorities. In addition, tax revenue may be further reduced from businesses claiming accelerated tax depreciation on capital outlays for machines and from other tax incentives related to indirect taxation, such as sales tax or value-added tax (VAT) exemptions.64
B. History of the Automation Scare
Fears of the consequences of automation have been expressed since the industrial revolution.65 In 1801, the writer Thomas Mortimer objected to ma- chines, “which are intended almost totally to exclude the labor of the human
61 Akshat Rathi, Stephen Hawking: Robots aren’t just taking our jobs, they’re making soci- ety more unequal, QUARTZ (Oct. 9, 2015), http://qz.com/520907/stephen-hawking-robots- arent-just-taking-our-jobs-theyre-making-society-more-unequal [https://perma.cc/A2BN- VYY5].
62 See OFFICE OF MGMT. & BUDGET, supra note 25, at 32–33 tbl.2.1.
63 See, e.g., CURTIS S. Dubay, THE HERITAGE FOUNDATION, THE RICH PAY MORE TAXES: TOP 20 PERCENT PAY RECORD SHARE OF INCOME TAXES (2009), http://www.heritage.org/pov- erty-and-inequality/report/the-rich-pay-more-taxes-top-20-percent-pay-record-share-income- taxes [https://perma.cc/N97H-VETS].
64 See infra Part III.
65 For that matter, broader social issues related to automation have been discussed since Aristotle’s time. See, e.g., JOHANNES HANEL, ASSESSING INDUCED TECHNOLOGY: SOMBART’S UNDERSTANDING OF TECHNICAL CHANGE IN THE HISTORY OF ECONOMICS 91 (2008) (noting Aristotle’s hope that machines could occupy the place of slaves in a utopian society).
 
477
2018] Should Robots Pay Taxes? 157
race.”66 In 1821, the economist David Ricardo argued that automation would result in inequality, and that “substitution of machinery for human labour, is often very injurious to the interests of the class of labourers. . . . [It] may render the population redundant, and deteriorate the condition of the la- bourer.”67 In 1839, the philosopher Thomas Carlyle more poetically wrote:
[T]he huge demon of Mechanism smokes and thunders, panting at his great task, in all sections of English land; changing his shape like a very Proteus; and infallibly, at every change of shape, over- setting whole multitudes of workmen, as if with the waving of his shadow from afar, hurling them asunder, this way and that, in their crowded march and course of work or traffic; so that the wisest no longer knows his whereabout[s].68
The Industrial Revolution even gave birth to a social movement and group protesting the use of new technologies: the Luddites.69 Luddites were primarily English textile workers who objected to working conditions in the nineteenth century. They believed that automation threatened their liveli- hoods, and they were opposed to the introduction of industrial machinery.70 Some Luddites engaged in violent episodes of machine-breaking, in re- sponse to which the English government made machine-breaking a capital offense.71
The Luddite movement died out, but automation concerns persisted throughout the twentieth century, often flaring during times of rapid techno- logical progress.72 For instance, the debate was revitalized in the 1950s and 1960s with the widespread introduction of office computers and factory ro-
66 THOMAS MORTIMER, LECTURES ON THE ELEMENTS OF COMMERCE, POLITICS, AND FI- NANCES 72 (London, A. Strahan, for T. N. Longman and O. Rees 1801).
67 DAVID RICARDO, ON THE PRINCIPLES OF POLITICAL ECONOMY AND TAXATION 283–84 (Batoche Books 2001) (3d ed. 1821).
68 2 THOMAS CARLYLE, THE WORKS OF THOMAS CARLYLE: CRITICAL AND MISCELLANE- OUS ESSAYS 141–42 (Henry Duff Traill ed., Cambridge Univ. Press 2010) (1899). Thomas Carlyle called the Industrial Revolution “the Mechanical Age.” Id at 59. Carlyle wrote that technology was causing a “mighty change” in their “modes of thought and feeling. Men are grown mechanical in head and in heart, as well as in hand.” Id. at 63.
69 See Richard Conniff, What the Luddites Really Fought Against, SMITHSONIAN MAG. (Mar. 2011), https://www.smithsonianmag.com/history/what-the-luddites-really-fought- against-264412/ [https://perma.cc/98RV-LNJ2].
70 See Ian Coulson, Power, Politics & Protest: The Growth of Political Rights in Britain in the 19th Century: Luddites, NAT’L ARCHIVES (U.K.), https://www.nationalarchives.gov.uk/edu- cation/politics/g3/ [https://perma.cc/96H4-4NAR].
71 See id.; see also Conniff, supra note 69. The “Luddite fallacy” now describes the fear that innovation will have long-term harmful labor effects. See Vivek Wadhwa, Sorry, but the jobless future isn’t a luddite fallacy, WASH. POST (July 7, 2015), https://www.washingtonpost .com/news/innovations/wp/2015/07/07/sorry-but-the-jobless-future-isnt-a-luddite-fallacy/?utm _term=.f52e3687022c [https://perma.cc/5JUA-YPUE].
72 In 1924, Mohandas Karamchand Gandhi wrote, “What I object to, is the craze for ma- chinery, not machinery as such. The craze is for what they call labour-saving machinery. Men go on ‘saving labour’, till thousands are without work and thrown on the open streets to die of starvation.” MOHANDAS K. GANDHI, YOUNG INDIA (1924), reprinted in ALL MEN ARE BROTH- ERS: LIFE AND THOUGHTS OF MAHATMA GANDHI AS TOLD IN HIS OWN WORDS 126 (Krishna Krapilani ed., 1958).
 
158 478 Harvard Law & Policy Review [Vol. 12
bots.73 In his 1960 election campaign, John F. Kennedy suggested that auto- mation offered “hope of a new prosperity for labor and a new abundance for America,” but that it also “carries the dark menace of industrial dislocation, increasing unemployment, and deepening poverty.”74
Despite these concerns, technological advances have historically re- sulted in overall job creation. The computer eliminated jobs, but created jobs for working with information created by computers. The automobile elimi- nated jobs, but created jobs in the motel and fast-food industries. The tractor and other agricultural advances eliminated jobs, but drove job growth in other areas of the economy. In 1900, forty-one percent of the workforce was employed in agriculture.75 In 2000, less than two percent of the employed labor force worked in agriculture.76 Yet this has not translated to a thirty-nine percent increase in unemployment. Even as agriculture-based employment and agriculture’s relative contribution to the GDP decreased, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the GDP increased.77 Indeed, in each era when concerns have been expressed about automation causing mass unemployment, technology has created more jobs than it has destroyed.
C. Is This Time Different?
The automation debate is resurfacing with a vengeance due to recent advances in AI and other automation technologies. Once more, prognosti- cators are divided into two camps: the optimists who claim there will be a net creation of jobs, and the pessimists who predict mass unemployment and growing inequality.78
History favors the optimists.79 They argue that technological advances will generate widespread benefits together with overall job creation. They
73 See Kremen, supra note 14 (“The dawn of the Atomic Age had witnessed the imple- mentation of a new technology that threatened to replace men with machines.”); see also Douglas A. Irwin, Comments, in JAGDISH BHAGWATI & ALAN S. BLINDER, OFFSHORING OF AMERICAN JOBS: WHAT RESPONSE FROM U.S. ECONOMIC POLICY? 79 (Benjamin M. Friedman ed., 2009).
74 Irwin, supra note 73, at 80.
75 See CAROLYN DIMITRI ET AL., U.S. DEP’T OF AGRIC., THE 20TH CENTURY TRANSFORMA- TION OF U.S. AGRICULTURE AND FARM POLICY 2 (June 2005), https://www.ers.usda.gov/ webdocs/publications/44197/13566_eib3_1_.pdf?v=41055 [https://perma.cc/FRJ7-V3QA].
76 See id.
77 Id.; see also JULIAN M. ALSTON ET AL., PERSISTENCE PAYS: U.S. AGRICULTURAL PRO- DUCTIVITY GROWTH AND THE BENEFITS FROM PUBLIC R&D SPENDING 43, 105 (2010).
78 See Schwab & Samans, supra note 9, at v–vi; see also Dorini, supra note 9, at 233 (“The ranks of the unemployed are swelling with former service sector workers, such as secre- taries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recog- nition devices, electronic mail, and inventory control and monitoring devices.”) (citation omitted).
79 See John Maynard Keynes, Economic Possibilities for our Grandchildren, in ESSAYS IN PERSUASION 321–32 (Palgrave Macmillan 2010) (1930) (predicting that the combination of technological innovation and capital accumulation will eventually solve the problem of mate- rial needs).
 
479
2018] Should Robots Pay Taxes? 159
also argue that current unemployment may relate more to globalization and offshoring than to technology, and that any future technological unemploy- ment would be “only a temporary phase of maladjustment.”80
But there is reason to think that this time may be different.81 Computers are improving exponentially, and there are fewer limits to what they can do than ever before. Computers can replace low-skilled workers and manual laborers as well as white-collar workers and professionals in a variety of fields. Computers are already working as doctors, lawyers, artists, and in- ventors.82 All of this is occurring at a time when labor costs are rising and computer costs are declining. In 2012, Vinod Khosla, the co-founder of Sun Microsystems, predicted that diagnostic software would take the jobs of eighty percent of physicians in the next twenty years.83
While the optimists and pessimists disagree about automation’s effects on long-term unemployment, both agree it causes short-term job losses and industry-specific disruption. During past episodes of widespread automation and technological change, it took decades to develop new worker skill sets on a significant scale and to build new job markets.84 Although the Industrial Revolution ultimately resulted in net job creation, it also resulted in periods of mass unemployment and human suffering. In the coming “Automation Revolution,” whether there are detrimental long-term effects, there will al- most certainly be significant short-term disruptions.85
80 Id. at 325; see also 1 JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY 97 (Cosimo Classics 2006) (1848).
81 See, e.g., Stevens, supra note 17, at 368–69 (“This time there may be some distinctions requiring widespread and perhaps novel solutions, unlike other periods in history.”).
82 See Parloff, supra note 3; see also Yonghui Wu et al., Google’s Neural Machine Trans- lation System: Bridging the Gap between Human and Machine Translation, CORNELL U. LIBR.: ARXIV 20 (Oct. 8, 2016), arxiv.org/abs/1609.08144 [https://perma.cc/KGU8-9RRB] (claiming that the Google Neural Machine Translation system is approaching human-level accuracy); see also Croft, supra note 3 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Remus & Levy, supra note 3 (arguing that AI will refocus rather than replace attorneys).
83 See Liat Clark, Vinod Khosla: Machines Will Replace 80 Percent of Doctors, WIRED UK (Sept. 4, 2012), http://www.wired.co.uk/article/doctors-replaced-with-machines [https:// perma.cc/QNL8-WP4M].
84 See Schwab & Samans, supra note 9, at 20.
85 For example, a substantial number of transportation workers are likely to be displaced by self-driving vehicles, and about three percent of the population is employed in the transpor- tation industry. See Richard Henderson, Industry Employment and Output Projections to 2024, BUREAU OF LAB. STAT.: MONTHLY LAB. REV. tbl. 1 (Dec. 2015), https://www.bls.gov/opub/ mlr/2015/article/industry-employment-and-output-projections-to-2024.htm [https://perma.cc/ 54FB-LDMM]. Tesla, for example, plans to make all its vehicles self-driving. See Tesla to Make All Its New Cars Self-Driving, BBC NEWS: TECH. (Oct. 20, 2016), http://www.bbc.co.uk/ news/technology-37711489 [https://perma.cc/DS4X-YYM2]. Tesla is only one of many com- panies developing such technologies. See 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehi- cles-corporations-list/ [https://perma.cc/4YNE-KDTZ]; see also Investment Into Auto Tech On Pace To Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/ blog/auto-tech-funding-h1-2016/ [https://perma.cc/HY5A-XGFH]. Elon Musk, the CEO of Tesla, has even claimed that self-driving cars will be so much safer than human drivers that there will need to be a future ban on human driving. See Stuart Dredge, Elon Musk: Self- driving Cars Could Lead to Ban on Human Drivers, GUARDIAN (Mar. 18, 2015, 3:22 AM),
 
160 480 Harvard Law & Policy Review [Vol. 12 D. Automation Social Policy
It is important that policy makers act to ensure that automation benefits everyone. Our policy goal should be to accommodate and even encourage advances that promote economic value, while redistributing benefits to those negatively affected. In the midst of the Industrial Revolution, the philoso- pher John Stuart Mill wrote that while automation would ultimately benefit laborers:
this does not discharge governments from the obligation of allevi- ating, and if possible preventing, the evils of which this source of ultimate benefit is or may be productive to an existing genera- tion. . . . [T]here cannot be a more legitimate object of the legisla- tor’s care than the interests of those who are thus sacrificed to the gains of their fellow-citizens and of posterity.86
Or, as the U.S. National Science and Technology Council Committee on Technology argued in 2016:
Public policy can address these risks, ensuring that workers are retrained and able to succeed in occupations that are complemen- tary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly, and assure that AI responsibly ushers in a new age in the global economy.87
Efforts to alleviate the harms and share the benefits of automation have focused on education and social benefits. As mentioned earlier, in December 2016, the Executive Office of the President, then under Barack Obama, is- sued a report which outlined policy responses to AI and automation, namely: to invest in AI, educate and train Americans for future jobs, and transition workers to ensure widespread benefits.88 In terms of education, it is thought that technologically unemployed workers need retraining to transition to new job types. Historically, numerous government and industry programs have combated technological unemployment with education.89 The nation’s first and most sweeping federal training program, the Manpower Development and Training Act of 1962, was signed into law by President Kennedy to train workers unemployed due to technological advances and automation.90 More
https://www.theguardian.com/technology/2015/mar/18/elon-musk-self-driving-cars-ban- human-drivers [https://perma.cc/5CPB-PVHS].
86 MILL, supra note 80, at 98.
87 COMM. ON TECH., supra note 8, at 2.
88 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3. 89 A particularly interesting example is the Armour Meat Packing Company, which cre-
ated “a special ‘automation fund’ for retraining purposes. The company paid a 14-cent levy into the fund, established in 1959, for every 100 tons of meat shipped, up to $500,000, to pay for retraining operations.” Kremen, supra note 14.
90 Id. Also of note, a year earlier, the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. Id. For extensive reviews of automation issues in the 1960s, see generally OFFICE OF MAN-
 
481
2018] Should Robots Pay Taxes? 161
recently, President Obama provided billions of dollars to fund worker train- ing in part to address technological unemployment.91 More ambitiously, he proposed a plan to make two years of community college free for “responsi- ble students” in his 2015 State of the Union Address, although this proposal was never adopted.92
As the third prong of President Obama’s 2016 strategy report notes, social benefit investments are also critical.93 The report advocates strength- ening the social safety net through greater investments in programs such as unemployment insurance and Medicaid.94 It also proposes the creation of new programs for wage insurance and emergency aid.95 In addition, it argues for building a twenty-first century retirement system, expanding health care access, and increasing worker bargaining power.96 President Trump’s admin- istration does not appear to have announced a policy response to AI and automation.97
POWER, AUTOMATION, & TRAINING, U.S. DEP’T OF LABOR, UNEMPLOYMENT AND RETRAINING: AN ANNOTATED BIBLIOGRAPHY OF RESEARCH (1965), https://babel.hathitrust.org/cgi/pt?id= umn.31951p010922940;view=1up;seq=1 [https://perma.cc/BZK8-3UYR]; see also SUND- QUIST, supra note 14, at 77.
91 See Press Release, The White House Office of the Press Sec’y, Fact Sheet: President Obama Proposes New ‘First Job’ Funding to Connect Young Americans with Jobs and Skills Training to Start Their Careers (Feb. 4, 2016), www.whitehouse.gov/the-press-office/2016/02/ 04/fact-sheet-president-obama-proposes-new-first-job-funding-connect-young [https://perma .cc/CV2T-SGB3].
92 John Morgan, Barack Obama Free Community College Plan Backed by $100M Fund- ing, TIMES HIGHER EDUC. (Apr. 27, 2016), www.timeshighereducation.com/news/barack- obama-free-community-college-plan-backed-by-one-hundred-million-dollar-funding [https:// perma.cc/5NTN-P3ZP].
93 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3–4. 94 See id.
95 See id. at 4.
96 See id.
97 Treasury Secretary Steve Mnuchin stated in March 2017 when asked about technologi- cal unemployment that, “In terms of artificial intelligence taking over American jobs, I think we’re . . . so far away from that that [it’s] not even on my radar screen . . . . I think it’s 50 or 100 more years.” Interview on Health Care and Tax Reform with Steven Mnuchin, Treasury Secretary, at 33:30–47, C-SPAN (Mar. 24, 2017), https://www.c-span.org/video/?425894-1/ treasury-secretary-steven-mnuchin-talks-axios-founder-mike-allen.&start=1992 [https://per ma.cc/6KYR-A82G]. By contrast, Larry Summers, the Obama administration’s first director of the National Economic Council, predicted that AI could result in about “a third of men be- tween the ages of 25 and 54 not working by the end of this half century.” Christopher Mat- thews, Summers: Automation is The Middle Class’ Worst Enemy, AXIOS, https://www.axios .com/summers-automation-is-the-middle-class-worst-enemy-1513302420-754facf2-aaca-478 8-9a41-38f87fb0dd99.html (last visited Jan. 7, 2018) [https://perma.cc/2UEA-PVU4]. Of note, China appears be adopting the findings of the White House strategy. On July 20, 2017, China’s State Council released its Next Generation Artificial Development Plan which adopts many of the policies proposed in the White House strategy. CHINA STATE COUNCIL, STATE COUNCIL NOTICE ON THE ISSUANCE OF THE NEXT GENERATION ARTIFICIAL INTELLIGENCE DE- VELOPMENT PLAN (Rogier Creemers et al. trans., 2017), https://www.newamerica.org/cyber- security-initiative/blog/chinas-plan-lead-ai-purpose-prospects-and-problems/ [https://perma .cc/5F3L-YE9K]. The plan argues that AI will be foundational to future economic growth and military dominance, and calls for China to surpass other nations in AI technology by 2030. See generally id.
 
162 482 Harvard Law & Policy Review [Vol. 12
Revitalized concerns about technological unemployment have breathed new life into an old social benefit proposal—guaranteed minimum income.98 The basic idea is that the government would provide a fixed amount of money to its citizens regardless of their situation. This has been implemented numerous times on a relatively small scale, most recently in Finland.99 In 2017, Finland began a pilot program to give about $600 per month to 2,000 unemployed citizens, with no other requirements.100 Proponents argue this will reduce unemployment, poverty, and disincentives for the unemployed to work (as under conventional unemployment schemes recipients generally lose their unemployment benefits after returning to work).101 It might also encourage education by providing support for a period of training. Critics have argued that a guaranteed minimum income will encourage recipients to remain unemployed and discourage additional education.102 In any case, Fin- land plans to eventually replace earnings-based insurance benefits with a basic income.103 Y Combinator, the Silicon Valley start-up incubator, has plans to launch a similar private program in Oakland, California.104
Improving education and social benefit systems will not be easy. Liber- als and conservatives alike can agree on the desirability of improving worker training as it will enlarge the productive labor force, but “[d]elivering this education and training will require significant investments.”105 Enhancing the social benefit system will also require significant investment, but such a goal is even more challenging because liberals and conservatives generally disagree that enhanced benefits are a desirable aim.106
That automation creates a need for greater government investment is well known, but what has so far been largely ignored in the automation debate is that automation will make it far more difficult for the government to make investments once tax revenues are reduced.
98 See supra note 16 and accompanying text.
99 See Mallett, supra note 17.
100 See Kevin Lui, Finland is Giving Nearly $600 a Month to 2,000 Jobless Citizens, No
Questions Asked, FORTUNE (Jan. 3, 2017, 1:26 AM), amp.timeinc.net/fortune/2017/01/03/fin- land-universal-basic-income-experiment/?source=dam [https://perma.cc/BFY3-JX2L]. It is also worth noting that the U.S. has operated a guaranteed basic income since 1999. The Alaska Permanent Fund pays each person who has lived the past year in Alaska $1,680. See Van Parijs, supra note 16.
101 See supra note 16 and accompanying text.
102 See id.
103 See Lui, supra note 100.
104 See Chris Weller, The Inside Story of One Man’s Mission to Give Americans Uncondi-
tional Free Money, BUS. INSIDER UK (June 27, 2016, 1:07 PM), uk.businessinsider.com/in- side-y-combinators-basic-income-project-2016-6?r=US&IR=T [https://perma.cc/48QT- H66H].
105 COMM. ON TECH., supra note 8, at 3.
106 See supra note 17 and accompanying text.
 
483
2018] Should Robots Pay Taxes? 163
II. CURRENT TAX POLICIES FAVOR AUTOMATION AND REDUCE TAX REVENUE
A. Introduction
Worker automation is often thought of as a matter of efficiency, where efficiency refers to the ratio of useful output to total input.107 For example, if a machine and a person create the same output, but the machine is less ex- pensive, then automation generates cost savings and improves efficiency.108 If a robot costs a firm $40,000 a year and a human worker costs $45,000 a year, with both workers producing the same output, the firm would yield a $5,000 annual cost savings by automating.
However, it may also be the case that the robot costs more than a human worker before taxes, and only becomes cheaper on a post-tax basis. For instance, the capital outlay for the robot, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as robots, together with the costs for operating the robot (electricity, etc.), might be estimated at $50,000 over some period, whereas the wages and other costs associated with an employee (healthcare, retirement funding, etc.) might be $45,000 over the same period. The robot may be associated with tax benefits that do not apply to human workers and which reduce its cost to $40,000. A firm using a rational cost-based decision model would choose to automate and realize the machine’s tax benefit. In this example, tax policy has ren- dered the robot a more efficient worker. In simple terms, the heavy relative taxation of the living worker drives the firm toward automation to generate tax savings.
The tax system is not neutral as between work performed by robots versus people.109 Automation provides several major tax advantages. Firms that automate avoid employee and employer wage taxes levied by federal, state, and local taxing authorities and claim accelerated tax depreciation on capital costs for automated workers. The tax system also provides indirect incentives for automated workers. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital.110 In
107 Expressed mathematically, efficiency “r” is equal to the amount of useful output (“P”) divided by the amount (“C”) of resources consumed: r=P/C.
108 See Stevens, supra note 17, at 373 (“Technology is very attractive to owners of capital. Machines require no pay, benefits, sick leave, vacation, lunch breaks, or weekends off. They are less prone to err and are more productive than human beings. In a race for the same job, it is therefore difficult for humans to compete with machines.”).
109 The analysis of the “neutrality” of taxation is a common practice in the field of taxa- tion. See generally Peggy Richman (Musgrave), Taxation of Foreign-Source Business Income and the Incentive to Foreign Investment, in PEGGY RICHMAN, TAXATION OF FOREIGN INVEST- MENT INCOME: AN ECONOMIC ANALYSIS (1963), reprinted in PEGGY R. MUSGRAVE, TAX POL- ICY IN THE GLOBAL ECONOMY: SELECTED ESSAYS 3–57 (E. Elgar Publishing 2002) (introducing the term “capital export neutrality”).
110 See MEISEL, supra note 32, at 220 (“An automation tax described as a payroll tax on computers conveys the basic concept. It helps level the playing field. The automation tax serves two purposes: (1) it provides an incentive for a company to create jobs by means such
 
164 484 Harvard Law & Policy Review [Vol. 12 fact, as described below, automated workers are taxed less than human
workers at both the employer and employee level.
B. Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to indi- viduals to fund social benefit programs including Social Security, Medicare, and Medicaid. Presently, in the United States, the employer and employee pay matching amounts totaling 12.4% of an employee’s salary, plus match- ing Medicare payments totaling 2.9% (applied on the first $127,200 of earn- ings), plus an additional 0.9% Medicare surcharge (applied on earnings over $200,000).111 Many states and localities also levy wage taxes that apply in addition to the federal levies.112
C. Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers
“Tax depreciation” refers here to the deduction (a reduction in the tax base) claimed by the firm in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow the firm to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction works differently for robot and human workers.
The timing of claiming a deduction may have a significant effect on a firm’s tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time).113 For example, assume a robot has a total
as investing in human-computer synergy; and (2) it proves governmental revenues that, prop- erly used, can create more consumption and thus boost the economy.”).
111 See I.R.C. § 3101(a) (2012 & Supp. II 2014), § 3111(a) (Supp. III 2016), § 3102(a) (2012), § 3121(a)(1) (2012 & Supp. II 2014); I.R.S., SOCIAL SECURITY AND MEDICARE WITH- HOLDING RATES (2017), www.irs.gov/taxtopics/tc751.html [https://perma.cc/3F5R-UEES]; see also Richard Winchester, The Gap in the Employment Tax Gap, 20 STAN. L. & POL’Y REV. 127, 132 (2009) (“The tax imposed by FICA has two components. The first is the old-age, survivors, and disability insurance component, often referred to as OASDI. It is [levied on] . . . ‘wages’ from employment. One half of the tax is deducted from the employee’s compensa- tion. The employer pays the other half. This component of the FICA tax is earmarked to cover social security benefits. There is a limit on the amount of wages that can be taxed. . . . The contribution and benefit base is adjusted each year to reflect increases in average wages of the U.S. economy.”) (citations omitted).
112 For an explanation of the U.S. states that levy sales taxes, see generally SCOTT DRENKARD & NICOLE KAEDING, TAX FOUND., STATE AND LOCAL SALES TAX RATES IN 2016 (2016), https://files.taxfoundation.org/legacy/docs/TaxFoundation_FF504.pdf [https://perma .cc/VLE2-KCHV]. For an explanation of EU tax policy including the VAT, see generally CE ́- CILE REMEUR, EUR. PARLIAMENTARY RESEARCH SERV., TAX POLICY IN THE EU: ISSUES AND CHALLENGES (2015), http://www.europarl.europa.eu/RegData/etudes/IDAN/2015/549001/ EPRS_IDA(2015)549001_EN.pdf. [https://perma.cc/FUE9-ZV58].
113 See Yoram Margalioth, Not a Panacea for Economic Growth: The Case of Accelerated Depreciation, 26 VA. TAX REV. 493, 494–95, 499 (2007) (“Accelerated depreciation policy can be traced back to an influential 1953 paper by Evsey Domar. . . . [Elaborating on the]
 
485
2018] Should Robots Pay Taxes? 165
capital cost of $100,000 and seven years of useful life, while an employee has a total wage cost of $100,000 over seven years. If accelerated deprecia- tion for capital is available,114 the firm may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year one rather than pro- rata over seven years.115 For instance, the firm might claim tax depreciation for an automated worker of $50,000 in year one, $30,000 in year two, $10,000 in year three, and in diminishing amounts to year seven. By con- trast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a present value benefit will accrue from claiming accelerated tax de- ductions for automated workers relative to the pro-rata tax deductions for employee wages, even where the $100,000 capital outlay is paid up-front.116 This is possible because the present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return the firm could make by investing the free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the eco- nomic depreciation.117 “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to buy the same robot next year than it costs today. The issue becomes sig- nificant where, as in the prior example, it was presumed for tax purposes that
Harrod-Domar model, [Domar predicted] that Gross Domestic Product (GDP) was propor- tional to the number of machines; namely, that investment is the key to growth. . . . A later model, developed by Nobel Laureate Robert Solow between 1956–57, points out that the Har- rod-Domar model cannot explain sustained growth. Solow showed that as capital per worker increases, the marginal productivity of capital declines until the capital-labor ratio approaches a steady-state level. At that point, savings . . . are just sufficient to replace worn out machines and equip new workers (assuming population growth), so productivity growth is zero.”) (citing Evsey D. Domar, Capital Expansion, Rate of Growth, and Employment, 14 ECONOMETRICA 137 (1946); Roy F. Harrod, An Essay in Dynamic Theory, 49 ECON. J. 14 (1939); Robert M. Solow, A Contribution to the Theory of Economic Growth, 70 Q. J. ECON. 65 (1956); Robert M. Solow, Technical Change and the Aggregate Production Function, 39 REV. ECON. & STAT. 312 (1957)).
114 See Margalioth, supra note 113, at 505 (“For tax reporting purposes, the Code allows the use of much more accelerated depreciation methods than the straight-line method.”).
115 See id. at 505–506 (“The vast majority of U.S. corporations use a depreciation method called ‘straight-line’ for financial reporting purposes. According to the straight-line deprecia- tion method, annual depreciation is calculated by subtracting the salvage value of the asset from the purchase price and dividing this number by the estimated useful life of the asset. The outcome is equal periodical deductions throughout the asset’s useful life. If the asset in the above example is depreciated under the straight-line method, its $1000 cost is allocated uni- formly over its useful life period of five years, resulting in $200 of depreciation deduction each year.”) (citations omitted).
116 Most large corporations have significant cash accumulations and do not need to borrow funds (and pay interest) to make capital expenditure on automation. Notably, if corporate bor- rowing is required to fund capital expenditure, then present value will depend on the adjusted cost of capital, taking into account the value of tax deductions for interest paid. In summary, accelerated tax depreciation yields an economic benefit where the firm has balance sheet cash earning a low rate of return that it can instead deploy to yield tax deductions on an accelerated basis.
117 See Margalioth, supra note 113, at 508 (“In times of inflation, recovery of the nominal cost of investment is not sufficient to match income and expenses. Because of inflation, the income generated by the asset is expressed in a larger number of dollars though it has the same purchasing power.”).
 
166 486 Harvard Law & Policy Review [Vol. 12
the robot wears out after seven years, but it turns out the robot actually increases in nominal value. An incremental tax benefit thus accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary) difference is never recaptured in the tax system. In the corporate setting, this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not ad- justed for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential inves- tors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit.118 Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate re- ported earnings could use automation to achieve such a timing benefit. This increase to reported earnings may be an even more significant motivation for large firms to automate than a cash tax savings.
D. Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the firm level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary examples are the Retail Sales Tax (RST) levied by states and municipalities in the United States and the VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportion- ately to offset the indirect tax.119 In the case of automated workers, however, the burden of indirect taxes is entirely avoided by the firm because it does not need to provide for a machine’s consumption.120 In general, business ex-
118 See id. at 505 (“Accounting for depreciation is also required for financial reporting purposes. Generally accepted accounting principles (GAAP) require the depreciation of the (depreciable) cost of income generating assets, usually, tangible assets. The cost has to be allocated among accounting periods on a systematic and rational basis that reflects the use of the asset in the revenue generating process over the asset’s operational life.”) (citations omitted).
119 See CTR. FOR RESEARCH ON THE PUB. SECTOR AT UNIV. BOCCONI, THE ROLE AND IMPACT OF LABOR TAXATION 14 (2011) [hereinafter BOCCONI].
120 The capital assets comprising automated workers might be subject to property taxation by some local jurisdictions as business personal property. However, such personal property taxation is often successfully mitigated by tax planning or with tax waivers by local jurisdic- tions and municipalities negotiated by municipalities. Further, human employees also engender some degree of attached personal property (e.g., office fixtures, personal computers), which are also subject to personal property taxation.
 
487
2018] Should Robots Pay Taxes? 167
penditures for capital assets such as machinery are exempt from indirect taxation or yield a deduction for RST or VAT.121
E. Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing.122 For 2015, the Internal Revenue Service (IRS) reported that out of the nearly $3 trillion in net collections, individual income taxes accounted for 49.8%, employment taxes 35.2%, business income taxes 11.7%, excise taxes 2.6%, and estate and gift taxes 0.7%.123 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now com- prises roughly one-half of its respective share compared to prior decades.124 In fact, the Trump administration’s recently enacted Tax Cuts and Jobs Act reduces the corporate tax rate from a maximum of 35 percent to a flat 21 percent beginning in 2018.125 In Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.126
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For in- stance, an employee cannot use transfer pricing techniques to shift earned income into a 0%-taxed entity in the Cayman Islands.127 Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human
121 See John Mikesell, Sales Tax Incentives for Economic Development: Why Shouldn’t Production Exemptions Be General?, 54 NAT’L TAX J. 557, 562 (2001).
122 See SOI Tax Stats – Collections and Refunds, by Type of Tax, IRS Data Book Table 1, I.R.S. (Aug. 28, 2017), https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by- type-of-tax-irs-data-book-table-1 [https://perma.cc/R282-7P76] (containing reported aggre- gate collections and refunds from 2015 to 1995). For example, from 1995 to 2015, business income taxes decreased from 12.3% of total collections to 11.7%, while individual taxes in- creased from 46.5% to 49.8%. Id.
123 See id. Individual income taxes here include estate and trust incomes taxes, which represent 1.1% of overall collections. Employment taxes consist of primarily old-age, survi- vors, disability, and hospital insurance, which is almost entirely Federal Insurance Contribu- tions payments and a small amount of Self-Employment Insurance Contributions. It also includes a small amount of Unemployment Insurance and Railroad retirement. Id.
124 See JOEL FRIEDMAN, CTR. ON BUDGET AND POL’Y PRIORITIES, THE DECLINE OF CORPO- RATE INCOME TAX REVENUES 3 (2003), https://www.cbpp.org/sites/default/files/atoms/files/10- 16-03tax.pdf [https://perma.cc/66JG-QSU3].
125 See Act of Dec. 22, 2017 (Tax Cuts & Jobs Act) Pub. L. No. 115-97, §13001, 131 Stat. 2054 (codified as amended at 26 U.S.C. § 11).
126 See MEISEL, supra note 32, at 223 (“What numbers are used in the ratio of revenues to employees? I recommend using revenues generated within the taxing country and employees within the country in the ratio.”).
127 The Cayman Islands has no corporate tax. See DELOITTE, INTERNATIONAL TAX: CAY- MAN ISLANDS HIGHLIGHTS 2017 1 (2017), https://www2.deloitte.com/content/dam/Deloitte/ global/Documents/Tax/dttl-tax-caymanislandshighlights-2017.pdf [https://perma.cc/FZL8- 33YR].
 
168 488 Harvard Law & Policy Review [Vol. 12
capital is also not depreciable, so a person does not typically get a tax deduc- tion for education or medical costs, at least not up to the full amount of the investment.128 By contrast, machinery or other equipment yields an immedi- ate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.129
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, this would be expected to have negative economic effects over the long term.
III. TAX POLICY OPTIONS FOR AN AUTOMATION TAX
The current tax system is designed to principally tax human workers and not robot workers. All else being equal, this creates a situation in which firms prefer robots since substantially less tax per output is accrued or remit- ted in respect of an automated worker. At the same time, the automation of large segments of the labor force threatens long-term fiscal solvency because of the potential reduction in tax collections.
A major automation policy issue is therefore how to adjust the tax sys- tem to be neutral as between robot and human workers, or even to create incentives for human rather than robot workers to incentivize employment. In doing so, it is important to consider that capital investment of any kind (including for robots) is thought to be beneficial to economic growth.130 Na- tions engage in tax competition to draw capital into their jurisdictions. Any disallowance of capital deduction would serve as a disincentive to invest- ment and would, theoretically, be economically undesirable. For example, if only one taxing jurisdiction disallowed tax deductions for automated work- ers, multinational firms might shift their capital investments to other juris-
 128 For the U.S. incentives with an election for deduction or credit on higher education costs, see 26 U.S.C. § 25A (2012 & Supp. III 2016).
129 See, e.g., BOCCONI, supra note 119, at 14.
130 See, e.g., Eduardo Borensztein et al., How Does Foreign Direct Investment Affect Eco- nomic Growth?, 45 J. INT’L ECON. 115, 116 (1998); see also Gert Wehinger, Fostering Long- Term Investment and Economic Growth: Summary of a High-Level OECD Roundtable, 2011 OECD J. FIN. MKT. TRENDS 1, 2 (2011).

489
2018] Should Robots Pay Taxes? 169
dictions.131 It is therefore important to consider international tax competition in evaluating various options to create an automation-neutral tax system.132
A. Disallowance of Corporate Tax Deductions for Automated Workers
A first option is to attempt to disallow the respective corporate income tax deductions for capital investments that give rise to the automation tax benefit. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The recent South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines.133
To begin with federal income taxation, the disallowance of tax prefer- ences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phase out.”134 Phase outs reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower-income taxpayers.135 For instance, student loan interest is deductible, but not for individuals with more than $80,000 in modified adjusted gross income (MAGI) ($160,000 for joint filers).136 Some phase outs reduce credits, others reduce deductions.137
A new code provision could be designed with a similar phase out, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker automation could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance.
In respect of indirect taxation, a simpler solution may be possible. Indi- rect tax preferences for capital outlay in respect to automated workers could
131 However, the shift would be from one high-tax jurisdiction to another high-tax juris- diction to claim the deduction’s full value, rather than a shift into tax havens with a zero percent corporate tax rate, where the capital tax deductions for automated workers would not have any value (i.e., the value of a tax deduction in a zero percent tax jurisdiction is zero). Thus, multinational firms should not be expected to make capital investment in robots in tax havens where the value of deductions is zero, especially where transfer pricing strategies are available to shift income arising from the automated workers.
132 See MEISEL, supra note 32.
133 See McGoogan, supra note 22.
134 See, e.g., Emmanuel Saez, Do Taxpayers Bunch at Kink Points?, 2 AM. ECON. J. ECON
POL’Y 180, 180 (2010).
135 See, e.g., I.R.S., TEN FACTS ABOUT THE CHILD TAX CREDIT (2011), https://www.irs
.gov/newsroom/ten-facts-about-the-child-tax-credit [https://perma.cc/3YEV-V8XD].
136 See I.R.S., TAX BENEFITS FOR EDUCATION 2 (2016), https://www.irs.gov/pub/irs-pdf/
p970.pdf [https://perma.cc/X5ZV-MXMX].
137 See generally I.R.S., TAX GUIDE 2016: FOR INDIVIDUALS 25, (2016), https://www.irs
.gov/pub/irs-pdf/p17.pdf [https://perma.cc/FS97-G5LE] (discussing the various types of cred- its and deductions available and the income levels at which they phase out).
 
170 490 Harvard Law & Policy Review [Vol. 12
be disallowed outright at the state level. Thus, for example, where the firm attempts to claim an RST/VAT exemption or refund for tax payments made to purchase and maintain automated workers, this would not be permitted.
These measures could achieve greater balance between taxing human workers and robots, but the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base used to fund social insurance benefits.
B. Levy of an Automation Tax
A second option is to levy an incremental federal “automation tax” to the extent workers are laid off or replaced by machines.138 A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings.139 Employers must pay into an unemployment insurance scheme based on their ratings, so a business which has more layoffs pays more in taxes for unemployment insurance.140 A federal automation tax could be de- signed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determined the layoffs were due to automation.
A potential drawback to the levy of an additional automation tax is that it would essentially increase the corporate effective tax rate for many firms, and also increase the relative complexity of the tax system. Economic theory suggests that higher rates and added complexity are negatives in terms of international tax competition.141 Another drawback is that firms might accel- erate layoffs upon passage (or debate) of the bill prior to implementation to
138 Cf. Michael Kraich, The Chilling Realities of the Telecommuting Tax: Adapting Twenti- eth Century Policies for Twenty-First Century Technologies, 15 U. PITT. J. TECH. L. & POL’Y 224 (2015) (providing a comprehensive discussion of a “telecommuting tax”).
139 See DAVID RATNER, FED. RESERVE BD., UNEMPLOYMENT INSURANCE EXPERIENCE RAT- ING AND LABOR MARKET DYNAMICS 1 (2013), https://www.federalreserve.gov/pubs/feds/2013/ 201386/201386pap.pdf [https://perma.cc/T5W3-5YJD] (“The United States is the only OECD country to finance unemployment insurance (UI) through a tax system which penalizes layoffs. The original intent of this institution, known as ‘experience rating,’ was to apportion the costs of UI to the highest turnover firms and thereby stabilize employment. Experience rating can stabilize employment through a layoff cost. The layoff cost is levied when a firm lays off a worker and is assessed a higher tax rate in the future.”).
140 See id.
141 See generally Michael Keen & Kai A. Konrad, The Theory of International Tax Com- petition and Coordination, in 5 HANDBOOK OF PUBLIC ECONOMICS 257 (Alan Auerbach et al. eds., 2013), http://gabriel-zucman.eu/files/teaching/KeenKonrad13.pdf [https://perma.cc/ 3VC5-HLN3] (exploring models that suggest a country might prefer to raise its tax rate in response to lower tax rates in other countries); but see Bret N. Bogenschneider, Causation, Science & Taxation, 10 ELON L. REV. (forthcoming, Spring 2018) (“The hypothesis that tax cuts cause economic growth is a central tenet of neoclassical economic theory. Yet, it is not clear why economists hold this belief, as empirical evidence of any posited causal relation is conspicuously absent. . . . The available evidence indicates to the contrary of the hypotheses that tax cuts cause economic growth is that higher ratios of taxation to gross domestic product are associated with higher rates of national economic growth in most countries.”).
 
491
2018] Should Robots Pay Taxes? 171
avoid the tax by reducing the number of employees upon the effective date of the law. Accordingly, a retroactive effective date for measurement of em- ployment levels for the automation tax would be a practical necessity.
C. Grant Offsetting Tax Preferences for Human Workers
A third option is to attempt to grant offsetting tax preferences for firms that employ human workers for each category of tax benefit. To begin with wage taxation, the tax preference could entail a repeal of the employer con- tributions to the Social Security and Medicare systems. The result would be that both human and automated workers would be exempt for the employer in terms of wage taxes—not just automated workers. However, this would accelerate the insolvency of the Social Security system unless the resultant decrease in tax collections were otherwise offset.142
In terms of income taxation, an offsetting preference for human work- ers could be designed as an accelerated deduction for future wage compensa- tion expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for automated workers. In terms of indirect tax- ation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income. This would constitute an incentive for firms to employ human workers.
D. Levy of a Corporate Self-Employment Tax
A fourth option is to increase corporate level taxation for firms that produce outputs without using human labor. The additional taxes would be a substitute amount for Social Security and Medicare wage taxes avoided by the firm with automated labor.143 In part, this is the corollary to the individ- ual self-employment tax where a small-business owner is required to pay monies into the Social Security system approximating the Social Security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work.144 The corporate self- employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount deter- mined by the Treasury (in reference to industry standards), then backup
142 This would require a very significant offset. Federal Insurance Contributions and Self- Employment Insurance Contributions currently make up about 34.6% of net federal tax collec- tions. Federal Insurance Contributions include both employee and employer payments to fund Social Security and Medicare. See I.R.S., supra note 122.
143 MEISEL, supra note 32, at 222–23 (“Returning to the payroll tax analogy, companies that hire fewer people pay fewer payroll taxes. The payroll tax in the US helps fund social security, Medicare, and unemployment insurance. In Europe, payroll taxes are even higher than in the US.”).
144 Id. at 227 (“The automation tax might encourage companies to prefer productivity improvements achieved by using a combination of human and computer capabilities.”).
 
172 492 Harvard Law & Policy Review [Vol. 12
withholding could apply on corporate profits. The gross amount of the auto- mation tax could be designed to match the wage taxes avoided by the firm with automated workers.
William Meisel has similarly proposed an “automation tax” which he referred to in lay terms as a “payroll tax on computers.”145 This would be like the corporate self-employment tax described here. Meisel wrote:
I propose that a national automation tax be based on the ratio of a company’s revenues (total sales) to their number of employ- ees. . . .[T]he automation tax should increase as a percentage as the revenue-per-employee [ratio] grows, making it more attractive to create jobs than to replace them with automation. . . . I prefer applying the percentage to revenues. . . . Profits can be manipu- lated with deductions and other accounting complexities much more than revenues.146
Meisel’s “automation tax” differs from our proposed corporate self-em- ployment tax in that the former uses a sales ratio as opposed to a profit ratio. A sales ratio may be unworkable in practice since the tax would prohibi- tively fall on firms with high sales but low profit margins, such as dis- counted retailers. Since automation often occurs in the high-tech industry among companies with high profit margins, it seems preferable that a viable “automation tax” using a ratio to employee expense should be premised on profits, not sales.
E. Increase the Corporate Tax Rate
A fifth option would be to significantly increase the corporate tax rate, with the intent of increasing the relative portion of the tax base borne by capital and decreasing that borne by labor. The counter-intuitive advantage of this approach is that higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment re- fers to incremental investment made only because of the tax system.147 As one of us has explained, “[t]he experienced tax attorney always counsels the client that marginal capital investment is tax deductible.”148 Thus, mul- tinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions to claim tax deductions of relatively higher value. In part for this reason, for smaller and growing firms that are reinvest- ing profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
145 Id. at 220 (“If software is to take over many jobs, why not have an income tax on software? We could perhaps think of it as a payroll tax on computers.”).
146 Id. at 221–23.
147 See Bret N. Bogenschneider, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74 (2015).
148 Id. at 61 (second emphasis added).
 
493
2018] Should Robots Pay Taxes? 173
The drawbacks to increasing the corporate tax rate are well-known and may be summarized as follows: First, the corporate tax rate might be a signal to firms about the tax climate of a jurisdiction, so higher tax rates could have a negative psychological effect on capital investment decision making.149 Second, accelerated tax deductions would be a stronger automation incen- tive with a higher corporate tax rate as the deduction would have greater value. This means that an increase in the corporate tax rate should be taken in combination with our other proposals. Third, the increase in corporate tax rates would affect all firms, even those not engaged in worker automation. Hence, the increase in corporate tax rate option might be viewed as one version of zero-sum analysis, in which tax policy is designed not to allow a shift of the tax burden from capital to other taxpayers. Further, any increase in corporate tax rates may prompt firms to attempt to shift the tax incidence to workers or consumers.150 Finally, increasing corporate tax rates may be politically unfeasible. As Meisel notes in an understated fashion, “[c]orporations might instinctively fight a corporate tax.”151
F. Issues in Economic Efficiency Relevant to Automation Tax Policy Proposals
The tax policy analysis developed here comes from the perspective of average effective tax rates as opposed to solely marginal rates.152 Any margi- nal tax rate methodology excludes an analysis of taxation relative to the overall share of the tax base. For example, technology and pharmaceutical companies often pay a very low average effective tax rate (e.g., less than 10%) but could also be correctly found to simultaneously have a high margi- nal effective tax rate (e.g., about 35%). A corporate taxpayer which pays very little tax relative to its level of taxable income could correctly describe its marginal tax rate as “high.” Accordingly, the last dollar of income may nearly always be found to be taxed at a “high” marginal tax rate, even where the average effective tax rate is relatively low.153
Economic models of taxation are typically designed by modeling the hypothetical effects of changes in marginal tax rates.154 Marginal tax rates
149 Id. at 60–61 (“Any income tax system is designed initially to favor active investors. This is because no matter how high the actual tax rate, it is levied only on what is referred to as ‘taxable income.’ Of course, ‘taxable income’ means the amount of profits less deductions. Every tax professional is aware of this feature of an income tax system and counsels the client accordingly.”).
150 See Kimberly Clausing, In Search of Corporate Tax Incidence, 65 TAX L. REV. 433, 468 (2012). Firms, however, behave as if they bear the incidence of corporate taxation.
151 MEISEL, supra note 32, at 225.
152 The calculation of a marginal tax rate is essentially the theoretical opposite of the calculation of taxation as a percentage of the share of the overall tax base.
153 For example, a firm may have an overall tax rate of 20% on all of its earnings; how- ever, with respect to a hypothetical decision of whether to earn incremental income, the margi- nal tax rate might be 35%.
154 For a discussion of marginal tax rates in economic analysis, see David Madden, The Poverty Effects of a ‘Fat Tax’ in Ireland, 24 HEALTH ECON. 104, 106 (2015) (“The difficulties
 
174 494 Harvard Law & Policy Review [Vol. 12
again represent incremental changes to the statutory tax rate on the last dol- lar of income.155 For example, a change in the statutory corporate tax rate from 35% to 30% would be reflected in economic models premised on mar- ginal rate analysis. The trouble with this form of economic modeling is that its validity relies on the presumption that firm decisions are made based on tax effects on the marginal investment and not based on an average. This approach has major implications for tax policy design as tax cuts to the stat- utory rate are nearly certain to have a marginal effect even where the firm does not pay a high tax rate overall. Thus, business and investment decisions are presumed not to proceed at the average tax rate for all earned income, but only with respect to incremental tax changes relevant to marginal income.
Other economic modeling proceeds on a marginal effective tax rate ba- sis (i.e., reflecting that corporate taxpayers do not pay the statutory rate). For example, the granting of an additional deduction for manufacturing activity to corporations could reduce the marginal effective tax rate on the last dollar of income from 30% to 27% where the statutory rate is 35%. By this method, the firm would be presumed to make an investment decision based on the average tax rate at the margin. Both approaches are distinguishable from analysis using simply an average effective tax rate, which for large corporations is now calculated at approximately 20% (including permanent deferrals) and trending downward.156 However, for many tech companies, the effective tax rate is below 10%. At such very low average effective tax rates, it is not clear that economic analysis of marginal effects of tax cuts is a realistic method of tax policy analysis. By such methods, significant macroeconomic benefits can be posited where corporate effective tax rates are reduced from very low levels to even lower levels (e.g., from 2% to 1%), but where it is likely that factors other than marginal taxation are likely to drive firm investment decisions. Also, the positing of economic growth from marginal tax cuts does not consider the effect changes in the composition of the overall tax base, where the taxation of one factor is substantially re- duced, namely capital, and the taxation of another factor is increased (or overall borrowing is increased). Further, multinational firms do not engage in tax avoidance planning to reduce income which they do not intend to earn.
associated with non-marginal tax reforms have led a number of analysts to concentrate on marginal tax reforms. This approach has the advantage of not requiring estimates of individual demand and utility functions.”) (internal citation omitted).
155 The U.S. federal statutory corporate tax rate is thirty-five percent for corporate income in excess of ten million. See I.R.C. § 11 (2012). Various individual U.S. states also levy an incremental state-level corporate tax. See generally NICOLE KAEDING, TAX FOUND., STATE INCOME CORPORATE TAX RATES AND BRACKETS FOR 2016 (2016), https://files.taxfoundation .org/legacy/docs/TaxFoundation-FF497.pdf [https://perma.cc/J37Z-Y8X6].
156 For effective tax rates on multinational firms including the delay in taxation of foreign earnings for U.S. multinationals, see generally Bret N. Bogenschneider, The Effective Tax Rates of U.S. Firms with Permanent Deferral, 145 TAX NOTES 1391 (2015).
 
495
2018] Should Robots Pay Taxes? 175
In summary, notwithstanding that the statutory corporate tax rate, or marginal corporate effective tax rates, might be correctly described as “high” in the economic theory of taxation, such analysis is also subject to a relative or zero-sum form of analysis, where tax cuts for one party are trans- ferred as tax increases to another party. The average effective tax rate on workers is relatively “high” where all types of taxation are taken into ac- count.157 The taxation of workers comprises the bulk of the tax base in the United States and that of most developed countries. As workers are substi- tuted or replaced by automation, follow-on effects are possible not only from the direct reduction in the tax base, but also indirectly where the relative taxes are transferred to other workers in the economy.
CONCLUSION
Automation promises to be one of the great social challenges of our generation. It can benefit everyone, or it can benefit the select few at the expense of the many. Tax is a critical component of any automation policy. Existing tax policies both encourage automation and dramatically reduce the government’s tax revenue. This means that attempts to craft policy solutions to deal with automation will be inadequate if they fail to take taxation into account. In this article, we have proposed a series of tax policy changes that could level the playing field for human workers. Whether these proposals are adopted may depend on whether policy makers are prepared to make politically challenging decisions about how to deal with automation.
 157 See Bogenschneider, supra note 27.

496
The Reasonable Computer: Disrupting the Paradigm of Tort Liability
Ryan Abbott* ABSTRACT
Artificial intelligence is part of our daily lives. Whether working as chauf- feurs, accountants, or police, computers are taking over a growing number of tasks once performed by people. As this occurs, computers will also cause the injuries inevitably associated with these activities. Accidents happen, and now computer-generated accidents happen. The recent fatality involving Tesla’s au- tonomous driving software is just one example in a long series of “computer- generated torts.”
Yet hysteria over such injuries is misplaced. In fact, machines are, or at least have the potential to be, substantially safer than people. Self-driving cars will cause accidents, but they will cause fewer accidents than human drivers. Because automation will result in substantial safety benefits, tort law should encourage its adoption as a means of accident prevention.
Under current legal frameworks, suppliers of computer tortfeasors are likely strictly responsible for their harms. This Article argues that where a sup- plier can show that an autonomous computer, robot, or machine is safer than a reasonable person, the supplier should be liable in negligence rather than strict liability. The negligence test would focus on the computer’s act instead of its design, and in a sense, it would treat a computer tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, and it would continue to reward sup- pliers for improving safety.
More importantly, principles of harm avoidance suggest that once com- puters become safer than people, human tortfeasors should no longer be mea- sured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against computers. To appropriate the immortal words of Justice Holmes, we are all “hasty and awkward” compared to the reasonable computer.
TABLE OF CONTENTS
INTRODUCTION ................................................. 2 I. LIABILITY FOR MACHINE INJURIES ..................... 8 A. A Brief History ..................................... 8
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at UCLA. Thanks to Hrafn Asgeirsson, Bret Bogenschneider, Richard Epstein, Marie Newhouse, Alexander Sarch, and Christopher Taggart for their insightful comments.
 January 2018 Vol. 86 No. 1
1

2
497 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
B. Tort Law as a Mechanism for Accident Prevention . 11 C. Negligence .......................................... 12 D. Strict and Product Liability ......................... 13
II. COMPUTER-GENERATED TORTS......................... 16
A. Automation Will Prevent Accidents ................. 16
B. Tort Liability Discourages Automation.............. 19
C. Computer-Generated Torts Should Be Negligence
Based ............................................... 22
D. Computer-Generated Torts as a Type of Machine
Injury............................................... 24
E. Implementation ..................................... 26
F. Financial Liability................................... 30
G. Alternatives to Negligence ........................... 32
III. THE REASONABLE ROBOT .............................. 35
A. When Negligence Is Strict ........................... 35
B. The New Hasty and Awkward ...................... 36
C. Reasonable People Use Autonomous Computers . . . . 39
D. The Reasonable Computer Standard for Computer
Tortfeasors.......................................... 41
E. The Automation Problem ........................... 42
CONCLUSION ................................................... 44
INTRODUCTION
An automation revolution is coming, and it is going to be hugely disruptive.1 Ever cheaper, faster, and more sophisticated computers are able to do the work of people in a wide variety of fields and on an unprecedented scale. They may do this at a fraction of the cost of existing workers, and in some instances, they already outperform their human competition.2 Today’s automation is not limited to manual la- bor; modern machines are already diagnosing disease,3 conducting le-
1 See generally JAMES MANYIKA ET AL., MCKINSEY & CO., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013).
2 See, e.g., Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible Are Jobs to Computerisation?, 114 TECHNOLOGICAL FORECASTING & SOC. CHANGE 254, 265–66 (2017) (reporting in a seminal paper that “47 percent of total US employment is [at] high risk” of automation, and stating that “recent developments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future”).
3 See Roger Parloff, Why Deep Learning Is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning [https://perma.cc/E3UA-N2TZ]. Several artificial intelligence systems are already capable of au- tomating medical diagnoses. See id. For instance, Freenome has a system for diagnosing cancer from blood samples that is competitive with pathologists. See id.; see also FREENOME, http:// www.freenome.com (last visited Jan. 4, 2018).
 
2018] 498 THE REASONABLE COMPUTER 3
gal due diligence,4 and providing translation services.5 For better or worse, automation is the way of the future—the economics are simply too compelling for any other outcome.6 But what of the injuries these automatons will inevitably cause? What happens when a machine fails to diagnose a cancer, ignores an incriminating email, or inadvertently starts a war?7 How should the law respond to computer-generated torts?
Tort law has answers to these questions based on a system of common law that has evolved over centuries to deal with unintended harms.8 The goals of this body of law are many: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.9 Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law.10 By creating a framework for loss shifting from injured victims to tortfeasors, tort law deters unsafe conduct.11 A purely financially motivated rational
4 See Jane Croft, Legal Firms Unleash Office Automatons, FIN. TIMES (May 16, 2016), https://www.ft.com/content/19807d3e-1765-11e6-9d98-00386a18e39d (discussing various software programs that can outperform attorneys and paralegals in document review); cf. Dana Remus & Frank S. Levy, Can Robots Be Lawyers? Computers, Lawyers, and the Practice of Law (Nov. 27, 2016), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2701092 (unpublished manuscript) (arguing that artificial intelligence will refocus rather than replace attorneys).
5 See Yonghui Wu et al., Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation (Sept. 26, 2016), https://arxiv.org/pdf/1609.08144.pdf (unpublished manuscript). Google now claims its Google Neural Machine Translation system is approaching human-level translation accuracy. Id. at 2.
6 See, e.g., DELOITTE, FROM BRAWN TO BRAINS: THE IMPACT OF TECHNOLOGY ON JOBS IN THE UK 4 (2015), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Growth/ deloitte-uk-insights-from-brawns-to-brain.pdf (suggesting that every nation and region of the U.K. has benefitted from automation and that automation has resulted in £140 billion to the U.K.’s economy in new wages).
7 See, e.g., Fiona Macdonald, The Greatest Mistranslations Ever, BBC (Feb. 2, 2015), http://www.bbc.co.uk/culture/story/20150202-the-greatest-mistranslations-ever (describing some of the unfortunate outcomes associated with mistranslation).
8 See generally MORTON J. HORWITZ, THE TRANSFORMATION OF AMERICAN LAW, 1780–1860 (1977) [hereinafter HORWITZ, 1780–1860]; MORTON J. HORWITZ, THE TRANSFORMA- TION OF AMERICAN LAW 1870–1960 (1992).
9 See George L. Priest, Satisfying the Multiple Goals of Tort Law, 22 VAL. U. L. REV. 643, 648 (1988).
10 See, e.g., George L. Priest, The Invention of Enterprise Liability: A Critical History of the Intellectual Foundations of Modern Tort Law, 14 J. LEGAL STUD. 461 (1985); see also Robert F. Blomquist, Goals, Means, and Problems for Modern Tort Law: A Reply to Professor Priest, 22 VAL. U. L. REV. 621 (1988) (arguing that economic theory and moral philosophy both require accident reduction to be the primary aim of tort law).
 11 See George L. Priest, Modern Tort Law and Its Reform, 22 VAL. U. L. REV. 1, 7 (1987).

4 499 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.12 This liability framework has far-reaching and sometimes complex impacts on be- havior. It can either accelerate or impede the introduction of new technologies.13
Most injuries people cause are evaluated under a negligence stan- dard where unreasonable conduct establishes liability.14 When com- puters cause the same injuries, however, a strict liability standard applies.15 This distinction has financial consequences and a corre- sponding impact on the rate of technology adoption.16 It discourages automation, because machines incur greater liability than people. It also means that in cases where automation will improve safety, the current framework to prevent accidents now has the opposite effect.
This Article argues that the acts of autonomous computer tortfeasors should be evaluated under a negligence standard, rather than a strict liability standard, in cases where an autonomous com- puter is occupying the position of a reasonable person in the tradi- tional negligence paradigm and where automation is likely to improve safety. For the purposes of ultimate financial liability, the computer’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments under standard principles of product liability law.
This Article employs a functional approach to distinguish an au- tonomous computer, robot, or machine from an ordinary product.17
12 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (applying rule that balances the burden of additional protections on the actor with the probability and gravity of an injury).
13 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Gideon Parchomovsky & Alex Stein, Torts and Innovation, 107 MICH. L. REV. 285, 286 (2008) (discussing how the role of custom in tort law impedes innovation). Nor is the idea that tort liability is a barrier to developments in machine intelligence new. See Steven J. Frank, Tort Adjudication and the Emergence of Artificial Intelligence Software, 21 SUFFOLK U. L. REV. 623, 639 (1987).
14 See infra text accompanying notes 62–71.
15 See infra text accompanying notes 93–100.
16 See, e.g., Amy Finkelstein, Static and Dynamic Effects of Health Policy: Evidence from
the Vaccine Industry, 119 Q.J. ECON. 527, 535 (2004) (explaining that establishment of the Vac- cine Injury Compensation Fund encouraged vaccine development by indemnifying manufactur- ers from liability).
17 Terms such as “robot,” “machine,” “artificial intelligence,” “machine intelligence,” and even “computer” are not used consistently even in the scientific literature. See, e.g., NEIL JOHN- SON ET AL., ABRUPT RISE OF NEW MACHINE ECOLOGY BEYOND HUMAN RESPONSE TIME 2 (2013), https://www.nature.com/articles/srep02627.pdf (discussing autonomy in the context of ar- tificial intelligence); Matthew U. Scherer, Regulating Artificial Intelligence Systems: Risks, Chal-
 
2018] 500 THE REASONABLE COMPUTER 5
Society’s relationship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are given tasks to complete and determine for themselves how to complete those tasks. For instance, a person could instruct a self-driving car to take them from point A to point B, but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. This distinction is analogous to the distinction be- tween employees and independent contractors, which centers on the degree of control and independence.18 As this Article uses such terms, autonomous machines or computer tortfeasors control the means of completing tasks, regardless of their programming.19
The most important implication of this line of reasoning is that just as computer tortfeasors should be compared to human tortfeasors, so too should humans be compared to computers. Once computers become safer than people and practical to substitute, com- puters should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical, reasonable person would have done in their situation, but what a computer would have done. In time, as computers come to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy,20 but people would engage in certain activi- ties at their own peril. Such a rule is entirely consistent with the ratio- nale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving
lenges, Competencies, and Strategies, 29 HARV. J.L. & TECH. 353, 359–61 (2016) (discussing difficulties with defining artificial intelligence); John McCarthy, What Is Artificial Intelligence? 2–3 (Nov. 12, 2007), http://jmc.stanford.edu/articles/whatisai/whatisai.pdf (discussing the lack of a standardized definition of artificial intelligence by the scientist who coined the term).
18 See Yewens v. Noakes [1880] 6 QB 530 at 532–33 (Eng.) (“A servant is a person subject to the command of his master as to the manner in which he shall do his work.”). Also see O’Connor v. Uber Technologies, Inc., No. 14-16078 (9th Cir. argued Sept. 20, 2017), for one of the many ongoing lawsuits against Uber highlighting modern challenges distinguishing between employees and independent contractors.
19 See, e.g., Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) (discussing types of machine architectures, including conventional knowledge-based systems with expert rules as well as types of machine intelligence algorithms that result in unexpected machine behavior).
20 See generally Richard M. Ryan & Edward L. Deci, Overview of Self-Determination The- ory: An Organismic Dialectical Perspective, in HANDBOOK OF SELF-DETERMINATION RESEARCH 3, 6 (Edward L. Deci & Richard M. Ryan eds., 2002) (arguing that people have three basic psychological needs: connectedness, autonomy, and feeling competent).
 
6 501 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
“reasonable computer” standard should even apply to computer tortfeasors, such that computers will be held to the standard of other computers. By this time, computers will cause so little harm that the primary effect of the standard would be to make human tortfeasors essentially strictly liable for their harms.
This Article uses self-driving cars as a case study to demonstrate the need for a new torts paradigm.21 There is public concern over the safety of self-driving cars, but a staggering ninety-four percent of crashes involve human error.22 These contribute to over 37,000 fatali- ties a year in the United States at a cost of about $242 billion.23 Auto- mated vehicles may already be safer than human drivers, but if not, they will be soon.24 Shifting to negligence would accelerate the adop- tion of driverless technologies, which, according to a report by the consulting firm McKinsey & Company, may otherwise not be wide- spread until the middle of the century.25
Automated vehicles may be the most prominent and disruptive upcoming example of robots changing society, but this analysis applies to any context with computer tortfeasors. For instance, IBM’s flagship artificial intelligence system, Watson, is working with clinicians at Me- morial Sloan Kettering to analyze patient medical records and provide
21 Others have written about tort liability and self-driving vehicles, although primarily dealing with how existing law deals with accidents involving autonomous vehicles. See, e.g., Jef- frey K. Gurney, Sue My Car Not Me: Products Liability and Accidents Involving Autonomous Vehicles, 2013 U. ILL. J.L. TECH. & POL’Y 247; F. Patrick Hubbard, “Sophisticated Robots”: Balancing Liability, Regulation, and Innovation, 66 FLA. L. REV. 1803, 1803 (2014) (arguing, using the example of self-driving vehicles, that the current framework “provides an appropriate balance of innovation and liability for personal injury”); Gary E. Marchant & Rachel A. Lindor, The Coming Collision Between Autonomous Vehicles and the Liability System, 52 SANTA CLARA L. REV. 1321 (2012).
22 See NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 115, CRITICAL REASONS FOR CRASHES INVESTIGATED IN THE NATIONAL MOTOR VEHICLE CRASH CAUSATION SURVEY 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublica- tion/812115.
23 General Statistics, INS. INST. FOR HIGHWAY SAFETY (Dec. 2017), http://www.iihs.org/ iihs/topics/t/general-statistics/fatalityfacts/overview-of-fatality-facts [https://perma.cc/2J5P- Y27C]; see NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 013, THE ECONOMIC AND SOCIETAL IMPACT OF MOTOR VEHICLE CRASHES, 2010 (REVISED) 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812013.
24 See Cadie Thompson, Why Driverless Cars Will Be Safer Than Human Drivers, BUS. INSIDER (Nov. 16, 2016, 9:24 PM), http://www.businessinsider.com/why-driverless-cars-will-be- safer-than-human-drivers-2016-11.
25 Michele Bertoncello & Dominik Wee, Ten Ways Autonomous Driving Could Redefine the Automotive World, MCKINSEY & CO. (June 2015), http://www.mckinsey.com/industries/auto- motive-and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
 
2018] 502 THE REASONABLE COMPUTER 7
evidence-based cancer treatment options.26 It even provides support- ing literature to human physicians to support its recommendations.27 Like self-driving cars, Watson does not need to be perfect to improve safety—it just needs to be better than people. In that respect, the bar is unfortunately low. Medical error is one of the leading causes of death.28 A 2016 study in the British Medical Journal reported that it is the third leading cause of death in the United States, ranking just be- hind cardiovascular disease and cancer.29 Some companies already claim their artificial intelligence systems outperform doctors, and that claim is not hard to swallow.30 Why should a computer not be able to outperform doctors when the computer can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?31
This Article is divided into three Parts. Part I provides back- ground on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negli- gence and strict product liability. Part II argues that while some forms of automation should prevent accidents, tort law may act as a deter- rent to adopting safer technologies. To encourage automation and im- prove safety, this Part proposes a new categorization of “computer- generated torts” for a subset of machine injuries. This would apply to cases in which an autonomous computer, robot, or machine is occupy- ing the position of a reasonable person in the traditional negligence paradigm and where automation is likely to improve safety. This Part contends that the acts of computer tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and it goes on to propose rules for implementing the system.
26 Oncology and Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-ge- nomics [https://perma.cc/Z6H7-S5W4].
27 Id.
28 See INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER HEALTH SYSTEM (Linda T. Kohn et al. eds., 2000); Martin A. Makary & Michael Daniel, Medical Error—The Third Leading Cause of Death in the US, 353 BMJ 2139, 2139 (2016). The landmark report published by the Institute of Medicine in 2000 was a wake-up call to the medical profession about the harmful effects of medical error. See INST. OF MED., supra. Yet the report was based on studies conducted in 1984 and 1992. See id.
29 Makary & Daniel, supra note 28, at 2143 fig.1.
30 Parloff, supra note 3. For example, Enlitic has a program for detecting and classifying
lung cancers which the company claims has already outperformed human radiologists. Id.
31 See, e.g., Saul N. Weingart et al., Epidemiology of Medical Error, 320 BMJ 774, 775 (2010) (discussing some of the causes of human medical error).
 
8 503 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
Finally, Part III argues that once computer operators become safer than people and automation is practical, the “reasonable computer” should become the new standard of care. It explains how this standard would work, argues the reasonable computer standard works better than a reasonable person using an autonomous machine, and consid- ers when the standard should apply to computer tortfeasors. At some point, computers will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
This Article is focused on the effects of automation on accidents, but automation implicates a host of social concerns. It is important that policymakers act to ensure that automation benefits everyone. Automation may increase productivity and wealth, but it may also contribute to unemployment, financial disparities, and decreased so- cial mobility. These and other concerns are certainly important to con- sider in the automation discussion, but tort liability may not be the best mechanism to address every issue related to automation.32
I. LIABILITY FOR MACHINE INJURIES
A. A Brief History
Injuries caused by machines are nothing new. For as long as peo- ple have used machines, injuries have resulted—and machines have been with us for quite some time. The earliest evidence of simple ma- chines—tools that redirect force to make work easier, like axes— dates back millions of years to the beginning of the Stone Age.33 In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as hand axes.34 The pri- mary function of these tools was to hunt and cut meat,35 but they were also used to facilitate violence against people.36 Machines used in the furtherance of intentional torts were likely used negligently as well.
32 See, e.g., Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL’Y REV. 145 (2018) (arguing that the tax system incentivizes automation even in cases where it is not otherwise efficient and that automation decreases government tax revenue, and proposing changes to existing tax policies as a solution).
33 Kate Wong, Ancient Cut Marks Reveal Far Earlier Origin of Butchery, SCI. AM. (Aug. 11, 2010), https://www.scientificamerican.com/article/ancient-cutmarks-reveal-butchery/.
34 See Stone Age, MERRIAM-WEBSTER, https://www.merriam-webster.com/dictionary/ Stone%20Age [https://perma.cc/U6W4-M4M8]. See generally Sonia Harmand et al., 3.3-Million- Year-Old Stone Tools from Lomekwi 3, West Turkana, Kenya, 521 NATURE 310 (2015).
35 Wong, supra note 33.
36 M. Mirazo ́n Lahr et al., Inter-group Violence Among Early Holocene Hunter-Gatherers
of West Turkana, Kenya, 529 NATURE 394, 396 (2016).
 
2018] 504 THE REASONABLE COMPUTER 9
Given that home knife accidents led to about a third of a million emergency room visits in the United States in 2011 alone, it is not difficult to imagine that during the Stone Age these simple machines caused accidents.37
As history progressed, and the use and complexity of simple ma- chines grew, so too did the resultant injuries38: Mesopotamian sur- geons botched procedures,39 Greek construction zones were so dangerous they required physicians on site,40 and Egyptian embalmers accidently left instruments in their subjects.41 Such injuries continued unabated from the time complex machines were invented by the an- cient Chinese and Greeks to the time of the first modern industrial machines.42
The Industrial Revolution marked a turning point in the role of machines in society.43 Major technological advances occurred during
37 See Joe Yonan, Knife Injuries and Other Kitchen Mishaps Afflict Both Top Chefs and Everyday Cooks, WASH. POST (Jan. 7, 2013), https://www.washingtonpost.com/national/health- science/knife-injuries-and-other-kitchen-mishaps-afflict-both-top-chefs-and-everyday-cooks/ 2013/01/07/92e191f8-4af0-11e2-b709-667035ff9029_story.html.
38 See generally Y.C. CHIU, AN INTRODUCTION TO THE HISTORY OF PROJECT MANAGE- MENT 19–115 (2010) (discussing the use of technology in industrial activities). For example, al- most half a million people died building the Great Wall of China, although the number of these deaths due to machine injuries is unknown. Great Wall of China, HISTORY.COM, http:// www.history.com/topics/great-wall-of-china [https://perma.cc/7MP5-FJX5]. So common were machine and industrial injuries in the ancient world that ancient Greek, Roman, Arab, and Chi- nese laws provided for compensation schedules for accidents. See Gregory P. Guyton, A Brief History of Workers’ Compensation, 19 IOWA ORTHOPAEDIC J. 106, 106 (1999). Under ancient Arab law, “loss of a joint of the thumb was worth one-half the value of a finger. The loss of a penis was compensated by the amount of length lost, and the value an ear was based on its surface area.” Id.
39 See Emily K. Teall, Medicine and Doctoring in Ancient Mesopotamia, 3 GRAND VALLEY J. HIST. 1, 5 (2014). Unfortunately for these doctors, medical malpractice in Babylon was corpo- rally punishable. Allen D. Spiegel & Christopher R. Springer, Babylonian Medicine, Managed Care and Codex Hammurabi, Circa 1700 B.C., 22 J. COMMUNITY HEALTH 69, 81 (1997); see also GUIDO MAJNO, THE HEALING HAND: MAN AND WOUND IN THE ANCIENT WORLD 53 (1975).
40 DAVID MATZ, VOICES OF ANCIENT GREECE AND ROME: CONTEMPORARY ACCOUNTS OF DAILY LIFE 58 (2012).
41 Granted, this example involves cadavers rather than living patients, or so one hopes. Owen Jarus, Oops! Brain-Removal Tool Left in Mummy’s Skull, LIVE SCI. (Dec. 14, 2012, 8:03 AM), http://www.livescience.com/25536-mummy-brain-removal-tool.html/. It certainly portends modern medical malpractice cases involving retained surgical instruments. See, e.g., Atul A. Ga- wande et al., Risk Factors for Retained Instruments and Sponges After Surgery, 348 NEW ENG. J. MED. 229, 230 (2003).
42 Peter J. Lu, Early Precision Compound Machine from Ancient China, 304 SCIENCE 1638 (2004); cf. Russell Fowler, The Deep Roots of Workers’ Comp, 49 TENN. B.J. 10, 10–12 (2013) (discussing historical development of workers’ compensation schemes from the medieval through the modern era).
 43 Economists have argued the Industrial Revolution was “certainly the most important

10 505 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
this period in textiles, transportation, and iron making, which resulted in the development of machines for shaping materials and the rise of the factory system.44 It also resulted in a dramatic increase in the num- ber and severity of machine injuries.45 Working in industrial settings was a dangerous business, in part because employers often had mini- mal liability for employee harms.46 These dangerous working condi- tions persisted well into the twentieth century before the U.S. government began collecting data on work-related injuries in a sys- tematic way.47 In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of 61 deaths per 100,000 workers.48
In the modern era, the rate of work-related injuries has declined significantly. In 2016, for example, the U.S. Bureau of Labor reported 5190 fatal work injuries, a rate of 3.6 per 100,000 workers.49 The rea- son for this decline is multifactorial: changes to tort liability, evolved societal and ethics norms that place a greater priority on human wel- fare, a modern system of regulations and criminal liability that pro- tects worker wellbeing, as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Workplace accidents were responsible for
event in the history of humanity since the domestication of animals and plants, perhaps the most important since the invention of language. It bids fair to free us all, eventually.” Deirdre Mc- Closkey, Review of The Cambridge Economic History of Modern Britain, PRUDENTIA (Jan. 15, 2004), http://www.deirdremccloskey.com/articles/floud.php [https://perma.cc/UAP4-6ZZ3].
44 See generally History of Technology: The Industrial Revolution (1750–1900), EN- CYCLOPæDIA BRITANNICA, https://www.britannica.com/technology/history-of-technology/The-In- dustrial-Revolution-1750-1900 [https://perma.cc/QV7K-LKLK].
45 See generally HENRY ROGERS SEAGER, SOCIAL INSURANCE: A PROGRAM OF SOCIAL REFORM 24–52 (1910) (including a chapter on industrial accidents in a classic exposition of the philosophical movement for social insurance).
46 John S. Haller, Jr., Industrial Accidents—Worker Compensation Laws and the Medical Response, 148 WEST J. MED. 341–48 (1988); see also HORWITZ, 1780–1860, supra note 8, at 90. 47 See Progressive Era Investigations, U.S. DEP’T LAB., https://www.dol.gov/dol/aboutdol/ history/mono-regsafepart05.htm [https://perma.cc/HUT4-5WQE]. The first systematic U.S. sur- vey of workplace fatalities found that 526 workers died in “work accidents” in Allegheny County from July 1906 to June 1907. Improvements in Workplace Safety—United States, 1900–1999, 48 CDC MORBIDITY & MORTALITY WKLY. REP. 461, 461 (1999). Of those fatalities, 195 were steel-
workers. Id. Contrast that with 17 national steelworker fatalities in 1997. Id.
48 Improvements in Workplace Safety—United States, 1900–1999, supra note 47, at 461. The National Safety Council estimated that 18,000–21,000 workers died from work-related inju-
ries in 1912. Id.
49 BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR, NATIONAL CENSUS OF FATAL
OCCUPATIONAL INJURIES IN 2016, at 1 (2017), http://www.bls.gov/news.release/pdf/cfoi.pdf [https://perma.cc/2YMS-RA8F].
 
2018] 506 THE REASONABLE COMPUTER 11
approximately 4000 deaths in the United States in 2014 and a total cost of about $140 billion.50 More broadly, there were a total of almost 200,000 injury-related deaths in 2014 in the United States, with all un- intentional injuries costing some $850 billion.51 Unintentional injuries are the fourth leading cause of death.52
B. Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law now provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health.53
A tort is a harmful civil act, other than under contract, where one person is damaged by another, and it gives way to a right to sue.54 A variety of goals have been proposed for tort law: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.55 Whether tort law is the best means for achieving all of these goals is a matter of endless dispute.56 Jurists are united, however, in considering accident reduction as one of the central goals of tort law, if not the primary goal.57 By creating a framework for loss shifting from injured
50 NAT’L SAFETY COUNCIL, INJURY FACTS: 2016 EDITION 3, 8 (2016).
51 Id. Lost quality of life from those injuries is valued at an additional $3345.5 billion. Id. at
8.
52 Id. at 2.
53 Tort law primarily grew out of a focus on bodily injury and physical property damage,
but protection in modern times has been extended beyond the physical to include harm to emo- tional well-being, and economic loss.
The range of torts is as broad as human experience and includes such wrongful conduct as negligence (personal injury law for unintentional harm), intentional torts (e.g., assault, battery, trespass to land), products liability (defective products), abnormally dangerous activities liability (e.g., blasting, aerial pesticide spraying), nuisance (e.g., air, water, and noise pollution), defamation (libel and slander), pri- vacy invasion (private area intrusion and personal autonomy interference), and fraud (misrepresentation). Tort law study also includes consideration of legislative measures related to torts and alternatives to tort liability, for example, automobile no-fault compensation systems.
DOMINICK VETRI ET AL., TORT LAW AND PRACTICE 3 (5th ed. 2016).
54 See id. at 2. A tort governs loss shifting from injured victims to tortfeasors, and it dic-
tates who can sue and what they can sue for. See id. It is “the set of legal rules establishing liability and compensation for personal injury and death caused by the intentional or careless conduct of a third party.” Id.
55 See, e.g., Priest, supra note 9, at 645 n.23, 648.
56 See, e.g., Priest, supra note 10.
57 See Blomquist, supra note 10, at 628–29 (arguing that economic theory and moral phi-
 losophy both require accident reduction to be the primary aim of tort law).

12 507 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
victims to tortfeasors, tort law deters unsafe conduct.58 A purely finan- cially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.59
On a broader level, the law of torts is one of the primary ways in which society choses to allocate liability. And allocating liability has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing and pulse oximeters, or it can discourage the use of new technologies, as is usually the case where the standard of care is based on custom.60
Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts which do not require fault.61 Covering the “great mass of cases” in the middle are harms involving negligence.62
C. Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms.63 In practice, to pre- vail in most personal injury cases, a plaintiff must prove by a prepon- derance of the evidence that the defendant owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages.64 This generally requires proof that the defendant acted neg- ligently, which is to say, acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circum-
58 See Priest, supra note 9, at 648.
59 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (stating that
liability calculations should consider whether the probability of injury times potential damages is lower than the burden imposed).
60 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Parchomovsky & Stein, supra note 13, at 306 (discussing how the role of custom in
 tort law 61
62 63 64
impedes innovation).
Oliver Wendell Holmes, Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873). Id.
See Thomas C. Grey, Accidental Torts, 54 VAND. L. REV. 1225, 1283–84 (2001). See RESTATEMENT (SECOND) OF TORTS § 281 (AM. LAW INST. 1965).

2018] 508 THE REASONABLE COMPUTER 13
stances.65 Thus, if the courts determined that a reasonable person would not have headed out to sea without a radio to warn of storm conditions,66 manufactured a ginger beer with a snail inside,67 or dropped heavy objects off the side of a building,68 then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compen- sating victims as well as encouraging economic growth and progress.69 One way that tort law attempts to achieve this balance is by permit- ting recovery in negligence only where there has been socially blame- worthy conduct.70 Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will gen- erally be no liability. Juries play a key role in determining the reasona- ble person standard as applied to the facts of a case.71
D. Strict and Product Liability
While negligence governs virtually all accidents, there are excep- tions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities such as hazardous waste disposal or blasting.72 Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy.73 Thus, a defendant cor-
65 The idea that negligence involves conduct that falls below an objective standard was first articulated by Baron Alderson in the case of Blyth v. Birmingham Waterworks Co.:
Negligence is the omission to do something which a reasonable man, guided upon those considerations which ordinarily regulate the conduct of human affairs, would do, or doing something which a prudent and reasonable man would not do. The defendants might have been liable for negligence, if, unintentionally, they omitted to do that which a reasonable person would have done, or did that which a person taking reasonable precautions would not have done.
(1856) 156 Eng. Rep. 1047, 1049; 11 Ex. 781, 784.
66 See The T.J. Hooper, 53 F.2d 107 (S.D.N.Y. 1931).
67 See Donoghue v. Stevenson [1932] AC 562 (HL) (appeal taken from Scot.).
68 See Byrne v. Boadle (1863) 159 Eng. Rep. 299.
69 VETRI ET AL., supra note 53, at 12.
70 See James Barr Ames, Law and Morals, 22 HARV. L. REV. 97, 99 (1908).
71 VETRI ET AL., supra note 53, at 10.
72 Id. at 11.
73 See Frederick Pollock, Duties of Insuring Safety: The Rule in Rylands v. Fletcher, 2 L.Q.
REV. 52, 53 (1886). While early English common law imposed strict liability for certain wrongs such as trespass, Rylands v. Fletcher (1868) 3 LRE & I App. 330 (HL), was the progenitor of the doctrine of strict liability for abnormally dangerous activities, and its ruling had a major impact on the development of tort law. Pollock, supra, at 52, 59. In the case, Fletcher’s reservoir burst and flooded a neighboring mine run by Rylands through no fault of Fletcher. Id. at 53. This court held that “the person who for his own purposes brings on his lands and collects and keeps there,
 
14 509 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
poration that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defec- tive or because its properties are falsely represented.74 Product inju- ries cause upwards of 200 million injuries a year in the United States.75 In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products.76 The bulk of product liability cases involve claims for damages against a manufac- turer or retailer by a person injured while using a product.77 Typically, a plaintiff will try to prove that an injury was the result of some inher- ent defect of a product or its marketing and that the product was flawed or falsely advertised.78 Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and ac- curately marketed.79 Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product or that something other than the product caused the harm.80
Product liability was not always governed by strict liability. Origi- nally, American courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise.81 Toward the end of the nineteenth century, however, states began increasingly em- ploying the doctrine of caveat venditor and an implied warranty of merchantable quality.82 Under this doctrine, “[s]elling for a sound price raises an implied warranty that the thing sold is free from de-
anything likely to do mischief if it escapes, must keep it in at his peril, and, if he does not do so, is prima facie answerable for all the damage which is the natural consequence of its escape.” Id. at 54. Critics of the case objected to its potential impact on economic activity. See, e.g., THOMAS C. GREY, FORMALISM AND PRAGMATISM IN AMERICAN LAW 248 (2014) (noting that many “prestig- ious judges and commentators” repudiated Rylands on the basis that “liberal principles of formal equality and economic freedom, or a devotion to economic development, required rejection of tort liability without fault”).
74 DAVID G. OWEN, PRODUCTS LIABILITY LAW 1 (3d ed. 2014).
75 Id. at 1.
76 Id. at 3.
77 Id.
78 Id.
79 Id.
80 Id.
81 Id. at 17–18.
82 Id. at 18.
 
2018] 510 THE REASONABLE COMPUTER 15
fects, known and unknown (to the seller).”83 Ultimately, the doctrine of implied warranty of merchantable quality was reduced to statutory form in the Uniform Sales Act of 1906.84 Yet even so, manufacturers were in large part able to avoid liability for defective products by ar- guing they lacked privity of contract with consumers.85 This was possi- ble because in most cases consumers purchased products from third- party retailers rather than directly from manufacturers.86
That changed in 1916 with the New York Court of Appeals deci- sion in MacPherson v. Buick Motor Co.87 The case involved a motorist who was injured when one of the wooden wheels of his Buick col- lapsed.88 He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if the manufacturer of such a foreseeably dangerous product knows that it “will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”89 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted MacPherson’s holding.90 This shift was ac- companied by growing public support for consumer protection to- gether with the understanding that liability would not unduly burden economic activity.91 Businesses are often in the best position to pre- vent product injuries and can distribute liability through insurance.92
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products, Inc.,93 which held that manufacturers of defec- tive products are strictly liable for injuries caused by such products.94
83 Id. (quoting S. Iron & Equip. Co. v. Bamberg, E. & W. Ry. Co., 149 S.E. 271, 278 (S.C. 1929)).
84 Id.; see U.C.C. § 2-314 (AM. LAW INST. & UNIF. LAW COMM’N 2014). See generally Friedrich Kessler, The Protection of the Consumer Under Modern Sales Law, Part 1, 74 YALE L.J. 262 (1964).
85 OWEN, supra note 74, at 18.
86 Id.
87 111 N.E. 1050 (N.Y. 1916).
88 Id. at 1051.
89 Id. at 1053.
90 OWEN, supra note 74, at 22. Maine was the last state to abolish the privity requirement
in negligence actions in 1982. Id.
91 See id. at 22–23.
92 See id.
93 377 P.2d 897 (Cal. 1963) (in bank).
94 Id. at 900. Of note, Justice Roger Traynor, who wrote the majority opinion in the case,
had suggested this strict liability rule nineteen years earlier in a concurring opinion in Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436 (Cal. 1944). He argued responsibility should “be
 
16 511 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
This case represents the birth of modern products liability law in America.95 After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s, with the American Law Institute memorializing the rule in Section 402A of the Restatement (Second) of Torts.96
Of course, today’s products liability law is not as simple as this brief narrative suggests.97 It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts.98 Since the 1960s, a variety of state statutes have attempted to reform prod- ucts liability law, often to limit the rights of consumers in order to protect manufacturers.99 For our purposes, however, it suffices to say that as a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.100
II. COMPUTER-GENERATED TORTS
A. Automation Will Prevent Accidents
On May 7, 2016, a Tesla driver was killed in the first known fatal crash of a self-driving car.101 Tesla reported that the autopilot system
fixed wherever it will most effectively reduce the hazards to life and health inherent in defective products that reach the market.” Id. at 440 (Traynor, J., concurring). A few years before this case, the Supreme Court of New Jersey found manufacturers strictly liable in warrantee to re- mote consumers in Henningsen v. Bloomfield Motors, Inc., 161 A.2d 69, 77, 84 (N.J. 1960).
95 OWEN, supra note 74, at 23.
96 RESTATEMENT (SECOND) OF TORTS § 402A (AM. LAW INST. 1965); see OWEN, supra note 74, at 23.
97 For a more comprehensive view on products liability, the American Law Institute pub- lished a Restatement specifically on products liability in 1998. RESTATEMENT (THIRD) OF TORTS: PRODUCTS LIABILITY (AM. LAW INST. 1998).
98 OWEN, supra note 74, at 4.
99 Id. at 23.
100 See Vandermark v. Ford Motor Co., 391 P.2d 168, 171–72 (Cal. 1964) (in bank) (“Retail-
ers like manufacturers are engaged in the business of distributing goods to the public. They are an integral part of the overall producing and marketing enterprise that should bear the cost of injuries resulting from defective products. In some cases the retailer may be the only member of that enterprise reasonably available to the injured plaintiff. In other cases the retailer himself may play a substantial part in insuring that the product is safe or may be in a position to exert pressure on the manufacturer to that end; the retailer’s strict liability thus serves as an added incentive to safety. Strict liability on the manufacturer and retailer alike affords maximum pro- tection to the injured plaintiff and works no injustice to the defendants, for they can adjust the costs of such protection between them in the course of their continuing business relationship.” (citation omitted)).
101 Sam Levin & Nicky Woolf, Tesla Driver Killed While Using Autopilot Was Watching Harry Potter, Witness Says, GUARDIAN (July 1, 2016, 1:43 PM), https://www.theguardian.com/
 
2018] 512 THE REASONABLE COMPUTER 17
did not apply the brakes after the car’s sensor system failed to detect an eighteen-wheel truck and trailer.102 The car attempted to drive full speed under the trailer and the bottom of the trailer impacted the car’s windshield.103 The driver, whom Tesla claims should have re- mained alert and who also failed to apply the brakes, may have been watching a Harry Potter movie at the time.104
Surveys of attitudes toward self-driving cars have produced mixed results, but they have often uncovered negative opinions.105 A survey by the American Automobile Association in March 2016 re- ported that three out of four U.S. drivers surveyed said they would feel “afraid” to ride in a self-driving car.106 Only one in five said they would trust a driverless car to drive itself while they were inside.107 Another recent survey found that most U.K. citizens would feel un- comfortable with self-driving vehicles on the road, and more than
technology/2016/jul/01/tesla-driver-killed-autopilot-self-driving-car-harry-potter; see Anjali Singhvi & Karl Russell, Inside the Self-Driving Tesla Fatal Accident, N.Y. TIMES (July 12, 2016), http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html?_r=0. This has been the first reported fatality, but not the only reported crash for which a self-driving vehicle has been at fault. See, e.g., Tan Weizhen, Self-Driving Car in Accident with Lorry at One-North, TODAY (Oct. 18, 2016), http://www.todayonline.com/singapore/self-driving-car-involved-acci- dent-one-north. Other, nonfatal accidents have been attributed to self-driving vehicles. See Dave Lee, Google Self-Driving Car Hits a Bus, BBC NEWS (Feb. 29, 2016), http://www.bbc.co.uk/news/ technology-35692845. The National Highway Traffic Safety Administration (“NHTSA”) investi- gated this accident and issued a report in January 2017 stating that “[a] safety-related defect trend has not been identified at this time and further examination of this issue does not appear to be warranted.” NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., INVESTI- GATION PE 16-007 (2017), https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF. The NHTSA found the accident was beyond the capabilities of the vehicle’s Autopilot and Auto- matic Emergency Breaking systems. Id. The report went on to state that overall crash rates decreased by nearly forty percent after installation of Tesla’s Autosteer technology. Id. at 10.
102 Levin & Woolf, supra note 101.
103 Id.
104 Id.
105 Similarly, a poll of 1869 registered voters in January 2016 by Morning Consult found
that forty-three percent of registered voters said self-driving cars were unsafe, while only thirty- two percent said they were safe. Amir Nasr & Fawn Johnson, Voters Aren’t Ready for Driverless Cars, Poll Shows, MORNING CONSULT (Feb. 8, 2016), https://morningconsult.com/2016/02/08/vot- ers-arent-ready-for-driverless-cars-poll-shows/. Fifty-one percent of respondents said they would not ride in a driverless car, while twenty-five percent said they would. Id.; see Paul Lienert, Tesla Crash Does Little to Sway Public Opinion on Self-Driving Cars, AUTOMOTIVE NEWS (July 29, 2016, 2:21 PM), http://www.autonews.com/article/20160729/OEM06/160729812/tesla-crash-does- little-to-sway-public-opinion-on-self-driving-cars (discussing the results of other surveys).
106 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM (Mar. 1, 2016), http://newsroom.aaa.com/2016/03/three-quarters-of-ameri- cans-afraid-to-ride-in-a-self-driving-vehicle/.
 107 Id.

18 513 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
three-quarters would want to retain a steering wheel.108 Regulators are more optimistic than the public, but they are still cautious.109 Until very recently, California required human drivers to be present in all self-driving cars being tested on public roads.110 Two laws passed in 2016, however, now permit unmanned vehicles to operate on public roads under certain circumstances.111
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether computers are perfect (they are not), but whether they are safer than people (they are). Nearly all crashes involve human error.112 A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs.113 The U.S. Department of Transportation reports that more than 35,000 people died from motor vehicle accidents in the United States in 2015.114 It estimates the economic costs of those accidents at over $240 billion.115
By contrast, the Tesla fatality was the first known autopilot death in about 130 million miles driven by the system.116 It is also important to note that driverless technologies are in their infancy. Imagine how improved such technologies will be in ten years. One academic expert predicted in September 2016 that self-driving cars will be ten times safer than human drivers in three years, and one hundred times safer in ten years.117 At the point where automated cars are ten times safer
108 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, INQUIRER (Oct. 17, 2016), http://www.theinquirer.net/inquirer/news/2474351/over-half-of-brits- wont-feel-safe-using-the-streets-with-driverless-cars.
109 This caution is reflected, for example, in guidelines released in September 2016 by the Department of Transportation for safe design, development, and testing of self-driving cars. NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., FEDERAL AUTOMATED VE- HICLES POLICY: ACCELERATING THE NEXT REVOLUTION IN ROADWAY SAFETY 5–7 (2016), https://www.transportation.gov/sites/dot.gov/files/docs/AV%20policy%20guidance%20PDF.pdf.
110 Susmita Baral, Driverless Car Laws in California Get Major Changes in September, INT’L BUS. TIMES (Oct. 3, 2016, 5:40 PM), http://www.ibtimes.com/driverless-car-laws-california- get-major-changes-september-2425689.
111 Id.
112 NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., supra note 109, at 5.
113 ALEXANDER HARS, TOP MISCONCEPTIONS OF AUTONOMOUS CARS AND SELF-DRIVING
VEHICLES 1, 6 (2016), http://www.inventivio.com/innovationbriefs/2016-09/Top-misconceptions- of-self-driving-cars.pdf.
114 General Statistics, supra note 23.
115 Id.
116 A Tragic Loss, TESLA (June 30, 2016), https://www.tesla.com/en_GB/blog/tragic-loss
[https://perma.cc/LZ8X-UW2F].
117 Michael Belfiore, Self-Driving Cars Will Be 10x Safer Than Human Drivers in 3 Years,
MICHAEL BELFIORE BLOG (Sept. 20, 2016), http://michaelbelfiore.com/2016/09/20/self-driving- cars-will-be-10x-safer-than-human-drivers-in-3-years/ [https://perma.cc/4T78-CEWD]. Similarly,
 
2018] 514 THE REASONABLE COMPUTER 19
than human drivers, that could reduce the annual number of motor vehicle fatalities to about 3500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicted au- tonomous vehicles would reduce the number of auto deaths by about 30,000 a year.118 However, the report estimated that self-driving tech- nologies would not be adopted widely enough to permit this outcome until the middle of the century.119
B. Tort Liability Discourages Automation
To see why tort law discourages automation, it is important to look at the question of when it makes economic sense for a business to replace a human operator with a machine operator. In practice, it might be complex to calculate the cost of each operator. Human em- ployees have costs in excess of their salaries and wages, such as tax liability for employer portions of Social Security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; em- ployer portions of health insurance; paid holidays, vacations, and sick days; contributions toward retirement, pension, savings, and profit- sharing plans, etc.120 Computer costs may be simpler to estimate, but they may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision.121 For example, a person may require vocational training or be unable to work due to sickness; a computer may require software updates or be unable to work due to malfunction. Human operators may result in greater expenses for le- gal fees, administrative and overhead costs, as well as compliance with regulatory and employment requirements.122 Automation may provide
Bob Lutz, former General Motors (“GM”) vice chairman, predicted that GM’s first autonomous cars would have an accident rate about ten percent of those of human drivers. Michelle Fox, Self- Driving Cars Safer than Those Driven by Humans: Bob Lutz, CNBC (Sept. 8, 2014, 3:30 PM), http://www.cnbc.com/2014/09/08/self-driving-cars-safer-than-those-driven-by-humans-bob-lutz. html.
118 Bertoncello & Wee, supra note 25.
119 Id.
120 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level,
145 TAX NOTES 117, 118 (2014); see also WAYNE F. CASCIO, COSTING HUMAN RESOURCES (4th ed. 2000).
121 See ALFRED MARSHALL, PRINCIPLES OF ECONOMICS 368, 376 (8th ed. 1920).
122 See Cost of Small Business Employment, CTR. FOR ECON. & BUS. RES., www.cebr.com/
reports/cost-of-small-business-employment/ [https://perma.cc/V3F6-USE6].
 
20 515 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
tax benefits,123 but may infringe patents or result in negative public- ity.124 Whether to staff with a person or a machine may also take into account broader social policies. For instance, automation may pro- mote income inequality and unemployment. But businesses are re- quired to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to pro- mote social responsibility.125
The decision of whether to employ a computer or human opera- tor, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making—estimating uncertain future costs relatively accurately and making decisions as rational economic actors.126 Tort liability will only be one factor to consider when deciding whether to employ a computer or human op- erator. But, in the aggregate, tort liability will influence automation.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, businesses may not be directly liable for harms caused by autonomous computers.127 The computer’s manufacturer and other members of the supply chain will generally be liable. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability, for instance, by relying on independent contrac-
 123 See Abbott & Bogenschneider, supra note 32.
124 See, e.g., Kate Taylor, McDonald’s Ex-CEO Just Revealed a Terrifying Reality for Fast-
Food Workers, BUS. INSIDER (May 25, 2016, 10:05 AM), http://www.businessinsider.com/ mcdonalds-ex-ceo-takes-on-minimum-wage-2016-5 (discussing criticism of McDonald’s for re- placing workers with machines).
125 See generally Dodge v. Ford Motor Co., 170 N.W. 668, 682–84 (Mich. 1919). Of course, many companies argue they promote corporate social responsibility, and in some circumstances, there may be a business case for doing so. See, e.g., Archie B. Carroll & Kareem M. Shabana, The Business Case for Corporate Social Responsibility: A Review of Concepts, Research and Practice, 12 INT’L J. MGMT. REVS. 85 (2010).
126 See, e.g., Hugh Courtney, Jane Kirkland & Patrick Viguerie, Strategy Under Uncertainty, HARV. BUS. REV., Nov.–Dec. 1997.
127 See Mark A. Chinen, The Co-Evolution of Autonomous Machines and Legal Responsi- bility, 20 VA. J.L. & TECH. 338, 347–48 (2016).

2018] 516 THE REASONABLE COMPUTER 21
tors.128 Businesses are not usually liable for negligent harms caused by their independent contractors.129
Yet even in cases where liability rests with a business’s supplier or an independent contractor, such liability may indirectly impact a busi- ness. A manufacturer or retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent con- tractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product.130 Yet the fact that tort liability may be indirect and complex or that firms may purchase insurance to manage risk does not change the fact that tort liability has a financial cost which influences behavior.
Leaving aside tort liability, if both operators cost a business the same amount to employ, the decision of whether to utilize a person or computer should be neutral. If a business introduces the variable of tort liability into the decision, a human operator would be preferred. Harms caused by a person will be evaluated in negligence, but the same harms caused by a computer will be evaluated in strict liability. It is easier to establish strict liability than negligence.131 Strict liability does not require careless manufacturer behavior, only that a defect be present in a product.132 At least with regard to tort liability, the law
128 See, e.g., Kleeman v. Rheingold, 614 N.E.2d 712, 715 (N.Y. 1993). There are, however, limits on the extent to which businesses can rely on independent contractors or attempt to clas- sify employees as independent contractors. See, e.g., In re Morton, 30 N.E.2d 369, 371 (N.Y. 1940). As another example of how business can avoid tort liability for the actions of human operators, employers are not generally liable for intentional torts committed by employees. See, e.g., Ocana v. Am. Furniture Co., 91 P.3d 58, 70–71 (N.M. 2004).
129 See Kleeman, 614 N.E.2d at 715.
130 See generally RBB ECONOMICS, COST PASS-THROUGH: THEORY, MEASUREMENT, AND
POTENTIAL POLICY IMPLICATIONS (2014).
131 See Cronin v. J.B.E. Olson Corp., 501 P.2d 1153, 1162 (Cal. 1972) (in bank) (“[T]he very purpose of our pioneering efforts in [strict product liability] was to relieve the plaintiff from problems of proof inherent in pursuing negligence and warranty remedies, and thereby ‘to insure that the costs of injuries resulting from defective products are borne by the manufacturers . . . .’” (ellipsis in original) (citations omitted) (quoting Greenman v. Yuba Power Prods., Inc., 377 P.2d 897, 901 (Cal. 1963))); see also Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436, 441 (Cal. 1944) (Traynor, J., concurring) (“It is to the public interest to discourage the marketing of products having defects that are a menace to the public. If such products nevertheless find their way into the market it is to the public interest to place the responsibility for whatever injury they may cause upon the manufacturer, who, even if he is not negligent in the manufacture of the product, is responsible for its reaching the market. However intermittently such injuries may occur and however haphazardly they may strike, the risk of their occurrence is a constant risk and a general one. Against such a risk there should be general and constant protection and the manufacturer is best situated to afford such protection.”).
 132 See Cronin, 501 P.2d at 1162.

22 517 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
thus favors people over machines. This will hold true as long as com- puters are treated as “ordinary products” as to which strict liability is the default rule.
C. Computer-Generated Torts Should Be Negligence Based
Holding computer-generated torts to a negligence standard will result in an improved outcome: it will accelerate the adoption of auto- mation where doing so would reduce accidents. Of course, moving from a strict liability to a negligence standard would have some draw- backs. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to insure against loss. Indeed, this is why courts initially adopted strict product liability.133 Computer-gen- erated torts, however, differ from other product harms in that—once machines become safer than people—automation will result in net safety gains.
To illustrate this, imagine that with current technology a com- puter driver would be ten times safer than a human driver. In this case, it would be better that one human driver is replaced by a ma- chine than that the same machine becomes 100 times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for a computer driver (model C-A) is one fatality per 1 billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now imagine that we are able to improve C-A an additional ten- fold such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of 10 billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, how- ever, instead of focusing our efforts on improving C-A we simply re- place the human driver with another C-A, then over the course of 10 billion miles driven by C-A & C-A there will be a total of 20 fatalities. Once computers become safer than people, and particularly once computers become substantially safer than people, very significant re- ductions in accident rates will be gained by automation. Therefore—at some point—it is preferable to weaken the incentive to gain incremen-
 133 See, e.g., Greenman, 377 P.2d at 901.

2018] 518 THE REASONABLE COMPUTER 23
tal improvements in product safety to increase the adoption of safer technologies.
Also, even under a negligence standard, manufacturers will be incentivized to improve the safety of their computer systems because they may still be liable for accidents. Manufacturers will likely have the best information available to determine whether it would be bet- ter to pay to further reduce accident risks, e.g., whether an additional $10,000 per vehicle is worth a one percent reduction in accident risk, or whether to pay claims for additional accidents. Higher safety levels are not always better; inefficiently high safety levels may result in pro- hibitively high prices for consumers.134 To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, non-tort mechanisms could be brought to bear, such as regula- tory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.135
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and “computer tortfeasors” like Tesla’s autonomous driving software. Society’s rela- tionship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, computers are stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from a computer tortfeasor in this system are the concepts of independence and con- trol. Autonomous computers, robots, or machines are given tasks to complete, but they determine for themselves the means of completing those tasks.136 In some instances, machine learning can generate un- predictable behavior such that the means are not predictable either by those giving tasks to computers or even by the computer’s original programmers.137 But the difference between ordinary products and
134 David G. Owen, Rethinking the Policies of Strict Products Liability, 33 VAND. L. REV. 681, 710 (1980).
135 Id. at 694.
136 Curtis E.A. Karnow, The Application of Traditional Tort Theory to Embodied Machine
Intelligence, in ROBOT LAW 51, 52 (Ryan Calo et al. eds., 2016).
137 Id. Unlike Karnow, the author does not agree that the relevant distinction between autonomous and nonautonomous machines should be the degree to which they are unpredict- able. See id. at 55. Tort law should pursue functional solutions, and for the purposes of accident reduction, it should not matter whether a self-driving car operates per expert rules or per unpre-
 
24 519 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
autonomous computers should not be based on predictability, only on social and practical outcomes.138 It makes no difference to a person run over by a self-driving car what type of computer was operating the vehicle. Whether a computer acts according to fixed or expert rules created by a programmer or more complex machine-learning algo- rithms such as neural networks that generate new and sometimes un- foreseen behaviors, the physical outcome is the same.139 Leaving aside difficulties with courts attempting to distinguish between different types of computer architecture, ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.
D. Computer-Generated Torts as a Type of Machine Injury
Not all machine injuries would be computer-generated torts. To illustrate, consider two hypothetical accidents:
1) A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop off.
2) A crane operator is manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and one may assume) that the operator is at fault (al- though a creative plaintiff’s attorney might argue the crane was negli- gently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a fac- tual cause of the injury in torts vernacular, the machine did not inter- rupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an exten- sion of the operator, in the same way that the operator could commit a battery by throwing a rock at another person.140 In the second hypo- thetical, allocating fault is once again intuitively obvious. The machine
dictable machine-learning algorithms. See Abbott, supra note 19, at 1109 (arguing in the patent context that it would be impossible or impractical to distinguish between different computer architectures for determining whether a computer qualifies as an inventor and that the distinc- tion is irrelevant to promoting innovation).
138 Cf., e.g., David C. Vladeck, Machines Without Principals: Liability Rules and Artificial Intelligence, 89 WASH. L. REV. 117, 127 (2014) (arguing different liability rules may need to apply to injuries caused by computers that cannot be traced to a “design, manufacturing, or programming defect”).
139 See, e.g., Jack M. Balkin, The Path of Robotics Law, 6 CALIF. L. REV. CIR. 45, 45–46 (2015) (arguing against a focus on formalism and essentialism in the law).
140 See, e.g., R v. Day (1845) 1 Cox 207, 208 (holding that slashing a victim’s clothing with a knife constitutes battery).
 
2018] 520 THE REASONABLE COMPUTER 25
is at fault rather than the operator. The operator acted with reasona- ble care, and the injury was due to (one may assume) a flawed crane. These two scenarios would result in very different liability out- comes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby even if the manufacturer had exercised the utmost care in the design and con-
struction of the crane.
In both scenarios, an operator is using a crane in much the same
way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dy- namic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza Pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.141
Now imagine a third scenario:
3) A computer-operated, unmanned crane drops a steel frame on a passerby after incorrectly identifying the location for drop off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a ma- chine is being used as a tool. In Example 3, a computer has stepped into the shoes of the worker; it has replaced a person, and it is per- forming in essentially the same manner as a person. If the computer were a person, the computer would be liable in negligence and held to the standard of a reasonable person.142
Holding computer tortfeasors to a negligence standard requires rules for distinguishing between computer-generated torts and other
141 See J.J. Coulton, Lifting in Early Greek Architecture, 94 J. HELLENIC STUD. 1, 1, 12, 15–17 (1974).
142 The author has previously argued for a similar rule in the intellectual property context, where he proposed that computers should be recognized as authors and inventors if they inde- pendently perform creative acts. See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 187, 187 (Cassidy R. Sugimoto et al. eds., 2016); Abbott, supra note 19, at 1081. This rule would generate innovation by creating financial incentives for developing creative computers. See Abbott, supra; Abbott, supra note 19, at 1081.
 
26 521 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is a “computer tortfeasor” (Example 3).
Computer-generated torts could be those cases in which an au- tonomous computer occupies the position of a reasonable person in the negligence calculus and where automating promotes safety. It is only beneficial to encourage automation when doing so would reduce accidents. It would be harmful to encourage automation while human drivers outperform self-driving cars (though, it might still be beneficial to encourage automation for a subset of cases, for instance, the class of bad drivers). To shift from strict liability to negligence, manufactur- ers should have the burden to show by a preponderance of the evi- dence that a computer tortfeasor is safer on average than a person.
E. Implementation
Automation may occur on a more or less permanent basis, or it may be situational. For example, an autonomous vehicle may only permit machine control, or it may allow a person to switch between human and machine control. Where automation is all-or-nothing, the relevant inquiry should be whether a specific instance of automation would be expected to result in a net reduction in accidents, rather than to reduce the risk of the specific harm that occurred. For instance, if self-driving cars were better than people at avoiding collisions with other vehicles, but worse at avoiding collisions specifically with white cars, a computer driver might decrease the overall risk of accidents but increase the risk of colliding with white cars. In a case involving a collision with a white car, a negligence standard should still apply. Better that there should be more collisions with white cars so long as there are fewer collisions in total (assuming collisions with white cars do not result in disproportionate harm).
Even where automation is situational, it makes sense to apply a negligence standard. Hypothetically, if a self-driving car is on average ten times safer than a person, but only half as safe as a person in rainy conditions, a person should rely on autonomous driving software most of the time but operate the vehicle conventionally in the rain. If some- one instead uses self-driving software in the rain, the computer should still be evaluated under a negligence standard. It may be difficult for a user to know in advance what circumstances an autonomous computer is likely to encounter as well as when an autonomous computer will outperform a person. In addition, the manufacturer—as the liable

2018] 522 THE REASONABLE COMPUTER 27
party—may not have input into how its computers are used situation- ally. Manufacturers could utilize non-tort mechanisms to prevent un- safe uses, such as by warning users that self-driving cars may not be operated in the rain or by building in technological safeguards to pre- vent self-driving cars from operating in the rain. If self-driving cars prove to be less safe than human drivers in the rain, it is likely manu- facturers would still be liable for accidents in negligence.
Similarly, software used to diagnose disease based on medical imaging may outperform physicians generally, but underperform at detecting certain diseases. Ideally, this might result in human-machine collaborative review of imaging. If a machine were to underperform detecting lung cancer, for example, it should still be evaluated in negli- gence for its failures. The computer will likely be liable if a physician should have detected the lung cancer. In instances where a computer is generally safer than a person but underperforms in a certain area, it is likely to be liable in negligence when underperforming. This retains the ex ante incentive to improve an autonomous computer to reduce accidents and still allows victims to be compensated.
The basic inquiry about automation safety should focus on whether automation reduces, or is expected to reduce, overall acci- dents, not whether it did in fact reduce accidents in a specific instance. If Tesla can prove its self-driving cars are more likely safer overall than human drivers, this should be sufficient to shift to negligence even in a case where a particular substitution of a human driver with a self-driving car results in more accidents. Better that there should be fewer accidents in total even if one normal self-driving car gets in more accidents than the class average.
This new standard might sometimes involve complex problems of proof. A manufacturer would have the initial burden to prove its com- puters are safer than people, which creates an incentive to misrepre- sent a computer’s safety.143 Even when manufacturers are acting in good faith, it may be difficult to determine whether a computer is safer than a person. Research conducted to the highest scientific stan- dards sometimes fails to accurately predict real-world outcomes.144 It may be that Tesla has reason to believe its self-driving cars are signifi- cantly safer than human drivers, but once its cars enter the market-
143 Ryan Abbott, Big Data and Pharmacovigilance: Using Health Information Exchanges to Revolutionize Drug Safety, 99 IOWA L. REV. 225, 232–37 (2013) (discussing differences between premarket and postmarket data for evaluating safety in the pharmaceutical context and the in- centive for manufacturers to misrepresent safety profiles).
 144 Id.

28 523 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
place, they fail to meet expectations. For instance, Tesla’s research might fail to consider the reactions of drivers to self-driving vehicles in states other than California.145 In practice, automation may turn out to be safer or more dangerous than initially predicted. Decisions often must be made based on incomplete information, and waiting for per- fect knowledge risks sacrificing probable benefits at the altar of precaution.146
Adversarial legal proceedings are well suited for resolving such factual issues, and plaintiffs could use those proceedings to challenge manufacturer claims of safety.147 Thus, if Tesla presents evidence that its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs show that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles, that should shift the standard back to strict liability. It is worth noting that postmarket data is not always superior to premarket data; sometimes premarket data may be more predictive of future outcomes, particularly where postmarket data is limited or skewed.148
145 For example, although Google’s self-driving vehicles have been involved in accidents, nearly all accidents involving these vehicles have been the fault of human drivers. Chris Ziegler, A Google Self-Driving Car Caused a Crash for the First Time, VERGE (Feb. 29, 2016, 1:50 PM), http://www.theverge.com/2016/2/29/11134344/google-self-driving-car-crash-report. Pre-2017 monthly reports of accidents involving Google’s self-driving cars were originally available on Google’s website. See Steve Kovach, Google Quietly Stopped Publishing Monthly Accident Re- ports for Its Self Driving Cars, BUS. INSIDER (Jan. 18, 2017, 6:32 PM), http:// www.businessinsider.com/waymo-ends-publishing-self-driving-car-accident-reports-website- 2017-1. However, in 2017 the Google Self-Driving Car Project rebranded as Waymo, and Waymo no longer publishes monthly accident reports. See id.
146 Ryan Abbott & Ian Ayres, Evidence and Extrapolation: Mechanisms for Regulating Off- Label Uses of Drugs and Devices, 64 DUKE. L.J. 377, 380 (2014).
147 See Abbott, supra note 143, at 266 (discussing benefits of adversarial dispute resolu- tion). Alternately, manufacturers could have a duty to evaluate the safety of automation technol- ogies before sale and an ongoing duty to monitor their postmarket performance. This could mean that instead of plaintiffs and defendants engaging in a “battle of the experts” focused on objective safety outcomes, a manufacturer’s good faith belief that its computers were safe would be sufficient to give rise to a negligence standard. Plaintiffs could only rebut the presumption that a manufacturer acted in good faith. Thus, Tesla would remain liable in negligence if it could prove its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs could prove that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles. Unless plaintiffs could prove Tesla knew, or should have known, that its initial predictions were not accurate or prove that Tesla failed to monitor the performance of its cars, Tesla would not be liable. But this would create a greater risk that manufacturers would fail to aggressively monitor, or that manufacturers would fail to monitor appropriately despite their best efforts. Better to base the standard on objective evidence of safety than a manufacturer’s subjective knowledge. Better also to empower plaintiffs’ attorneys to hold manufacturers to account than to put foxes in charge of guarding henhouses.
148 See generally Ryan Abbott, The Sentinel Initiative as a Cultural Commons, in GOV- ERNING MEDICAL KNOWLEDGE COMMONS (Katherine J. Strandburg et al. eds., 2017), https://
 
2018] 524 THE REASONABLE COMPUTER 29
It should not be necessary for a computer tortfeasor to physically replace a human operator for negligence to apply. It should be suffi- cient that a computer is performing a task which a person could rea- sonably do. For example, if a new taxi company goes into business using a fleet of only self-driving vehicles, computers would not have replaced human operators, but they would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software, e.g., the engine, could not be reasona- bly substituted. A person could drive a taxi instead of a computer, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as a computer tortfeasor, but the other parts of the vehicle would not.
Once a manufacturer establishes that a computer tortfeasor is safer than a person, the negligence test should focus on whether the computer’s act was negligent, rather than whether the computer was negligently designed or marketed. Again, the computer is taking the place of a person in the traditional negligence paradigm, and this par- adigm would treat the computer more like a person than a product. It makes no difference to an accident victim what a computer was “thinking”; only how the computer acted.149 Accident victims have a right to demand careful conduct regardless of how well a computer tortfeasor may have been designed.150
Applying the above rules to the crane examples, Example 1 would result in human liability because the human operator acted carelessly and the crane did not interrupt a foreseeable chain of events. It would retain strict manufacturer liability for Example 2 be- cause a person could not reasonably be substituted for a crane. It would permit negligent manufacturer liability for Example 3 (because the computer was automating a task which a person could have per- formed), but only if the computer tortfeasor is on average safer than a human operator.
www.cambridge.org/core/books/governing-medical-knowledge-commons/sentinel-initiative-as-a- knowledge-commons/FE736CE30779C4FFE5BA740F2A0FBBFE/core-reader (discussing diffi- culties with using real-world data to predict safety outcomes in an example using the medication Dabigatran).
149 To appropriate criminal law terminology, we are interested in the actus reus rather than the mens rea. See generally DENNIS J. BAKER, TEXTBOOK OF CRIMINAL LAW 167 (3d ed. 2012) (explaining the concept of actus reus). There is no benefit to punishing computer tortfeasors for wrongful actions, even under civil law.
150 See Oliver Wendell Holmes, Lecture III: Torts—Trespass and Negligence, in 3 THE COL- LECTED WORKS OF JUSTICE HOLMES 154, 157–58 (Sheldon M. Novick ed., 1995).
 
30 525 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
In the context of automated driving, human drivers would be lia- ble for harms they cause due to their own driving decisions, while a manufacturer would be strictly liable for harms caused by defective machines that are not automating human functions (as would be the case for MacPherson’s Buick151), but manufacturers would be liable in negligence rather than strict liability for errors made by autonomous driving software if the software were proven safer on average than a person.
F. Financial Liability
Autonomy exists on a continuum. In practice, the divide between an ordinary product and an autonomous computer may not be clear cut. In the self-driving car context, for example, under one widely adopted framework, vehicles are categorized on a zero to five scale based on who does what, when.152 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction be- tween person and machine. When computers and people share deci- sionmaking, traditional principles of joint and several liability should apply.153 For instance, where a human driver and a computer driver are both at fault, as may be the case where a Tesla system fails to detect a truck while a human driver is watching a movie, both drivers could be liable for either the entire injury or in proportion to their wrongdoing.154
Whether in strict liability or negligence, computers could not be financially liable for their harms. Computers do not have property rights, are owned as chattel, and would not be influenced by the spec- ter of liability in the way a person might be influenced. For the pur- poses of financial liability, the computer’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product or where several parties are involved in the distribu-
151 See supra notes 87–89 and accompanying text.
152 See SAE INT’L, AUTOMATED DRIVING (2014) (on file with the Law Review) (describing
the SAE taxonomy).
153 See generally Richard W. Wright, The Logic and Fairness of Joint and Several Liability, 23 MEM. ST. U. L. REV. 45 (1992) (reviewing and analyzing the public policy debate over joint and several liability).
 154 Id. at 46.

2018] 526 THE REASONABLE COMPUTER 31
tion chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which Gen- eral Motors licenses and incorporates in its vehicles, and the vehicles are then leased by an independent retailer to Lyft. Common law liabil- ity rules could be altered by firms in the supply chain. That would be particularly likely to occur where manufacturers and retailers are large, sophisticated entities. For example, General Motors might in- demnify Apple, Delphi, and Lyft in return for more favorable licens- ing and leasing terms.
Alternately, the computer’s owner could be liable for its harms. That would be somewhat akin to treating computer tortfeasors as em- ployees and making owners liable under theories of vicarious liabil- ity.155 It is particularly easy to imagine owners purchasing insurance for harms caused by autonomous computers in the self-driving car context, where insurance policies may soon come with a rider (or dis- count) for autonomous software. Owner liability might further incen- tivize the production of autonomous computers given that manufacturers would have less liability, but it might reduce adoption because owners would be taking on that liability. These two effects might offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of computer tortfeasors, and because manufacturers are in the best position to im- prove product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self-driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars be- come significantly safer than human drivers, insurance rates will de- crease for self-driving cars and perhaps increase for human drivers.156 This should have a nudging effect on self-driving car adoption as fi- nancially sensitive individuals take auto premiums into account in de- ciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower pre- miums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsi- ble for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
155 See generally Fleming James, Jr., Vicarious Liability, 28 TUL. L. REV. 161 (1954).
156 See supra text accompanying notes 112–19.
 
32 527 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 G. Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the cre- ation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation.157 For exam- ple, incentives could take the form of grants for research and develop- ment,158 loans to build production facilities,159 enhanced intellectual property rights,160 prizes,161 preferential tax treatments,162 or govern- ment guarantees.163
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allow- ance Rebate System (“CARS”), better known as “cash for clunkers.”164 CARS provided consumers trading in old vehicles with
157 See generally Nancy Gallini & Suzanne Scotchmer, Intellectual Property: When Is It the Best Incentive System?, in 2 INNOVATION POLICY AND THE ECONOMY 51 (Adam B. Jaffe et al. eds., 2002).
158 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes De- bate, 92 TEX. L. REV. 303, 321 (2013) (discussing the role of government grants in innovation policy).
Today, direct federal R&D spending (which includes the very small amount cur- rently spent on prizes) is about $130–$140 billion per year—slightly more than half of which is defense-related. Many states also provide direct R&D support: in fiscal year 2009, states spent $3.6 billion on support for R&D at state universities and another $1.3 billion on other grants and facilities for in-state research.
Id. (footnote omitted).
159 See, e.g., Joe Stephens & Carol D. Leonnig, Solyndra: Politics Infused Obama Energy
Programs, WASH. POST (Dec. 25, 2011), https://www.washingtonpost.com/solyndra-politics-in fused-obama-energy-programs/2011/12/14/gIQA4HllHP_story.html?utm_term=.Bb171adb15da (providing background information on the billions in unexpected costs to taxpayers from contro- versial loans defaulted on by green technology programs).
160 See, e.g., Ryan Abbott, Treating the Health Care Crisis: Complementary and Alternative Medicine for PPACA, 14 DEPAUL J. HEALTH CARE L. 35, 62–98 (2011) (noting that pharmaceu- tical manufacturers can receive market exclusivity, extended patent terms, or even sui generis forms of intellectual property protection for preferred technologies).
161 See, e.g., Richard A. Posner, Intellectual Property: The Law and Economics Approach, 19 J. ECON. PERSP. 57, 58–59 (2005).
162 See, e.g., Nick Bloom et al., Do R&D Tax Credits Work? Evidence from a Panel of Countries 1979–1997, 85 J. PUB. ECON. 1, 2 (2002); Bronwyn Hall & John Van Reenen, How Effective Are Fiscal Incentives for R&D? A Review of the Evidence, 29 RES. POL’Y 449, 449 (2000).
163 See, e.g., Gunhild Berg & Michael Fuchs, Bank Financing of SMEs in Five Sub-Saharan African Countries: The Role of Competition, Innovation, and the Government (World Bank, Pol- icy Research Working Paper No. 6563, 2013).
164 TED GAYER & EMILY PARKER, CASH FOR CLUNKERS: AN EVALUATION OF THE CAR ALLOWANCE REBATE SYSTEM 1 (2013), https://www.brookings.edu/wp-content/uploads/2016/06/ cash_for_clunkers_evaluation_paper_gayer.pdf.
 
2018] 528 THE REASONABLE COMPUTER 33
vouchers of between $3500 and $4500 to purchase new cars.165 It was a nearly $3 billion U.S. federal program designed as a short-term eco- nomic stimulus and to benefit U.S. auto manufacturers.166 It was also intended to promote safer, cleaner, more fuel-efficient vehicles.167 Ul- timately, while critics dispute the effectiveness of the program at stim- ulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel efficiency and safety, and it was popular with consumers.168 In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alter- natives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that place caps on damages, limit contingency fees, eliminate joint and several liability, mandate periodic payments, or reduce the statute of limitations.169
Finally, the government could promote safety by means of regula- tion. This could involve requirements for industries to achieve mini- mum safety targets or direct requirements to adopt certain technologies.170 At the point where self-driving cars become ten or a
165 Id.
166 See id. at 1–2; $2 Billion More for Clunker Car Trade-Ins Passes Senate, N.Y. TIMES: CAUCUS (Aug. 6, 2009, 9:05 PM), https://thecaucus.blogs.nytimes.com/2009/08/06/2-billion-more- for-clunker-car-trade-ins-passes-senate/.
167 See GAYER & PARKER, supra note 164, at 1–2.
168 The Department of Transportation reported the program succeeded at boosting eco-
nomic growth and creating jobs. Press Release, Nat’l Highway Traffic Safety Admin., Secretary LaHood Touts Success of Cash for Clunkers; Responds to Reports by DOT Inspector General, GAO (Apr. 29, 2010), https://www.nhtsa.gov/press-releases/secretary-lahood-touts-success-cash- clunkers-responds-reports-dot-inspector-general. Others were less bullish. One study found that the total costs of the program outweighed the benefits by $1.4 billion. See Burton A. Abrams & George R. Parsons, Is CARS a Clunker?, ECONOMISTS’ VOICE, Aug. 2009, at 4. Another study argued that the program increased short-term spending, but decreased overall spending on new cars. Mark Hoekstra et al., Cash for Corollas: When Stimulus Reduces Spending 23 (Nat’l Bureau of Econ. Research, NBER Working Paper Series No. 20349, 2014), http://www.nber.org/papers/ w20349.pdf. With regard to fuel efficiency, one study found that the program improved the aver- age fuel economy of all vehicles purchased by 0.6 mpg in July 2009, and by 0.7 mpg in August 2009. MICHAEL SIVAK & BRANDON SCHOETTLE, U. MICH. TRANSP. RESEARCH INST., THE EF- FECT OF THE “CASH FOR CLUNKERS” PROGRAM ON THE OVERALL FUEL ECONOMY OF PUR- CHASED NEW VEHICLES 4 (2009), http://deepblue.lib.umich.edu/bitstream/2027.42/64025/1/ 102323.pdf.
169 These are some of the reforms created by the Medical Injury Compensation Reform Act of 1975 (“MICRA”) enacted by the California legislature to lower medical malpractice lia- bility insurance premiums. Cal. Civ. Code §§ 3333–3333.2 (West 2016).
170 See generally HEALTH & SAFETY EXEC., A GUIDE TO HEALTH AND SAFETY REGULA- TION IN GREAT BRITAIN 11 (2013), http://www.hse.gov.uk/pubns/hse49.pdf (outlining the occu-
 
34 529 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
hundred times safer than human drivers, nonautonomous driving could be prohibited.171 Regulatory solutions may be most appropriate where the benefits of automation are overwhelming and where it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer- and business-friendly so- lution. While consumers would have more difficulty seeking to re- cover for accidents, they would also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. For busi- nesses, it would lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not re- quire government funding, additional regulatory burdens on industry, or new administrative responsibilities. Additionally, it is an incremen- tal solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than more radical solutions.172 For all the above reasons, shifting to negli- gence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer cred- its, combined with direct regulations in certain instances.
Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable be- cause it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy.173 Indeed, automation
pational health and safety system in Great Britain and the various types of safety standards imposed on businesses).
171 See Stuart Dredge, Elon Musk: Self-Driving Cars Could Lead to Ban on Human Driv- ers, GUARDIAN (Mar. 18, 2015, 3:22 AM), https://www.theguardian.com/technology/2015/mar/18/ elon-musk-self-driving-cars-ban-human-drivers.
172 Indeed, some critics argued that CARS primarily subsidized Japanese auto manufactur- ers, while a similar Japanese stimulus program excluded American auto manufacturers. John Crawley, Japanese, Koreans Gain Most from Cash for Clunkers, REUTERS (Aug. 26, 2009, 5:34 PM), http://www.reuters.com/article/retire-us-usa-clunkers-sales-idUSTRE57P5C220090826; Douglas Stanglin, U.S. Cars Excluded from Japan’s Cash-for-Clunkers Program, USA TODAY (Dec. 11, 2009, 2:09 PM), http://content.usatoday.com/communities/ondeadline/post/2009/12/us- cars-excluded-from-japans-cash-for-clunkers-program-/1#.WDwOQXfc-t8.
173 See, e.g., Scherer, supra note 17, at 389–90 (discussing the reactionary nature of court proceedings); see also Bibb v. Navajo Freight Lines, Inc., 359 U.S. 520, 524 (1959) (“Policy deci- sions are for the . . . legislature . . . .”).
 
2018] 530 THE REASONABLE COMPUTER 35
to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could independently adopt these rules. Lawmakers would then have the option of modifying the common law.
III. THE REASONABLE ROBOT
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
—Oliver Wendell Holmes, Jr.174
A. When Negligence Is Strict
Negligence may function almost like strict liability for people with below average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.”175 This issue was at the heart of Vaughan v. Menlove176 in 1837, which con- cerned a defendant who lacked normal intelligence.177 The defense ar- gued that it would thus be unfair to hold him to the standard of an ordinary person and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, hold- ing that ordinary prudence should apply in every case of negligence.178 As Oliver Wendell Holmes, Jr., articulated in 1881, “The law consid- ers . . . what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”179 That re- mains the case today; a modern defendant cannot generally escape
174 O.W. HOLMES, JR., THE COMMON LAW 108 (1881).
175 Id. Holmes did distinguish between a lack of “intelligence and prudence” and “distinct
defect[s]” which he believed did not generally lead to strict liability. Id. at 108–10.
176 (1837) 132 Eng. Rep. 490, 492; 3 Bing. (N.C.) 468, 471.
177 Id. at 492.
178 Id. at 490, 492.
Instead, therefore, of saying that the liability for negligence should be co-extensive with the judgment of each individual, which would be as variable as the length of the foot of each individual, we ought rather to adhere to the rule which requires in all cases a regard to caution such as a man of ordinary prudence would observe. That was in substance the criterion presented to the jury in this case, and therefore the present rule must be discharged.
Id. at 493.
179 HOLMES, supra note 174, at 108.
 
36 531 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
liability for causing a motor vehicle accident because she has slow re- flexes, poor vision, or anxiety while driving.180
There are benefits to such a rule. Logistically, as Justice Tindal noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state.181 Better for administrative purposes to work with an external, objective stan- dard than to prove individual capacities and state of mind. Substan- tively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal con- duct of others.182 As Holmes articulated, damages caused by individu- als with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare, but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else perhaps “should refrain from operating an automobile” at all.183
B. The New Hasty and Awkward
Collectively, people are not the best drivers, even when they re- frain from drinking behind the wheel,184 falling asleep on the high- way,185 or colliding into police cars while playing Poke ́mon Go.186 But compared to computers? It will not be long until computers are safer than the average person and then safer than any human driver. Princi- ples of harm avoidance suggest that once it becomes practical to auto- mate, and once doing so is safer, a computer should become the new “reasonable person” or standard of care.
180 See, e.g., Roberts v. Ring, 173 N.W. 437, 437–38 (Minn. 1919).
181 Vaughan, 132 Eng. Rep. at 493.
182 See Holmes, supra note 150, at 154–55.
183 Roberts, 173 N.W. at 438. In this case, a seventy-seven-year-old defendant with defec-
tive sight and hearing was held liable for running over a seven-year-old boy when it was estab- lished that a reasonable driver could have stopped the car. Id.
184 See J. Michael Kennedy, Allowed in 26 States: Drinking and Driving: A Legal Mix, L.A. TIMES (Jan. 26, 1985), http://articles.latimes.com/1985-01-26/news/mn-13688_1_container-law (noting that until recently, it was even legal in many states to “sip[] on a Scotch and soda while cruising down the interstate”).
185 See David Boroff, Two Women Dead as Greyhound Bus Driver Falls Asleep at Wheel During California Crash; Driver was ‘Fatigued,’ Police Say, N.Y. DAILY NEWS (Jan. 19, 2016, 9:01 PM), http://www.nydailynews.com/news/national/greyhound-bus-crash-kills-2-injures-18-ar- ticle-1.2501658.
186 See Sarah Begley, Driver Hits Cop Car While Playing Poke ́mon Go. The Whole Thing Was Caught on Video, TIME (July 20, 2016), http://time.com/4414998/pokemon-go-hits-cop-body- cam/ (discussing a driver playing Poke ́mon Go who collided with a police car and had the inci- dent captured on video, and quoting the driver as saying, “That’s what I get for playing this dumb – game”).
 
2018] 532 THE REASONABLE COMPUTER 37
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defen- dant would be judged against what a computer would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an automated car would have prevented the injury. In fact, it may be that the automated vehicle is only able to prevent such an accident because it has superhuman abili- ties. It may have software capable of ultrafast decisionmaking, monitors that surpass human senses, and external cameras that ex- pand peripheral view beyond that of a person.187
With the reasonable person test, jurors are asked to put them- selves in the shoes of a reasonable person and decide what that person would have done.188 It may be a challenge for a juror to follow that reasoning in the case of a reasonable computer (or reasonable robot or machine). The reasonable computer, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasona- ble” in the context of a computer is an anthropomorphism to assist people conceptually. In fact, computers largely function according to fixed rules which—when all goes well—result in foreseeable behav- ior.189 Even those computers which can generate unpredictable behav- ior are still likely to be more predictable than people, particularly where such machines have been found to improve safety.190 It should be more or less possible to determine what a computer would have done in a particular situation.
To take a simple case, imagine an individual driving on dry pave- ment at forty miles per hour colliding with a child running into the road 150 feet ahead of the driver’s vehicle.191 To determine whether the driver is liable under the reasonable computer standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable computer would not have collided with
187 See supra notes 116–19 and accompanying text.
188 See supra notes 65, 71 and accompanying text.
189 THOMAS A. PETERS, COMPUTERIZED MONITORING AND ONLINE PRIVACY 97 (1999).
Malfunctioning computers would not be “reasonable” computers.
190 See id.
191 See Why Your Reaction Time Matters at Speed, NAT’L HIGHWAY TRAFFIC SAFETY AD-
MIN. (Aug. 2015), www.nhtsa.gov/nhtsa/Safety1nNum3ers/august2015/S1N_Aug15_Speeding_ 1.html.
 
38 533 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of self-driving software would be known, and the breaking distance could be standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software. Even in more com- plex cases, it should be easier to predict how a computer would have behaved than a person because computers are more predictable. Thus, it is possible to have a more objective test for the reasonable com- puter than for the reasonable person.
A defendant might argue that it is unfair for his best efforts to result in liability. A computer standard of care essentially makes peo- ple strictly liable for their accidental harms. That is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below- average driver due to computers. It may appear unfair to impose lia- bility on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or a computer.
Tort liability would not prohibit people from driving even at the point where computers become substantially safer than people. If that were a desired outcome it could be accomplished through command- and-control legislation.192 Instead, a computer standard of care would mean that people drive at their own risk. If a driver causes an acci- dent, he or she will be liable for the resultant damages. A tort-based incentive may be superior to an inflexible statutory mandate because there may be benefits to human driving unrelated to accidents, for instance, promoting freedom and autonomy.193 Individuals who partic- ularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, a computer standard of care is likely to have a significant impact on behavior. Making individ- uals and businesses essentially strictly liable for their harms will strongly discourage certain undertakings. In the self-driving car con- text, it would likely result in far fewer human drivers as insurance
192 See Orly Lobel, The Renew Deal: The Fall of Regulation and the Rise of Governance in Contemporary Legal Thought, 89 MINN. L. REV. 342, 371–404 (2004) (discussing the trend from regulations to incentive-based regimes).
193 See generally Ryan & Deci, supra note 20, at 6–7 (arguing that people have three basic psychological needs: (1) connectedness, (2) autonomy, and (3) feeling competent).
 
2018] 534 THE REASONABLE COMPUTER 39
rates for traditional vehicles become prohibitively expensive relative to rates for self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. Automatons may be prohibitively expensive or only available in limited quantities. That is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. There- fore, to introduce a computer standard of care, a plaintiff should have to show by a preponderance of the evidence that a person was per- forming a task that could be performed by a computer and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of a computer operator where 1) no such operator existed at the time of the accident, 2) no computer operator was available to the defendant, 3) a computer operator was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory require- ments for a human operator). If Tesla could manufacturer a com- pletely safe autonomous vehicle but at a cost of $1 million dollars, it would not be reasonable to require consumers to automate.
C. Reasonable People Use Autonomous Computers
As an alternative to the reasonable computer standard, the rea- sonable person could be a person using an autonomous computer. For example, once self-driving cars become safer than traditional vehicles, a jury might find that it is unreasonable to drive yourself rather than to use a self-driving car. Applying the “reasonable person using an autonomous computer” standard to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on her driving in the first place. A reasonable person would not have driven.
Under either the reasonable person or reasonable computer stan- dard, a human driver would be compared with a self-driving car, but in different ways. With the reasonable computer standard, courts would evaluate the human driver’s proximally harmful act, whereas with the reasonable person standard, courts would evaluate the human driver’s a priori decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable per- son standard would be more in line with the existing negligence re- gime, and it would be a less radical way to accomplish the goal of incentivizing automation to improve safety.

40 535 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
While keeping the reasonable person standard would be concep- tually easier, in practice it would be less desirable. The goal is to com- pare the harmful act of the person and computer, not to target the initial decision to automate. It is problematic to base liability on the decision to automate because it either must focus on the question of whether automation is generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform a machine. A situational focus must still compare the harmful act of a person versus a computer.
It is likely that as autonomous computers are introduced they will be safer at automating certain activities than others. For instance, au- tomated computers working to diagnose disease may be superior to physicians at detecting certain conditions, but not others. Self-driving cars may be safer than human drivers on average, but not safer than professional or above-average drivers. Autonomous vehicles may also be safer under most conditions, but might be relatively poor at, for example, driving off road. So, while automation may generally im- prove safety, optimal accident reduction may require a mix of com- puter and human activity.
Suppose a self-driving car is ten times safer than a human driver generally, but only half as safe as a human driver in icy conditions. Now suppose a human driver encounters a patch of black ice and causes an accident under circumstances in which she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable computer, she would es- cape liability if the computer would have been unable to avoid the accident (which is likely if the computer is half as safe in icy condi- tions). If the reasonable person using an autonomous computer test focuses on whether an autonomous computer is generally safer, how- ever, she would be liable. That test would conclude that it would have been unreasonable not to use a self-driving car because self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an autonomous com- puter evaluation could be situational. For instance, it could be reason- able not to use an autonomous computer, but only in icy conditions. However, this is just a more convoluted version of the reasonable computer test because it requires evaluating whether a computer would be safer than a person in a particular instance. That essentially asks how the computer would have acted in a situation—which is the

2018] 536 THE REASONABLE COMPUTER 41
reasonable computer standard.194 It would then require asking, based on that knowledge, which might be impractical for a person to have, whether an earlier decision to automate was reasonable. On top of that, it presupposes the ability to activate and deactivate automation as needed. In the black ice hypothetical, it could require the driver to know in advance of activating self-driving software whether there were icy conditions and how the computer would perform in icy con- ditions. It might require the driver to activate or deactivate automa- tion only during icy conditions or to understand whether the risk of using the computer in icy conditions outweighed the benefits of using the computer for other parts of the trip.
D. The Reasonable Computer Standard for Computer Tortfeasors
This Article proposes holding computer tortfeasors to a negli- gence standard and comparing their acts to the acts of a reasonable person after technology has advanced to the point that computers have been proven safer than people.195 It also proposes replacing the reasonable person standard with the reasonable computer standard, again, once this point has been reached.196 This means that computer tortfeasors would be held to the reasonable computer standard.
There will be instances in which it still makes sense to apply the reasonable person standard to computer tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of a computer, for instance, where auto- mation is prohibitively expensive or where computer operators are not widely available. We would not want to hold a computer tortfeasor to a higher standard than a human defendant. In some in- dustries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become customary or to meet the criteria proposed earlier for adopting the reasonable com- puter standard.
Eventually, once a reasonable computer becomes the standard of care, it would also be the standard for computer tortfeasors. For in- stance, if a self-driving Audi collided with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Volvo cars. There are a variety of ways to deter- mine the reasonable computer standard, for example, considering the industry customary, average, or safest technology. Under any stan-
194 See supra text accompanying notes 184–93.
195 See supra text accompanying notes 184–93.
196 See supra text accompanying notes 184–93.
 
42 537 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
dard, this is a different test than the current strict liability standard, in which the inquiry focuses on whether a product was defectively de- signed or its properties falsely represented.
As computers improve, the reasonable computer standard would grow stricter. That is alright, because once the reasonable computer is exponentially safer than a person, it is likely that computer tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable computer standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until computers are exponentially safer than people.
E. The Automation Problem
The impact of automation goes far beyond accident reduction. Just focusing on autonomous vehicles, the widespread adoption of this technology could have revolutionary benefits. It will allow people to be more productive and mobile, and it will reduce emissions and con- gestion.197 One autonomous vehicle could replace up to twelve normal cars.198 Given that the average automobile spends about ninety-five percent of its time sitting in place, self-driving cars may also eliminate the need for most parking.199 Getting rid of parking just in the United States would free up space the size of Connecticut and could allow redesigned, pedestrian-friendly urban areas.200 Automation will in- crease freedom for the disabled, blind, and unlicensed. It might elimi- nate traffic lights and the need for private car ownership.201 The net result of self-driving cars could be substantial environmental, eco- nomic, and social benefits.202
Driverless technologies may also result in the displacement of human workers, increased unemployment, greater wealth disparities, and a reduction of the tax base. Automation threatens the jobs of truck, bus, and taxi drivers who collectively make up about three per- cent of the working population.203 In other industries, automation has
197 DEP’T FOR TRANSPORT, THE PATHWAY TO DRIVERLESS CARS: SUMMARY REPORT AND ACTION PLAN 6 (2015).
198 Clive Thompson, No Parking Here, MOTHER JONES (Jan.–Feb. 2016), http:// www.motherjones.com/environment/2016/01/future-parking-self-driving-cars.
199 Id.
200 Id.
201 DEP’T FOR TRANSPORT, supra note 197, at 6.
202 Id.
203 RICHARD HENDERSON, INDUSTRY EMPLOYMENT AND OUTPUT PROJECTIONS TO 2024,
 
2018] 538 THE REASONABLE COMPUTER 43
resulted in reduced workforces.204 For instance, employment at com- puter and electronic companies decreased forty-five percent from 2001 to 2016.205 Employment at semiconductor makers decreased by half during the same period.206
These are all important issues to consider in formulating automa- tion policies, but tort law may not be the best mechanism to address these broader concerns.207 Ultimately, tort liability alone will not de- termine whether automation occurs. Consumer demand and the eco- nomics of automation will bring about increasing automation in the absence of laws prohibiting it.208 Tesla, for example, is planning to make all its cars self-driving, and Tesla is far from alone in automating vehicles.209 Billions of dollars have been invested in self-driving tech- nologies by at least forty-four corporations including Apple, Google, and General Motors.210
at 2 (2015); see AUSTL. BUREAU OF STATISTICS, 2011 CENSUS COMMUNITY PROFILES, http:// www.censusdata.abs.gov.au/census_services/getproduct/census/2011/communityprofile/0?open document&navpos=220 (last updated Jan. 12, 2017) (select “Working Population Profile”).
204 For example, WhatsApp had fifty-five employees when Facebook acquired it for $21.8 billion in 2014. Jon Swartz, Tech’s Gilded Glory Didn’t Mean Much to Trump’s Supporters, USA TODAY (Nov. 14, 2016), http://www.usatoday.com/story/tech/2016/11/14/techs-gilded-glory-didnt- mean-much-trumps-supporters/93598484/. Amazon, Tesla, and other companies have developed production lines that minimize the use of people. Id.
205 Id. 206 Id.
207 See, e.g., Priest, supra note 11, at 5–6.
208 See Brad Templeton, Robotaxi Economics, BRAD IDEAS (Sept. 8, 2016, 2:07 PM), http://
ideas.4brad.com/robotaxi-economics [https://perma.cc/T4JU-D866]; see also Who’s Self-Driving Your Car?, ECONOMIST (Sept. 22, 2016), http://www.economist.com/news/business/21707600-bat tle-driverless-cars-revs-up-whos-self-driving-your-car (noting a tight race between major tech- nology companies competing to make autonomous driving software due to financial expectations).
209 Tesla to Make All Its New Cars Self-Driving, BBC NEWS (Oct. 20, 2016), http:// www.bbc.co.uk/news/technology-37711489. Not all autonomous vehicles are created equal. A va- riety of technologies are in development to automate cars to a greater or lesser degree—ranging from driverless cars to self-parking vehicles. See generally SCIENCEWISE EXPERT RES. CTR., AU- TOMATED VEHICLES: WHAT THE PUBLIC THINKS (2014), http://www.sciencewise-erc.org.uk/cms/ assets/Uploads/Automated-Vehicles-Update-Jan-2015.pdf.
210 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehicles-corporations-list/ [https:// perma.cc/JM38-TR7D]; see Investment into Auto Tech on Pace to Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/blog/auto-tech-funding-h1-2016/ [https:// perma.cc/ZTE9-MH7E].
 
44 539 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 CONCLUSION
In the coming decades, as people and machines compete in an expanding array of activities, it is vital that appropriate legal and pol- icy frameworks be put in place to guide the development of technol- ogy and to ensure its widespread benefits.211 It is particularly important that tort liability be structured to optimize accident deterrence.
Technological advances present new challenges to existing frameworks. At some point in the future, there are likely to be few or no activities for which computers cannot outperform people.212 Self- driving cars may eventually be a thousand times safer than the best human driver.213 At some point, computers will cause so little harm that the economics of negligence versus strict liability will be irrele- vant. Autonomous computers will have become so ubiquitous that the constantly improving reasonable computer should be the benchmark for most or all areas of accident law. In fact, autonomous computers are likely to become so safe that regulatory mandates for automation will be desirable.
In the meantime, creating incentives for developing and adopting safer technologies could prevent countless accidents. It has become acceptable for more than a million people a year to die in traffic acci- dents worldwide, but only because there has not been a reasonable alternative until now.214 We could soon be living in a world where no one dies from unintended injury, or from medical error for that mat- ter. Once the third and fourth leading causes of death are eliminated, that would just leave us to deal with the leading two causes of death:
211 See, e.g., Press Release, European Parliament, Robots: Legal Affairs Committee Calls for EU-Wide Rules (Jan. 12, 2017), http://www.europarl.europa.eu/sides/getDoc.do?type=IM- PRESS&reference=20170110IPR57613&language=EN&format=XML (“EU rules for the fast- evolving field of robotics, to settle issues such as compliance with ethical standards and liability for accidents involving driverless cars, should be put forward by the EU Commission, urged the Legal Affairs Committee . . . .”).
212 See generally RAY KURZWEIL, THE SINGULARITY IS NEAR 7 (2005) (predicting that machines will be able to automate all human work in “a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed”).
213 See Dredge, supra note 171.
214 See Press Release, United Nations Secretary-General, Traffic Accidents Kill 1.3 Million
People Each Year, but with Commitment Roads Can Be Made Safer for All, Secretary General Says in Video Message (May 6, 2013), https://www.un.org/press/en/2013/sgsm15005.doc.htm [https://perma.cc/B2QQ-UN59].
 
2018] 540 THE REASONABLE COMPUTER 45 cardiovascular disease and cancer. Automation may eliminate those as
well.215
 215 See Abbott, supra note 19, at 1118 (hypothesizing about how artificial intelligence could cure cancer in an article about creative computers that are already independently generating patentable subject matter).

541
  Punishing Artificial Intelligence: Legal Fiction or Science Fiction
Ryan Abbott†* and Alex Sarch**
Whether causing flash crashes in financial markets, purchasing illegal drugs, or running over pedestrians, AI is increasingly engaging in activity that would be criminal for a natural person, or even an artificial person like a corporation. We argue that criminal law falls short in cases where an AI causes certain types of harm and there are no practically or legally identifiable upstream criminal actors. This Article explores potential solutions to this problem, focusing on holding AI directly criminally liable where it is acting autonomously and irreducibly. Conventional wisdom holds that punishing AI is incongruous with basic criminal law principles such as the capacity for culpability and the requirement of a guilty mind.
Drawing on analogies to corporate and strict criminal liability, as well as familiar imputation principles, we show how a coherent theoretical case can be constructed for AI punishment. AI punishment could result in general deterrence and expressive benefits, and it need not run afoul of negative limitations such as punishing in excess of culpability. Ultimately, however, punishing AI is not justified, because it might entail significant costs and it would certainly require radical legal changes. Modest changes to existing criminal laws that target persons, together with potentially expanded civil liability, are a better solution to AI crime.
TABLE OF CONTENTS
INTRODUCTION ................................................................................... 325
I.
†
*
** Alex Sarch, Reader (Associate Professor) in Legal Philosophy, University of Surrey School of Law. Thanks to Antony Duff, Sandra Marshall, Mark D’Souza, and Steve Bero for their insightful comments.
323
ARTIFICIAL INTELLIGENCE AND PUNISHMENT........................... 329 A. Introduction to Artificial Intelligence ................................. 329
Copyright © 2019 Ryan Abbott and Alex Sarch.
 Ryan Abbott, Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at University of California, Los Angeles.

542
  324
University of California, Davis [Vol. 53:323
A Framework for Understanding AI Crime ........................ 332 A Mainstream Theory of Punishment ................................. 337 1. Affirmative Reasons to Punish ................................... 338 2. Negative (Retributive) Limitations ............................ 341 3. Alternatives to Punishment........................................ 343 4. Putting the Pieces Together ....................................... 344
B. C.
II. THE AFFIRMATIVE CASE ........................................................... 344
A. Consequentialist Benefits ................................................... 344
B. Expressive Considerations ................................................. 346
III. RETRIBUTIVE AND CONCEPTUAL LIMITATIONS.......................... 349
A. The Eligibility Challenge ................................................... 349
1. Answer 1: Respondeat Superior ................................. 350
2. Answer 2: Strict Liability............................................ 352
3. Answer3:AFrameworkforDirectMensRea
Analysis for AI ............................................................ 354
B. Further Retributivist Challenges: Reducibility and
Spillover ............................................................................ 360 1. Reducibility................................................................361
2. Spillover ...................................................................... 362
C. Not Really Punishment?..................................................... 364 IV. FEASIBLE ALTERNATIVES ........................................................... 368
A. First Alternative: The Status Quo....................................... 369
1. WhattheAIcriminalgapisnot:reducibleharmful conduct by AI ............................................................. 369
2. WhattheAIcriminalgapis:irreduciblecriminal conduct by AI ............................................................. 373
B. The Costs of Punishing AI.................................................. 374
C. Second Alternative: Minimally Extending Criminal Law.... 378
D. Third Alternative: Moderate Changes to Civil Liability...... 381
E. Concluding Thoughts ......................................................... 383

543
  2019] Punishing Artificial Intelligence 325 INTRODUCTION
In 2015, an artist going by the moniker “Random Darknet Shopper” (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit.1 This was part of a performance project where RDS was given $100 in the cryptocurrency bitcoin each week to make a purchase from an online marketplace. The items were then shipped to a Swiss art gallery and put on exhibit. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.2
What makes this story interesting for our purposes is that RDS was an artificial intelligence (“AI”), and hardly the first to have a run-in with law enforcement.3 If RDS had been a natural person located in the United States, it could be criminally prosecuted under U.S. law.4 For that matter, entities involved in this activity other than RDS could also be criminally prosecuted, such as those supplying the bitcoin and hosting the exhibition.5 Luckily for RDS and crew, the Swiss authorities were art fans.6
Cases like this will pose new challenges, including for criminal law doctrine.7 The RDS case may be relatively straightforward, but programs exist that are autonomous, decentralized, and “unstoppable.”8 What if
1 Arjun Kharpal, Robot with $100 Bitcoin Buys Drugs, Gets Arrested, CNBC (Apr. 22, 2015, 5:09 AM), https://www.cnbc.com/2015/04/21/robot-with-100-bitcoin-buys- drugs-gets-arrested.html.
2 See id.
3 See Matt Novak, Was This the First Robot Ever Arrested?, GIZMODO (Feb. 18, 2014,
12:00 PM), https://paleofuture.gizmodo.com/was-this-the-first-robot-ever-arrested- 1524686968 (describing police confiscation in 1982 of a robot: “The police considered citing [its owner] for failing to obtain a permit for advertising . . . but no charges were filed and the robot was ultimately returned.”). Robot encounters with law enforcement are becoming more common. See, e.g., Peter Dockrill, A Robot Was Just ‘Arrested’ by Russian Police, SCI. ALERT (Sept. 20, 2016), https://www.sciencealert.com/a-robot-was- just-arrested-by-russian-police.
4 See 21 U.S.C. § 841(a)(1) (2019) (criminalizing distribution and possession with intent to distribute a controlled substance).
5 See 18 U.S.C. § 2(a) (2019) (criminalizing aiding and abetting offenses).
6 Random Darknet Shopper was eventually returned to its creators together with
all of the purchases except the Ecstasy. See Kharpal, supra note 1 (noting the prosecutor’s comment that “the possession of Ecstasy was indeed a reasonable means for the purpose of sparking public debate about questions related to the exhibition”). Apparently, the Hungarian passport was also returned. See id.
7 See Christopher Markou, We Could Soon Face a Robot Crimewave. . .The Law Needs to Be Ready, CONVERSATION (Apr. 11, 2017, 9:36 AM), https://theconversation. com/we-could-soon-face-a-robot-crimewave-the-law-needs-to-be-ready-75276.
8 See infra Part I.A (discussing The Decentralized Autonomous Organization (“The DAO”)).
 
544
  326 University of California, Davis [Vol. 53:323
RDS had been open source software that individuals from around the world independently helped program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike.9 Perhaps the best-known defender of punishing AI is Gabriel Hallevy. He contends that “[w]hen an AI entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”10 In his view, “[i]f all of its specific requirements are met, criminal liability may be imposed upon any entity — human, corporate or AI entity.”11 Drawing on the analogy to corporations,12 Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations and on AI entities.”13 “Modern times,” he contends, “warrant modern legal
9 See, e.g., Gabriel Hallevy, The Punishibility of Artificial Intelligence Technology, in LIABILITY FOR CRIMES INVOLVING ARTIFICIAL INTELLIGENCE SYSTEMS 185-229 (2014); J.K.C. Kingston, Artificial Intelligence and Legal Liability, in RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XXXIII: INCORPORATING APPLICATIONS AND INNOVATIONS IN INTELLIGENT SYSTEMS XXIV 269 (Max Bramer & Miltos Petridis eds., 2016), https://arxiv.org/pdf/1802.07782.pdf; Christina Mulligan, Revenge Against Robots, 69 S.C. L. REV. 579, 580 (2018); Jeffrey Wale & David Yuratich, Robot Law: What Happens If Intelligent Machines Commit Crimes?, CONVERSATION (July 1, 2015, 8:06 AM), http://theconversation.com/robot-law-what-happens-if-intelligent-machines-commit- crimes-44058; infra Part I.A (discussing The DAO).
10 Gabriel Hallevy, The Criminal Liability of Artificial Intelligence Entities — From Science Fiction to Legal Social Control, 4 AKRON INTELL. PROP. J. 171, 191 (2010).
11 Id. at 199.
12 See id. at 200 (asking why AI entities should be treated “different from corporations”).
13 Id. at 200-01.
 
545
  2019] Punishing Artificial Intelligence 327
measures.”14 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.15 Her arguments likewise draw heavily on the analogy to corporate criminal liability.16
In contrast to AI punishment expansionists like Hallevy and Hu, skeptics might be inclined to write off the idea of punishing AI from the start as conceptual confusion — akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental concepts of the criminal law like culpability — a “guilty mind” that is characterized by insufficient regard for legally protected values17 — would be misplaced. One might think the whole idea of punishing AI can be easily dispensed with as inconsistent with basic criminal law principles.
The idea of punishing AI is due for fresh consideration. This Article takes a measured look at the proposal, informed by theory and practice alike. We argue punishment of AI cannot be categorically ruled out. Harm caused by a sophisticated AI may be more than a mere accident where no wrongdoing is implicated. Some AI-generated harms may stem from difficult-to-reduce behaviors of an autonomous system,
14 Id. at 199.
15 Ying Hu, Robot Criminals, 52 MICH. J.L. REFORM 487, 531 (2019); see also id. at
490 (“[A]n argument can be made for robot criminal liability, provided that the robot satisfies three threshold conditions . . . . [T]he robot must be (1) equipped with algorithms that can make nontrivial morally relevant decisions; (2) capable of communicating its moral decisions to humans; and (3) permitted to act on its environment without immediate human supervision.”).
16 See Ying Hu, Robot Criminal Liability Revisited, in DANGEROUS IDEAS IN LAW 494, 497-98 (Jin Soo Yoon, Sang Hoon Han & Seong Jo Ahn eds., 2018) (arguing that corporations are “structurally similar” to “robots that are equipped with machine learning algorithms to determine the appropriate course of actions in specific circumstances,” and concluding that “if there is reason to treat corporations as moral agents, there is reason to treat sophisticated robots as moral agents as well”); Hu, supra note 15, at 520-21 (“One may argue that a smart robot can act intentionally in the same way that a corporation can. A robot’s moral algorithms are functionally similar to a corporation’s internal decision structure . . . . By analogy, . . . any act made pursuant to a smart robot’s moral algorithms is an act done for the robot’s own reasons and would therefore amount to an intentional action.”). Unlike Hu, we do not argue that AIs have genuine moral responsibility. We focus on the legal notion of culpability, which involves institutional design constraints that allow it to diverge from moral responsibility or blameworthiness.
17 Alexander Sarch, Who Cares What You Think? Criminal Culpability and the Irrelevance of Unmanifested Mental States, 36 L. & PHIL. 707, 709 (2017) [hereinafter Who Cares].
 
546
  328 University of California, Davis [Vol. 53:323
whose actions resemble those of other subjects of the criminal law, especially corporations. These harms may be irreducible where, for a variety of reasons, they are not directly attributable to the activity of a particular person or persons.18 Corporations similarly can directly face criminal charges when their defective procedures generate condemnable harms19 — particularly in scenarios where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals.20
It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented into criminal law doctrine, and to consider the alternatives. Our primary focus is not what form AI punishment would take, which could directly target AIs through censure, deactivation, or reprogramming, or could involve negative outcomes directed at natural persons or companies involved in the use or creation of AI.21 Rather, our focus is the prior question of whether the doctrinal and theoretical commitments of the criminal law can be reconciled with criminal liability for AI.
Our inquiry focuses on the strongest case for punishing AI: scenarios where crimes are functionally committed by machines and there is no identifiable person who has acted with criminal culpability. We call these Hard AI Crimes. This can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important negative, retributivist-style limitations that apply to persons need not prohibit AI punishment. On the other hand, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep.22 In
18 See infra Part II.B.
19 See MODEL PENAL CODE § 2.07 (AM. LAW INST. 1962) (outlining conditions under
which a corporation could be convicted of an offense).
20 See William S. Laufer, Corporate Bodies and Guilty Minds, 43 EMORY L.J. 647, 664- 68 (1994) (outlining prevalent models of “genuine corporate culpability” including proactive fault, reactive fault, corporate ethos, and corporate policy); infra notes 166– 168 and accompanying text (discussing ways to defend the irreducibility of corporate culpability).
21 See Hu, supra note 15, at 529-30 (discussing the question of how a robot should be punished, and proposing “a range of measures [that] might be taken to ensure that the robot commits fewer offenses in the future”);
  “robot death penalty”). 22 See infra Part III.
Mark A. Lemley & Bryan Casey,
 Remedies for Robots 86 U. CHI. L. REV. 1311, 1316, 1389-93 (2019)
(discussing the

547
  2019] Punishing Artificial Intelligence 329
the end, our conclusion is this: While a coherent theoretical case can be made for punishing AI, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This Article proceeds as follows. Part I provides a brief background of AI and “AI crime.” It then provides a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Part II considers potential benefits to AI punishment, and argues it could provide general deterrence and expressive benefits. Part III examines whether punishment of AI would violate any of the negative limitations on punishment that relate to desert, fairness, and the capacity for culpability. It finds that the most important constraints on punishment, such as requiring a capacity for culpability for it to be appropriately imposed, would not be violated by AI punishment.
Finally, Part IV considers feasible alternatives to AI punishment. It argues the status quo is or will be inadequate for properly addressing AI crime. While direct AI punishment is a solution, this would require problematic changes to criminal law. Alternately, AI crime could be addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. We argue that civil liability is generally preferable to criminal liability for AI activity as it is proportionate to the scope of the current problem and a less significant departure from existing practice with fewer costs. In this way, the Article aims to map out the possible responses to the problem of harmful AI activity and makes the case for approaching AI punishment with extreme caution.
I. ARTIFICIAL INTELLIGENCE AND PUNISHMENT A. Introduction to Artificial Intelligence
We use the term “AI” to refer to a machine that is capable of completing tasks otherwise typically requiring human cognition.23 AI only sometimes has the ability to directly act physically, as in the case of a “robot,” but it is not necessary for an AI to directly affect physical activity to cause harm (as the RDS case demonstrates).
23 AI lacks a standard definition, but its very first definition in 1955 holds up reasonably well: “[T]he artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” J. MCCARTHY ET AL., A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE (1955), http://www-formal.stanford.edu/jmc/history/dartmouth/ dartmouth.html.
 
548
  330 University of California, Davis [Vol. 53:323
AI is rapidly improving, driven by advances in software, computing power, and big data.24 Hardly a day goes by without a new report of some impressive feat achieved by AI. In 2017, Alphabet’s flagship DeepMind AI beat the world champion of the board game Go.25 This was considered an important feat in the AI community, because of the sheer complexity of the game.26 There are more possible Go board configurations than there are atoms in the universe.27 Thus, a machine designed to play Go cannot simply be preprogrammed with optimal predetermined moves, or solely rely on a brute force approach to considering a large number of future moves.28 Go was the last traditional board game in which people had been able to outperform machines.29
In some areas, AI already makes significant practical contributions. For instance, Google Translate supports more than 100 languages, including 37 by photo input, 32 by voice input, and 27 in “augmented reality mode.”30 The increasing prevalence and capability of AI will lead to widespread social benefit, but will also cause harm. Virtually all activity involves a risk of harm, and as AI comes to do more it will inevitably cause more harm.31
A few features of AI are important to highlight. First, AI has the potential to act unpredictably.32 Some leading AIs rely on machine learning or similar technologies which involve a computer program, initially created by individuals, further developing in response to data without explicit programming.33 This is one means by which AI can
24 See Ryan Abbott, Everything Is Obvious, 66 UCLA L. REV. 2, 23-28 (2019).
25 See id. at 24.
26 See id.
27 See id.
28 See id.
29 See id.
30 GOOGLE TRANSLATE, https://translate.google.com/intl/en/about/languages/ (last
visited Oct. 9, 2019).
31 See, e.g., Daisuke Wakabayashi, Self-Driving Uber Car Kills Pedestrian in Arizona,
Where Robots Roam, N.Y. TIMES (Mar. 19, 2018), https://www.nytimes.com/2018/03/19/ technology/uber-driverless-fatality.html.
32 See, e.g., Taha Yasseri, Never Mind Killer Robots — Even the Good Ones Are Scarily Unpredictable, PHYS.ORG (Aug. 25, 2017), https://phys.org/news/2017-08-mind-killer- robots-good-scarily.html; Why Did the Neural Network Cross the Road?, AI WEIRDNESS (2018), http://aiweirdness.com/post/174691534037/why-did-the-neural-network-cross- the-road (describing a programmer who made her machine learning algorithm attempt to tell jokes).
33 See, e.g., Davide Castelvecchi, Can We Open the Black Box of AI?, NATURE (Oct. 5, 2016), https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731.
 
549
  2019] Punishing Artificial Intelligence 331
engage in activities its original programmers may not have intended or foreseen.34
Second, AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.35 This has led to some AIs being described as “black box” systems.36 For instance, an algorithm may refuse a credit application but not be able to articulate why the application was rejected.37 That is particularly likely in the case of AIs that learn from data, and which may have been exposed to millions or billions of data points.38 Even if it is theoretically possible to explain an AI outcome, it may be impracticable given the potentially resource intensive nature of such inquiries, and the need to maintain earlier iterative versions of AI and specific data.
Third, AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual. Suppose an individual creates an AI to steal financial information by mimicking a bank’s website, stealing user information, and posting that information online. While the theft may be entirely reducible to an individual who is using the AI as a tool, the AI may continue to act in harmful ways without further human involvement. It may even be the case that the individual who sets an AI in motion is not able to regain control of the AI, which could be by design.39
Fourth, while AI can already outperform people in spectacular fashion in some domains, like playing board games, in other domains AI is not even competitive with toddlers.40 That is because all AI is
34 There has been a recent focus on biased decisions by machine learning algorithms — sometimes due to a programmer’s implicit bias, sometimes due to biased training data. See, e.g., Chris DeBrusk, The Risk of Machine-Learning Bias (and How to Prevent It), MIT SLOAN MGMT. REV. (Mar. 26, 2018), https://sloanreview.mit.edu/article/the-risk-of- machine-learning-bias-and-how-to-prevent-it/.
35 See, e.g., Castelvecchi, supra note 33.
36 Id.
37 See id.
38 See id.
39 “The DAO” was the most famous attempt to create a decentralized autonomous
organization. See Samuel Falkon, The Story of the DAO — Its History and Consequences, THE STARTUP (Dec. 24, 2017), https://medium.com/swlh/the-story-of-the-dao-its- history-and-consequences-71e6a8a551ee. The concept was to deploy an entity that could no longer be controlled by its creators and would act without further direction. The DAO would operate through smart contracts, or pre-programmed rules dictating future behavior. A DAO might be used to create an entity operating according to publicly available, unalterable code on a distributed ledger to prevent corporate mismanagement. Unfortunately, the DAO failed shortly after launch on Ethereum due to programming flaws and hacker interference. See id.
40 See Abbott, supra note 24, at 40.
 
550
  332 University of California, Davis [Vol. 53:323
designed to perform “narrow” or “specific” tasks.41 DeepMind can beat the world’s best human player at Go, but it could not translate English to French without being programmed to do so.42 By contrast, the holy grail of computer science research is developing “general” AI that would be able to perform any task that a person could perform.43 Experts are divided on whether, and when, general AI will be developed. For now, the weight of expert opinion holds there will likely be no general AI for at least a couple of decades.44
Of course, it is possible for a conventional machine to perform unpredictably, unexplainably, or autonomously. However, at a minimum, AI is far more likely to exhibit these characteristics, and to exhibit them to a greater extent. Even a sufficient difference in degree along several axes makes AI worth considering as a distinctive phenomenon, possibly meriting novel legal responses.
Finally, general AI, and even super- or ultra-intelligent AI,45 is different than the sort of self-aware, conscious, sentient AIs that are common in science fiction. The latter sorts of AIs, sometimes referred to as “strong AI,” are portrayed as having a human-like abilities to cognitively reason and to be morally culpable for their actions.46 Today, even the prospect of such machines is safely within the realm of science fiction.47 We will not consider punishment of strong AI.48
B. A Framework for Understanding AI Crime
We use the term “AI crime” as a loose shorthand for cases in which an AI would be criminally liable if a natural person had performed a similar act. Machines have caused harm since ancient times, and robots have caused fatalities since at least the 1970s.49 However, besides
41 Id. at 25.
42 See id. at 24-25.
43 Id. at 25.
44 See generally Vincent C. Müller & Nick Bostrom, Future Progress in Artificial
Intelligence: A Survey of Expert Opinion, in FUNDAMENTAL ISSUES OF ARTIFICIAL INTELLIGENCE 555 (Vincent C. Müller ed., 2016) (describing a survey finding that experts think AI superintelligence will not be a reality for at least a few decades).
45 See Abbott, supra note 24, at 23-28 (describing super- and ultra-intelligent AI).
46 See Jesus Rodriguez, Gödel, Consciousness and the Weak vs. Strong AI Debate,
TOWARDS DATA SCI. (Aug. 23, 2018), https://towardsdatascience.com/g%C3%B6del- consciousness-and-the-weak-vs-strong-ai-debate-51e71a9189ca.
47 See id.
48 If and when such machines come into existence, we will certainly enjoy reading
their works on AI criminal liability.
49 See Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. 1, 8 (2018) [hereinafter The Reasonable Computer];
 
551
  2019] Punishing Artificial Intelligence 333
machines being intentionally used to inflict harm (as when a person runs someone over with their car), most harms caused by machines are seen as mere accidents. The exception is when the culpable carelessness of people using a machine caused the harm (as when negligence in using drilling machinery caused the BP oil spill).50 Such harms are not mere accidents; rather, they are accidents that implicate criminal law.51 Nonetheless, even in such cases, criminal law is not usually deployed against the harmful machines themselves (at least outside of some intriguing but archaic prosecutions of inanimate objects).52 It may be that AI crimes are no different than any other harm for which a machine is involved.
Yet AI can differ from conventional machines in a few essential ways that make the direct application of criminal law more worthy of consideration. Specifically, AI can behave in ways that display high degrees of autonomy and irreducibility.53 In terms of autonomy, AI is capable of acting largely independently of human control. AI can receive sensory input, set targets, assess outcomes against criteria, make decisions and adjust behavior to increase its likelihood of success — all without being directed by human orders.54 Reducibility is also critical
Bryan Young, The First ‘Killer Robot’ Was Around Back in 1979, HOW STUFF WORKS (Apr. 9, 2018), https://science.howstuffworks.com/first-killer-robot-was-around-back-in- 1979.htm.
50 See Clifford Krauss & John Schwartz, BP Will Plead Guilty and Pay Over $4 Billion, N.Y. TIMES (Nov. 15, 2012), https://www.nytimes.com/2012/11/16/business/global/ 16iht-bp16.html.
51 See BP Exploration and Production Inc. Agrees to Plead Guilty to Felony Manslaughter, Environmental Crimes and Obstruction of Congress Surrounding Deepwater Horizon Incident, U.S. DEP’T JUSTICE (Nov. 15, 2012), https://www.justice.gov/opa/pr/bp-exploration-and- production-inc-agrees-plead-guilty-felony-manslaughter-environmental (outlining BP’s guilty plea to criminal offenses).
52 See 1 WILLIAM BLACKSTONE, COMMENTARIES *302; OLIVER WENDELL HOLMES, THE COMMON LAW 7, 24 (1881) (during Edward I’s reign “[i]f a man fell from a tree, the tree was deodand” — forfeited as an “accursed thing” and given to God); Albert W. Alschuler, Comment, Ancient Law and the Punishment of Corporations: Of Frankpledge and Deodand, 71 B.U. L. REV. 307, 312 (1991) (“Just as primitive people hated and punished the wheel of a cart that had run someone over . . . some of us truly manage to hate the corporate entity.”).
53 We will not attempt to articulate the non-functional differences between human and algorithmic reasoning, a subject which has fascinated and confounded computer scientists since the 1950s. See, e.g., A.M. Turing, Computing Machinery and Intelligence, 59 MIND 433 (1950). Functionally, AI and people can exhibit similar patterns of behavior and information processing, regardless of whether machines “think” or understand what they do. See David J. Chalmers, Facing Up to the Problem of Consciousness, 2 J. CONSCIOUSNESS STUD. 200, 216 (1995) (distinguishing intellectual capacities from phenomenal consciousness).
54 See supra notes 24–37 and accompanying text.
 
552
  334 University of California, Davis [Vol. 53:323
because if an AI engages in an act that would be criminal for a person and the act is reducible, then there typically will be a person that could be criminally liable.55 If an AI act is not effectively reducible, there may be no other party that is aptly punished, in which case intuitively criminal activity could occur without the possibility of punishment.
Almost all AI crimes are likely to be reducible. For instance, if an individual develops an AI to hack into a self-driving car to disable vital safety features, that individual has directly committed a crime.56 If someone strikes another person with a rock, the rock has not committed battery — the individual throwing the rock has. Even where AI behaves autonomously, to the extent that a person uses AI as a tool to commit a crime, and the AI functions foreseeably, the crime involves an identifiable defendant causing the harm. Even when AI causes unforeseeable harm, it may still be reducible — for example, if an individual creates an AI to steal financial information, but a programming error results in the AI shutting down an electrical grid that disrupts hospital care. This is a familiar problem in criminal law.57 If someone commits a robbery and in so doing injures bystanders in unforeseeable ways (imagine a tripped bank alarm startles the animals in a neighboring zoo and they break loose and trample pedestrians), criminal law has doctrinal tools by which liability could still be imposed.58
Sometimes, however, it may be difficult to reduce AI crime to an individual due to AI autonomy, complexity, or lack of explainability. A large number of individuals may contribute to development of an AI over a long period of time. For instance, with some open source software, thousands of people can collaborate informally to create an AI.59 In the case of AI that develops in response to training with data, it may be difficult to attribute responsibility for an AI output where the
55 See infra Part IV.A.
56 See Jeffrey K. Gurney, Driving into the Unknown: Examining the Crossroads of
Criminal Law and Autonomous Vehicles, 5 WAKE FOREST J.L. & POL’Y 393, 433 (2015) (discussing crimes applicable to this scenario).
57 See infra Part IV.
58 See infra Part IV.A (discussing constructive liability offenses).
59 In 2017, for instance, more than 4,500 Microsoft employees contributed to open
source software hosted on GitHub. See Matt Asay, Who Really Contributes to Open Source, INFOWORLD (Feb. 7, 2018), https://www.infoworld.com/article/3253948/open- source-tools/who-really-contributes-to-open-source.html#tk.twt_ifw. GitHub is a development platform that hosts open source code. See Frederic Lardinois & Ingrid Lunden, Microsoft Has Acquired GitHub for $7.5B in Stock, TECHCRUNCH (June 4, 2018, 6:08 AM), https://techcrunch.com/2018/06/04/microsoft-has-acquired-github-for-7-5b- in-microsoft-stock/.
 
553
  2019] Punishing Artificial Intelligence 335
machine has learned how to behave based on accessing millions or billions of data points from heterogeneous sources.60 Thus, it may be more difficult to assign fault to individuals where AI is concerned versus a conventional product such as a car, where just one component is faulty. In fact, it may be practically impossible to reduce an AI generated harm to the actions of individuals.
Even where AI developers are known, an AI might end up causing harm without any unreasonable human behavior. Suppose two experienced and expert programmers separately contribute code for the software of an autonomous vehicle, but the two contributions unforeseeably interact in ways that cause the vehicle to deliberately collide with individuals wearing striped shirts. If this was the result of some not-reasonably-foreseeable interactions between the two programmers’ contributions, then presumably neither programmer would have criminal liability. Generally, to be criminally liable, an individual has to intend a certain prohibited socially undesirable outcome — or at least act recklessly, which is acting despite being aware of a substantial and unjustified risk that one’s conduct may produce a prohibited outcome.61 Sometimes, although more controversially,62 criminal liability can be imposed on a negligence basis when one causes harm that a reasonable person would have foreseen and taken precautions to avoid.63 At least in a case where AI activity has, from the perspective of a reasonable person, unforeseeably caused harm, individuals would not generally face criminal liability, as this would not even meet the threshold for criminal negligence. In some cases, they would not even be civilly liable if their actions were not negligent under the tort standard.64
There are several possible grounds on which criminal law might deem AI crime to be irreducible.65
60 See Lothar Determann & Bruce Perens, Open Cars, 32 BERKELEY TECH. L.J. 915, 988 (2017).
61 See MODEL PENAL CODE § 2.02(2)(a)-(c) (AM. LAW INST. 1962) (defining purpose and recklessness).
62 See LARRY ALEXANDER & KIMBERLY KESSLER FERZAN, CRIME AND CULPABILITY: A THEORY OF CRIMINAL LAW 70 (2009) (arguing there should be no criminal liability for negligence).
63 See § 2.02(2)(d) (defining negligence).
64 Developers may be civilly liable other than under tortious negligence. For instance, if it were a defective commercial “product,” its supplier might be subject to strict product liability. Abbott, The Reasonable Computer, supra note 49, at 13-16 (discussing product liability law). However, such liability only applies in limited situations. See id.
65 See infra Part III.B.1.
 
554
  336
University of California, Davis [Vol. 53:323
1) Enforcement Problems: A bad actor is responsible for an AI crime, but the individual cannot be identified by law enforcement. For example, this might be the case where the creator of a computer virus has managed to remain anonymous.66
2) Practical Irreducibility: It would be impractical for legal institutions to seek to reduce the harmful AI conduct to individual human actions, because of the number of people involved, the difficulty in determining how they contributed to the AI’s design, or because they were active far away or long ago. Criminal law inquiries do not extend indefinitely for a variety of sound reasons.67
3) Legal Irreducibility: Even if the law could reduce the AI crime to a set of individual human actions, it may be bad criminal law policy to do so. For example, unjustified risks might not be substantial enough to warrant being criminalized. Perhaps multiple individuals acted carelessly in insubstantial ways, but their acts synergistically led to AI causing significant harm. In such cases, the law might deem the AI’s conduct to be irreducible for reasons of criminalization policy.
We will largely set aside enforcement-based reasons for irreducibility as less interesting from a legal design perspective. Enforcement problems exist without AI. Other forms of irreducibility may exist, such as moral irreducibility, but we will not focus on these here because they are controversial and undertheorized.68
Instead, our analysis will focus on what we take to be less controversial forms of irreducibility: those where it is not practically feasible to reduce the harmful AI conduct to human actors, or where the harmful AI
66 The chance of being prosecuted for a cyberattack in the United States is estimated at a mere 0.05% versus 46% for a violent crime. See William Dixon, Fighting Cybercrime — What Happens to the Law When the Law Cannot Be Enforced?, WORLD ECON. FORUM (Feb. 19, 2019), https://www.weforum.org/agenda/2019/02/fighting-cybercrime-what- happens-to-the-law-when-the-law-cannot-be-enforced/. Incidentally, cybercrime is predicted to cost the global economy $6 trillion by 2021. See id.
67 See infra Part III.B.1 (discussing the reducibility challenge as applied to corporate liability).
68 It’s conceivable the law might adopt a “moral irreducibility” view. That is, the law might deem an AI (perhaps incorrectly) to be a full-blown moral agent (i.e., genuinely responsible for its conduct) and for that reason the law might regard its conduct as irreducible. However, while this might be logically possible, we have doubts about it — especially if sufficient creativity is used to identify bad human behavior nearby. See generally infra Part III.B.1.
 
555
  2019] Punishing Artificial Intelligence 337
conduct was just the result of human misconduct too trivial to penalize. In these instances, AI can be seen as autonomously committing crimes in irreducible ways, where there is no responsible person. This is what we refer to as “Hard AI Crime” and what we take to provide the strongest case for holding AI criminally liable in its own right.
C. A Mainstream Theory of Punishment
To anchor our analysis, this section introduces a theory of punishment that reflects the broad consensus in the literature.69 We use the term “punishment” roughly as defined by H.L.A. Hart in terms of five elements:
(i) It must involve pain or other consequences normally considered unpleasant;
(ii) It must be for an offense against legal rules;
(iii) It must be of an actual or supposed offender for his offense;
(iv) It must be intentionally administered by human beings other than the offender; and
(v) It must be imposed and administered by an authority constituted by a legal system against which the offense is committed.70
Thus, “punishment” requires a conviction for a legally recognized offense following accepted procedures.71 Under this definition, imprisonment, fines, or asset forfeiture carried out in response to a proper conviction for a specified offense would count as punishment, but a range of other activities that most people might consider “punishment” in a loose sense would not.72 For instance, harsh
69 See generally Mitchell N. Berman, The Justification of Punishment, in THE ROUTLEDGE COMPANION TO PHILOSOPHY OF LAW 141, 144-45 (Andrei Marmor ed., 2012) (noting the convergence on this sort of theory of punishment).
70 H.L.A. HART, PUNISHMENT AND RESPONSIBILITY: ESSAYS IN THE PHILOSOPHY OF LAW 4-5 (2d ed. 2008).
71 This is likewise supported by the principle of legality, built into any well- functioning legal system. This principle (nulla poena sin lege) provides that it is not legally permitted to penalize someone for an action without a law prohibiting that conduct. See Douglas N. Husak & Craig A. Callender, Wilful Ignorance, Knowledge, and the “Equal Culpability” Thesis: A Study of the Deeper Significance of the Principle of Legality, 1994 WIS. L. REV. 29, 30 (1994).
72 See HART, supra note 70, at 5.
 
556
  338 University of California, Davis [Vol. 53:323
treatment by private citizens for violating informal social norms, preventative detention of people suffering from mental illnesses on grounds of their being a danger to themselves or others, or asset forfeiture carried out in advance of a conviction would not count as punishment.73 Civil penalties, while violations of legal norms, do not count as an “offense” for criminal law purposes, as criminal law seeks to condemn egregious categories of conduct.74
Punishment is justified only if its affirmative justifications outweigh its costs and it does not otherwise offend applicable negative limitations on punishment. Affirmative justifications are the positive benefits that punishment might produce like harm reduction, increased safety, enhanced well-being, or expressing a commitment to core moral or political values. Such benefits can give reason to criminalize certain types of conduct and impose sanctions on actors who perform those types of acts. Affirmative justifications are distinct from negative limitations on punishment, which are commonly associated with culpability-focused retributivist views of criminal law. For example, it is widely held to be unjust to punish the innocent — or to punish wrongdoers in excess of what they deserve in virtue of their culpability — even if this would promote aggregate well-being in society.75 This so- called “desert constraint” imposes a limitation, grounded in justice, on promoting social welfare through punishment.76
1. Affirmative Reasons to Punish
It is common to be a pluralist about the benefits of punishment.77 U.S. federal law refers to the most widely acknowledged benefits, including
73 It remains open, on this definition, whether mere arrest is itself a form of punishment. It is properly carried out only in response to a suspected criminal offense, although it is in advance of a conviction and therefore it is unclear whether it satisfies the “imposed for an offense” requirement in (i). As Hart notes, these may be punishment-like in some respects, but do not fall within the core concept of punishment. See id.
74 To deem some conduct an offense is to condemn it, to mark it out as culpable and to label the one who commits it an offender. See R A DUFF, THE REALM OF CRIMINAL LAW 19-20 (2018) [hereinafter REALM]. The expression of condemnation and declaring someone convicted of a crime to be an offender who is guilty is a feature of core punishment, but not civil liability. Because Hart’s definition is couched in terms like “offense” and “offender,” carrying as they do connotations of culpability and condemnation, civil liability would not qualify as punishment.
75 See infra notes 94–95 and accompanying text.
76 See id.
77 See Berman, supra note 69, at 141-42 (noting the “converg[ence] on a desert-
constrained pluralism” about the justifications of punishment, describing it as
 
557
  2019] Punishing Artificial Intelligence 339
the need to “afford deterrence to criminal conduct,” to “protect the public from further crimes of the defendant,” to “provide the defendant with” rehabilitative treatment of various kinds, as well as to reflect “the seriousness of the offense” which covers the culpability of the act and the desert of the actor.78
For simplicity, the affirmative aims of punishment can be grouped into two main categories: (a) consequentialist aims and (b) retributivist aims. Some theorists also mention (c) expressive aims, though these are largely reducible to the aims in the first two categories.79 Consequentialist benefits cover the good consequences that punishment can bring about, usually understood as enhancing the aggregate well-being of the members of society by reducing harm. The main type of consequentialist benefit of punishment is preventive, in that punishment can reduce crime.
Punishment can reduce crime several ways. The simplest is incapacitation: when the offender is locked up, he or she is physically limited from committing further crimes while incarcerated.80 The next and arguably most important way punishment prevents harm is through deterrence — namely by threatening negative consequences for the commission of a crime that give would-be offenders reasons to refrain from prohibited conduct.81 Deterrence comes in two forms: (i) specific deterrence and (ii) general deterrence. Specific deterrence is the process whereby punishing a specific individual discourages that person from committing more crime in the future.82 General deterrence occurs when punishing an offender discourages other would-be offenders from committing crimes.83 It is a matter of punishing an offender in order to “send a message” to other potential offenders. There can be affirmative
“something approaching a consensus” view); Michael T. Cahill, Punishment Pluralism, in RETRIBUTIVISM: ESSAYS ON THEORY AND POLICY 25, 25 (Mark D. White ed., 2011) (“[M]any have proposed a hybrid model of ‘limiting retributivism’ that explicitly purports to combine aspects of both the canonical theories” of consequentialism and retributivism, suggesting that “the ascendant view of punishment is more openly pluralistic about its purposes . . . .”).
78 18 U.S.C. § 3553 (2019).
79 See Berman, supra note 69, at 148 (discussing whether expressivism reduces to
consequentialism).
80 See Consequentialist Accounts, STAN. ENCYCLOPEDIA PHIL. (July 18, 2017), https://plato.stanford.edu/entries/legal-punishment/#PurConPun (“It is commonly suggested that punishment can help to reduce crime by deterring, incapacitatiing [sic], or reforming potential offenders . . . .”).
81 See id.
82 See Berman, supra note 69, at 145 (discussing types of deterrence).
83 See id.
 
558
  340 University of California, Davis [Vol. 53:323
benefits to punishing those who qualify for an insanity defense because it may deter sane individuals from committing crimes and attempting to rely on an insanity defense.84
These are not the only kinds of consequentialist benefits that can support punishment. Besides incapacitation and deterrence, punishment can reduce harm through rehabilitation of the offender.85 Insofar as punishment helps the offender to see the error of his or her ways, or training or skills are provided during incarceration, this, too, can help prevent future crimes.
Besides crime prevention, there also may be non-consequentialist benefits that can provide additional affirmative grounds for punishment. Most importantly, it may be intrinsically valuable to give culpable actors what they deserve in a way that does not just reduce to the value of harm reduction.86 In other words, the idea is that retribution, giving offenders what they are due in virtue of the culpability of what they did, is intrinsically valuable or fitting.87 Retribution matters, for example, because it allows society to sufficiently distance itself from the offender’s wrongdoing and prevents it from being complicit (or overly tolerant) of culpable wrongdoing.88
84 Hart offered this response to Bentham’s argument that because children and the insane are not deterrable, they should not be punished. Hart argues more soberly that “though . . . the threat of punishment could not have operated on [children or the insane], the actual infliction of punishment on those persons, may secure a higher measure of conformity to law on the part of normal persons.” HART, supra note 70, at 19. While there are other reasons for not punishing children and the insane (i.e., reduced culpability), Bentham’s “undeterrability” argument is not a convincing reason.
85 See Berman, supra note 69, at 145 (discussing rehabilitation).
86 To illustrate, suppose punishing a murderer will do absolutely nothing to prevent
future crime or reduce harm to others. Maybe the offense was committed decades ago and the offender is now too infirm to reoffend. Suppose the punishment is guaranteed to remain a complete secret from the public. Punishment would thus not result in specific or general deterrence, but there may still be a retributive reason to punish. The reason would stem from the value (if any) inherent in giving offenders what they are due in virtue of their culpability.
87 See VICTOR TADROS, THE ENDS OF HARM: THE MORAL FOUNDATIONS OF CRIMINAL LAW 26 (2011) [ENDS OF HARM] (identifying retributivism with the claim that it is “intrinsically valuable” that offenders suffer in proportion to their desert); John Cottingham, Varieties of Retribution, 29 PHIL. Q. 238, 238 (1979); Doug Husak, Retributivism in Extremis, 32 L. & PHIL. 3, 4-5 (2013) (defending broader versions of retributivism).
88 See Leora Dahan Katz, Response Retributivism: Defending the Duty to Punish 16-17 (Oct. 10, 2018) (unpublished manuscript), https://papers.ssrn.com/sol3/papers.cfm? abstract_id=3264139.
 
559
  2019] Punishing Artificial Intelligence 341
While virtually everyone agrees that the good consequences of preventing crime must be a major part of what justifies punishment,89 there is more debate about whether retributivist reasons also exist.90 While retributivist reasons for punishment are worth taking seriously, here we assume that the lion’s share of the affirmative case in favor of punishment will involve harm reduction and similar desirable consequences.
One last group of affirmative reasons that merit mention are expressive reasons.91 Punishment involves the communication of society’s collective commitment to certain core values. The state, through punishment, conveys official condemnation of culpable conduct through the mechanism of a criminal conviction. Victims may benefit psychologically to see the state reaffirm their rights which were violated by a criminal act. Officially expressing condemnation of culpable conduct may also affect behavior and attitudes in general by reinforcing positive social values.92
Some question whether expressive benefits are a distinct category of reason to punish, or whether they simply reduce to consequentialist or retributivist reasons.93 After all, many of the benefits in the expressive category center around harm prevention, such as the deterrent effects of signaling that society will not stand for seriously culpable conduct. Expressive reasons might also reduce to retributivist reasons insofar as the value of expressing condemnation is that it involves giving offenders their due. In what follows, we assume expressive benefits reduce to consequentialist or retributive reasons, but not much turns on it. Our arguments are also compatible with the contrary view.
2. Negative (Retributive) Limitations
Punishment also should not violate deeply held normative commitments such as justice or fairness. The most important of these limitations focus on the culpability of those subject to the criminal law. One such limitation on punishment is the desert constraint, which figures into most retributivist views.94 The desert constraint claims that
89 See TADROS, ENDS OF HARM, supra note 87, at 21.
90 See, e.g., id. at 60.
91 See R A DUFF, ANSWERING FOR CRIME: RESPONSIBILITY AND LIABILITY IN THE CRIMINAL
LAW 140-46 (2007) [hereinafter ANSWERING].
92 See PAUL H. ROBINSON, INTUITIONS OF JUSTICE AND THE UTILITY OF DESERT 161-62
(2013).
93 E.g., Berman, supra note 69, at 148.
94 See id. at 144 (on retributivism, punishment is justified if, but only to the extent
that, “it is deserved or otherwise fitting, right or appropriate, and not [necessarily
 
560
  342 University of California, Davis [Vol. 53:323
an offender may not, in justice, be punished in excess of his or her desert. Desert, in turn, is understood mainly in terms of the culpability one incurs in virtue of one’s conduct. The main effect of the desert constraint is to rule out punishments that go beyond what is proportionate to one’s culpability.95 Thus, it would be wrong to execute someone for jaywalking even if doing so would ultimately save lives by reducing illegal and dangerous pedestrian crossings.
What supports the desert constraint? Intuition, for one thing. It seems unjust to punish someone who is completely innocent even if it would produce significant benefits through general deterrence.96 Similarly, it seems unjust to impose a very severe punishment on someone who only committed a minor crime. Beyond its intuitive plausibility, the desert constraint is also supported by the argument — tracing back at least to Kant — that it is wrong to use people merely as a means to one’s ends without also treating them as ends in themselves (i.e., without respecting their value as persons).97 The idea is that punishing the innocent to obtain broader social benefits is a paradigmatic example of treating people merely as means, which fails to show individuals the respect they are due. Under some Kantian views, the desert constraint is absolute: It is not appropriate to treat someone merely as a means to one’s ends regardless of the costs of respecting their value as persons. Others have a more nuanced view, such that violating a negative limitation could be overall justified if the benefits were sufficiently weighty.98
There are limitations on punishment other than the desert constraint. Most importantly, criminal law requires certain prerequisites, such as a capacity for culpability, that defendants must meet in order to be properly subject to punishment. It is a fundamental aim of criminal law to condemn culpable wrongdoing, and it is the default position in criminal law doctrine that punishment may only be properly imposed
because of] any good consequences” it may have); see also id. at 151 (discussing desert- constrained consequentialism).
95 Negative retributivism is the view that the desert of the offender only prohibits punishing in excess of desert (even if it has good consequences). Positive retributivism says that the offender’s desert provides an affirmative reason for punishment.
96 Cf. HART, supra note 70, at 12 (discussing the famous example of the small southern town).
97 See, e.g., TADROS, ENDS OF HARM, supra note 87, at 114 (defending a version of the means principle).
98 See, e.g., HART, supra note 70, at 12 (“In extreme cases many might think it right to resort to these expedients but we should do so with the sense of sacrificing an important principle.”).
 
561
  2019] Punishing Artificial Intelligence 343
in response to culpable wrongdoing.99 Without the requisite capacities of deliberation and agency, an entity is not an appropriate subject for criminal punishment — as can be seen from the fact that lacking such capacities altogether can give rise to an incapacity defense.100 Thus, capacity for culpability is an eligibility requirement for being aptly subject to regulation by criminal law.
3. Alternatives to Punishment
For punishment to be justified, it is not enough for it to have affirmative benefits and to be consistent with the negative limitations for punishment. In addition, there cannot be better, feasible alternatives, including doing nothing. This is an obvious point that is built into policy analysis of all kinds.101
Even if punishing AI has affirmative benefits, and even if the practice did not seriously violate any negative limitations, it still would not be justified if, for example, civil liability, licensure, or industry standards provide a better solution. It is often claimed that when seeking to exert social control, criminal law should be a tool of last resort.102 After all, criminal law sanctions are the harshest form of penalty society has available, involving as they do both the possible revocation of personal freedom as well as the official condemnation of the offender. Thus, the third requirement for a given punishment to be justified is the absence of better alternatives.
99 See MODEL PENAL CODE § 1.02(c) (AM. LAW INST. 1962) (declaring that one of the “general purposes” of the Code is “to safeguard conduct that is without fault from condemnation as criminal”); Nicola Lacey & Hanna Pickard, To Blame or to Forgive? Reconciling Punishment and Forgiveness in Criminal Justice, 35 OXFORD J. LEGAL STUD. 665, 666 (2015), https://doi.org/10.1093/ojls/gqv012 (observing that on retributivist theories, “punishment is only justified if the condition of responsible agency is met”); DUFF, REALM, supra note 74, at 19-20 (noting that “censure . . . is essential to a criminal conviction” and that a legal system “that criminalizes conduct that is not even alleged to be or portrayed as being wrongful is, necessarily, a perversion of criminal law”).
100 See MODEL PENAL CODE § 4.01 (outlining the incapacity defense based on mental defect as when a person is unable “either to appreciate the criminality . . . of his conduct or to conform [it to] the law”).
101 See Sven Ove Hansson, Philosophical Problems in Cost-Benefit Analysis, 23 ECON. & PHIL. 163, 164 (2007) (“In cost-benefit analysis, an alternative is not evaluated by itself but in comparison to other alternatives (or, at least, in comparison to not choosing that alternative).”).
102 E.g., Doug Husak, The Criminal Law as Last Resort, 24 OXFORD J. LEGAL STUD. 207, 235 (2004) (discussing the view that “[a]mong those modes of social control we are likely to deem acceptable, the criminal law should be used only as a last resort”).
 
562
  344 University of California, Davis [Vol. 53:323 4. Putting the Pieces Together
Determining whether a given punishment is appropriate requires investigation of three questions:
a) Affirmative Benefits: Are there sufficiently strong affirmative reasons in favor of punishment? This chiefly concerns consequentialist benefits of harm reduction but may also include retributive and expressive benefits.
b) Negative Limitations: Would punishment be consistent with applicable negative limitations? This primarily concerns culpability-focused principles like the desert constraint as well as basic prerequisites of apt criminal punishment such as capacity for culpability.
c) Feasible Alternatives: Is punishment a better response to the harms or wrongs in question, compared to alternatives like civil liability, regulation, or doing nothing?
In the remainder of this Article, we will apply this theory to investigate whether the direct punishment of AI is justified. We will begin in Part II with the question of Affirmative Benefits, consider Negative Limitations in Part III, then Feasible Alternatives in Part IV.
II. THE AFFIRMATIVE CASE
This Part considers the affirmative benefits that might be adduced to support punishing AI. The discussion focuses primarily on consequentialist benefits. Even if retribution can also count in favor of punishment, we assume that such benefits would be less important than consequentialist considerations centering on harm reduction.103 This Part does not aim to completely canvass the benefits of punishing AI. Instead, it argues that punishing AI could produce at least some significant affirmative benefits.
A. ConsequentialistBenefits
Recall that, arguably, the paramount aim of punishment is to reduce harmful criminal activity through deterrence. Thus, a preliminary objection to punishing AI is that it will not produce any affirmative harm-reduction benefits because AI is not deterrable. Peter Asaro argues that “deterrence only makes sense when moral agents are capable of
103 See TADROS, ENDS OF HARM, supra note 87, at 25-28.
 
563
  2019] Punishing Artificial Intelligence 345
recognizing the similarity of their potential choices and actions to those of other moral agents who have been punished for the wrong choices and actions — without this . . . recognition of similarity between and among moral agents, punishment cannot possibly result in deterrence.”104 The idea is that if AIs cannot detect and respond to criminal law sanctions in a way that renders them deterrable, there would be nothing to affirmatively support punishing AI. It is likely true that AI, as currently operated and envisioned, will not be responsive to punishment, although responsive AI is theoretically possible.105
The answer to the undeterrability argument requires distinguishing specific deterrence from general deterrence.106 Specific deterrence involves incentivizing a particular defendant not to commit crimes in the future.107 By contrast, general deterrence involves incentivizing other actors besides the defendant from committing crimes. We must further distinguish two types of general deterrence: deterring others from committing offenses of the same type the defendant was convicted of, offense-relative general deterrence, and deterring others from committing crimes in general, unrestricted general deterrence.
Punishing AI could provide general deterrence. Presumably, it will not produce offense-relative general deterrence to other AIs, as such systems are not designed to be sensitive to criminal law prohibitions and sanctions. Nonetheless, AI punishment could produce unrestricted general deterrence. That is to say, direct punishment of AI could provide unrestricted general deterrence as against the developers, owners, or users of AI and provide incentives for them to avoid creating AIs that cause especially egregious types of harm without excuse or justification. Depending on the penalty associated with punishment, such as destruction of an AI, what Mark Lemley and Brian Casey have
104 Peter M. Asaro, A Body to Kick, but Still No Soul to Damn: Legal Perspectives on Robotics, in ROBOT ETHICS: THE ETHICAL AND SOCIAL IMPLICATIONS OF ROBOTICS 169, 181 (Patrick Lin et al. eds., 2012). Asaro is ultimately skeptical of punishing robots because of questions about how to make AI be directly responsive to punishments. See id. at 182.
105 It is conceivable that AIs could be programmed to follow court orders or adapt their behavior in response to a conviction. This may be a less effective way to ensure AI lawfulness, however, than programming the AI ex ante not to run afoul of criminal law. This will be more challenging with criminal laws that are standards rather than simple rules. It is comparatively easy to program a self-driving car not to run a red light compared to programming it not to run a red light except in unspecified emergency conditions. Cf. Louis Kaplow, Rules Versus Standards: An Economic Analysis, 42 DUKE L.J. 557 (1992) (arguing that standards are more costly to interpret and apply).
106 See HART, supra note 70, at 19.
107 See Berman, supra note 69, at 145.
 
564
  346 University of California, Davis [Vol. 53:323
termed the “robot death penalty,”108 punishing AI directly could deprive such developers, owners or users of the financial benefits of the systems. This penalty may thereby incentivize such human parties to modify their behavior in socially desirable ways. The deterrence effect may be stronger if capitalization requirements are associated with some forms of AI in the future, or if penalties associated with punishment are passed on to, for example, an AI’s owner.
B. ExpressiveConsiderations
Punishment of AI may also have expressive benefits.109 Expressing condemnation of the harms suffered by the victims of an AI could provide these victims with a sense of satisfaction and vindication. Christina Mulligan has defended the idea that punishing robots can generate victim-satisfaction benefits, arguing that, “taking revenge against wrongdoing robots, specifically, may be necessary to create psychological satisfaction in those whom robots harm.”110 On her view, “robot punishment — or more precisely, revenge against robots — primarily advances . . . the creation of psychological satisfaction in robots’ victims.”111 Punishment conveys a message of official condemnation that could reaffirm the interests, rights, and ultimately the value of the victims of the harmful AI.112 This, in turn, could produce an increased sense of security among victims and society in general.
This sort of expressivist argument in favor of punishing AI may seem especially forceful in light of empirical work demonstrating the human tendency to anthropomorphize and attribute mentality to artificial
108 Lemley & Casey, supra note 21, at 100.
109 Analogous considerations could apply to provide support for punishing inanimate objects and corporations.
110 Christina Mulligan, Revenge Against Robots, 69 S.C. L. REV. 579, 580 (2018); cf. David Lewis, The Punishment That Leaves Something to Chance, 18 PHIL. & PUB. AFF. 53, 54 (1989) (discussing but rejecting the idea of defending puzzling features of criminal law on the ground that when harm results the population may demand blood).
111 Mulligan, supra note 110, at 593.
112 See DUFF, ANSWERING, supra note 91, at 114; Guyora Binder, Victims and the
Significance of Causing Harm, 28 PACE L. REV. 713, 733 (2008) (“We punish not only in order to admonish the offender . . . but also . . . to show the victim our own respect. If so, we are punishing harm for a purpose that transcends doing justice to the offender.”); Jack Boeglin & Zachary Shapiro, A Theory of Differential Punishment, 70 VAND. L. REV. 1499, 1503 (2017) (arguing that victims’ interests should be taken “into account when determining how severely criminal offenders should be punished”).
  
565
  2019] Punishing Artificial Intelligence 347 113
  persons like corporations.
The same sorts of tendencies are likely to
 be even more powerful for AI-enabled robots that are specifically designed to seem human enough to elicit emotional responses from
  humans.
114
In the corporate context, some theorists argue that
 corporations should be punished because the law should reflect lay perceptions of praise and blame, “folk morality,” or else risk losing its
  perceived legitimacy.
115
This sort of argument, if it succeeds for
 corporate punishment, is likely to be even more forceful as applied to punishing AI, which often are deliberately designed to piggy-back on
  the innate tendency to anthropomorphize.
116
Were the law to fail to
 express condemnation of robot-generated harms despite robots being widely perceived as blameworthy (even if this is ultimately a mistaken perception), this could erode the perception of the legitimacy of
 criminal law.
Thus, a number of benefits could be obtained through the expressive function of punishment.117
Nonetheless, there are a range of prima facie worries about appealing to expressive benefits like victim satisfaction in order to justify the
113 See Mihailis E. Diamantis, Corporate Criminal Minds, 91 NOTRE DAME L. REV. 2049, 2078 (2016) (arguing that “[w]hen groups exhibit high levels of coherence, as do most corporations, humans perceive them as possessing many of the attributes traditionally associated with individuals,” thus rendering “‘blame and punishment [of] these groups . . . psychologically sensible and sustainable’”); id. at 2077-79 (collecting psychology sources).
114 See Matthias Scheutz, The Inherent Dangers of Unidirectional Emotional Bonds Between Humans and Social Robots, in ROBOT ETHICS: THE ETHICAL AND SOCIAL IMPLICATIONS OF ROBOTICS 205, 205-22 (Patrick Lin et al. eds., 2012); Sherry Turkle, In Good Company? On the Threshold of Robotic Companions, in CLOSE ENGAGEMENTS WITH ARTIFICIAL COMPANIONS 3, 3 (Yorick Wilks ed., 2010); Luisa Damiano & Paul Dumouchel, Anthropomorphism in Human-Robot Co-evolution, FRONTIERS IN PSYCHOL. (Mar. 26, 2018), https://doi.org/10.3389/fpsyg.2018.00468 (discussing “social robotics” which sees anthropomorphism not as “a cognitive error” but as a useful tool “to facilitate social interactions between humans and . . . social robots”); see Ryan Calo, Robotics and the Lessons of Cyberlaw, 103 CALIF. L. REV. 513, 538 (2015).
115 See Diamantis, supra note 113, at 2088-89 (“[A] criminal legal system that is more responsive to society’s perceptions of blameworthiness may foster forces, like respect for and confidence in the law, that ultimately increase compliance by individuals. Conversely, ignoring lay perceptions of blameworthiness . . . threatens to undermine the broader effectiveness of the criminal law in preventing crime.”). See generally ROBINSON, supra note 92, at 176-88 (discussing lay observers’ ideas as they bear on the legitimacy of the criminal justice system).
116 See, e.g., Margaret Rhodes, The Touchy Task of Making Robots Seem Human — But Not Too Human, WIRED (Jan. 19, 2017, 7:00 AM), https://www.wired.com/2017/01/ touchy-task-making-robots-seem-human-not-human/.
117 Some might worry that expressive benefits just are consequentialist reasons to punish AI. While conceptually interesting, not much of practical importance turns on this issue for our purposes.
 
566
  348 University of California, Davis [Vol. 53:323
punishment of AI. First, punishing AI to placate those who want retaliation for AI-generated harms would be akin to giving in to mob justice. Legitimizing such reactions could enable populist calls for justice to be pressed more forcefully in the future. The mere fact that punishing AI might be popular would not show the practice to be just. As David Lewis observed, if it is unjust for the population to “demand blood” in response to seeing harm, then satisfying such demands through the law would itself be unjust — even if “it might be prudent to ignore justice and do their bidding.”118 Simply put, the popularity of a practice does not automatically justify it, even if popularity could be relevant to its normative justification. Second, punishing AI for expressivist purposes could lead to further bad behavior that might spill over to the way other humans are treated.119 Thus, Kate Darling has argued robots should be protected from cruelty in order to reflect moral norms and prevent undesirable human behavior.120
Third, expressing certain messages through punishment may also carry affirmative costs which should not be omitted from the calculus. Punishing AI could send the message that AI is itself an actor on par with a human being, which is responsible and can be held accountable through the criminal justice system. Such a message is concerning, as it could entrench the view that AI has rights to certain kinds of benefits, protections and dignities that could restrict valuable human activities.
In sum, punishing AI may have affirmative benefits. It could result in general deterrence for developers, owners, and users, as well as produce expressive benefits (if also potential costs). Whether these benefits would provide sufficient justification for punishing AI when compared to the feasible alternatives will be discussed in Part IV. Before that, we turn to another kind of threshold question: whether punishing AI violates the culpability-focused negative limitations on punishment.
118 Lewis, supra note 110, at 54.
119 This is similar to Kant’s point that although he thought animals are not strictly
speaking moral persons, there are still good reasons to discourage the mistreatment of animals because it could embolden people to mistreat other human beings. See The Moral Status of Animals, STAN. ENCYCLOPEDIA PHIL. (Aug. 23, 2017) https://plato.stanford.edu/entries/moral-animal/ (discussing Kant’s view of ethical treatment of animals: if one unfairly harms a dog “he does not fail in his duty to the dog . . . but . . . [he] must practice kindness towards animals, for he who is cruel to animals becomes hard also in his dealings with men” (quoting Kant’s Lectures on Ethics)).
120 See Kate Darling, Extending Legal Protection to Social Robots: The Effects of Anthropomorphism, Empathy, and Violent Behavior Towards Robotic Objects, in ROBOT LAW 213, 228 (Ryan Calo, A. Michael Froomkin & Ian Kerr eds., 2016). Relatedly, in the United Kingdom, laws criminalizing animal cruelty exist to disapprove of offensive human conduct. See Animal Welfare Act 2006, c. 45 (Gr. Brit.), http://www.legislation.gov.uk/ukpga/2006/45/contents.
 
567
  2019] Punishing Artificial Intelligence 349 III. RETRIBUTIVE AND CONCEPTUAL LIMITATIONS
This Part considers retributivist (culpability-focused) limitations on punishment. Section A asks whether AI is the right kind of entity to be eligible for punishment — what we call The Eligibility Challenge. Where criminal law’s fundamental prerequisites are not satisfied, its sanctions are not legitimately deployed. Section B considers two further retributivist objections to the punishment of AI. The Reductionist Challenge insists that any apparent AI culpability is fully reducible to the actions of persons who are better targets for punishment. This challenge purports to show that there is no need for the direct punishment of AI. Finally, the Spillover Objection insists it would be unjust to punish AI if this would predictably harm innocent people who develop, own, or use such systems. Finally, Section C considers the conceptual objection that AI punishment is not actually punishment at all.
A. The Eligibility Challenge
The Eligibility Challenge is simple to state: AI, like inanimate objects, is not the right kind of thing to be punished. AI lacks mental states and the deliberative capacities needed for culpability, so it cannot be punished without sacrificing the core commitments of the criminal law. The issue is not that AI punishment would be unfair to AI. AIs are not conscious and do not feel (at least in the phenomenal sense),121 and they do not possess interests or well-being.122 Therefore, there is no reason to think AI gets the benefit of the protections of the desert constraint, which prohibits punishment in excess of what culpability merits.123 The Eligibility Challenge does not derive from the desert constraint.
Instead, the Eligibility Challenge, properly construed, comes in one narrow and one broad form. The narrow version is that, as a mere machine, AI lacks mental states and thus cannot fulfill the mental state (mens rea) elements built into most criminal offenses. Therefore, convicting AI of crimes requiring a mens rea like intent, knowledge, or recklessness would violate the principle of legality. This principle stems from general rule of law values and holds that it would be contrary to law to convict a defendant of a crime unless it is proved (following applicable procedures and by the operative evidentiary standard) that the defendant satisfied all the elements of the crime.124 If punishing AI
121 See Chalmers, supra note 53, at 201 (describing phenomenal experiences as those personally felt or experienced).
122 See id. (discussing the hard problem of consciousness).
123 See supra notes 94–96 and accompanying text.
124 See Husak & Callender, supra note 71, at 32-33.
 
568
  350 University of California, Davis [Vol. 53:323
violates the principle of legality, it threatens the rule of law and could weaken the public trust in the criminal law.
The broad form of the challenge holds that because AI lacks the capacity to deliberate and weigh reasons, AI cannot possess broad culpability of the sort that criminal law aims to respond to.125 A fundamental purpose of the criminal law is to condemn culpable wrongdoing, as it is at least the default position in criminal law doctrine that punishment may be properly imposed only in response to culpable wrongdoing.126 The capacity for culpable conduct thus is a general prerequisite of criminal law, and failing to meet it would remove the entity in question from the ambit of proper punishment — a fact that is encoded in law, for example, in incapacity defenses like infancy and insanity. Thus, the broad version of the Eligibility Challenge holds that because AI lacks the practical reasoning capacities needed for being culpable, AI does not fall within the scope of criminal law. Punishing AI despite its lack of capacity would not only be conceptually confused, but would fail to serve the retributive aims of criminal law — namely, to mark out seriously culpable conduct for the strictest public condemnation.
Here we develop three answers to the Eligibility Challenge.
1. Answer 1: Respondeat Superior
The simplest answer to the Eligibility Challenge has been deployed with respect to corporations. Corporations are artificial entities that might also be thought ineligible for punishment because they are incapable of being culpable in their own right.127 However, even if corporations cannot literally satisfy mens rea elements, criminal law has
125 See generally Douglas Husak, “Broad” Culpability and the Retributivist Dream, 9 OHIO ST. J. CRIM. L. 449, 456-57 (2012) (distinguishing narrow culpability as merely mens rea categories from broad culpability, which is the underlying normative defect that criminal law aims to respond to).
126 See MODEL PENAL CODE § 1.02(c) (AM. LAW INST. 1962); see also MICHAEL S. MOORE, PLACING BLAME: A GENERAL THEORY OF THE CRIMINAL LAW 35 (1997) (arguing for a presumption in favor of punishing “all and only those who are morally culpable in the doing of some morally wrongful action”); DUFF, REALM, supra note 74, at 20 (a legal system “that criminalizes conduct that is not even alleged to be . . . wrongful is, necessarily, a perversion of criminal law”). While strict liability crimes exist, these are only justified in exceptional circumstances and are otherwise unjust. See W. Robert Thomas, On Strict Liability Crimes: Preserving a Moral Framework for Criminal Intent in an Intent-Free Moral World, 110 MICH. L. REV. 647, 647-50 (2012).
127 See, e.g., Albert W. Alschuler, Two Ways to Think About the Punishment of Corporations, 46 AM. CRIM. L. REV. 1359, 1367-69 (2009) (arguing against corporate punishment).
 
569
  2019] Punishing Artificial Intelligence 351
developed doctrines that allow culpable mental states to be imputed to corporations. The most important such doctrinal tool is respondeat superior, which allows mental states possessed by an agent of the corporation to be imputed to the corporation itself provided that the agent was acting within the scope of her employment and in furtherance of corporate interests.128 Some jurisdictions also tack on further requirements.129 Since imputation principles of this kind are well- understood and legally accepted, thus letting actors guide their behavior accordingly, respondeat superior makes it possible for corporations to be convicted of crimes without violating the principle of legality.130
If this kind of legal construction of mental states is a promising mechanism by which corporations can be brought back within the ambit of proper punishment and avoid the Eligibility Challenge, the same legal device could be used to make AI eligible for punishment. The culpable mental states of AI developers, owners, or users could be imputed to the AI under certain circumstances pursuant to a respondeat superior theory.131
It may be more difficult to use respondeat superior to answer the Eligibility Challenge for AI than for corporations — at least in cases of Hard AI Crime. Unlike a corporation, which is literally composed of the humans acting on its behalf, an AI is not guaranteed to come with a
128 See Ashley S. Kircher, Corporate Criminal Liability Versus Corporate Securities Fraud Liability: Analyzing the Divergence in Standards of Culpability, 46 AM. CRIM. L. REV. 157, 157 (2009) (“[R]espondeat superior has been the most traditionally accepted method of imputing criminal liability to a corporation.”); Eli Lederman, Models for Imposing Corporate Criminal Liability: From Adaptation and Imitation Toward Aggregation and the Search for Self-Identity, 4 BUFF. CRIM. L. REV. 641, 654-55 (2000) (explaining that under respondeat superior, “a corporation is liable for the deeds of any of its agents or employees . . . as long as . . . [t]he agent was acting within the course and scope of his or her employment, having the authority to act for the corporation . . . at least in part in furtherance of the corporation’s business interests” (internal alterations and quotation marks omitted)).
129 See MODEL PENAL CODE § 2.07(1)(c) (AM. LAW INST. 1962) (adopting respondeat superior but restricting it to the mental states of high corporate officials).
130 Granted, this is a legal fiction. But the principle of legality does not obviously require that corporations literally — as opposed to legally — satisfy the mens rea element. See Paul H. Robinson, Imputed Criminal Liability, 93 YALE L.J. 609, 611-12 (1984). Even if one thinks imputation principles are in tension with the principle of legality, strictly construed, the costs we normally fear from violating it — like weakening public trust in criminal law — are not likely to be very serious. So even if corporations’ literal lack of mental states were to remain a formalistic problem for corporate punishment, it would not be a very weighty one.
131 See Hallevy, supra note 10, at 201 (arguing “there is no substantive legal difference between the idea of criminal liability imposed on corporations and on AI entities”).
 
570
  352 University of California, Davis [Vol. 53:323
ready supply of identifiable human actors whose mental states can be imputed.132 This is not to say there will not also be many garden-variety cases where an AI does have a clear group of human developers. Most AI applications are likely to fall within this category and so respondeat superior would at least be a partial route to making AI eligible for punishment. Of course, in many of these cases when there are identifiable people whose mental states could be imputed to the AI — such as developers or owners who intended the AI to cause harm — criminal law will already have tools at its disposal to impose liability on these culpable human actors. In these cases, there is less likely to be a need to impose direct AI criminal liability.
Thus, while respondeat superior can help mitigate the Eligibility Challenge for AI punishment in many cases, this is unlikely to be an adequate response in cases of Hard AI Crime.
2. Answer 2: Strict Liability
A different sort of response to the Eligibility Challenge is to look for ways to punish AI despite its lack of a culpable mental state. That is not to simply reach for a consequentialist justification133 of the conceptual confusion or inaptness involved in applying criminal law to AI. Within criminal law, we take this to be a justificatory strategy of last resort — especially given the blunt form of consequentialism it relies on. Rather, what is needed is a method of cautiously extending criminal law to AI that would not entail weighty violations of the principle of legality.
One way to do this would be to establish a range of new strict liability offenses specifically for AI crimes — i.e., offenses that an AI could commit even in the absence of any mens rea like intent to cause harm, knowledge of an inculpatory fact, reckless disregard of a risk or negligent unawareness of a risk. In this sense, the AI would be subject to liability without “fault.” This would permit punishment of AI in the absence of mental states. Accordingly, strict liability offenses may be one familiar route by which to impose criminal liability on an AI without sacrificing the principle of legality.
132 Although, Shawn Bayern has argued that existing American LLC statutes allow an organization not to be legally associated with human members. Shawn Bayern, Are Autonomous Entities Possible?, 114 NW. U. L. REV. ONLINE 23, 26 (2019). He argues on this basis that organizational statutes are thus flexible enough to give something like legal personhood to software systems, because an AI can also direct the activities of an organization. Id.
133 See supra notes 77–82 (explaining the idea of justifying punishment based on its good consequences).
 
571
  2019] Punishing Artificial Intelligence 353
Many legal scholars are highly critical of strict liability offenses. For example, as Duff argues, strict criminal liability amounts to unjustly punishing the innocent:
That is why we should object so strongly . . . : the reason is not (only) that people are then subjected to the prospect of material burdens that they had no fair opportunity to avoid, but that they are unjustly portrayed and censured as wrongdoers, or that their conduct is unjustly portrayed and condemned as wrong.134
Yet this normative objection applies with greatest force to persons. The same injustice does not threaten strict criminal liability offenses for AI because AI does not obviously enjoy the protections of the desert constraint135 (which prohibits punishment in excess of culpability).136
This strategy is not without problems. Even to be guilty of a strict liability offense, defendants still must satisfy the voluntary act requirement.137 LaFave’s criminal law treatise observes that “a voluntary act is an absolute requirement for criminal liability.”138 The Model Penal Code, for example, holds that a “person is not guilty of an offense unless his liability is based on conduct that includes a voluntary act or the omission to perform an act of which he is physically capable.”139 Behaviors like reflexes, convulsions or movements that occur unconsciously or while sleeping are expressly ruled out as non- voluntary.140 To be a voluntary act, “only bodily movements guided by conscious mental representations count”.141 If AI cannot have mental states and is incapable of deliberation and reasoning, it is not clear how any of its behavior can be deemed to be a voluntary act.
There are ways around this problem. The voluntary act requirement might be altered (or outright eliminated) by statute for the proposed class of strict liability offenses that only AI can commit. Less
134 DUFF, REALM, supra note 74, at 19.
135 There may be unfairness to adjacent innocent people who own or rely on the AI,
but that is a separate problem which afflicts any punishment. See infra Part III.B.2.
136 Matters would be different if AIs, perhaps like many animals, could experience pleasure and pain, or were conscious or otherwise in possession of morally salient interests. It would indeed seem unfair to subject animals to extreme suffering just for general deterrent benefits (if not as unfair as for a human being).
137 See WAYNE R. LAFAVE, 1 SUBSTANTIVE CRIMINAL LAW § 6.1(c) (3d ed. 2018) (“[C]riminal liability requires that the activity in question be voluntary.”).
138 Id.
139 MODEL PENAL CODE § 2.01(1) (AM. LAW INST. 1962).
140 See id. § 2.01(2).
141 Gideon Yaffe, The Voluntary Act Requirement, in THE ROUTLEDGE COMPANION TO
THE PHILOSOPHY OF LAW 174, 175 (Andrei Marmor ed., 2012).
 
572
  354 University of California, Davis [Vol. 53:323
dramatically, even within existing criminal codes, it is possible to define certain absolute duties of non-harmfulness that AI defendants would have to comply with or else be guilty by omission of a strict liability offense. The Model Penal Code states that an offense cannot be based on an omission to act unless the omission is expressly recognized by statute or “a duty to perform the omitted act is otherwise imposed by law.”142 A statutory amendment imposing affirmative duties on AI to avoid certain kinds of harmful conduct is all it would take to enable an AI to be strictly liable on an omission theory.
Of course, this may also carry costs. Given that one central aim of criminal law is usually taken to be responding to and condemning culpable conduct, if AI is punished on a strict liability basis, this might risk diluting the public meaning and value of the criminal law.143 That is, it threatens to undermine the expressive benefits that supposedly help justify punishing AI in the first place.144 This is another potential cost to punishing AI that must be weighed against its benefits.
3. Answer 3: A Framework for Direct Mens Rea Analysis for AI
The last answer is the most speculative. A framework for directly defining mens rea terms for AI — analogous to those possessed by natural persons — could be crafted. This could require an investigation of AI behavior at the programming level and offer a set of rules that courts could apply to determine when an AI possessed a particular mens rea — like intent, knowledge or recklessness — or at the very least, when such a mens rea could be legally constructed.145 This inquiry could draw on expert testimony about the details of the AI’s code, though it need not. By way of analogy, juries assess mental states of human defendants by using common knowledge about what mental states (intentions, knowledge, etc.) it takes to make a person behave in the observed fashion.146 Similarly, in AI cases, experts might need only
142 MODEL PENAL CODE § 2.01(3) (AM. LAW INST. 1962).
143 See DUFF, REALM, supra note 74, at 19-20.
144 See supra Part II.B.
145 In Part III.A, we discussed respondeat superior as a mode of taking an existing
human mental state and carrying it over to an AI. This section, by contrast, discusses possible methods of legally constructing AI mental states that no person already possesses. Cf. infra note 164 (discussing the collective knowledge doctrine for corporations).
146 See generally Peter Carruthers, Mindreading in Infancy, 28 MIND & LANGUAGE 141, 143 (2013) (discussing how infants attribute beliefs and intentions to others); David Premack & Guy Woodruff, Does the Chimpanzee Have a Theory of Mind?, 1 BEHAV. BRAIN SCI. 515, 515 (1978) (defining “theory of mind” as the system whereby “the individual
 
573
  2019] Punishing Artificial Intelligence 355
to testify in broad terms about how the relevant type of AI (say, a neural network) functions and how its information-processing architecture could have generated the observed behavior. Thus, direct mens rea analysis for AI could, but need not, require “looking under the hood” at the details of the code. Instead, it would be enough to simply guide the legal determination of what mens rea the AI can be deemed to possess.
Towards this end, a framework is needed to steer decision-makers in conducting direct mens rea analysis for AI, and it must consist of two parts. First, to answer the broad Eligibility Challenge, we need a general conception of what it would mean for AI to be culpable in its own right. Second, to answer the narrow version of the challenge, we need to offer a set of rules for when an AI may be deemed to possess a given mens rea.
To begin with, a coherent concept of AI culpability could be legally constructed in the following way. The prevailing theory holds that a person is criminally culpable for an action to the extent that he or she manifests insufficient regard for legally protected interests or values.147 These protected interests and values provide legally recognized reasons bearing on how to behave. Insufficient regard is a form of ill will or indifference that produces mistakes in the way one recognizes, weighs, and responds to the applicable legal reasons for action.148 The criminal
imputes mental states to himself and to others” and noting that it is “not directly observable [but] can be used to make predictions . . . about the behavior of other organisms”).
147 See ALEXANDER & FERZAN, supra note 62, at 67-68 (2009) (“[I]nsufficient concern [is] the essence of culpability.”); Peter K. Westen, An Attitudinal Theory of Excuse, 25 L. & PHIL. 289, 374-75 (2006) (“[A] person is normatively blameworthy for engaging in conduct that a statute prohibits if he was motivated by an attitude of disrespect for the interests that the statute seeks to protect . . . .”); see also VICTOR TADROS, CRIMINAL RESPONSIBILITY 250 (2005) (“[I]f [a defendant] is convicted of a serious offence, the state communicates . . . that [his] behaviour manifested an inappropriate regard for other citizens and their interests....”); Gideon Yaffe, Intoxication, Recklessness, and Negligence, 9 OHIO ST. J. CRIM. L. 545, 552-53 (2012).
148 See, e.g., GIDEON YAFFE, ATTEMPTS 38 (2010) (an action is culpable to the degree that “it is a product of a faulty mode of recognition or response to reasons for action”). Note that legal culpability may or may not be the same as moral blameworthiness. Compare Mark Dsouza, Criminal Culpability After the Act, 26 KING’S L.J. 440, 453 (2015) (distinguishing moral from legal culpability), and Sarch, Who Cares, supra note 17, at 710, with Michael S. Moore, Choice, Character, and Excuse, 7 SOC. PHIL. & POL’Y 29, 30- 31 (1990) (taking moral and legal culpability to be presumptively the same). We are agnostic on how to understand moral blameworthiness, which may be more fine- grained and searching of one’s inner mental states than legal culpability. Compare Pete Graham, A Sketch of a Theory of Blameworthiness, 88 PHIL. & PHENOMENOLOGICAL RES. 388, 403 (2014) (“[W]hat people are truly blameworthy for are the motivations from which [their] actions spring, rather than the actions themselves.”), with NOMY ARPALY & TIMOTHY SCHROEDER, IN PRAISE OF DESIRE 170 (2014) (defending a notion of
 
574
  356 University of California, Davis [Vol. 53:323
law typically does not demand that we are motivated by respect for others, or even respect for law; all it demands is that we do not put our disrespect on display by acting in ways that are inconsistent with attaching proper weight to protected interests and values. Thus, criminal culpability can be seen as being more about what one’s behavior manifests and less about the nuances of one’s private motivations, thoughts, and feelings.149 There are good institutional design reasons — such as clarity, the need for the law to be able to guide the conduct of normal citizens, and the demand for the law not to intrude too heavily into the private sphere — for criminal law not to be overly concerned with the specific motives or private mental states involved in law-breaking. Thus, as long as one crosses the line and has no affirmative defense, we may treat the presumption that one’s illegal action manifests insufficient regard as being unrebutted — i.e., as legally conclusive.
By way of analogy, this notion of culpability can account for corporate culpability. If only the legal notion of criminal culpability is required for proper punishment, then eligibility for punishment requires being capable of behaving in ways that manifest insufficient regard for the legally recognized reasons. All that avoiding legal culpability requires is to abstain from actions that are reasonably interpreted as disrespectful forms of conduct stemming from a legally deficient appreciation of the legal reasons.150 This provides a recipe for how to regard corporations as being criminally culpable in their own right. They possess information-gathering, reasoning, and decision-making procedures in virtue of the hierarchy of employees they are made up of. Thus, corporations can be seen as having the capacity for criminal culpability. Through their members, they weigh and act on the reasons that criminal law demands not displaying insufficient regard for in action.151
blameworthiness that is similar to criminal culpability as described here). Our focus here, regardless, is legal culpability.
149 See Sarch, Who Cares, supra note 17, at 709-10.
150 See GIDEON YAFFE, THE AGE OF CULPABILITY: CHILDREN AND THE NATURE OF CRIMINAL RESPONSIBILITY (2018) (developing an evidentialist account of manifestation of insufficient regard); see also Sarch, Who Cares, supra note 17, at 727-33.
151 See CHRISTIAN LIST & PHILIP PETTIT, GROUP AGENCY 158-63 (2011) (arguing that corporations can have decision-making structures that satisfy the main preconditions for responsibility); W. Robert Thomas, The Ability and Responsibility of Corporate Law to Improve Criminal Punishment, 78 OHIO ST. L.J. 601, 612-13 (2017) (“Corporations have free will in a narrow sense: they can deliberate and act consistent with their self- identified interests and separate from outside pressures. Corporations are willing participants in . . . our normative practices, even if they may not be objects of moral consideration in . . . themselves. For example, through contract law, corporations routinely participate in a normatively laden practice akin to promising.”).
 
575
  2019] Punishing Artificial Intelligence 357
Corporations can engage in conduct that puts on display their insufficient regard for the legally recognized interests of others. For example, if a corporation learns, through its employees, that its manufacturing processes generate dangerous waste that is seeping into the drinking water in the nearby town, this is a legally recognized reason for the corporation to alter its conduct. If the corporation continues its manufacturing activities unchanged, this demonstrates — through its information-sharing and decision-making procedures — that it did not attach sufficient weight to the legally recognized reasons against continuing its dangerous activities. This is paradigmatic criminal culpability.152
AI could qualify as criminally culpable in an analogous manner. Sophisticated AI may have built-in goals with a greater or lesser autonomy to determine the means of completing those goals.153 AI may gather information, process it, and determine the most efficient means to accomplish its goals.154 Accordingly, the law might deem some AIs to possess the functional equivalent of sufficient reasoning and decision- making abilities to manifest insufficient regard. If the AI is programmed to be able to take account of the interests of humans and consider legal requirements, but ends up behaving in a way that is inconsistent with taking proper account of these legally recognized interests and reasons, then the AI can be reasonably seen as manifesting insufficient regard — which is to say, be deemed in law to be criminally culpable.155
152 One might object that a corporation’s practical reasoning and decision-making capacities merely derive from, or are composed out of, those of the corporation’s members. However, this is merely a worry about reducibility. See infra Part III.B. It does not undermine corporations’ threshold eligibility for punishment. See Thomas, supra note 151, at 613 (noting that if “corporate attitudes derive from the contributions of individuals who themselves are uncontroversially moral agents...it would be surprising that every emergent corporate attitude would be stripped of normative content”).
153 See supra notes 31–48.
154 See id.
155 This idea is similar in some respects to Hu’s argument that robots may be
punished only if they possess “moral algorithms.” See Hu, supra note 15, at 496. These are “algorithms that are capable of making nontrivial morally relevant decisions,” (i.e., ones that “concern[] a choice between or among two or more courses of actions that might be considered right or wrong by ordinary members of our society.”). Id. As Hu notes, these could be taken on the basis of strict rules, guiding principles or an effort to weigh the competing interests at play in order to determine what would maximize some expected utility function. See id. at 497-98; see also id. at 505 (considering the analogy between corporate moral responsibility and similar responsibility for AI). Unlike Hu, our arguments do not assume or require that AI are moral persons or have moral responsibility. We are concerned with legal culpability, the demands of which are less exacting than true moral responsibility.
 
576
  358 University of California, Davis [Vol. 53:323
This gives a flavor of how criminal culpability might broadly be understood for AI, but we still need a framework for determining when sophisticated AIs can be said to possess a functional analog of a standard mens rea-like purpose or knowledge. We do not attempt here to formulate necessary and sufficient conditions for an AI mens rea, but rather to sketch some possible approaches.
Work in the Philosophy of Action characterizing the functional role of human intentions could be extended to AI. On Bratman’s well-known account,156 actors who intend (i.e., act with the purpose) to bring about an outcome “guide [their] conduct in the direction of causing” that outcome.157 This means that in the normal case, “one [who intends an outcome] is prepared to make adjustments in what one is doing in response to indications of one’s success or failure in promoting” that outcome.158 Suppose an actor is driving with the intention to hit a pedestrian. In that case, if the actor detects that conditions have changed so that behavioral adjustments are required to make this outcome more likely, then the actor will be disposed to make these adjustments. Moreover, actors with this intention will be disposed to monitor the circumstances to find ways to increase the likelihood of the desired outcome. Merely foreseeing the outcome, but not intending it, does not similarly entail that one will guide one’s behavior in these ways to promote the outcome in question (i.e., make it more likely).
This conception of intention could be applied to AI. One conceivable way to argue that an AI (say, an autonomous vehicle) had the intention (purpose) to cause an outcome (to harm a pedestrian) would be to ask whether the AI was guiding its behavior so as to make this outcome more likely (relative to its background probability of occurring). Is the AI monitoring conditions around it to identify ways to make this outcome more likely? Is the AI then disposed to make these behavioral adjustments to make the outcome more likely (either as a goal in itself or as a means to accomplishing another goal)? If so, then the AI plausibly may be said to have the purpose of causing that outcome. Carrying out this sort of inquiry will of course require extensive and technically challenging expert testimony regarding the nature of the programming — and could thus be prohibitively difficult or expensive.
156 See Michael E. Bratman, What Is Intention?, in INTENTIONS IN COMMUNICATION 15, 23-27 (Philip R. Cohen et al. eds., 1990); see also Alex Sarch, Double Effect and the Criminal Law, 11 CRIM. L. & PHIL. 453, 467-68 (2015).
157 Bratman, supra note 156, at 26.
158 See id.
 
577
  2019] Punishing Artificial Intelligence 359
But it does not seem impossible in principle, even if difficult questions remain.159
Similar strategies may be developed for arguing that an AI possessed other mens rea, like knowledge. For example, on dispositional theories, knowledge may be attributed to an actor when the actor has a sufficiently robust set of dispositions pertaining to the truth of the proposition — such as the disposition to assent to the proposition if queried, to express surprise and update one’s plans if the proposition is revealed to be false, to behave consistently with the truth of the proposition, or to depend on it carrying out one’s plans.160 In criminal law, knowledge is defined as practical certainty.161 Thus, if we extend the above dispositional theory to AI, there is an argument for saying an AI knows a fact, F, if the AI displays a sufficiently robust set of dispositions associated with the truth of F — such as the disposition to respond affirmatively if queried (in a relevant way) whether F is practically certain to be true, or the disposition to revise plans upon receiving information showing that F is not practically certain, or the disposition to behave as if F is practically certain to be true. If enough
159 For example, suppose the autonomous vehicle is actually aiming not to harm pedestrians by hitting them, but rather aims for something that merely correlates with hitting pedestrians — such as reducing the amount of shadows objects cast on the streets (as fewer shadows increases other metrics of reliable driving, which is the car’s primary goal). Should this be construed as intentionally hitting the pedestrians, or merely hitting them knowingly? This is a familiar problem from criminal law theory and philosophy of action. See, e.g., Adam Feltz & Joshua May, The Means/Side-Effect Distinction in Moral Cognition: A Meta-Analysis, 166 COGNITION 314-17 (2017). We need not resolve this difficult question here to establish our main point that it is possible to make progress on extending mens rea terms to AI. Nonetheless, by analogy, we suspect this case would plausibly be construed as intentionally hitting the pedestrian as a means to the self-driving car’s other goals. If the AI regulates its conduct to make hitting pedestrians more likely, this is not simply a “foreseen byproduct” of the AI behavior, but something it pursues as a means to accomplishing its deeper aims. Intending harm as a means suffices for showing purpose in the criminal law. If you kill a relative merely as the means to getting your inheritance, the killing still is purposeful. Alternatively, perhaps the “intended as a means/foreseen as a side-effect” distinction should be jettisoned as unworkable.
160 See Belief, STAN. ENCYCLOPEDIA PHIL. (June 3, 2019), https://plato.stanford.edu/ entries/belief/#1.2 (“Traditional dispositional views of belief assert that for someone to believe some proposition P is for [her] to possess [relevant] behavioral dispositions pertaining to P. Often cited is the disposition to assent to utterances of P in [appropriate] circumstances . . . . Other relevant dispositions might include the disposition to exhibit surprise should the falsity of P [become] evident, the disposition to assent to Q if . . . shown that P implies Q, and the disposition to depend on P’s truth in [acting]. [More generally, this amounts to] being disposed to act as though P is the case.”).
161 See MODEL PENAL CODE § 2.02(2)(b) (AM. LAW INST. 1962) (defining knowledge as practical certainty).
 
578
  360 University of California, Davis [Vol. 53:323
of these dispositions are proven, then knowledge that F could be attributed to the AI.162 One could take a similar approach to arguing that recklessness is present as well, as this requires only awareness that a substantial risk of harm is present — i.e., knowledge that the risk has a mid-level probability of materializing (below practical certainty).163
Finally, as an alternative to direct arguments for showing AI mens rea, one could develop new imputation rules for AI. For example, one might follow the model of the collective knowledge doctrine, which identifies culpable interference with the flow of information within an organization and uses this as the basis for pretending as if the organization itself “knew” the facts it prevented itself from learning.164 The idea as applied here would be to take culpable conduct by the AI’s developers and use this as the basis for pretending the AI possessed a culpable mens rea itself. For example, if AI developers were reckless (or negligent) in their design, testing or production, and the AI goes on to cause harm, this could provide an argument for treating the AI itself as if it were reckless (or negligent) as to the harm caused.
Although much more needs to be said for such arguments to be workable,165 it at least suggests that it may be possible to develop a set of legal doctrines by which courts could deem AIs to possess the mens rea elements of crimes.
B. Further Retributivist Challenges: Reducibility and Spillover
Even assuming AI is eligible for punishment, two further culpability- focused challenges remain. The first concerns the reducibility of any putative AI culpability, while the second concerns spillover of AI punishment onto innocent people nearby. This Section offers answers to both.
162 Cf. Eric Schwitzgebel, In-Between Believing, 51 PHIL. Q. 76 (2001) (defending this approach to determining when to attribute beliefs to humans).
163 See MODEL PENAL CODE § 2.02(2)(c) (AM. LAW INST. 1962) (defining recklessness).
164 See United States v. Bank of New England, 821 F.2d 844, 856 (1st Cir. 1987) (embracing one version of collective knowledge doctrine); ALEXANDER SARCH, CRIMINALLY IGNORANT 246 (2019) (defending the collective knowledge doctrine as an equal culpability imputation rule for corporations).
165 Among other problems there may not be deterrence benefits to punishing autonomous vehicles that hit pedestrians due to code that could be reconstructed as embodying a culpable maxim (like “if you flip me off then I run you over”), but withholding such punishment from unexplainable machine learning code that results in the same thing. Why the latter should not generate independent liability while the former would seems to be a distinction without a normative difference.
 
579
  2019] Punishing Artificial Intelligence 361 1. Reducibility
One might object that there is never a genuine need to punish AI because any time an AI seems criminally culpable in its own right, this culpability can always be reduced to that of nearby human actors — such as developers, owners, and users. The law could target the relevant culpable human actors instead.
This objection has been raised against corporate punishment too. Skeptics argue that corporate culpability is always fully reducible to culpable actions of individual humans.166 Any time a corporation does something intuitively culpable — like causing a harmful oil spill through insufficient safety procedures — this can always be fully reduced to the culpability of the individuals involved: the person carrying out the safety checks, the designers of the safety protocols, or the managers pushing employees to cut corners in search of savings. For any case offered to demonstrate the irreducibility of corporate culpability,167 a skeptic may creatively find additional wrongdoing by other individual actors further afield or in the past to account for the apparent corporate culpability.168
This worry may not be as acute for AI as it is for corporations. AI seems able to behave in ways that are more autonomous from its developers than corporations are from their members. Corporations, after all, are simply composed of their agents (albeit organized in particular structures). Also, AI may sometimes behave in ways that are less predictable and foreseeable than corporate conduct.
Nonetheless, there are ways to block the reducibility worry for corporate culpability as well as AI. The simplest response is to recall that it is legal culpability we are concerned with, not moral blameworthiness. Specifically, it would be bad policy for criminal law
166 See, e.g., Andras Szigeti, Are Individualist Accounts of Collective Responsibility Morally Deficient?, in INSTITUTIONS, EMOTIONS, AND GROUP AGENTS: CONTRIBUTIONS TO SOCIAL ONTOLOGY 329 (2014) (Anita Konzelmann Ziv & Hans Bernhard Schmid eds., 2013) (arguing that the individualist analysis does not leave any responsibility-deficit that would require a genuine group culpability).
167 Consider List and Pettit’s notion of a “responsibility deficit.” LIST & PETTIT, supra note 151, at 165. Perhaps “the individuals are blamelessly ignorant [or] act under such felt pressure that they cannot be held fully responsible for their contribution to a bad outcome; they can each argue that the circumstances mitigate their personal . . . responsibility.” Id. If the individuals have lowered culpability, then the total culpability for the group harm might seem greater than the sum of individual culpability. Whether such responsibility deficits can really arise, however, remains debatable. After all, when the individuals are excused, might that lower the total amount of blame to be attributed for the group harm?
168 See id. at 158.
 
580
  362 University of California, Davis [Vol. 53:323
to always allow any putative corporate criminal culpability to be reduced to individual criminal liability. This would require criminalizing very minute portions of individual misconduct — momentary lapses of attention, the failure to perceive emerging problems that are difficult to notice, tiny bits of carelessness, mistakes in prioritizing time and resources, not being sufficiently critical of groupthink, and so on. Mature legal systems should not criminalize infinitely fine-grained forms of misconduct, but rather should focus on broader and more serious categories of directly harmful misconduct that can be straightforwardly defined, identified, and prosecuted. Criminalizing all such small failures — and allowing law enforcement to investigate them — would be invasive and threatening to values like autonomy and the freedom of expression and association.169 It would also increase the risk of abuse of process. Instead, we should expect “culpability deficits”170 in any well-designed system of criminal law, and this in turn creates a genuine need for corporate criminal culpability as an irreducible concept.
Similar reasoning could be employed for AI culpability. There is reason to think it would be a bad system that encouraged law enforcement and prosecutors to, any time an AI causes harm, invasively delve into the internal activities of the organizations developing the AI in search of minute individual misconduct — perhaps even the slightest negligence or failure to plan for highly unlikely exigencies. The criminal justice system would be disturbingly invasive if it had to create a sufficient number of individual offenses to ensure that any potential AI culpability can always be fully reduced to individual crimes. Hence, where AI is concerned, we do not think the Reducibility Challenge — at least as applied to legal culpability — imposes a categorical bar to punishing AI.
2. Spillover
A final retributivist challenge to punishing AI is the “spillover problem,” again familiar from the corporate context.171 Because corporate punishments (usually in the form of fines) amount to a hit to the corporation’s bottom line, these punishments inevitably spill over
169 Cf. HART, supra note 70, at 1-27.
170 See LIST & PETTIT, supra note 151, at 165 (defending “responsibility deficits” as
creating a need for irreducible corporate accountability).
171 See Thomas, supra note 151, at 619.
 
581
  2019] Punishing Artificial Intelligence 363
onto innocent shareholders.172 This might seem to violate the desert constraint against the state harming people in excess of their desert. The same objection has been raised against punishing AI. Mulligan worries that “[o]ne could . . . imagine situations where the notion of separating a rogue robot from its owner [or damaging or restricting the robot in punishing it] would create a disproportionate burden on the owner, for example if a robot was unique, unusually expensive relative to the harm caused, or difficult to replace.”173 This is just a version of the spillover problem. If the AI system unforeseeably causes harm, it may seem unfair or disproportionate to its innocent owner or operator to damage the AI system in punishment.
There are familiar responses to the spillover objection for corporations. First, one might contend that spillover does not qualify as punishment because it is not imposed on a shareholder for her offense.174 Nonetheless, this definitional answer is somewhat unsatisfying, as there clearly are strong reasons for the state not to knowingly harm innocent bystanders even if the harm does not strictly count as punishment.
A better answer is that spillover is not a special problem for corporate or AI punishment. Most forms of punishment — including punishment of individual wrongdoers — has the potential to harm the innocent, as when a convicted person has dependent children. Spillover objections may simply expose general problems with criminal law. The fact that punishment tends to harm the innocent suggests a need to reform criminal law as well as prisons, reentry programs and similar initiatives to lessen the collateral consequences of punishment of all types. In the corporate context, some have recently responded to the spillover objection by defending reforms to corporate punishments so the “pain” they impose is more accurately distributed to the culpable actors within the company who contributed to the crime.175 For example, Will Thomas argues that managers found to have contributed to a crime committed by the corporation should have their incentive compensation clawed back to satisfy the criminal fines that were levied against the corporation in the first instance.176
172 See GLANVILLE WILLIAMS, CRIMINAL LAW: THE GENERAL PART 863 (2d ed. 1961) (noting that “a fine imposed on the corporation is in reality aimed against shareholders who are not . . . responsible for the crime, i.e., is aimed against innocent persons”).
173 Mulligan, supra note 9, at 594.
174 See HART, supra note 70, at 4-5.
175 See Thomas, supra note 151, at 647.
176 See id. at 647-49.
 
582
  364 University of California, Davis [Vol. 53:323
Similar thinking applies to AI punishment, which likewise should be narrowly tailored. Destroying an AI, for example, would be a blunt remedy that is more likely to harm the innocent. More tailored remedies might be implemented instead, such as reprogramming the AI, or civil remedies directed at responsible persons. In such ways, the punishment of AI systems could be crafted to minimize the spillover effects. Further, spillover may be less of a concern in the case of Hard AI Crime, where there may be little nexus between AI punishment and harm to innocent individuals. Even here, spillover could be largely addressed through well-designed mechanisms like the ex-ante creation of a financially responsible party or creation of a fund to cover criminal liability as a condition of operation the AI system (akin to criminal liability insurance). We explore such implementation ideas further in the next Part. The spillover problem thus is not an absolute bar to AI punishment. It is an omnipresent problem with criminal punishment, which should be addressed for any novel mode of criminal punishment — whether for corporations or AI.
C. Not Really Punishment?
We end this Part by considering another challenge to AI punishment — that AI cannot be truly “punished.” Even if an AI was convicted of an offense and subject to negative treatment — such as being reprogrammed or terminated — this may not be punishment under our working definition. On Hart’s definition introduced in Part I.C, punishment “must involve pain or other consequences normally considered unpleasant.”177 However, AI cannot experience things as being painful or unpleasant.178
A first response is to argue that AI punishment does satisfy Hart’s definition because prong (i) requires only that the treatment in question must be “normally considered unpleasant” — not that it be actually unpleasant or unwelcome to a convicted party. This is what allows Hart’s definition to accommodate people who, for idiosyncratic reasons, do not experience their sentence as unpleasant or bad and to still regard this as punishment. The mere fact that a convicted party overtly wants to be imprisoned, like the Norwegian mass murderer Anders Bering Breivik, who wanted to be convicted and imprisoned to further his political agenda, does not mean that doing so pursuant to a conviction
177 See HART, supra note 70, at 4.
178 See supra note 53 and accompanying text.
 
583
  2019] Punishing Artificial Intelligence 365
ceases to be punishment.179 Something similar might be said for AI as well as defendants who may be physically or psychologically incapable of experiencing pain or distress. Having one’s actions frozen or being terminated really are the kind of thing that can “normally be considered unpleasant.”
This response can be developed further. Why might punishment need to be normally regarded as unpleasant? Why does it still seem to be punishment, for example, to imprison a person who in no way experiences it as unpleasant or unwelcome? The answer may be that defendants can have interests that are objectively set back even when they do not experience these setbacks as painful, unpleasant or unwelcome.180 Some philosophers argue it is intrinsically bad for humans to have their physical or agential capacities diminished — regardless of whether this is perceived as negative.181 If correct, this suggests that what prong (i) of Hart’s definition, properly understood, requires is that punishment involve events that objectively set back interests, and negative subjective experiences are merely one way to objectively set back interests.
Can an AI have interests that are capable of being set back? AI is not conscious in the phenomenal sense of having subjective experiences and thus cannot experience anything as painful or unpleasant.182 However, one could maintain that being incapacitated or destroyed is objectively bad for AIs even if the AI does not experience it as such — in much the same way that things like nutrition, reproduction, or physical damage can be said to be good or bad for biological entities like plants or animals.183 Some philosophers argue that it is in virtue of
179 See Anders Breivik Found Sane: The Verdict Explained, TELEGRAPH (Aug. 24, 2012, 10:15 AM), https://www.telegraph.co.uk/news/worldnews/europe/norway/9496641/ Anders-Breivik-found-sane-the-verdict-explained.html (discussing why Breivik would “want to be sent to prison” rather than getting the benefit of an insanity defense).
180 See Guy Fletcher, A Fresh Start for the Objective-List Theory of Well-Being, 25 UTILITAS 206, 206 (2013) (defending objective theories of well-being from familiar objections); Alexander Sarch, Multi-Component Theories of Well-Being and Their Structure, 93 PAC. PHIL. Q. 439, 439-41 (defending a partially objective theory of well- being, where both subjective experiences and some objective components can impact well-being).
181 See Elizabeth Harman, Harming As Causing Harm, in HARMING FUTURE PERSONS 137, 139 (Melinda A. Roberts & David T. Wasserman eds., 2009) (arguing that an event harms an agent when that event causes P to be in an intrinsically bad state, where such states include “pain, mental or physical discomfort, disease, deformity, disability, or death”).
182 See supra note 53 and accompanying text.
183 See PHILIPPA FOOT, NATURAL GOODNESS 26 (2001) (noting that “features of plants
and animals have what one might call an ‘autonomous’, ‘intrinsic’, or as I shall say
 
584
  366 University of California, Davis [Vol. 53:323
something’s having identifiable functions that things can be good or bad for it. Most notably, Philippa Foot defends this sort of view (tracing it to Aristotle) when she argues that the members of a given species can be evaluated as excellent or defective by reference to the functions that are built into its characteristic form of life.184 From this evaluation as flourishing or defective, facts about what is good or bad for the entity can be derived. Thus, if having interests in this broad, function-based sense is all that is required for punishment to be sensible, then perhaps AI fits the bill. AIs also have a range of functions — characteristic patterns of behavior needed to continue in good working order and succeed at the tasks it characteristically undertakes. If living organisms can in a thin sense be said to have an interest in survival and reproduction, ultimately in virtue of their biological programming, then arguably an AI following digital programming could have interests in this thin sense as well.
Other philosophers reject this view, however. They insist that only those entities capable of having beliefs and desires, or at least phenomenal experiences such as of pleasure and pain, can truly be said to have full-blooded interests that are normatively important. Legal philosopher Joel Feinberg took the capacity for cognition as the touchstone full-blooded interests, that is as a precondition for having things really be good or bad for us.185 He notes “we do say that certain conditions are ‘good’ or ‘bad’ for plants” (unlike rocks), but he denies that they have full-blooded interests.186 Although “Aristotle and Aquinas took trees [and plants] to have their own ‘natural ends’” (in much the same sense that Foot argues for), Feinberg denies plants “the status of beings with interests of their own” because “an interest, however the concept is finally to be analyzed, presupposes at least rudimentary cognitive equipment.”187 Interests, he thinks, “are compounded out of desires and aims, both of which presuppose
‘natural’ goodness and defect that may have nothing to do with the needs or wants of the members of any other species of living thing”).
184 See id. at 33 (“The way an individual should be is determined by what is needed for development, self-maintenance, and reproduction: in most species involving defence, and in some the rearing of the young . . . . Thus, evaluation of an individual living thing in its own right, with no reference to our interests or desires, is possible [by reference to the functions of the thing, as captured in] Aristotelian categoricals (life- form descriptions relating to the species)”).
185 See Joel Feinberg, The Rights of Animals and Unborn Generations, in PHILOSOPHY AND ENVIRONMENTAL CRISIS 43, 49-51 (William T. Blackstone ed., 1974).
186 Id. at 51.
187 Id. at 52.
 
585
  2019] Punishing Artificial Intelligence 367
something like belief, or cognitive awareness.”188 Since AIs are not literally capable of cognitive awareness (notwithstanding the discussion in Part III.A of how mens rea might be imputed), they cannot literally possess full-blooded interests of the kind Feinberg has in mind.189
Thus, the pertinent question for present purposes is what sense of interest an entity must have for it to be intelligible to talk of punishing it — the thin sense of function-based interests of the kind Foot defended or the full-blooded, attitudinally-based interests Feinberg had in mind? This is ultimately a question about how to understand prong (i) of Hart’s definition of punishment, and one that goes to the heart of what criminal law is and what it is for. We simply note that this is one possible way of defending the idea of AI punishment as sensible.
A different, perhaps stronger, type of reply is to distinguish between conviction and punishment, where the latter covers the sentence to which the convicted party is subject. Even if no form of treatment can count as punishment unless the entity in question experiences it negatively, this is not a precondition for a conviction. Perhaps for it to be intelligible to convict X of an offense, it is only required that X acted in ways that violated a prohibition and this can be sensibly construed as culpable (a manifestation of insufficient regard). If so, then one might accept that while punishing AI is not conceptually possible, applying criminal law to AIs, so they could be convicted of offenses, is. Thus, society might still benefit from AI convictions while not running afoul of the conceptual confusion that results from purporting to punish AIs.
Convicting AIs may require, or allow, subjecting other parties to punishment in place of the AI. Criminal law roundly rejects “vicarious punishment” where people are concerned190 — not least where it risks the injustice of strict criminal liability imposed on innocent actors.191 Corporate punishment might seem to involve vicarious punishment when officers or employees of the corporation are made to suffer due to the criminal fines imposed on the corporation. However, such cases are better understood not as vicarious liability, strictly speaking, but as convicting the corporation of an offense directly and then allowing the
188 Id.
189 See id. at 49-50 (arguing that an entity cannot have full-blooded interests if it has “no conative life: no conscious wishes, desires, and hopes; or urges and impulses; or unconscious drives, aims, and goals; or latent tendencies, direction of growth, and natural fulfillments”).
190 See Joel Feinberg, Collective Responsibility, 65 J. PHIL. 674, 680 (1968) (“[S]urely there is no going back to [collective punishment] . . . . On the contrary, the changes that have come with modern times have dictated quite inevitably that [individual punishment] replace [collective punishment].”).
191 See supra notes 133–134 and accompanying text.
 
586
  368 University of California, Davis [Vol. 53:323
sentence to be distributed to the different individuals out of which the corporation is made up.192 In the case of an AI, it could be argued that if human owners or users accept responsibility for operating the AI safely, then were the AI convicted of an offense in its own right, these responsible parties would be the appropriate persons to whom the sentence could be distributed by virtue of their voluntarily undertaking such responsibility. We explore a simple version of this idea further in the next Part.
A final type of reply, always available as a last resort, is that even if applying criminal law to AIs is conceptually confused, it could still have good consequences to call it punishment when AIs are convicted. This would not be to defend AI punishment from within existing criminal law principles, but to suggest that there are consequentialist reasons to depart from them.
IV. FEASIBLE ALTERNATIVES
We have argued that punishing AI could have benefits and that doing so would not be ruled out by the negative limitations and retributive preconditions of punishment. But this does not yet show the punishment of AI to be justified. Doing so requires addressing the third main question in our theory of punishment: Would the benefits of punishing AI outweigh the costs, and would punishment be better than alternative solutions? These solutions might involve doing nothing, or relying on civil liability and regulatory responses, perhaps together with less radical or disruptive changes to criminal laws that target individuals.
Ideally a cost-benefit analysis would involve more than identifying various costs and benefits, and would include quantitative analysis. If only a single Hard AI Crime is committed each decade, there would be far less need to address an AI criminal gap than if Hard AI Crime was a daily occurrence. The absence of evidence suggesting that Hard AI Crime is common counsels against taking potentially more costly actions now, but this balance may change as technological advances result in more AI activity.
Section A focuses on Hard AI Crime, and finds that existing criminal law coverage will likely fall short. Section B argues that AI punishment has significant costs that suggest alternative approaches may be preferable. In Sections C and D, we map out some alternative approaches to managing AI crime. In particular, we examine moderate expansions of criminal law as well as tools available within civil law,
192 See Thomas, supra note 151, at 612-13.
 
587
  2019] Punishing Artificial Intelligence 369 and we argue that they have the resources to provide preferable
solutions to the problem of Hard AI Crime.
A. First Alternative: The Status Quo
In considering the alternatives to direct punishment of AI, we begin with asking whether it would be preferable to simply do nothing. This section answers that existing criminal law falls short: there is an AI criminal gap. The impact of this gap is an empirical question we do not attempt to answer here.
1. What the AI criminal gap is not: reducible harmful conduct by AI
We begin by setting aside something that will not much concern us: cases where responsibility for harmful AI conduct is fully reducible to the culpable conduct of individual human actors. A clear example would be one where a hacker uses AI to steal funds from individual bank accounts. There is no need to punish AI in such cases, because existing criminal offenses, like fraud or computer crimes, are sufficient to respond to this type of behavior.193
Even if additional computer-related offenses must be created to adequately deter novel crimes implemented with the use of AI, criminal law has further familiar tools at its disposal, involving individual- focused crimes, which provide other avenues of criminal liability when AI causes foreseeable harms. For example, as Hallevy observes, cases of this sort could possibly be prosecuted under an innocent “agency model” (assuming AI can sensibly be treated as meeting the preconditions of an innocent agent, even if not of a fully criminally responsible agent in its own right).194 Under the innocent agency doctrine, criminal liability attaches to a person who acts through an agent who lacks capacity — such as a child or someone with an insanity defense. For instance, if an adult uses a five-year-old child to deliver illegal drugs, the adult rather than the child would generally be criminally liable.195 This could be analogous to a person programming a sophisticated AI to break the law: the person has liability for
193 See 18 U.S.C. § 1030(a)(1)-(7) (2019) (defining offenses such as computer trespass and computer fraud); id. § 1343 (wire fraud statute).
194 Hallevy, supra note 10, at 179-81.
195 See Sanford H. Kadish, Complicity, Cause and Blame: A Study in the Interpretation
of Doctrine, 73 CALIF. L. REV. 323, 372-73 (1985) (“Most criminal actions can readily be committed through the instrumentality of another person.”).
 
588
  370 University of California, Davis [Vol. 53:323
intentionally causing the AI to bring about the external elements of the offense.196
This doctrine requires intent (or at least the knowledge) that the innocent agent will cause the prohibited result in question.197 This means that in cases where someone does not intend or foresee that the AI system being used will cause harm, the innocent agency model does not provide a route to liability. In such cases, one could instead appeal to recklessness or negligence liability if AI creates a foreseeable risk of a prohibited harm.198 For example, if the developers or users of AI foresee a substantial and unjustified risk that an AI will cause the death of a person, these human actors could be convicted of reckless homicide.199 If such a risk was merely reasonably foreseeable (but not foreseen), then lower forms of homicide liability would be available.200 Similar forms of recklessness or negligence liability could be adopted where the AI’s designers or users actually foresaw, or should have foreseen, a substantial and unjustified risk of other kinds of harms as well — such as theft or property damage.201
Hallevy also discusses this form of criminal liability for AI-generated harms, calling it the “natural and probable consequences model” of liability.202 This is an odd label, however, since the natural and probable consequences doctrine generally applies only when the defendant is already an accomplice to — i.e., intended — the crime of another. More
196 One might have doubts about this model of liability, too. After all, if AI is merely a tool, one would simply prosecute the user of the AI on a direct liability model. However, if AI is to be analogized to some kind of autonomous actor, which could break the chain of causation, akin to a child perhaps, then the innocent agency model would seem more apt. In any case, we argued in Part III that AI might plausibly count as an agent at least for legal purposes. Therefore, we think it is not ruled out at least in principle that the innocent agency model of liability could be applied to actors who cause AI to produce criminally prohibited results.
197 See Peter Alldridge, The Doctrine of Innocent Agency, 2 CRIM. L. F. 45, 70-71 (1990); 18 U.S.C. § 2(b) (2019) (“Whoever willfully causes an act to be done which [is a crime] is punishable as a principal.”). This intent requirement for innocent agency is similar to complicity liability, used where the actor assists or encourages another full- fledged agent with capacity to do a crime, which also requires intent or knowledge by the accomplice that the principal actor will do the crime. Rosemond v. United States, 572 U.S. 65, 79-80 (2014) (clarifying mens rea for complicity).
198 See MODEL PENAL CODE § 2.02(2)(c)-(d) (AM. LAW INST. 1962) (defining recklessness and negligence).
199 See id. § 210.3(a) (recklessly causing death suffices for manslaughter).
200 See id. § 210.4 (negligent homicide).
201 See, e.g., id. § 220.1(2) (reckless burning or exploding); id. § 220.2(2) (risking
catastrophe); id. § 220.3 (criminal mischief). 202 See Hallevy, supra note 10, at 181-84.
 
589
  2019] Punishing Artificial Intelligence 371
specifically, the “natural and probable consequences” rule provides that where A intentionally aided B’s underlying crime C1 (say theft), but then B also goes on to commit a different crime C2 (say murder), then A would be guilty of C2 as well, provided that C2 was reasonably foreseeable.203
Despite his choice of label, Hallevy seems alive to this complication and correctly observes that there are two ways in which negligence liability could apply to AI-generated harms that are reasonably foreseeable. He writes:
the natural-probable-consequence liability model [applied] to the programmer or user differ in two different types of factual cases. The first type of case is when the programmers or users were negligent while programming or using the AI entity but had no criminal intent to commit any offense. The second type of case is when the programmers or users programmed or used the AI entity knowingly and willfully in order to commit one offense via the AI entity, but the AI entity deviated from the plan and committed some other offense, in addition to or instead of the planned offense.204
In either sort of scenario, there would be a straightforward basis for applying existing criminal law doctrines to impose criminal liability on the programmers or users of an AI that causes reasonably foreseeable harms. Thus, no AI criminal gap exists here.
A slightly harder scenario involves reducible harms by AI that are not foreseeable, but this is still something criminal law has tools to deal with. Imagine hackers use an AI to drain a fund of currency, but this ends up unforeseeably shutting down an electrical grid which results in widespread harm. The hackers are already guilty of something — namely, the theft of currency (if they succeed) or the attempt to do so (if they failed). Therefore, our question here is whether the hackers can be convicted of any further crime in virtue of their causing harm through their AI unforeseeably taking down an electrical grid.205
203 The rule holds that the aider and abettor “of an initial crime . . . is also liable for any consequent crime committed by the principal, even if he or she did not abet the second crime, as long as the consequent crime is a natural and probable consequence of the first crime.” Baruch Weiss, What Were They Thinking?: The Mental States of the Aider and Abettor and the Causer Under Federal Law, 70 FORDHAM L. REV. 1341, 1424 (2002); see also United States v. Barnett, 667 F.2d 835, 841 (9th Cir. 1982) (adopting natural or probable consequences doctrine).
204 Hallevy, supra note 10, at 184.
205 Compare this case to the one where some kids are illegally using fireworks in
their back yard, and this causes a massive forest fire destroying many homes. Sure, they
 
590
  372 University of California, Davis [Vol. 53:323
At first sight, it might seem that the hackers would be in the clear for the electrical grid. They could argue that they did not proximately cause those particular harms. Crimes like manslaughter or property damage carry a proximate cause requirement under which the prohibited harm must at least be a reasonably foreseeable type of consequence of the conduct that the actors intentionally carried out.206 But in this case, taking down the electrical grid and causing physical harm to human victims were assumed to be entirely unforeseeable even to a reasonable actor in the defendant’s shoes.
Criminal law has tools to deal with this kind of scenario, too. This comes in the form of so-called constructive liability crimes. These are crimes that consist of a base crime which require mens rea, but where there then is a further result element as to which no mens rea is required. Felony murder is a classic example.207 Suppose one breaks into a home one believes to be empty in order to steal artwork. Thus, one commits the base crime of burglary.208 However, suppose further that the home turns out not to be empty, and the burglar startles the homeowner who has a heart attack and dies. This could make the burglar guilty of felony murder.209 This is a constructive liability crime because the liability for murder is constructed out of the base offense (burglary) plus causing the death (even where this is unforeseeable). According to one prominent theory of constructive liability crimes, they are normatively justifiable when the base crime in question (burglary) typically carries at least the risk of the same general type of harm as the constructive liability element at issue (death).210
can be convicted of any offenses, if any, related to illicitly using the fireworks. But can they also be convicted of offenses related to the massive forest fire and destroyed homes? 206 See, e.g., MODEL PENAL CODE § 2.03 (AM. LAW INST. 1962) (characterizing
proximate or legal causation requirement using a “scope of the risk” test).
207 See WAYNE R. LAFAVE, 2 SUBSTANTIVE CRIMINAL LAW § 14.5 (3d ed.) (explaining felony murder as the doctrine that “one whose conduct brought about an unintended
death in the commission or attempted commission of a felony was guilty of murder”).
208 See MODEL PENAL CODE § 221.1 (defining burglary).
209 See LAFAVE, supra note 207, § 14.5.
210 See A. P. Simester, Is Strict Liability Always Wrong?, in APPRAISING STRICT LIABILITY
21, 45 (A. P. Simester ed., 2005) (arguing that constructive liability as to a result is justified when the result is risked by the base offense) (“Where the risk [of Y] is intrinsic [to D’s doing the base offense, X], there seems no difficulty about holding D responsible and culpable for Y.”). To the extent one has normative qualms about the inclusion of such strict liability elements, one could mitigate this worry by requiring the mens rea of negligence as to the further harm element — though that would prevent this kind of crime from being of any use when the further harm is unforeseeable, as it is stipulated to be in the cases in question here.
 
591
  2019] Punishing Artificial Intelligence 373
This tool, if extended to the AI case, provides a familiar way to hold the hackers criminally liable for unforeseeably taking down the electrical grid and causing physical harm to human victims.
It may be beneficial to create a new constructive liability crime that takes a criminal act like the attempt to steal currency using AI as the base offense, and then taking the further harm to the electrical grid, or other property or physical harm, as the constructive liability element, which requires no mens rea (not even negligence) in order to be guilty of the more serious crime. This constructive liability offense, in a slogan, could be called Causing Harm Through Criminal Uses of AI.
New crimes could be created to the extent there are not already existing crimes that fit this mold. Indeed, in the present example, one might think there are already some available constructive liability crimes. Perhaps felony murder fits the bill insofar as attempting to steal currency may be a felony, and the conduct subsequently caused fatalities. However, this tool would be of no avail in respect to the property damage caused. This is why a new crime like Causing Harm Through Criminal Uses of AI may be necessary. In any case, no AI criminal gap is present here because criminal law has familiar tools available for dealing with unforeseeable harms of this kind.
2. What the AI criminal gap is: irreducible criminal conduct by AI
Consider a case of irreducible AI crime inspired by RDS. Suppose an AI is designed to purchase class materials for incoming Harvard students, but, through being trained on data from online student discussions regarding engineering projects, the AI unforeseeably “learns” to purchase radioactive material on the dark web and has it shipped to student housing. Suppose the programmers of this “Harvard Automated Shopper” did nothing criminal in designing the system and in fact had entirely lawful aims. Nonetheless, despite the reasonable care taken by the programmers — and subsequent purchasers and users of the AI (i.e., Harvard) — the AI caused student deaths.
In this hypothetical, there are no upstream actors who could be held criminally liable. Innocent agency is blocked as a mode of liability because the programmers, users and developers of the AI did not have the intent or foresight that any prohibited or harmful results would ensue — as is required for innocent agency to be available.211 Moreover, in the case of RDS, if the risk of the AI purchasing the designer drugs was not reasonably foreseeable, then criminal negligence would also be blocked. Finally, constructive liability is not available in cases of this
211 See supra notes 192–194 and accompanying text.
 
592
  374 University of California, Davis [Vol. 53:323
sort because there is no “base crime” — no underlying culpable conduct by the programmers and users of the AI — out of which their liability for the unforeseeable harms the AI causes could be constructed.
One could imagine various attempts to extend existing criminal law tools to provide criminal liability for developers or users. Most obviously, new negligence crimes could be added for developers that make it a crime to develop systems that foreseeably could produce a risk of any serious harm or unlawful consequence, even if a specific risk was unforeseeable. The trouble is that this does not seem to amount to individually culpable conduct, particularly as all activities and technologies involve some risks of some harm. This expansion of criminal law would stifle innovation and beneficial commercial activities. Indeed, if there were such a crime, most of the early developers of the internet would likely be guilty of it.212
B. The Costs of Punishing AI
Earlier, we discussed some of the potential costs of AI punishment, including conceptual confusion, expressive costs, and spillover. Even aside from these, punishment of AI would entail serious practical challenges as well as substantial changes to criminal law. Begin with a practical challenge: the mens rea analysis.213 For individuals, the mens rea analysis is generally how culpability is assessed. Causing a given harm with a higher mens rea like intent is usually seen as more culpable than causing the same harm with a lower mens rea like recklessness or negligence.214 But how do we make sense of the question of mens rea for AI?
Part III considered this problem, and argued that for some AI, as for corporations, the mental state of an AI’s developer, owner, or user could be imputed under something like the respondeat superior doctrine. But for cases of Hard AI Crime that is not straightforwardly reduced to human conduct — particularly where the harm is unforeseeable to designers and there is no upstream human conduct that is seriously unreasonable to be found — nothing like respondeat superior would be appropriate. Some other approach to AI mens rea would be required.
212 For related reasons, we would reject proposals to impose strict criminal liability on developers of AI that autonomously causes harms. Strict liability crimes for designers amounts to punishing the innocent. See supra notes 134–135 and accompanying text.
213 See supra Part III.A (discussing the Eligibility Challenge).
214 E.g., Kenneth W. Simons, Should the Model Penal Code’s Mens Rea Provisions Be
Amended?, 1 OHIO ST. J. CRIM. L. 179, 195-96 (2003) (“The MPC views its four basic mental states or culpability terms as hierarchically ordered . . . .”).
 
593
  2019] Punishing Artificial Intelligence 375
A regime of strict liability offenses could be defined for AI crimes. However, this would require a legislative work-around so that AI are deemed capable of satisfying the voluntary act requirement, applicable to all crimes.215 This would require major revisions to the criminal law and a great deal of concerted legislative effort. It is far from an off-the- shelf solution. Alternately, a new legal fiction of AI mens rea, vaguely analogous to human mens rea, could be developed, but this too is not currently a workable solution. This approach could require expert testimony to enable courts to consider in detail how the relevant AI functioned to assess whether it was able to consider legally relevant values and interests but did not weight them sufficiently, and whether the program has the relevant behavioral dispositions associated with mens rea-like intention or knowledge. In Part III.A, we tentatively sketched several types of argument that courts might use to find various mental states to be present in an AI. However, much more theoretical and technical work is required and we do not regard this as a first best option.
Mens rea, and similar challenges related to the voluntary act requirement, are only some of the practical problems to be solved in order to make AI punishment workable. For instance, there may be enforcement problems with punishing an AI on a blockchain. Such AIs might be particularly difficult to effectively combat or deactivate.
Even assuming the practical issues are resolved, punishing AI would still require major changes to criminal law. Legal personality is necessary to charge and convict an AI of a crime, and conferring legal personhood on AIs would create a whole new mode of criminal liability, much the way that corporate criminal liability constitutes a new such mode beyond individual criminal liability.216 There are problems with implementing such a significant reform.
Over the years, there have been many proposals for extending some kind of legal personality to AI.217 Perhaps most famously, a 2017 report
215 See supra notes 137–142 and accompanying text.
216 See Thomas J. Bernard, The Historical Development of Corporate Criminal Liability,
22 CRIMINOLOGY 3, 3-4 (1984) (describing criminal law as having “always had as its primary concern the regulation of relationships between individual persons,” while for practical reasons the “legal fiction” of corporate personality — and later corporate crime — developed).
217 See, e.g., SAMIR CHOPRA & LAWRENCE F. WHITE, A LEGAL THEORY FOR AUTONOMOUS ARTIFICIAL AGENTS (2011) (arguing that AI could and should be given legal personality in the near future); Asaro, supra note 104, at 169-86 (proposing a Turing test to decide if an AI agent that caused harm is legally fit to stand trial for a criminal offense); Lawrence B. Solum, Legal Personhood for Artificial Intelligences, 70 N.C. L. REV. 1231
 
594
  376 University of California, Davis [Vol. 53:323
by the European Parliament called on the European Commission to create a legislative instrument to deal with “civil liability for damage caused by robots.”218 It further requested the Commission to consider “a specific legal status for robots,” and “possibly applying electronic personality” as one solution to tort liability.219 Even in such a speculative and tentative form this proposal proved highly controversial.220
Full-fledged legal personality for AIs equivalent to that afforded to natural persons, with all the legal rights that natural persons enjoy, would clearly be inappropriate. To take a banal example, allowing AI to vote would undermine democracy, given the ease with which anyone looking to determine the outcome of an election could create AIs to vote for a particular candidate.221 However, legal personality comes in many flavors, even for natural persons such as children who lack certain rights and obligations enjoyed by adults. Crucially, no artificial person enjoys all of the same rights and obligations as a natural person.222 The best- known class of artificial persons, corporations, have long enjoyed only a limited set of rights and obligations that allows them to sue and be sued, enter contracts, incur debt, own property, and be convicted of crimes.223 However, they do not receive protection under constitutional provisions, such as the Fourteenth Amendment’s Equal Protection
(1992); Amanda Wurah, We Hold These Truths to Be Self-Evident, That All Robots Are Created Equal, 22 J. FUTURE STUD. 61 (2017).
218 Report with Recommendations to the Commission on Civil Law Rules on Robotics, at 16 (Jan. 27, 2017), http://www.europarl.europa.eu/doceo/document/A-8-2017-0005_ EN.pdf.
219 See id. at 18.
220 For instance, more than 150 AI “experts” subsequently sent an open letter to the
European Commission warning that, “[f]rom an ethical and legal perspective, creating a legal personality for a robot is inappropriate whatever the legal status model.” Open Letter to the European Commission Artificial Intelligence and Robotics, ROBOTICS- OPENLETTER.EU, http://www.robotics-openletter.eu/ (last visited Oct. 13, 2019).
221 Indeed, even without the right to vote, AI may have been used to attempt to undermine democracy. Bots have been employed to influence election outcomes, inflate online follower counts, spread fake news, or intimidate users expressing particular opinions. See, e.g., Nicole M. Radziwill & Morgan C. Benton, Evaluating Quality of Chatbots and Intelligent Conversational Agents, (Apr. 15, 2017) (unpublished manuscript), https://arxiv.org/pdf/1704.04579.pdf. More generally, a lot of online content is generated by AI. See id.
222 See S. M. Solaiman, Legal Personality of Robots, Corporations, Idols and Chimpanzees: A Quest for Legitimacy, 25 ARTIFICIAL INTELLIGENCE & L. 155 (2017).
223 Elvia Arcelia Quintana Adriano, The Natural Person, Legal Entity or Juridical Person and Juridical Personality, 4 PENN. ST. J.L. & INT’L AFF. 363, 365 (2015). The first U.S. federal criminal conviction of a company was United States v. N.Y. Cent. & Hudson River R.R. Co., 212 U.S. 509 (1909).
 
595
  2019] Punishing Artificial Intelligence 377
Clause, and they cannot bear arms, run for or hold public office, marry, or enjoy other fundamental rights that natural persons do.224 Thus, granting legal personality to AI to allow it to be punished would not require AI to receive the rights afforded to natural persons, or even those afforded to corporations. AI legal personality could consist solely of obligations.
Even so, any sort of legal personhood for AIs would be a dramatic legal change that could prove problematic.225 As discussed earlier, providing legal personality to AI could result in increased anthropomorphisms.
life.”227
Strengthening questionable anthropomorphic tendencies regarding AI could also lead to more violent or destructive behavior directed at AI, such as vandalism or attacks.228 Further, punishing AI could also affect human well-being in less direct ways, such as by producing anxiety about one’s own status within society due to the perception that AIs are given a legal status on a par with human beings.
Finally, and perhaps most worryingly, conferring legal personality on AI may lead to rights creep, or the tendency for an increasing number of rights to arise over time.229 Even if AIs are given few or no rights initially when they are first granted legal personhood, they may gradually acquire rights as time progresses. Granting legal personhood to AI may thus be an important step down a slippery slope. In a 1933 Supreme
224 See U.S. CONST., amend. XIV, § 1, cl. 2; Nw. Nat’l Life Ins. Co. v. Riggs, 203 U.S. 243, 255 (1906) (“The liberty referred to in [the 14th] Amendment is the liberty of natural, not artificial, persons.”); see also Richard A. Epstein, Of Citizens and Persons: Reconstructing the Privileges or Immunities Clause of the Fourteenth Amendment, 1 N.Y.U. J.L. & LIBERTY 334, 341 (2005).
225 Cf. Hu, supra note 15, at 527-28 (discussing whether recognizing legal personhood for “smart robots” would be harmful, and addressing a number of Ryan Calo’s concerns about anthropomorphizing robots or other AI entities).
226 See Jakub Zlotowski et al., Anthropomorphism: Opportunities and Challenges in Human–Robot Interaction, INT’L J. SOC. ROBOTICS 347, 352 (2014).
227 Damiano & Dumouchel, supra note 114, at 4.
228 Cf. Diamantis, supra note 113, at 2078-80.
229 See David S. Law & Mila Versteeg, The Evolution and Ideology of Global
Constitutionalism, 99 CALIF. L. REV. 1163, 1170 (2011) (defining “rights creep”).
 226
People anthropomorphizing AI expect it to adhere
 to social norms and have higher expectations regarding AI
  capabilities.
This is problematic where such expectations are
 inaccurate and the AI is operating in a position of trust. Especially for vulnerable users, such anthropomorphisms could result in “cognitive and psychological damages to manipulability and reduced quality of
 These outcomes may be more likely if AI were held accountable
 by the state in ways normally reserved for human members of society.
 
596
  378 University of California, Davis [Vol. 53:323
Court opinion, for instance, Justice Brandeis warned about rights creep, and argued that granting corporations an excess of rights could allow them to dominate the State.230 Eighty years after that decision, Justice Brandeis’ concerns were prescient in light of recent Supreme Court jurisprudence such as Citizens United v. Federal Election Commission and Burwell v. Hobby Lobby Stores, which significantly expanded the rights extended to corporations.231 Such rights, for corporations and AI, can restrict valuable human activities and freedoms.
C. Second Alternative: Minimally Extending Criminal Law
There are alternatives to direct AI punishment besides doing nothing. The problem of Hard AI Crime would more reasonably be addressed through minimal extensions of existing criminal law. The most obvious would be to define new crimes for individuals. Just as the Computer Fraud and Abuse Act criminalizes gaining unauthorized access or information using personal computers,232 an AI Abuse Act could criminalize malicious or reckless uses of AI. In addition, such an Act might criminalize the failure to responsibly design, deploy, test, train, and monitor the AIs one contributed to developing. These new crimes would target individual conduct that is culpable along familiar dimensions, so they may be of limited utility with regard to Hard AI Crimes that do not reduce to culpable actors. Accordingly, a different way to expand the criminal law seems needed to address Hard AI Crime.
In cases of Hard AI Crime, a designated adjacent person could be punished who would not otherwise be directly criminally liable — what we call a Responsible Person. This could involve new forms of criminal negligence for failing to discharge statutory duties (perhaps relying on strict criminal liability) in order to make a person liable in cases of Hard AI Crime. It could be a requirement for anyone creating or using an AI
230 See Louis K. Liggett Co. v. Lee, 288 U.S. 517, 549 (1933) (Brandeis, J., dissenting).
231 See Citizens United v. Fed. Election Comm’n, 558 U.S. 310, 341 (2010) (curtailing government’s ability to restrict political speech by companies). Citizens United held that the free speech clause of the First Amendment prohibits the government from restricting independent expenditures for communications by companies. See id. at 341-43 (“
Burwell v. Hobby Lobby Stores, Inc., 573 U.S. 682 (2014) (recognizing a for-profit company’s claim to religious belief).
232 See 18 U.S.C. § 1030(a) (2019).
  The Court has recognized that First Amendment
  protection extends to corporations
....
The Court has thus rejected the argument that
 political speech of corporations or other associations should be treated differently under the First Amendment simply because such associations are not ‘natural persons.’”);

597
  2019] Punishing Artificial Intelligence 379
to ex ante register a Responsible Person for the AI.233 It could be a crime to design or operate AI capable of causing harm without designating a Responsible Person.234 This would be akin to the offense of driving without a license.235 The registration system might be maintained by a federal agency. However, a registration scheme is problematic because it is difficult to distinguish between AI capable of criminal activity and AI not capable of criminal activity, especially when dealing with unforeseeable criminal activity. Even simple and innocuous seeming AI could end up causing serious harm. Thus, it might be necessary to designate a Responsible Person for any AI. Registration might involve substantial administrative burden and, given the increasing prevalence of AI, the costs associated with mandatory registration might outweigh any benefits.
A default rule rather than a registration system might be preferable. The Responsible Person could be the AI’s manufacturer or supplier if it is a commercial product. If it is not a commercial product, the Responsible Person could be the AI’s owner, developer if no owner exists, or user if no developer can be identified. Even non-commercial AIs are usually owned as property, although that may not always be the case, for instance, with some open source software. Similarly, all AIs have human developers, and in the event an AI autonomously creates another AI, responsibility for the criminal acts of an AI-created AI could reach back to the original AI’s owner. In the event an AI’s developer cannot be identified, or potentially if there are a large number of developers, again in the case of some open source software, responsibility could attach to an AI’s user. However, this would fail to catch the rare, perhaps only hypothetical, case of the non-commercial AI with no owner, no identifiable developer, and no user. To the extent that a non-commercial AI owner, developer, and user working together
233 A new criminal offense — akin to driving without a license — could be imposed for cases where programmers, developers, owners or users have unreasonably failed to designate a Responsible Person for an AI.
234 The Responsible Person should also be liable for harms caused by an AI where the AI, if a natural person, would be criminally liable together with another individual. Otherwise, there is a risk that sophisticated AI developers could create machines that cause harm but rely on co-conspirators to escape liability.
235 There is precedent for such a Responsible Person registration scheme. In the corporate context, executives may be required to attest to the validity of some SEC filings and held strictly liable for false statements even where they have done nothing directly negligent. If the Responsible Person is a person at a company where a company owns the AI, it would have to be an executive to avoid the problem of setting up a low- level employee as “fall guy.” The SEC for this reason requires a C-level executive to attest to certain statements on filings.
 
598
  380 University of California, Davis [Vol. 53:323
would prefer a different responsibility arrangement, they might be permitted to agree to a different ex ante selection of the Responsible Person.236 That might be more likely to occur with sophisticated parties where there is a greater risk of Hard AI Crime. The Responsible Person could even be an artificial person such as a corporation.237
It would be possible to impose criminal liability on the Responsible Person directly in the event of Hard AI Crime. For example, if new statutory duties of supervision and care were defined regarding the AI for which the Responsible Person is answerable, criminal negligence liability could be imposed on the Responsible Person should he or she unreasonably fail to discharge those duties. Granted, this would not be punishment for the harmful conduct of the AI itself. Rather, it would be a form of direct criminal liability imposed on the Responsible Person for his or her own conduct.
More boldly, if this does not go far enough to address Hard AI Crime, criminal liability could also be imposed on the Responsible Person on a strict liability basis — particularly if the relevant punishments are only fines rather than incarceration. Generally, strict liability crimes are restricted to minor infractions or regulatory offenses or “violations,”238 though some examples of more serious strict criminal liability can also be found (such as statutory rape in some jurisdictions).239 This could be defended by claiming that there is a special duty owed to society at large to provide special assurances that certain especially serious risks will be mitigated as much as possible.240 A Responsible Person accepting
236 It might also be likely that parties with more negotiating power would attempt to offload their liability. For instance, AI suppliers might attempt to shift liability to consumers. At least in the case of commercial products, it should not be possible for suppliers to do this.
237 This raises potential concerns about corporations with minimal capital being used to avoid liability. However, this same concern exists now with human activities, where thinly capitalized corporations are exploited as a way to limit the liability of individuals. Still, there are familiar legal tools to block this sort of illicit liability avoidance. To the extent a bad actor is abusing the corporate form, courts can, for instance, pierce the corporate veil.
238 See MODEL PENAL CODE § 2.05(1) (AM. LAW INST. 1962).
239 See, e.g., N.Y. PENAL LAW §§ 130.25-50 (2001) (defining statutory rape offenses);
Funari v. City of Decatur, 563 So. 2d 54, 55 (Ala. Crim. App. 1990) (holding that an Alabama “statute which prohibits the selling of alcohol to minors does not contain any language requiring knowledge or intent,” and “the very purpose of the statute clearly indicates a legislative intent to impose strict liability”).
240 Cf. DUFF, ANSWERING, supra note 91, at 170 (suggesting in the mala prohibitum context that “we owe it to each other not merely to ensure that we act safely, but to assure each other that we are doing so, in a social world in which we lack the personal knowledge of others that could give us that assurance”).
 
599
  2019] Punishing Artificial Intelligence 381
strict criminal liability could serve this function. Especially in the case of AI where user trust is critical to realizing the benefits of AI, this approach could be warranted to combat the perception that unsafe AI is being employed. Accordingly, AI could become another context in which strict criminal liability on the Responsible Person is imposed.
Yet we have serious reservations about strict liability crimes applied to persons.241 If justifiable at all, they can only be justifiably used as a last resort in exigent circumstances — as in cases of unusually dangerous activities. However, it is not obvious that the use of AI qualifies as unusually dangerous. To the contrary, in many areas of activity it would be unreasonable not to use AI, as when safety can be improved over human actors such as may soon be the case with self- driving cars.242 Most bad human actors using AI systems to commit crimes will still be caught under existing criminal laws, and so far there have not been high-profile cases of Hard AI Crimes. As a result, we are not yet convinced that Hard AI Crime is a significant enough social problem to merit the use of strict criminal liability.
At the end of the day, a Responsible Person regime accompanied by new statutory duties, which carry criminal penalties if these duties are negligently or recklessly breached, provides an attractive approach to dealing with Hard AI Crime. While it is only a minimal expansion of criminal law, by expressing condemnation through a criminal conviction of the Responsible Person, much of the expressive benefit from a direct conviction AI can be achieved — but without as serious a loss of public trust as the legal fictions needed to punish AI directly could create.
D. Third Alternative: Moderate Changes to Civil Liability
A further alternative to dealing with Hard AI Crime is to look to the civil law, primarily tort law, as a method of both imposing legal accountability and deterring harmful AI. Some AI crime will no doubt already result in civil liability, however, if existing civil liability falls short, new liability rules could be introduced. A civil liability approach could even be used in conjunction with expansions to criminal liability.
241 See Kenneth W. Simons, When Is Strict Criminal Liability Just?, 87 J. CRIM. L. & CRIMINOLOGY 1075, 1075-76 (1997) (discussing retributive views that denounce strict liability) (“Strict liability appears to be a straightforward case of punishing the blameless, an approach that might have consequential benefits but is unfair on any retrospective theory of just deserts.”).
242 See, e.g., Abbott, The Reasonable Computer, supra note 49.
 
600
  382 University of California, Davis [Vol. 53:323
While it is beyond the scope of this Article to canvas gaps in civil liability for AI crime, it is worth noting that existing civil liability frameworks come with built-in limitations. Very few laws specifically address AI-generated harms, which means civil liability must usually be established under a traditional negligence or product liability framework or under contractual liability.243 Negligence generally requires a person to act carelessly, so where this cannot be established there may be no recovery. Product liability may require both that an AI is a commercial product (e.g., this may not apply where AI is just software or the use of AI is a “service”), and that there be a defect in the product (or that its properties are falsely represented).244 In the case of complex AI, it may be difficult to prove a defect, and AI may cause harm without a “defect” in the product liability sense. For these reasons, the European Commission has created Expert Groups to determine whether new technologies necessitate a revision of the Product Liability Directive, which harmonizes product liability across the European Union, and whether even more ambitious changes are needed.245 Civil liability may also derive from contractual relationships, but this usually only applies where there is privity of contract between parties, and it may also have significant limitations.246
To the extent there is inadequate civil liability for Hard AI Crimes, the Responsible Person proposal sketched above could be repurposed so that the Responsible Person might only be civilly liable. The case against a Responsible Person could be akin to a tort action if brought by an individual or a class of plaintiffs, or a civil enforcement action if brought by a government agency tasked with regulating AI. At trial, an AI would not be treated like a corporation, where the corporation itself is held to have done the harmful act and the law treats the company as a singular acting and “thinking” entity. Rather, the question for adjudication would be whether the Responsible Person discharged his or her duties of care in respect of the AI in a reasonable way — or else civil liability could also be imposed on a strict liability basis (a less troubling prospect than it is within criminal law).
A Responsible Person scheme is not the only solution to inadequate civil liability for Hard AI Crimes. An insurance scheme is another
243 See, e.g., id.
244 See, e.g., id.
245 See, e.g., Register of Commission Expert Groups and Other Similar Entities: Group
Details - Commission Expert Group, EUROPEAN COMMISSION (Jun. 17, 2019), http://ec.europa.eu/transparency/regexpert/index.cfm?do=groupDetail.groupDetail& groupID=3592.
246 See Abbott, The Reasonable Computer, supra note 49, at 15-16.
 
601
  2019] Punishing Artificial Intelligence 383
approach.247 Owners, developers, or users of AI, or just certain types of AI, could pay a tax into a fund to ensure adequate compensation for victims of Hard AI Crime. The cost of this tax would be relatively minor compared to the financial benefits of AI. This could either replace the Responsible Person solution or apply to cases where no appropriate Responsible Person exists. An AI compensation fund could operate like the National Vaccine Injury Compensation Program (“VICP”).248 Vaccines create widespread social benefits but are known in rare cases to cause serious medical problems. VICP is a no-fault alternative to traditional tort liability that compensates individuals injured by a VICP- covered vaccine. It is funded by a tax on vaccines that is paid by users.249 Other models for insurance schemes exist, such as the Price Anderson Act for nuclear power, which establishes a pool of funds to compensate victims in the event of a nuclear incident through a chain of indemnity regardless of who was ultimately at fault. 250
E. Concluding Thoughts
This Article has argued that, confronted with the growing possibility of Hard AI Crime, we should not overreact and reach for the radical tool of punishing AI. Alternative approaches could provide substantially similar benefits and would avoid many of the pitfalls and difficulties involved in punishing AI. A natural alternative, we argued, involves modest expansions to criminal law, including, most importantly, new negligence crimes centered around the improper design, operation, and testing of AI applications as well as possible criminal penalties for designated parties who fail to discharge statutory duties. Expanded civil liability could supplement this framework.
We took a careful look at how a criminal law regime that punished AI might be constructed and defended. In so doing, we showed that it is all too easy to underestimate the ability of criminal law theory to accommodate substantial reforms. We explored the ways in which
247 Indeed, New Zealand has replaced tort law with a publicly funded insurance scheme to compensate victims of accidents. See, e.g., Peter H. Schuck, Tort Reform, Kiwi- Style, 27 YALE L. & POL’Y REV. 187, 187-90 (observing that New Zealand “abolished the most important areas of tort law more than three decades ago” in favor of an insurance scheme that awards compensation to victims on a no-fault basis).
248 See National Vaccine Injury Compensation Program, HEALTH RESOURCES & SERV. ADMIN., https://www.hrsa.gov/vaccine-compensation/index.html (last visited Oct. 13, 2019).
249 See id.
250 See The Price-Anderson Act, BACKGROUND INFO. (Cent. For Nuclear Sci. & Tech.
Info., La Grange Park, Ill.), Nov. 2005.
 
602
  384 University of California, Davis [Vol. 53:323
criminal law can — and, where corporations are involved, already does — appeal to elaborate legal fictions to provide a basis within the defensible boundaries of criminal law theory for punishing some artificial entities. We showed what a system of punishment for AI might look like and showed how some hasty arguments against it can be answered.
The use of legal fictions to solve difficult conceptual questions or practical problems — such as how to conceptualize or prove particular sorts of mental elements for AI or misbehavior by its developers — gives criminal law theory impressive plasticity. Legal fictions help turn the criminal law into a pragmatic tool for solving social problems. Nonetheless, legal fictions must be used with caution, as their overuse risks eroding public trust and weakening the rule of law. Moreover, allowing legal fictions to proliferate unchecked can lead to widespread injustice either through punishing the innocent or by punishing more harshly than one’s culpability calls for. While some legal fictions can be justified,251 they must be used judiciously. For this reason, there is and should be an onerous burden to meet before we can be confident that a particular legal fiction — such as legal personality for AI or the invention of culpable mental states for AI — is adopted. Embracing legal fiction without meeting this justificatory burden would be tantamount to believing in science fiction.
 251 See SARCH, supra note 164, at 141 (defending certain restricted uses of particular legal fictions based on culpably preserving ignorance).

603
Conclusion
 Submission Themes
My thesis has investigated how the law should regulate human-like activity by AI, and how the phenomenon of AI behaving like a person should change how people are regulated. It has argued that the default position of the law should be not to treat behavior by people and AI differently when they are engaged in the same activities. It has explained in depth how a principle of AI legal neutrality could be applied in tax, tort, intellectual property, and criminal law to help these various areas of the law to better achieve their underlying normative goals, and it has argued that not discriminating between AI and human behavior will tend to promote efficiency. Different areas of the law do have significantly different applications of AI legal neutrality—for instance AI should not be directly liable for personal injuries, should not directly own patents, and should not be held directly criminally liable. In the concluding chapter of my monograph, I considered what I believe are the most frequent or strongest criticisms of this line of reasoning. In addition, my research has not considered most areas of the law, and it has not exhaustively considered all of the challenges posed by AI within the four legal areas I have considered. Significantly more work is required to consider the application of AI legal neutrality in different contexts—and in different jurisdictions.
In the course of writing this thesis, I have grappled with precisely what technologies I am concerned with, and thus what terminology to use. For example, I used the terms “autonomous machine” and “creative computers” in earlier works. Those terms worked, and the concept of autonomy does play a central role in my research - but it has become clear to me that “artificial intelligence,” according to its original definition, is the phenomenon that is most relevant to my work. I have focused on the ways in which machines or algorithms are stepping into the shoes of people and behaving in human like ways. That is really the concept at the heart of
 
604
artificial intelligence. It has to do with creating something that will behave like a person – not something that is like a person. When you have something that behaves like a person but is not like a person, it tests laws designed to regulate standards of human behavior. My research has thus considered how is it that something that acts like, but is not, a person doing human like things will impact the law, and how will that something be impacted itself by the law. At least in terms of social consequentialist outcomes, these questions are less a matter of how the law will impact AI per se than how the law will influence the people behind the AI who are making, using, and developing it. The law should act to encourage the development of AI in socially beneficial ways – whether that has to do with distribution of income, improving safety, encouraging innovation, or preventing crime.
There are many people now working in the emerging field of AI law, or more generally in law and technology. There are also a number of scholars working in more traditional fields such as torts, criminal law, and tax law that have considered the impact of AI on their fields. AI legal research has not been limited to academia, there has been research by think tanks, industry, and governments on the many of the challenges posed by AI such as those related to algorithmic bias, data ownership, and privacy.1 In turn, some of these efforts have resulted in best principles for AI regulation, such as accountability, transparency, and human agency. My work, while not minimizing the importance of such principles, has been the first to argue in favor of legally neutral treatment of AI and people when they are engaged in the same activities. I argue this will tend to produce improved outcomes that will ultimately benefit human wellbeing.
That is not to say there should be legally identical treatment of people and AI – there are some respects in which AI and people will clearly need to be treated differently to improve social
 1
 Calo, Ryan, Artificial Intelligence Policy: A Primer and Roadmap (August 8, 2017). Available at
 http://dx.doi.org/10.2139/ssrn.3015350
.
 
605
outcomes. AI should not have rights such as the right to own property or the right to vote. There are some artificial persons like corporations that do have some rights, such as the right to own property, but these rights are given to corporations because it is thought to be the best means of encouraging certain sorts of socially beneficial human activities like entrepreneurship. Corporations do not have rights because they are directly morally deserving of rights. Some rights we grant because they have consequentialist benefits, and others we grant for deontological reasons about how people ought to be treated. There may be times where giving AI rights does produce social benefits, perhaps acknowledging an AI as an inventor. But we would not grant these rights to AI for the AI’s sake, in the case of inventorship it would be because this maintains the value and meaning of human inventorship and accurately informs the public about the actual devising of an invention.
In the concluding chapter of the book included with this thesis, I discuss some of the challenges and objections to AI legal neutrality. While AI legal neutrality is not something that is always applied in a simple or straightforward manner – it does not, say, involve giving robots personhood and treating them exactly like a person – complexities in application is common with any principle of regulation. Privacy is socially valuable but may come at the cost of improved national security. Transparency is socially valuable but may come at the cost of privacy, or allowing companies to protect their confidential information. So, like other regulatory principles, AI legal neutrality is a guiding concept that can be used when thinking about how to regulate AI in new areas.
Areas for further research
I plan to broadly continue this research agenda along two dimensions. First, I plan to broaden my research agenda by consider the implications of AI legal neutrality in other areas of the law. For example, I intend to consider how AI legal neutrality applies with respect to smart
 
606
contracts—something neither smart nor a contract, but self-executing code in a distributed ledger. Smart contracts promise to eliminate much of the need for trust in contracts and for intermediaries, but aside from challenges related to enforcement and multi-jurisdictional issues – I think that the incorporation of AI into contracts will produce new sorts of both harms and benefits. I also think that AI has an important role also to play in dispute resolution generally and specifically with respect to smart contracts, as it is a natural fit for parties looking to circumvent traditional legal frameworks and to avoid third-party human interference. However, the role of AI in dispute resolution presents numerous challenges – AI has the potential to have new sorts of algorithmic bias as well as to reduce or eliminate traditional sorts of human biases. As another example, I plan to consider AI legal neutrality with respect to civil procedure and evidence. Evidence from people has complex rules about its admissibility, but such rules may not function effectively where evidence is gathered and submitted by an AI or with the assistance of an AI. AI may also become a new challenging phenomenon in the context of “deepfakes” and the need to use AI to determine the authenticity of evidence or of human testimony.
As a second dimension for continuing my research agenda, I plan to deepen my research agenda by considering additional and further implications of AI legal neutrality in areas I have already considered. For example, Professor Sarch and I considered whether criminal punishment of AI is consistent with the doctrinal and theoretical limitations of criminal law. In doing so, we only dealt with some of the implications of this analysis. As I considered further in the introduction of my monograph, AI punishment sheds light on how and why we punish people. Culpability and retribution – someone’s moral blameworthiness and giving them what they deserve independently of any consequentialist benefits that may flow from that – are critical to understanding criminal law. But, machines which behave in predetermined ways, even very complex predetermined ways resulting from machine learning techniques, cannot reasonably

607
be said to be morally blameworthy. Yet, they engage in the same sorts of antisocial behaviors that criminal law is designed to respond to. If criminal punishment of AI actually did result in a number of socially positive outcomes, it is not clear that the retributivist limitations on criminal law would prevent AI punishment. This may suggest that legal culpability for people is more functional and broader than culpability in a moral sense – which suggests someone’s intent in engaging in antisocial behavior may not need to occupy such a central position in criminal law. That helps to defend criminal law against attacks by hard determinists who argue people are not morally blameworthy for antisocial behavior because they lack free will and the ability to make truly independent decisions. These arguments may not matter, or matter so much.
This same analysis may also help to justify corporate punishment. Corporate punishment is criticized for harming innocent shareholders when it may be individual corporate agents that should be punished. Rather than relying on respondeat superior, corporations may be better thought of as being legally culpable in their own rights because their systems and processes functionally generate antisocial behavior.
Tax, tort, and intellectual property law all have challenges posed by AI that I have yet to fully address. Work I am doing now is proceeding along this dimension, for instance, I am leading a group of colleagues in submitting a test case to the US Patent and Trademark Office, European Patent Office, United Kingdom Intellectual Property Office, and other patent offices for two inventions autonomously generated by a machine. The outcome of this submission should result in greater certainty for IP stakeholders and hopefully legal precedent or legislative changes which will result in fairer and more efficient patents. It has already attracted a significant amount of attention internationally from policymakers, academics, and industry since the story was announced by the Financial Times in August 2019. I anticipate this project

608
will involve one or more related law review articles, and potentially a monograph that will consider the impact of AI on IP standards broadly. It could also involve an edited volume on the topic of AI and IP. In addition, I am planning a case study of an AI-generated work created by a large technology company.
AI has other IP implications, such as it functionally infringing on third-party IP in human like ways - AI may engage in text-and-data mining activities on copyrighted databases, create derivative works through relying on machine learning, and so forth. This phenomenon does not naturally fit into rules for whether and when an individual infringes, because an individual may not know, or may not even reasonably expect, an AI will engage in infringing activity. Where an AI is not functioning as a tool in a foreseeable fashion, theories of indirect liability may be a grounds for finding infringement, such as vicarious infringement, contributory infringement, or infringement by inducement – however this requires more doctrinal and theoretical work. Similarly, different jurisdictions have exceptions to infringement – such as fair use defences to copyright infringement in the US, or fair dealing defences to copyright infringement in the UK – with uncertain application to AI-based activities. AI also challenges other areas in patent law, such as requirements for enablement and disclosure. It is not clear that many of the patents on “computer-implemented inventions” that rely on AI are able to adequately enable and disclose without also providing detailed source code and training data sets that are typically not provided in applications. AI has so much disruptive potential, and my work so far has only scratched the surface.
