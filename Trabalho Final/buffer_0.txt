1
The Reasonable Robot: Artificial Intelligence and the Law
by
Professor Ryan Abbott
Submitted for the Degree of Doctor of Philosophy
School of Law
Faculty of Arts and Social Sciences University of Surrey
Supervisors:
Dr Arman Sarvarian Dr Christopher Taggart
©Ryan Abbott, 2020
 
2
Acknowledgements
Thanks to my principal supervisor Dr Arman Sarvarian for his support and guidance, including in navigating a higher educational institution in England as well as a course-of-study novel to the law school in institutional memory.
Thanks to my secondary supervisor, Dr Christopher Taggart, for his generous availability, constant positivity, and insightful comments.
Thanks to my former head of school Professor Veronica Rodriguez-Blanco, and to my current head of school Professor Alexander Sarch, for creating an environment conducive to completing this thesis.
Thanks to Charlotte, Ryan, and Theodora for substantially increasing the difficulty level of this thesis in the most adorable way possible.
Finally, thanks most of all to my wife—for tolerating yet one more degree.
 
3
Statement of Originality
 I confirm that the submitted work is my own work and that I have clearly identified and fully acknowledged all material that is entitled to be attributed to others (whether published or unpublished) using the referencing system set out in the programme handbook. I agree that the University may submit my work as a means of checking this. I confirm that I understand that assessed work that has been shown to have been plagiarised will be penalised.
Guildford, 24 Jan 2020
Ryan Abbott
 
4
Abstract
   This thesis argues in favor of a novel principle for AI regulation: AI legal neutrality. Namely, that the law should not discriminate between activity by people and activity by AI when people and AI are performing the same tasks. This will reduce market distortions and help to ensure that decisions are made on the basis of efficiency. Efficiency is not the only principle that should guide AI regulation, but not discriminating between people and AI will tend to improve human well-being.
We do not currently have a neutral legal system as between human and AI activity. An AI that is significantly safer than a person may be the best choice for driving a vehicle, but existing laws may prohibit driverless vehicles. A person may be a better choice for packing boxes at a warehouse, but a business may automate because AI receives preferential tax treatment. AI may be better at generating certain types of innovation, but businesses may not want to use AI if this restricts future intellectual property rights. In all of these instances, neutral legal treatment would ultimately benefit society as a whole.
As AI increasingly steps into someone’s shoes, it will need to be treated more like a person. Sometimes more importantly, as AI is incorporated into our legal standards, people will need to be treated more like AI. In many areas of the law, standards are set by human behavior—the degree of care expected of a driver, the quantum of creativity required to protect a new invention, even the state of mind required for criminal punishment. As AI takes people out-of- the-loop and comes to be the normal way that tasks are performed, the AI’s behavior should set our benchmarks. AI is not part of our moral community, but it should be part of our legal community.
      
5
Table of Contents
VOL I
I. Introduction...................................................................................... 6
A. Research Context ....................................................................... 6
B. Literature Review ...................................................................... 7
C. My Research ........................................................................... 12
D. Research Question and Thesis Statement .......................................... 18
E. Impact .................................................................................. 20
F. Methodology .......................................................................... 23
II. Monograph...................................................................................... 25
A. The Reasonable Robot: Artificial Intelligence and the Law..................... 25
1. Introduction: Artificial Intelligence and the Law........................ 32
2. Understanding Artificial Intelligence...................................... 69
3. Should Artificial Intelligence Pay Taxes? .............................. 103
4. The Reasonable Robot..................................................... 133
5. Artificial Inventors.......................................................... 172
6. Everything is Obvious...................................................... 219
7. Punishing Artificial Intelligence.......................................... 255
8. Alternative Perspectives on AI Legal Neutrality....................... 308
VOL II
III. Articles......................................................................................... 324
A. I Think, Therefore I Invent: Creative Computers and the Future of Patent Law............................................................................................. 324
B. Hal the Inventor: Big Data and its Use by Artificial Intelligence............. 372
C. Inventive Machines: Rethinking Invention and Patentability.................. 391
D. Artificial Intelligence, Big Data and Intellectual Property: Protecting
Computer-Generated Works in the United Kingdom.................................... 398
E. Everything is Obvious............................................................... 414
F. Should Robots Pay Taxes? Tax Policy in the Age of Automation............ 465
G. The Reasonable Computer: Disrupting the Paradigm of Tort
Liability........................................................................................ 496
H. Punishing Artificial Intelligence: Legal Fiction or Science Fiction........... 541
IV. Conclusion..................................................................................... 603
A. Submission Themes.................................................................. 603
B. Areas for further research............................................................ 605
   
6
Research Context
Introduction
  Artificial intelligence (AI) is experiencing a period of unparalleled growth driven by improvements in software, computing power and big data.1 This has already had a dramatic economic and social impact; for instance, on the financial services sector and on the way that consumers interact with online content in the digital economy.2 AI now promises to radically transform industries such as health care and transportation.3 Studies on AI’s economic impact by McKinsey and PricewaterhouseCoopers suggest that AI is likely to generate trillions of pounds in value annually by 2030.4
The extent to which AI can provide these benefits depends in large part on user trust and public perceptions of AI.5 This in turn requires adequate resolution of a host of legal and ethical challenges, including those relating to consumer privacy, cybersecurity, biased data, technological unemployment, and the use of force.6 The European Parliament has recently launched a series of expert groups that have proposed making Trustworthy AI the foundation of regional legislative activity.7 Similarly, the UK Government has released an ambitious industrial strategy that seeks to make the UK the world-center for Ethical AI.8 Yet the legal
1 This thesis uses the following definition of AI: “an algorithm or machine capable of
completing tasks that would otherwise require cognition”. For a discussion of this definition and other definitions of AI, see THE REASONABLE ROBOT: ARTIFICIAL INTELLIGENCE AND THE LAW, (hereinafter “REASONABLE ROBOT”), Chapter 2, Section 2. For a discussion of the relevance of software, computing power, and big data, see REASONABLE ROBOT, Chapter 2, Section 5.
2 Id. Chapter 2, Section 5.
3 Id.
4 Id.
5 European Commission Press Release IP/18/3362, Artificial Intelligence: Commission Outlines a European Approach to Boost Investment and set Ethical Guidelines (Apr. 25, 2018), https://europa.eu/rapid/press- release_IP-18-3362_en.htm.
6 See REASONABLE ROBOT, Chapter 7.
7 European Commission Press Release IP/18/3362, Artificial Intelligence: Commission Outlines a European Approach to Boost Investment and set Ethical Guidelines (Apr. 25, 2018), https://europa.eu/rapid/press- release_IP-18-3362_en.htm.
8 HM GOVERNMENT, INDUSTRIAL STRATEGY: BUILDING A BRITAIN FIT FOR THE FUTURE, 40 (2017) (“We will lead the world in safe and ethical use of data and artificial intelligence giving confidence and clarity to citizens and business”).
 
7
framework governing AI, and for that matter new technologies generally, has historically been slow to develop due to concerns that an overly burdensome regulatory environment would slow innovation.9
Literature Review
I broadly divide the literature relevant to this thesis into two categories. The first category is literature relevant to AI within a specific legal discipline - for example, literature on how AI will impact tort or tax law. This literature is reviewed in detail within my individual publications following this introduction and to a lesser extent in the corresponding book chapters. For instance, as discussed in greater detail in my article, “I Think, Therefore I Invent”, people have been writing about the phenomenon of AI-generated works10 since at least the 1960s.11 There has been a variable but steady stream of materials published since then as advances in AI, or other technological advances in areas such as software, challenged intellectual property (IP) standards more generally and reintroduced the concept of AI- generated works to academic discourse.12 Established narratives prior to my work largely denied the desirability of attributing creative acts to AI,13 and argued it was undesirable to provide IP protection for such advances.14 With respect to the skilled person standard used to evaluate inventive step,15 existing scholarship has largely considered the impact of AI from the perspective of a tool augmenting human inventors, rather than as an entity automating the
9 REASONABLE ROBOT, Chapter 1, Section 1.
10 REASONABLE ROBOT, Chapter 1 (defining an “AI-generated work”).
11 Karl F. Milde, Jr., Can a Computer Be an “Author” or an “Inventor”? 51 J. PAT. OFF. SOC’Y 378, 379 (1969). 12 Id, 1080.
13 Pamela Samuelson, arguing against considering computers to be authors, argues that, “[o]nly those stuck in the doctrinal mud could even think that computers could be ‘authors.’” Pamela Samuelson, Allocating Ownership Rights in Computer-Generated Works, 47 U. PITT. L. REV. 1185, 1200 (1986).
14 Ralph D. Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up? 71 TUL. L. REV. 1675, 1681, 1702–03 (1997) (arguing the output of creative computers cannot and should not be protected by federal intellectual property laws and that such results enter the public domain).
15 See REASONABLE ROBOT, Chapter 5 (discussing the skilled person standard,).
  
8
inventive act itself.16 My individual chapters following this introduction contain discussions of the main findings of existing scholarship and highlight when and how my work diverges from established doctrine.
The second category relates more broadly to regulation of technology. I would further divide this into literature about 1) principles of AI regulation and 2) regulation of technology generally. In terms of AI regulatory principles, the past few years have witnessed a flurry of activity, with at least dozens of principles having been developed, published, and endorsed by governments and high-profile organizations such as OECD17 and the G20.18 These principles in turn derive from a combination of general regulatory principles and focus on some of the unique challenges posed by AI. For instance, there are particular concerns about allowing AI to make life or death decisions in military contexts without human supervision, or with minimal human supervision.19 This has even resulted in a social movement and campaigning against killer robots. This concern in turn is reflected in many of the principles of AI regulation such as the OECD’s principles for accountability and requirements to respect the rule of law. To my knowledge, none of these principles have included a principle like AI legal neutrality - the principle that the law should not discriminate between people and AI when they are engaged in the same activities. To the contrary, other principles of AI regulation tend to focus on approaches such as maintaining the supremacy of human agency and ensuring that people are kept “in-the-loop” or “on-the-loop” with AI. While I believe that approach is and will be proper in many circumstances, I do not agree that it is a universally beneficial principle. There will be
16 See REASONABLE ROBOT, Chapter 5.
17 OECD Principles on AI (2019), https://www.oecd.org/going-digital/ai/principles/.
18 G20 Ministerial Statement on Trade and Digital Economy (2019), https://www.mofa.go.jp/files/000486596.pdf.
19 See, e.g., Open Letter to the Prime Minister of Canada, Call for an International Ban on the Weaponization of Artificial Intelligence (2017), https://techlaw.uottawa.ca/bankillerai#letter.
 
9
times where people underperform compared to AI, and where having a person in- or on-the- loop will produce negative outcomes.
With respect to more general principles of regulation, my thesis shares the most similarity with a principle of technology neutrality – the idea that laws should regulate behaviour rather than technology.20 This is a widely adopted policy by academics and lawmakers,21 albeit with variable application.22 It has been perhaps most prominently applied in the context of IP law, for example, with the 1976 Copyright Act in the U.S. which attempted to make copyright law neutral in the face of disruptive technologies such as cable transmission.23 Like AI legal neutrality, technologically neutral laws are more concerned with behaviour itself rather than how that behaviour occurs. This is done to promote the longevity of regulations – so that laws are more adaptable to technological advances.24 It also promotes fairness by reducing discrimination against a given technology by virtue of some factor other than how it behaves.25 In the 1976 Copyright Act, a work is protected by copyright so long as it is fixed in any technology someone can perceive regardless of whether Congress mentioned it specifically.26 Both AI legal and technological neutrality share the aims of simplifying and future-proofing
20 See Bert-Japp Koops, Should ICT Regulation Be Technology-Neutral? in STARTING POINTS FOR ICT REGULATION: DECONSTRUCTING PREVALENT POLICY ONE-LINERS 77 (Bert-Jaap Koops, Miriam Lips, Corien Prins & Maurice Schellekens eds., 2006).
21 See, e.g., Carys J. Craig, Technological Neutrality: (Pre)Serving the Purposes of Copyright Law, in THE COPYRIGHT PENTALOGY: HOW THE SUPREME COURT OF CANADA SHOOK THE FOUNDATIONS OF CANADIAN COPYRIGHT 271, 272-73 (Geist ed., 2013) (analying how technological neutrality has been favored under Canadian law in the copyright context); OECD, Council Recommendation on Principles for Internet Policy Making (2011), https://www.oecd.org/sti/ieconomy/49258588.pdf (noting technological neutrality is a key feature of Internet regulation).
22 Winston Maxwell and Marc Bourreau, Technology Neutrality in Internet, Telecoms and Data Protection Regulation 1 C.T.L.R. 1 (2015).
23 Pub. L. No. 94-553, 90 Stat. 2541 (1976); Melville B. Nimmer and David Nimmer, NIMMER ON COPYRIGHT § 12A.16(b) (1963) (arguing technology neutrality is a “unifying theme” of the 1976 Copyright Act).
24 Bert-Japp Koops, Should ICT Regulation Be Technology-Neutral? in STARTING POINTS FOR ICT REGULATION: DECONSTRUCTING PREVALENT POLICY ONE-LINERS 77 (Bert-Jaap Koops, Miriam Lips, Corien Prins & Maurice Schellekens eds., 2006).
25 Paul Ohm, The Argument Against Technology-Neutral Surveillance Laws, 88 TEX. L. REV. 1685, 1691–2 (2010).
26 17 U.S.C. § 102(a).
 
10
the law and reducing inefficient legal distinctions, and the body of literature related to technological neutrality has relevance to AI legal neutrality.27
I address some of the criticisms levied at technological neutrality in the context of AI legal neutrality in final chapter of my book, such as technology not being inherently neutral but rather reflecting particular political, social, or historical values.28 Technological neutrality differs from AI legal neutrality, among other reasons, because technological neutrality is about treating different types of technologies the same way as between one another, whereas AI legal neutrality focuses on comparing people to technology. For instance, technological neutrality might be concerned with one medium of broadcasting which is more efficient than a pre- existing standard, whereas AI legal neutrality might be concerned with comparing a person’s behaviour to a self-driving car.
Professor Brad Greenberg criticizes technological neutrality on several grounds.29 First, that is it difficult to predict new technologies and the costs of applying existing laws to these technologies. He also criticizes technologically neutral laws for being difficult to interpret due to inherent vagueness, for being infrequently updated, and for protectionist treatment of incumbent technologies. Professor Greenberg ultimately argues, in the copyright context, in favour of replacing technological neutrality with a general right to commercialize protected works within different categories of technology.30
27 See, e.g., Orin S. Kerr, Applying the Fourth Amendment to the Internet: A General Approach, 62 STAN. L. REV. 1005, 1015 17 (2010) (arguing the Fourth Amendment has been applied in a technologically neutral manner).
28 Jesús Romero Moñivas, The Problem and Socio-Political Confusion of ‘Technological Neutrality’: The Case of the Observatorio de Neutralidad Tecnológica in Spain. CURRENT SOCIOLOGY 59(3) 310–327; Chris Reed, Taking Sides on Technology Neutrality, 4 SCRIP T-ed 263, 265 (2007).
29
30 Id.
  Brad A. Greenberg, Rethinking Technology Neutrality, 100 MINN L. REV. 1495 (2016) (providing a critical
 review of technological neutrality).

11
Dr. Carys Craig has argued for an expanded understanding of technological neutrality.31 She argues slavish adherence to not distinguishing between technologies is more akin to “technological blindness,” and that technological neutrality should be seen as a “normative quest.”32 In other words, it is not that the law should be independent of any particular technology, but that the law should change as technology develops to achieve normative equibrium.33 In the face of technological development, the law should continue to foster socially beneficial values.
Other scholars have dealt more broadly with some of the social, philosophical, and economic issues associated with AI also covered in this thesis. However, these works have not focused predominantly on legal implications of technological advances, and none have advanced something like a principle of AI legal neutrality. For example, in The Singularity is Near (2005), as well as in some of his other works, Ray Kurzweil predicts that people will transcend their biological limitations through harnessing technology to usher in a new age of human progress.34 More recently, Max Tegmark has considered a similar phenomenon and argued that we are entering a time known as Life 3.0 as organisms emerge able to upgrade their own hardware and software.35 He similarly grapples with many of the implications of a world in which machines and machine-augmented individuals have superhuman performance. In Superintelligence: Paths, Dangers, Strategies,36 Nick Bostrom considers how people have historically used their relatively superior cognitive abilities to achieve dominance, and how people may prosper once machine superintelligence exists. Other works, such as The Glass
31 Carys J. Craig, Technological Neutrality: Recalibrating Copyright in the Information Age, 17 Theoretical Inquiries L. 601 (2016).
32 Id, at 605
33 Id.
34 Ray
35 Max Tegmark, 36 Nick
   Kurzweil, THE SINGULARITY IS NEAR: WHEN HUMANS TRANSCEND BIOLOGY (2006).
 LIFE 3.0 : BEING HUMAN IN THE AGE OF ARTIFICIAL INTELLIGENCE (2017).
 Bostrom, SUPERINTELLIGENCE: PATHS, DANGERS, STRATEGIES (2014).

12
Cage: Where Automation is Taking Us,37 and The Rise of the Robots: Technology and the Threat of Mass Unemployment,38 consider the phenomenon of automation, and how this will change the nature of work and employment, as well as how people learn and solve problem. Finally, other titles such as Robot Ethics: The Ethical and Social Implications of Robotics39 and Robot Law40 consider a variety of topics in ethics and law relevant to AI and physically embodied AI.
My Research
Since joining the University of Surrey as Professor of Law and Health Sciences in May 2016, the primary focus of my research has been on the legal implications of advances in AI. I have examined how advances in AI should change how the law is applied and developed in variety of legal areas—tax, tort, IP, and criminal—and how the law should change AI development. My analysis has focused primarily on US law, with comparative references mainly to UK (England and Wales) law, but I argue that my thesis is relevant to all legal systems and areas of law. Prior to this work, my research focused on IP as well as health law and policy.
My first work in “AI law” was a book chapter (‘Hal the Inventor: Big Data and its Use by Artificial Intelligence’)41 which I worked on around the same time as my law review article in the Boston College Law Review (‘I Think, Therefore I Invent: Creative Machines and the Future of Patent Law.’)42 These works came about as a result of the research I was doing in both IP and life sciences. I was teaching my patent law class the case law on inventorship,
37 Nicholas Carr, THE GLASS CAGE: AUTOMATION AND US (2014).
38 Martin Ford, THE RISE OF THE ROBOTS: TECHNOLOGY AND THE THREAT OF MASS UNEMPLOYMENT (2015).
39 ROBOT ETHICS: THE ETHICAL AND SOCIAL IMPLICATIONS OF ROBOTICS (Patrick Lin, Keith Abney, and George
A. Bekey, eds., 2012). 40
41 Ryan Abbott, Hal the Inventor: Big Data and its Use by Artificial Intelligence in BIG DATA IS NOT A MONOLITH (Hamid Ekbia et al eds., 2016).
42 Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079 (2016).
   ROBOT LAW (
 Ryan Calo, A. M. Froomkin and Ian Kerr eds., 2016).

13
while at the same time I was reading material about work being done by AI in the life sciences that resembled work done by human patent inventors. This led me to wonder whether any existing scholarship had considered issues such as whether an AI could be an inventor for purposes of patent law. I subsequently discovered people had been writing about AI inventorship academically for decades.
The prevailing narrative when I started working in this area held that AI-generated inventions (those made without a traditional human inventor) should not be protected by IP rights (IPRs) because the prospect of a patent could not incentivize an AI to invent. Thus, because patent law was primarily economically concerned with incentivizing innovation it did not justify IPRs for AI-generated inventions. Also, patent law protects the moral rights of inventors, and the prevailing narrative similarly held that there was no benefit to protecting the moral rights of a machine.
I was the first to argue that the ability to receive patents would incentivize innovation by motivating those building, using, and owning AI.43 I further argue that the AI should be listed as the inventor when it is functionally inventing, and that the AI’s owner should own any resultant IPRs. This article was received positively enough that it was solicited for republication in Landslide, the IP magazine of the American Bar Association’s IP chapter, and in Mitteilungen, a leading journal for German patent attorneys.
I further developed my arguments in favour of IPRs for AI-generated inventions as a result of a book chapter I was invited to contribute to an Edward Elgar handbook by Tanya Aplin, who is Professor of Intellectual Property Law at King’s College.44 She suggested I consider the
43 Id.
44 Ryan Abbott, Artificial Intelligence, Big Data and Intellectual Property: Protecting Computer-Generated Works in the United Kingdom in RESEARCH HANDBOOK ON INTELLECTUAL PROPERTY AND DIGITAL TECHNOLOGIES (Tanya Aplin ed., forthcoming Jan. 2020).
  
14
issues from a UK and an EU perspective. In doing so, I further argued that listing an AI as an inventor when it is functionally inventing is important to preserve the integrity of the patent system by maintaining accuracy of inventorship and informing the public of who the actual devisor of an invention is, and by preserving the moral rights of inventors. That is because while taking credit for an AI’s work would not be unfair to an AI, it would be unfair to other human inventors. It would equate legitimately inventive work with someone merely asking an AI to solve a problem.
It was also useful to consider the phenomenon in the context of different legal systems. The UK (England and Wales) is a good comparative example with the US because while neither has an established legal framework for patents and AI-generated inventions, they have different systems with respect to copyright and AI-generated works. The UK allows these to be protected by statute, whereas the US Copyright Office has a policy of prohibiting copyright protection for AI-generated works. Given that two polar opposite legal systems have been in place since at least 1988, it provides useful insight into the costs and benefits of one system versus the other. Although, perhaps because there are no requirements to register copyright in either jurisdiction (it is not even possible to register copyright in the UK), there have been a limited number of cases involving AI-generated works in either jurisdiction.
Again, neither jurisdiction has a law specifically about AI-generated works and patents, but both have laws requiring an inventor to be a natural person. I argue that this requirement was designed to ensure inventors were given credit because most patents are owned by businesses, but this was not done with AI-generated works in mind and should not operate to prohibit their subsistence in an arbitrary manner. In the US at least, it is accepted that courts can dynamically interpret statutes to best achieve their underlying policy goals. While I argue in favour of a dynamic interpretation of the law, a legislative solution may ultimately be required. As a further

15
contribution to this thesis, comments I made along these lines at a conference hosted by CEIPI were published.45
As my final included work on AI and IP, I wrote a follow up article, “Everything is Obvious,” that focused on how AI would change the standard of the “skilled person” in patent law.46 This is a standard used to judge, among other things, the inventive step or obviousness standard which requires a patent application to be nonobvious to a skilled person. The skilled person represents essentially the average worker in a scientific field. I argued that as AI increasingly augments average researchers, making them more knowledgeable and sophisticated, this should be reflected in the standard – raising the bar to obviousness. In time, as inventive AI comes to widely automate, rather than augment, inventive activity, inventive AI should come to represent the skilled person. Prior scholarship in this area had not considered an AI as a substitute for a skilled person. I argue a skilled person standard based on an inventive machine would further raise the bar to inventive step. In time, as specific artificial intelligence gives way to general artificial intelligence and then to super-intelligent AI, everything should be obvious to a sufficiently sophisticated AI. This would mean no more patents would issue, which would be fine because at that point the cost of innovation would be negligible. This article has also been republished by Landslide and in a three-part series by Mitteilungen. It was additionally republished as one of the final posts on the popular website IP Watch as well as in a book chapter in the Cambridge Handbook on Law and Algorithms.
My AI law research has also considered tort liability. In, “The Reasonable Computer: Disrupting the Paradigm of Tort Liability,” I argued that where an AI’s manufacturer can show
45 Ryan Abbott, Inventive Machines: Rethinking Invention and Patentability in INTELLECTUAL PROPERTY AND DIGITAL TRADE IN THE AGE OF ARTIFICIAL INTELLIGENCE AND BIG DATA (Xavier Seuba, Christophe Geiger and Julien Penin eds., 2018).
46 Ryan Abbott, Everything is Obvious, 66 UCLA. L. REV. 2 (2019)
   
16
that an AI is safer than a reasonable person and automating human activity which causes harm, the supplier should be liable in negligence rather than strict liability.47 This negligence test would focus on the AI’s act instead of its design, and in a sense, it would treat an AI tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, avoid many of the challenges associated with the complexity and black-box nature of AI-generated accidents, and it would continue to reward manufacturers for improving safety. More importantly, I argue that principles of harm avoidance suggest that once AIs become safer than people, human tortfeasors should no longer be measured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against AIs.
In AI and tax, together with Dr Bret Bogenschneider we wrote, “Should Robots Pay Taxes? Tax Policy in the Age of Automation.”48 This was the foundational law review article advocating for a “robot tax.” We argued that tax laws treat people and machines differently even when they are in the same role, because automation allows businesses to avoid employee and employer wage taxes levied by central and local taxing authorities. A potentially even greater tax advantage to automation may be that businesses can accelerate tax deductions for machines but not for human workers. Finally, employers also receive a variety of indirect tax incentives to automate.
Even more concerning is that robots do not pay taxes. This is a significant problem because income tax combined with employment taxes are the largest sources of revenue for the government, together accounting for the great majority of federal tax revenue. Robots do not
47 Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. 1 (2018).
48 Ryan Abbott and Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL. REV. 145 (2018).
 
17
pay income tax or require employment taxes. On top of that, they do not purchase goods and services so are not charged sales taxes, and they do not purchase or rent property, so they do not pay property taxes. Robots are simply not taxpayers, at least not to the same extent as human workers. If all work was to be automated tomorrow, most of the tax base would immediately disappear. We argue that when firms automate, the government loses revenue—potentially hundreds of billions of dollars in the aggregate. The solution is to fundamentally change the extent to which we tax capital vs. labor and workers vs. businesses. This could be done, for example, by eliminating payroll taxes and increasing the effective corporate tax rate (or property or consumption taxes).
Finally, in criminal law, Professor Alex Sarch and I wrote an article recently published in UC Davis Law Review considering direct criminal punishment of AI.49 We examine the case, as being advanced by some scholars, for direct criminal punishment of AI by evaluating whether it would be consistent with the theoretical and doctrinal limitations on criminal law, the costs and benefits of such a practice, and whether an alternate approach would be preferred. We argue that legal fictions in criminal law are sufficiently flexible to accommodate criminal punishment of AI, and that similar fictions are used to make artificial persons in the form of corporations criminally liable. However, we argue the practice would have significant costs and that any benefits would be more effectively achieved through more traditional sorts of expansions of criminal and civil law.
I had a book proposal accepted by Cambridge University Press based on my above research on AI law. The completed book has now been accepted and it is due to be released mid-2020.50 The book is essentially a thesis for a PhD-by-publication program because it gathers together
49 Ryan Abbott and Alexander Sarch, ‘Punishing Artificial Intelligence: Legal Fiction or Science Fiction’ 53 UC Davis Law Review (2019).
50 REASONABLE ROBOT.
 
18
my preexisting work and synthesizes it to find a unifying thesis. While the various projects described above have been in different legal areas, they have all examined the ways in which AI has been automating and augmenting traditionally human-centric activities and my analysis has focused on what legal changes are consequently required to best achieve the law’s underlying goals. I have found that the law often treats activity by AI and activity by people differently, and that this can have negative outcomes. My thesis is that lawmakers should embrace a novel regulatory principle – AI legal neutrality. I argue this will tend to improve human wellbeing by promoting efficiency.
The book advances and explains this thesis in the introduction chapter, which is essentially the introductory chapter of a PhD-by-publication, and then the first chapter is an introduction that provides general background information about AI. Chapters 2-6 deep dive into specific fields of the law to explore how a principle of AI legal neutrality would work in practice, and the final chapter responds to criticisms of my thesis and some of its implications. Substantively, chapters 2-6 of the book incorporate my primary AI law articles, but with changes that reflect both streamlining and the evolution of my thinking since the articles were published. I have always found it the case in the years after publishing a law review article that I would like to go back and do some major edits. The book provided such an opportunity, even where the edits were primarily a matter of framing and streamlining. As the CUP monograph is attempting to appeal to a more general audience than the law review articles, many of the citations in the articles – along with the traditional lengthy, discursive footnotes typical of US law review articles (my primary medium) – have been omitted in the book.
Research Question and Thesis Statement
I am concerned with how the law should regulate human-like activity by AI, and how the phenomenon of AI behaving like a person should change how people are regulated. My thesis
 
19
is that the law should tend not to discriminate between AI and human behavior. That is because the law should focus on conventionalist or utilitarian outcomes, and it may not matter whether an actor is human or machine with respect to the social impact of activities. I argue that more equal treatment of AI and human behavior will tend to help the law to achieve its underlying goals.
As sub-questions to my broader research question, in intellectual property, I am concerned with AI autonomously engaging in creative and inventive activity and stepping into the shoes of human authors and inventors. My thesis is that creative and inventive works autonomously made by AI should be accorded protection, that AI should be listed as an author or inventor when it otherwise meets criteria for authorship and inventorship, and that the AI’s owner should own any resultant intellectual property rights. Also, that as AI increasingly augments and eventually automates researchers, that this should raise the bar to inventive step and require the nonobviousness inquiry to focus more on economic than cognitive factors. In tax law, I am concerned with how similar work done by a person vs an AI are subject to different tax regimes, and how this will affect tax revenue. My thesis is that work by an AI is favored over work by a person, and also that automation will tend to decrease government tax revenue. I argue the solution is to reduce or eliminate labor taxes while at the same time increasing other forms of taxation to ensure revenue. In tort law, I am concerned with people and AI both engaging in activities with a risk of harm, and whether current liability standards promote or discourage automation. My thesis is that AI activity is subject to stricter liability standards which will tend to discourage automation even when it would promote safety. Also, that people should be held to the standard of AI once automation is practicable, and that this will further promote safety. In criminal law, I am concerned with AI engaging in criminal sorts of behavior in ways that does not reduce to criminal conduct by individuals. My thesis is that direct criminal liability for AI would be inappropriate, but that there will likely be a need to expand civil and possibly criminal

20
liability for individuals involved in the development and use of AI. Further, that direct criminal punishment of AI is broadly consistent with the conceptual and theoretical limitations of the criminal law, and that this provides support for punishment of anti-social behavior even without the same level of reliance on moral fault.
Impact
This body of work has already had a significant impact on the field of AI regulation. With respect to AI and IP, I was selected as one of the 50 most influential people in the world of IP in 2019 by Managing Intellectual Property Magazine. The article in which I was selected states, “Ryan Abbott is a leading academic writing about law and technology, IP and life sciences. His work on artificial intelligence (AI) and IP has contributed to the international dialogue on how new technologies are challenging existing legal standards.” 51 My AI and IP work has also been discussed in depth and played a significant substantive role in a number of law review articles.52 The BCLR article alone has been cited 79 times according to Google Scholar. Not only have academics engaged with this work, it has been relied upon outside of academia, for instance in a White Paper by the World Economic Forum’s Center for the Fourth Industrial Revolution which cited me heavily.53 In June 2019, United Kingdom Supreme Court Justice Lord Kitchin spoke about my scholarship at an event co-sponsored by the UK Intellectual Property Office and the World Intellectual Property Office (WIPO) on AI and IP.54
51 50 Most Influential People in IP (2019), https://www.managingip.com/Article/3907240/Top-50-in-IP-2019- notable-individuals.html.
52 See, e.g., W. Michael Schuster, Artificial Intelligence and Patent Ownership, 75 WASH. & LEE. L. REV. 1945 (2019).
53 ARTIFICIAL INTELLIGENCE COLLIDES WITH PATENT LAW (2018), https://www.weforum.org/whitepapers/artificial-intelligence-collides-with-patent-law.
54 UKIPO - WIPO Conference Keynote Speech Lord Kitchin, Justice of the Supreme Court (Jun., 18 2019), https://www.supremecourt.uk/docs/speech-190618.pdf (e.g., “Professor Ryan Abbott points out in one of his many interesting papers in this area that [autonomous machine invention] has been a reality for some time.”).
  
21
Perhaps to some degree in direct response to my work, but in any event due to increased public attention to AI and IP issues which I have helped to generate, the United States Patent and Trademark Office (USPTO) issued a request for comments on AI and patent law in late 2019, and then another request for comments on AI and other types of IP law.55 The USPTO has reported it is seeking these comments as a first step in developing policies for AI and IP. Although the comments touch on a wide variety of areas, they focus in large part on AI- generated inventions and AI-generated works, along with other issues I have directly addressed in my research such as the impact of AI on the skilled person standard. WIPO recently issued a similar request for comments.56 Some of the replies to these calls for comments, for instance from the Information Technology & Innovation Foundation (ITIF) have relied on my academic work.57
Finally, my academic AI and IP works have also formed the basis for a legal test case that I have spearheaded – the first cases of patent filings for AI-generated works. At the present time, these applications have rejected by the UK and European Patent offices, resulting in new policies. The UK rejection noted, “inventions created by AI machines are likely to become more prevalent in future and there is a legitimate question as to how or whether the patent system should handle such inventions. I have found that the present system does not cater for such inventions and it was never anticipated that it would, but times have changed and technology has moved on. It is right that this is debated more widely and that any changes to the law be considered in the context of such a debate, and not shoehorned
55 Request for Comments on Patenting Artificial Intelligence Inventions (Aug. 27, 2019), https://www.federalregister.gov/documents/2019/08/27/2019-18443/request-for-comments-on-patenting- artificial-intelligence-inventions.
56 Impact of Artificial Intelligence on IP Policy: Call for Comments (last visited, Jan. 22, 2019), https://www.wipo.int/about-ip/en/artificial_intelligence/call_for_comments/
57 Information Technology and Innovation Foundation, Response to Requests for Comments on Patenting Artificial Intelligence Inventions (2019), http://www2.itif.org/2020-ustpo-ip-ai.pdf.
 
22
arbitrarily into existing legislation.”58 Both rejections are pending appeal. The cases are pending consideration in other patent offices including in the US, Germany, Israel, Republic of Korea, China, and Taiwan. The cases have already received significant attention by the Financial Times, Wall Street Journal, and Business Insider among others.59
My AI work outside of IP has also had significant impact. For instance, my torts article has been cited and engaged with in a variety of subsequent academic literature.60 I was appointed as an expert on the European Commission Expert Group on New Technologies Formation and Liability based on the article, and it has been cited 31 times according to Google Scholar. Similarly, my tax article has since been translated to Chinese and published in the Economic Law Review.61 It has been cited 59 times according to Google Scholar, including in publications by OECD and in the Yale Law Journal.62 It was also prominently covered in the New York Times and VICE.63
I have been invited to present on my AI legal work at academic events including at Oxford, Cambridge, Stanford, Yale, MIT, and King’s College. I have also presented my work at the World Intellectual Property Organization (WIPO), at the European Union Intellectual Property Office (EUIPO), at government sponsored events in the United Kingdom, Moscow and Luxembourg, and at industry events such as the American Chemical Society’s annual meeting in Boston, the Association Internationale pour la Protection de la Propriété
58 UKIPO DECISION BL O/741/19, BRITISH PATENT APPLICATIONS, GB1816909.4 & GB1818161.0 (2019).
59 See, e.g., Jared Council, Can an AI System be an Inventor, W.S.J., Oct. 11, 2019.
60 See, e.g., Bryan Casey, Robot Ipsa Loquitur. 107 GEO L.J. 14 (2019).
61 Ryan Abbott and Bret Bogenschneider. Should Robots Pay Taxes? Tax Policy in the Age of Automation, 18
62
(2017); OECD Economic Survey of the United States: Key Research Findings (2019), https://doi.org/10.1787/9789264310278-en.
63 Eduardo Porter, Don’t Fight the Robots. Tax Them, N.Y. TIMES, Feb. 23, 2019; Ankita Rao, Millennial Democrats like Alexandra Ocasio-Cortez and Pete Buttigieg Are Ready to Face Off Against Job- Stealing Robots VICE, Apr. 1, 2019-09899999999p988888888p9.
  (2) ECONOMIC LAW REVIEW 239 (2018) (Chinese).
 Cynthia L. Estlund, What Should We Do After Work? Automation and Employment Law, 128 YALE L.J. 254

23
Intellectuelle’s (AIPPI) biannual international conference in Israel, and INTERPAT’s annual meeting.
Methodology
My research is qualitative, utilizing primarily doctrinal and legal theory research methods. In several areas of the law, I analyse and synthesize legal precedent and legislative interpretation related to AI and draw connections between different areas of the law. I explain areas of difficulty and predict future developments, and also provide a critical conceptual analysis while suggesting new rules, principles, norms, interpretive guidelines and values. My research is reform oriented, as well as comparative among several legal systems, particularly the US and UK. The decision to focus primarily on US and secondarily on UK systems was based on the fact that I am legally qualified and have taught in both jurisdictions, and because the US and UK have substantially different legal treatment of AI activity in some of the areas I examine.
My thesis is also interdisciplinary, based on my background as a physician scientist and my understanding of technology in the health sciences context. For instance, I consider disruption caused by AI in light of duties of health care providers and their ethical obligations in the context of tort law, including issues related to informed consent, privacy, respect for persons, beneficence, and justice. I draw upon health sciences literature with respect to likely development of future technologies, the extent to which identified challenges have been previously addressed, and to put my research and methodology into context. For instance, I consider how inventive AI will impact R&D in the field of biotechnology, and more particularly with development of new monoclonal antibodies and novel indications for existing antibodies. I further draw insights from socio-legal studies, in that the thesis considers the law’s wider social context and the economic and social policy implications of proposed legal changes.
 
24
The law review articles outside of the book are narrow in the sense that they provide in-depth analysis in several legal fields, but the book is broad in the sense that it extrapolates from those specific instances utilizing inductive reasoning to propose a generalized conclusion about AI regulation. Judge Frank Easterbrook once argued at a conference on the Law of Cyberspace that there was no more a “law of cyberspace” than a “law of the horse.”64 He was focused on cyberlaw, the study of the law’s interaction with the Internet, but he would no doubt echo the same sentiment about the law’s interaction with AI. I argue in this thesis that to the contrary, AI requires a particular regulatory response – and that regulating AI has lessons about regulating generally. AI presents different challenges than a motor vehicle, telephone, or even the Internet. Activity by an AI is analogous to activity by a person, but demands fresh thinking – should the law treat AI and human activity in the same way or differently? Should the law change in light of AI activity or should the law try to change the activity? It may be that the role of law will be to restore the status quo ex ante in light of disruption caused by AI, but AI may also provide an opportunity to more fundamentally restructure the law in order to better promote the public interest.65
 64 Frank H. Easterbrook, Cyberspace and the Law of the Horse, U. CHI. L. FORUM 207 (1996). 65
 Other works have similarly considered how technologies, like the Internet, will disrupt the law and what
 responses are needed to promote various normative goals. See, e.g., Lawrence Lessig, CODE AND OTHER LAWS
 OF CYBERSPACE (1999); Yochai Benkler, THE WEALTH OF NETWORKS (2006).

25
 half-title-page
Blurb and bio to follow
The Reasonable Robot
     AI and people do not compete on a level-playing field. From a safety perspective, AI may be the best choice for driving a vehicle, but laws often prohibit driverless vehicles. At the same time, a person may be better at providing customer service, but a business may automate because it saves on taxes. AI may be better at helping companies to innovate, but using AI may keep these companies from obtaining intellectual property rights. In The Reasonable Robot, Ryan Abbott argues that the law should not discriminate between people and AI when they are performing the same tasks, a legal standard that will ultimately improve human wellbeing. This work should be read by anyone interested in the rapidly evolving relationship between AI and the law.
Ryan Abbott is Professor of Law and Health Sciences at the School of Law, University of Surrey and Assistant Professor of Medicine at UCLA. A physician and patent attorney, Abbott’s research on law and technology has helped shape the international dialogue on these topics. He has served as an expert for the World Health Organization, World Intellectual Property Organization, the European Commission, and the UK Parliament. Abbott also spearheaded the first patent applications to disclose inventions made autonomously by an AI. In 2019, he was named one of the top 50 in Intellectual Property by IP Magazine.

26
 title-page
 The Reasonable Robot Artificial Intelligence and the Law
Ryan Abbott University of Surrey School of Law
 
27
 imprint-page
University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108472128
DOI: 10.1017/9781108631761
© Ryan Abbott 2020
This publication is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press.
First published 2020
Printed in <country> by <printer>
A catalogue record for this publication is available from the British Library.
    
28
Library of Congress Cataloging-in-Publication Data
ISBN 978-1-108-47212-8 Hardback ISBN 978-1-108-45902-0 Paperback
Cambridge University Press has no responsibility for the persistence or accuracy of URLs for external or third-party internet websites referred to in this publication and does not guarantee that any content on such websites is, or will remain, accurate or appropriate.

29
 table-of-contents
 Contents
Introduction: Artificial Intelligence and the Law
1 AI Legal Neutrality
2 Tax
3 Tort
4 Intellectual Property
5 Criminal
6 Regulating Artificial Intelligence
1 Understanding Artificial Intelligence
1 In the Beginning
2 Defining AI
3 Can AI Think?
4 Types of AI
5 Advances in AI
6 AI Characteristics
2 Should Artificial Intelligence Pay Taxes?
1 Automation and Technological Unemployment
2 Current Tax Policies Favor Automation and Reduce Tax Revenue
3 Options for Tax Neutrality
3 The Reasonable Robot
1 Liability for Machine Injuries
2 AI-Generated Torts
3 The Reasonable Robot
4 Artificial Inventors

30
1 AI-Generated Inventions
2 Intellectual Property Rights for AI-Generated Works
3 Implications of AI Inventorship
5 Everything is Obvious
1 Obviousness
2 Artificial Intelligence in the Inventive Process
3 A Post-Skilled World
6 Punishing Artificial Intelligence
1 Artificial Intelligence and Punishment
2 The Affirmative Case
3 Retributive and Conceptual Limitations
4 Feasible Alternatives
7 Alternative Perspectives on AI Legal Neutrality
1 What If There Is Never a Singularity?
2 Should the Law Discourage the Singularity?
3 Is AI Legal Neutrality a Coherent Principle?
4 Protecting Spheres of Human Agency
5 The Risks and Unique Dangers of AI
6 Concluding Thoughts
Third-Party Materials
Index

31
Introduction: Artificial Intelligence and the Law
The rise of powerful AI will be either the best or the worst thing ever to happen to humanity. We don’t yet know which.
- Stephen Hawking
Artificial intelligence (AI) is doing more than ever before, and often doing it cheaper, faster, and better than humans are. In 2017, the company DeepMind developed an AI, AlphaGo Master, that beat the human world champion of the board game Go. Many experts had predicted AI’s Go dominance would take another decade given the game’s complexity. There are more possible board configurations in Go than there are atoms in the universe. Later in 2017, a revised version of AlphaGo, AlphaGo Zero, beat AlphaGo Master one hundred games to zero. It did this after training for just three days by playing against itself. Unlike its predecessors, AlphaGo Zero never learned from human examples.
Go was the last traditional board game at which people could outperform machines. There is now an entire field of human activity at which AI outperforms people. While AlphaGo’s victory was an exciting technical landmark, it has had limited social impact because playing board games is not the most practical endeavor, or perhaps it might be because AI is still working toward video game dominance. But board games are one of the oldest measures of machine intelligence, and AI’s ascendency hints that it may soon automate a broader range of tasks, perhaps sooner than many anticipate, and it may do so in spectacular fashion.
Alphabet, which owns DeepMind, is not investing in AI to dominate the field for competitive board games. In principle, if an AI can train to recognize patterns in Go, then it

32
can train to recognize pneumonia in an X-ray or pedestrians on a road. Indeed, DeepMind is already being applied to solve practical challenges. In 2018, DeepMind’s AI AlphaFold outperformed all of its ninety-eight competitors in a challenge aimed at predicting the three-dimensional structure of proteins – a task critical to drug discovery. Unlike playing Go, predicting protein folding is an important, common, and real-life scientific problem. Similarly, again in 2018, researchers found that another DeepMind AI correctly referred patients with more than fifty distinct eye diseases for specialist care in 94 percent of cases, matching the performance of expert clinicians. In 2019, DeepMind AI was able to consistently predict development of acute kidney failure forty-eight hours earlier than human physicians, which could ultimately prevent around 30 percent of cases from ever occurring.
The future social impact of these advances is likely to be tremendous. Already, impressive-sounding era titles are being used to describe the coming disruption such as the Fourth Industrial Revolution, the Second Machine Age, and the Automation Revolution. Among other things, AI is predicted to generate a massive amount of wealth by changing the future of work. This has long been the experience with AI automating physical work, such as in automobile manufacturing, but AI is now moving into automating mental work, and not only relatively simple service activities like operating a cash register at McDonald’s. AI is completing tasks performed by doctors, lawyers, and scientists.
IBM’s flagship AI brand Watson, which famously won a game of Jeopardy! in 2011, is working in a range of fields. In health care, for example, Watson (now comprised of a variety of AI systems) analyzes the genetics of cancer patients to help select appropriate drug treatments, a task that a group of human experts can also do. While knowing a

33
patient’s genome, determining what treatment to provide remains a complex task. For some cancers, it can require around 160 collective work hours by a team of highly trained health care providers. By contrast, a 2017 study reports that Watson could outperform the standard practice and only require about 10 minutes to do so,1 although Watson’s performance has proven controversial.2
Several companies claim their AI can already outperform human doctors in certain areas of medical practice. This is not surprising. Machines are able to memorize every bit of medical literature ever created and process practice experience from countless human lifetimes. Plus, they never need a rest break. In 2017, a Chinese company reported its robot Xiao Yi took and passed, by a wide margin, China’s National Medical Licensing Examination, the test someone takes to become a medical doctor in China. Xiao Yi knows the contents of dozens of medical textbooks, millions of medical records, and hundreds of thousands of articles, but to pass the test it also had to learn, reason, and make judgments by itself. Researchers at IBM have even reported that Watson quietly passed the equivalent examinations in the United States after it was prohibited from formally taking the exam. Of course, just passing exams does not make someone, or something, a doctor. Once AI is consistently better than a doctor at diagnosing certain diseases, managing prescriptions, or performing surgeries, AI is still unlikely to completely automate medical care. But these
    1
et al.,
Kazimierz O.
Wrzeszczynski
Comparing Sequencing Assays and Human-Machine
 Analyses in Actionable Genomics for Glioblastoma
DOI: 10.1212/NXG.0000000000000164
2 Eliza Strickland, How IBM Watson Overpromised and Underdelivered on AI Health Care, IEEE SPECTRUM, Apr. 2, 2019, https://spectrum.ieee.org/biomedical/diagnostics/how-ibm- watson-overpromised-and-underdelivered-on-ai-health-care.
, 3   (2017),
NEUROL. GENET.
   
34
developments suggest that there are already aspects of medical care susceptible to automation, and that we will need fewer doctors once we have more efficient doctors augmented by AI.
1 AI Legal Neutrality
The law plays a critical role in the use and development of AI as the law establishes binding rules and standards of behavior to ensure social well-being and protect individual rights. An appropriate legal framework will help us realize the benefits of AI while minimizing its risks – which are significant. AI has caused flash crashes in the stock market, committed cybercrime, and been used for social and political manipulation. Famous technologists like Elon Musk and academics like Stephen Hawking have even argued that AI may doom the human race. Most concerns, however, focus on nearer-term and more practical problems such as technological unemployment, discrimination, and social inequality. But these concerns do not exist in a vacuum. How different jurisdictions elect to regulate AI will change how technologies develop. For instance, there is already a significant international divide with respect to whether companies or consumers “own” personal data vital to AI development, the extent to which AI can be used in state surveillance of its residents, and when individuals have a right to an explanation for decisions made by AI (ranging from credit approval to criminal sentencing). As the law will impact AI, AI will have no less significant an impact on the law. AI challenges fundamental assumptions underlying laws designed to regulate the behavior of human actors.

35
AI-centric laws have been slow to develop, perhaps due to a concern that an overly burdensome regulatory environment would slow innovation. This ignores the fact that AI is already subject to regulations that may have been created decades ago to deal with issues like privacy, security, and unfair competition. What is needed is not necessarily more or less law but the right law. In 1925, Justice Benjamin Cardozo admonishes a graduating law school class that “the new generations bring with them their new problems which call for new rules, to be patterned, indeed, after the rules of the past, and yet adapted to the needs and justice of another day and hour.”3 This is the case for AI, even if it only differs in degree from other disruptive technologies like personal computers and the Internet. A legal regime optimized for AI is even more important if AI turns out to be different in kind.
One might imagine that AI would fit comfortably into existing rules, or that there would be a single legal change, such as granting AI legal personality similar to a corporation, that will solve matters. While it is appealing to think a simple solution to the problem of regulating something as complex as AI exists, there is not a one-size-fits-all principle in every area of the law, which is why it is necessary to do the difficult work of thinking through the implications of AI in different settings. In this respect, it is promising that there have been efforts in recent years to articulate policy standards or best principles such as transparency, safety, accountability, and trustworthiness specifically for AI regulation by governments, think tanks, and industry. For example, the Organisation for Economic Co-operation and Development (OECD) adopted Principles on Artificial
3 BENJAMIN NATHAN CARDOZO, SELECTED WRITINGS OF BENJAMIN NATHAN CARDOZO 417 (Margaret E. Hall ed., 1947).
 
36
Intelligence in May 2019, and one month later the G20 adopted human-centered AI Principles guided by the OECD principles.
The central thesis of this book is that the law requires a new principle of AI regulation – AI legal neutrality – that the law should not discriminate between people and AI when they are engaged in the same activities. Currently, we do not have a neutral legal system. An AI that is significantly safer than a person might be the best choice for driving a vehicle, but existing tort laws might prohibit driverless vehicles. A person might be a better choice for delivering groceries, but a business might automate because it saves on taxes. AI might be better at generating certain types of innovation, but businesses might not want to use AI if this restricts future intellectual property rights. In all these instances, neutral legal treatment would ultimately benefit society as a whole, and this book argues that not discriminating between people and AI will tend to improve human well-being. The rise of AI is the rise of a new workforce – one that is treated differently under the law. Leveling the playing field is not necessary as a matter of fairness to machines, but it should be done because technologically neutral laws will improve efficiency. In turn, this will generate wealth, promote innovation, alleviate poverty, and reduce waste. Competition is vital to the operation of markets, and inappropriate government policies and legislation can obstruct fair competition.
AI legal neutrality does not require universally identical treatment of AI and people. There are obviously significant differences between them, the most important of which is that AI does not morally deserve rights; these differences will justify differential rules. Since AI lacks humanlike consciousness and interests, treating AI as if it were to have rights in the future should only be justified if this would benefit people. An example of this would

37
be if autonomous vehicles needed to directly hold funds or insurance policies to cover potential harms to pedestrians. This is essentially the rationale for corporations being allowed to enter into contracts and own property. Corporations’ legal rights exist only to improve the efficiency of human activities such as commerce and entrepreneurship, and like AI they do not morally deserve rights: They are a member of our legal community but not our moral community. This book does not advocate for AI’s having rights or legal personhood. Nor is a principle of AI legal neutrality a moral principle of nondiscrimination in the way that term is traditionally used. Antidiscrimination laws have helped improve conditions for historically marginalized groups, primarily as a matter of fairness. However, antidiscrimination laws also promote competition and efficiency.
Of course, efficiency should not be the driving force behind every decision. It should not come at the expense of other deeply felt normative values such as fairness and justice. A child laborer might be more efficient than an adult at fixing looms or sweeping chimneys, but for noneconomic reasons we decided long ago to prohibit child labor. A person might be better at mining minerals in hazardous conditions, but automation could be preferable based on safety considerations. An AI might be better at identifying and eliminating military targets, but there could be other reasons not to delegate life and death decisions to a machine. Rather than a dispositive policymaking principle, AI legal neutrality is an appropriate default that may be departed from when there are good reasons for so doing.
A child laborer may be more efficient than an adult at fixing looms or sweeping chimneys, but for noneconomic reasons we decided long ago to prohibit child labor. A person may be better at mining minerals in hazardous conditions, but automation may be preferable based on safety considerations. An AI may be better at identifying and

38
eliminating military targets, but there may be other reasons not to delegate life and death decisions to a machine. Rather than a dispositive policymaking principle, AI legal neutrality is an appropriate default that may be departed from when there are good reasons for so doing. This book examines how a principle of AI legal neutrality would impact four different areas of the law – tax, tort, intellectual property, and criminal. As AI increasingly steps into the shoes of persons, AI will need to be treated more like a person, and sometimes people will need to be treated more like AI.
2 Tax
Automation involves much more than technology’s putting people out of work, what economist John Maynard Keynes terms “technological unemployment,” but it is one of the things people are most concerned about. Today, this is a frequent topic of scholarship on labor markets, some of which predicts long-term technological unemployment and some that does not. It is also an old concern. The Luddites, a group of English workers opposed to automation’s eliminating jobs, periodically destroyed machinery in acts of protest during the First Industrial Revolution. In response, the British government made machine- breaking a capital offense. History has shown the Luddite fears about automation were misplaced, at least in regard to concerns about long-term unemployment. In the end, the machines resulted not just in vast gains in productivity but also in more jobs for everyone, and ever since new technologies have consistently resulted in overall job creation. Steam engines, electrical power, and personal computers all eliminated certain jobs, but they created more jobs than they eliminated. At the turn of the twentieth century, some 40

39
percent of the US workforce was employed in agriculture. Now, less than 2 percent of the workforce works in agriculture. This has not translated to a 38 percent increase in unemployment. In fact, even as agriculture-based employment and agriculture’s relative contribution to the economy have decreased, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the economy has increased.
For the Fourth Industrial Revolution, history repeating itself may not be so bad. Despite some naysaying, the risks of automation may be overstated and may again result in long-term employment gains. While that may be the case, it cannot be ignored that the First Industrial Revolution was accompanied by decades of pervasive social unrest, widening income disparities, and individual suffering. A proactive regulatory approach to a new industrial revolution should allow us to make the most of automation while limiting some of its harmful effects. If this is a new type of industrial revolution that results in permanently increased long-term unemployment, a proactive approach will be all the more important.
But for all the debate about AI putting people out of work, it turns out this may occur for a very surprising reason – taxes. Tax laws treat people and automation technologies like AI differently even when they are performing the same tasks. For instance, automation allows businesses to avoid employee and employer wage taxes. Employers and employees in the United States pay matching amounts totaling more than 12 percent of an employee’s salary and matching Medicare payments totaling nearly 3 percent (applied on the first $127,200 of earnings), plus almost 1 percent more for higher earners as a Medicare surcharge on earnings more than $200,000. If an AI can replace a person, the employer avoids these taxes. So, if an automatic cashier costs McDonald’s the

40
same or even a bit more before taxes than an employee who does the same job, it actually costs the company less to automate after taxes.
In addition to avoiding wage taxes, businesses can accelerate tax deductions for some AI when it has a physical component or falls under certain exceptions for software – but not for human workers. In other words, employers can claim a large portion of the cost of some AI up-front as a tax deduction, which may be more valuable to some large companies than delaying wage expenses over time. This occurs because many large companies have significant cash reserves that earn low rates of return, so it can be more valuable for them to get a present deduction than to invest existing cash. In fact, employers may get accelerated tax deductions even if AI increases in value over time, the same way that deductions can be claimed for depreciation in real estate for assets that increase in value. This latter fact allows firms to report profits to potential investors for the difference between an AI’s tax depreciation and the value of the AI on the firm’s financial statements. Finally, employers also receive a variety of indirect tax incentives to automate. In short, our tax laws keep people and AI from competing on a level-playing field. While the system was not designed to do this, but does primarily tax labor rather than capital. This has had the unintended effect of inefficiently incentivizing automation, once the capital (AI) is the labor (AI).
What is even more concerning is that AI does not pay taxes! This sounds ridiculous until you consider that income and employment taxes are the largest sources of revenue for the government, together accounting for about 85 percent of total federal tax revenue. By contrast, business income taxes only generated about 10 percent of revenue when statutory corporate tax rates were 35 percent. Under the 2017 Tax Cuts and Jobs Act, the

41
rate was cut to 21 percent, and corporate tax revenue has been trending sharply downward. Whatever the statutory rate, the effective corporate tax rate – what companies pay after taking tax breaks into account – is substantially less. In 2018, the S&P 500 annual tax rate, which refers to 500 large companies that have common stock listed on one of the three major US stock exchanges, was less than 18 percent.4 However, this includes all taxes from the federal and state levels as well as from foreign authorities. Amazon drew unwanted attention that year by reporting a US pretax profit of $11.2 billion together with a negative tax bill of $129 million.5 Amazon’s total effective tax rate for 2018 was 11 percent including foreign, state and deferred taxes.
So, AI does not pay income taxes or generate employment taxes. It does not purchase goods and services, so it is not charged sales taxes. It does not purchase or own property, so it does not pay property taxes. AI is simply not a taxpayer, at least not to the same extent as a human worker. If all work were to be automated tomorrow, most of the tax base would immediately disappear. What happens is that when firms automate, the government loses revenue – potentially hundreds of billions of dollars in the aggregate. This may be enough to significantly constrain the government’s ability to pay for things like social security, national defense, and health care. In the long run, the revenue loss should balance out if people rendered unemployed eventually return to similar types of work, and there should ultimately be revenue gains if automation makes businesses more productive
4 Matt Krantz, PepsiCo Paid No Tax? Neither Did These Other 33 Profitable S&P 500 Companies, INVESTOR’S BUSINESS DAILY, July 18, 2019, www.investors.com/etfs-and-
funds/personal-finance/corporate-tax-rate-zero-profitable-us-companies-sp500/.
5 https://www.wsj.com/articles/does-amazon-really-pay-no-taxes-heres-the-complicated-answer- 11560504602
      
42
and if people go on to find better-paying types of work. But even leaving aside potentially significant short-term disruptions, that will not be the case if we are headed to a future of work with higher unemployment rates unless increased productivity dramatically outstrips unemployment.
Only recently has public debate surfaced about taxing AI, and it has mainly been in relation to slowing the rate of automation, not as an attempt to craft tax-neutral policies or ensure government revenue. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing concerns of stifling innovation. The next day, Bill Gates said he thought governments should tax robots. Former US Secretary of the Treasury Lawrence Summers subsequently contended that Gates was profoundly misguided. Later that year, South Korea was credited with enacting the world’s first “robot tax,” which limited tax incentives for automated machines.
The question of how the law should respond remains. Automation should not be discouraged on principle; in fact, it should be welcomed when it improves efficiency. But, automating for the purpose of tax savings is not efficient – or, at least not socially efficient. Automating for tax savings may not make businesses any more productive or result in any consumer benefits - it may even result in productivity decreases to reduce tax burdens. This is not socially beneficial. The options - once policymakers agree that they do not want to advantage AI over human workers - could be to reduce the tax benefits AI receives over people, such as what South Korea, or to create new benefits, or reduce existing taxes, that only apply to human workers. For instance, payroll taxes could be eliminated, which might be a better way to achieving neutrality since it reduces tax complexity and ends taxation of

43
something of social value – namely human labor. However, this would eliminate around 35 percent of the US federal government’s current tax revenue.
There are many ways to ensure adequate tax revenue, such as by increasing property or sales taxes, which might be a more progressive way to tax because it would end up taxing income regardless of its source – labor or capital. It could certainly be progressively designed by applying relatively higher property taxes for higher-value properties and higher sales taxes for, say, luxury goods. Income taxes could also be increased either by raising the marginal tax rates for high earners or the effective tax rates through the elimination of things like the step-up in basis rule that reduces tax liability for inherited assets. More ambitiously, AI legal neutrality may prompt a more fundamental change in the way labor versus capital and workers versus businesses are taxed. While new tax regimes could target AI, as well as other automation technologies to which similar considerations apply, it would risk increasing compliance costs and tax complexity. It would also be “taxing innovation” in the sense that it may penalize business models that are legitimately more productive with less human labor.
A better solution would be to increase capital gains taxes and corporate tax rates to reduce reliance on labor taxes. Before AI entered the scene, there had been long-standing criticism about the extent to which capital is favored over labor in tax policy. The Fourth Industrial Revolution may provide the necessary impetus to finally address this issue. The downside of increased capital taxation is largely a concern about international tax competition. There is a historic belief that labor should be taxed over capital, because capital is more mobile and will leave jurisdictions with higher tax rates. These concerns may be overstated, particularly in large, developed markets such as the United States.

44
Historically, relatively high corporate tax rates have not been a barrier to US-based investments.
The United States, which has the world’s largest economy, does not have a relatively progressive tax system – which is to say one based on a person’s ability to pay. Wider wealth disparities exist in the United States than in any other developed country. With AI likely to result in massive but poorly distributed financial gains, AI will both require and enable us to rethink how we allocate resources and redistribute wealth. If we do choose to reduce income inequality, this should be accomplished primarily though taxation. With new laws ensuring that AI contributes its fair share to government revenue, we could fund retraining programs for workers and enhance social benefits. If AI does end up causing increased long-term unemployment, subsequent tax revenue could even fund a universal basic income that would enable governments to pay every citizen regardless of their employment.
3 Tort
AI will be doing all sorts of things that only a person used to do – like driving. While difficult to say exactly when this will happen, companies like Uber and Tesla claimed they would be using or selling fully autonomous vehicles (AVs) before 2020. Other automobile manufactures now state they will be selling AVs in the early 2020s. By contrast, in 2017, a European Commission expert group predicts that fully driverless vehicles will not be

45
commercially available before 2030.6 Nevertheless, survey research often reports negative public attitudes about AVs. Most people say they would feel unsafe being driven around by their car compared to a taxi driver, yet AVs may already be safer than people. Driving is dangerous. The US Department of Transportation reports that more than 35,000 people die each year from domestic motor vehicle accidents. It estimates the economic costs of those accidents at more than $240 billion. Worldwide, more than a million people die each year in motor vehicle accidents, and tens of millions are injured. This is almost exclusively the result of people’s being terrible drivers. About 94 percent of crashes involve human error, and a human driver causes a fatality about every 100 million miles.
There has been at least one fatality caused by an AV. Operated by Uber, the AV collided with a pedestrian in Arizona because it failed to detect her in time to stop, but it was not the only negligent party: The backup driver was watching a television show on her phone, and the pedestrian was walking her bicycle across an unlit and unmarked section of road at night. More recently, regulators reported that a Tesla “Autopilot” system may have been at fault in a March 2019 fatality. A Tesla spokesperson noted in response that Tesla drivers have logged more than a billion miles with Autopilot engaged, and that Autopilot tends to make drivers safer.7 Earlier reported AV fatalities involving Tesla’s Autopilot system were ultimately determined by regulators not to be the AV’s fault. However, those incidents speak to the challenges of human-machine interaction – human drivers are
6 GEAR 2030, FINAL REPORT OF THE HIGH-LEVEL GROUP ON THE COMPETITIVENESS AND SUSTAINABLE GROWTH OF THE AUTOMOTIVE INDUSTRY IN THE EUROPEAN UNION, at 40 (July 2017).
7 Timothy B. Lee, Autopilot Was Active When a Tesla Crashed into a Truck, Killing Driver, ARS TECHNICA, May 16, 2019, https://arstechnica.com/cars/2019/05/feds-autopilot-was- active-during-deadly-march-tesla-crash/?comments=1&post=37374819.
    
46
supposed to be prepared to retake control of the vehicle on short notice, but it is difficult for people to remain alert and engaged while an AV is driving. Inevitably, self-driving cars will cause fatalities. But the perfect should not be the enemy of the good. AVs do not need to be harmless to make people safer; they just need to be better drivers than people. Whether in 2020, 2030, or 2040, AVs will not only be safer drivers than people but much safer than people. AVs are rapidly improving and human drivers are not, which is important with respect to legal liability for wrongful acts.
Tort law defines what constitutes a legal injury and determines those cases in which someone may be held financially, as opposed to criminally, accountable. For accidents caused by people, we generally require negligence for liability. Negligence means that someone’s actions fell below the standard of a reasonable person. To apply this test, over the course of centuries, courts have developed the concept of a hypothetical reasonable person who does not really exist but who sets the standard for human behavior. If a reasonable person would not have stopped a car in time to avoid hitting a child who ran out into the street, then the driver would not be held liable. If a reasonable person would have stopped, then she would be liable. This concept of a reasonable person is not exclusive to torts; it is a standard that applies in many areas, including criminal and contract law. People mention tort liability when speaking about the law and self-driving cars, often in the context of worrying over who – or what – to hold liable for accidents caused by self-driving cars.
Those concerns are overblown. AVs are products, and there is already a legal regime built around injuries caused by products. Product liability law could simply be applied to self-driving car accidents. To oversimply, there is a different standard for accidents caused

47
by people than for accidents caused by products. The law holds manufacturers and retailers of products strictly liable for harms caused when a machine is defective, or when its properties are misrepresented. Strict liability refers to the fact that liability is based on causation: Did the machine cause an injury without regard to whether a manufacturer’s conduct was socially blameworthy? Strict liability is a lower bar for liability, which is a good rule for most products. There is more liability for manufacturers, helping incentivize manufacturers to make safer products. Manufacturers are in the best position to improve product safety and to profit from decreasing accidents. However, more liability for manufacturers does not necessarily translate to fewer accidents if a product is safer than the existing standard. In that case, product liability law would make people less safe. When AI has more liability than a person, it makes automation costlier. This is not a desirable outcome. We want to encourage, or at least not discourage, automation through tort liability in situations where it would improve safety.
Instead of applying standard product liability law to AI, the law should evaluate accidents caused by AI under a negligence standard. In a sense, this would treat AI like a person, and it would focus on the AI’s act rather than its design. The law would ask whether the AI behaved in such a way that if a person had done such a thing, the act would have fallen below the standard of a reasonable person. As with human defendants, the law does not usually concern itself with what a person was thinking or whether he thought what he was doing seemed reasonable. The law looks objectively at whether a reasonable person would have committed the act.
Here, as with tax law, AI and people compete at the same sorts of activities and exhibit similar behaviors and responsibilities. While tax law currently aids AI in the

48
workplace, tort law favors people. A principle of AI legal neutrality that holds AI to a negligence standard would encourage the development and adoption of safer technologies. Again, this would not treat AI and people entirely the same legally in that AI would not be personally liable for injuries. AI is owned as property, does not have financial resources, and is not influenced by the specter of liability the way people are. Manufacturers are still in the best position to improve safety and to weigh the risks and benefits of new technologies. This would function as a market-based mechanism to encourage the introduction of technologies that improve safety with the benefit of not requiring government funding, additional regulatory burdens on industry, or new administrative responsibilities.
Applying a negligence framework to AI is the least important part of the torts story. The time is coming when AI performance will not just be safer but substantially safer than a person’s – to the point where self-driving cars may almost never cause accidents. And once that happens, it really will not matter which liability regime we apply to AI; it will matter which liability regime we apply to ourselves because by then we – people – will be the biggest danger on the road. The courts, at this point, should hold human drivers to the standard of self-driving cars – the reasonable robot standard, although it may be more accurate, if less catchy, to call it the reasonable AI standard – rather than the reasonable person standard.
Once AVs are exponentially safer than people, almost any accident someone causes would be negligent by comparison to an AI. Today, if a child runs in front of a person’s car while she is driving at night and is unable to stop, the person probably would not be liable. However, in a future where the reasonable person standard is AI, if an AV would have been

49
able to stop, even if it detected the child through radar, then a person would be liable. A reasonable robot or AI standard would not prohibit people from driving, although that is a possible response. Instead, this standard would mean that people drive at their own risk and they are responsible for any harms they cause. This standard internalizes the risk of driving and might be a better system than banning human drivers because it safeguards their freedom and autonomy while simultaneously providing a strong financial incentive for safer practices.
Self-driving cars are only one example of how AI will disrupt tort law. With evidence suggesting AI can outperform people at some aspects of health care and escalating health care costs, people may soon be going to see Doctor Watson for care. Right now, AI can only outperform people at very narrow aspects of medical practice, but it is getting better quickly, and human doctors are not. What should be remembered is that Watson does not have to be perfect to improve safety – just a little bit better than human doctors – and that bar is low. Make no mistake, human doctors are downright dangerous. If you think a lot of people die from car accidents, think about the fact that medical error kills far more. Estimates vary on the exact numbers, but the medical profession has acknowledged for about two decades that medical error is a leading cause of human death. In fact, for human doctors who take an oath to first do no harm, it would be unethical to allow them to compete with AI.

50
4 Intellectual Property
It should come as little surprise that AI has been autonomously generating artistic works and scientific inventions for decades. The law provides legal protections for human actors for many types of intellectual property – copyrights for books and music, patents for certain types of discoveries – but the law remains quite stubborn in its protection when AI creates what we think of as "products of the mind.” The fourth chapter in this book discusses intellectual property law, the branch of law concerning property rights in certain intangible creations. Legally, it is unclear whether AI-generated works are eligible for intellectual property protection and who would own them if they are. In most cases ownership rights initially go to an author or inventor, and most jurisdictions require the author or inventor to be a natural person. Authors and inventors do have the ability to transfer their rights to others; this can happen automatically when employees create something within the scope of employment. As a matter of fact, most patents are owned not by human inventors but artificial persons in the form of businesses. Still, the requirement that a natural person be listed as an author or inventor ensures, even when companies own the intellectual property rights, the right of human creators to be acknowledged. These laws were not designed with AI in mind and have not yet largely been applied to AI- generated works where there is no person who qualifies as an author or inventor.
With respect to patents, the first one for an AI-generated invention may have been issued in 1998 for an invention made by a type of AI called a Creativity Machine. A Creativity Machine has an artificial neural network that has been trained on data that it stores by varying the connection weights between its neurons. This network then self-

51
perturbs these connection weights, altering the data to which it has been exposed, and generates novel output. A second artificial neural network, a critic network, monitors the first network for new ideas and identifies those ideas that are sufficiently novel compared to the AI’s preexisting knowledge base and that meet some criteria for usefulness. The Creativity Machine, if a person, would likely be an inventor in these circumstances. In the case of the 1998 patent, the US Patent and Trademark Office granted a patent for the AI’s invention but did so in the name of the AI’s owner, an easy decision as the patent application did not disclose the AI’s role – the AI’s owner listed himself as the inventor. This Creativity Machine may have been the first AI inventor, but it has certainly not been the last. More patented inventions were created autonomously by AI in the 2000s, such as the Invention Machine, which relied on an entirely different type of AI called genetic programming to design a new controller.
Right now, there are probably very few AI systems functionally inventing; most AI is involved in the process as a simple tool. Using a calculator, even if a person is bad at math, does not make the calculator an inventor on a patent. Similarly, having another person use an abacus to multiply some number would not make him an inventor. Some machines do a lot more than a calculator, but that work is still not inventive. If a machine, or a PhD student, conducts an experiment that a person has designed, and the results are patentable, the person who designed the experiment will probably still be the only inventor. However, if a person only asks someone to solve a problem (say, design a better battery), and she does, in the eyes of the law she – not the person asking her to design a better battery – will generally be the inventor (of the battery). Likewise, if a person asks Apple’s personal assistant Siri to develop a faster semiconductor, and it does, Siri would be the inventor, at

52
least it would if Siri were a natural person. What complicates AI’s involvement in invention with regard to its recognition in the process is that sometimes there is not a natural person who, as defined by current statutes, would traditionally qualify as an inventor. This raises important policy questions regarding whether AI-generated inventions are patentable, who the owner of such a patent should be, and who – or what – should be credited as an inventor. As of 2019, there is no law specifically about this in any jurisdiction. As such, it remains unclear whether an AI or a person would be an inventor, or even that the output of an inventive AI would be patentable. This uncertainty is bad for business and innovation. What it creates is opportunity for gaming patent offices and taking credit for the work of AI. While not unfair to AI, it might be unfair to human inventors who are legitimately inventing.
Somewhat more law on AI-generated works can be found with respect to copyright. The United Kingdom, the first jurisdiction to explicitly protect so-called computer- generated works in 1988, remains one of only a handful of countries worldwide that does so: The “producer” of the work is considered the author and owns the copyright. In the United States, a Copyright Office policy from 1973 prohibits nonhuman authorship, but the Copyright Office appears to have rejected AI-generated music as early as the 1950s. AI- generated works in the United States automatically enter the public domain and cannot receive copyright protection, which makes it tempting for people to take credit for something AI has done. In support of the current iteration of its “Human Authorship Policy,” the Copyright Office cites the 1884 case of Burrow-Giles v. Sarony . Photographer Napoleon Sarony sued the Burrow-Giles Lithographic Company for copyright infringement of a famous photograph of Oscar Wilde. The company alleged that the photographer could

53
not be the photograph’s author because a photograph is a mechanical reproduction of a natural phenomenon, but the US Supreme Court disagreed and held that any form “by which the ideas in the mind of the author are given visible expression” is eligible for copyright protection.8 The case thus deals explicitly with whether the use of a machine would negate human authorship and implicitly with whether a camera could be considered an author. The Copyright Office has interpreted this to mean that copyright requires ideas and mental products, which it has determined AI lacks. While a nonbinding judicial opinion from the Gilded Age should not answer the question of whether DeepMind’s AI can be an author, it is. Analogizing to the patent context, it could be that the AI-generated works are unprotectable. Other scholars have argued that AI does not need incentives to innovate, and that protecting AI-generated works could chill future human innovation.
Not only should the law permit copyrights and patents for AI-generated works, it should recognize AI as an author or inventor when the AI otherwise meets author- or inventorship criteria. The primary reason is based on why the law grants patents and copyrights in the first place: to encourage certain socially valuable activities. The US Constitution states that Congress shall have the power to “promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries.”9 Compared to physical or real property like a car or a house, it can be difficult to prevent people from copying or using intangible products like a song or a design. Without some form of intervention, the market would underproduce certain types of innovation because there would not be adequate incentives
8 Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53, 56 (1884). 9 U.S. CONST., Art. I, § 8, Cl. 8.
 
54
to generate new content if third parties could use it for free. This is referred to as the free rider problem, for which patents and copyrights are possible solutions. Patents and copyrights provide an inventor or author with a temporary monopoly over an invention or work by preventing third parties from using or copying it without the inventor or author’s permission. The prospect of a patent or copyright thus provides an additional financial motivation for inventors and authors.
Even though machines do not care about patents and copyrights, people who build, own, and use AI do. There is value – monetary and moral – in acknowledging AI as authors and inventors. Allowing patents for AI-generated works would make inventive AI more valuable and incentivize AI development, translating to rewards for effort upstream from the stage of invention. By contrast, failing to allow patents for AI-generated works would discourage businesses from using inventive AI, even in instances where it would be more effective than a person. Further, acknowledging AI as inventors would safeguard human moral rights because it would prevent people from receiving undeserved acknowledgment. Taking credit for an AI’s work would not be unfair to a machine, but it would diminish the accomplishments of people who have invented without using inventive AI. In addition, acknowledging AI as inventors would acknowledge AI developers, and it would reduce gamesmanship with patent offices. As with self-driving cars, AI would not own its patents. The AI’s owner should own patents on the AI’s inventions.
What about the argument we only protect the results of intellectual or mental activity, and the Copyright Office’s determination that AI does not think? That is not the right focus – whether and how an AI thinks simply should not matter. Congress realized that a functional standard for invention was needed back in the 1950s. Before then, courts

55
had devised a “Flash of Genius” test, which required that the inventive spark come to a person in a moment of clarity rather than as the result of methodical, laborious research. The nature of the test was never entirely clear, but it involved judges subjectively reasoning about what an applicant might have been thinking. Eventually people realized that trying to figure out what was going on in someone’s head was a terrible idea. It is hard to do, but more importantly it does not matter whether invention comes from Einstein or a room full of monkeys. What we care about is generating socially beneficial innovation. With AI, how an algorithm is designed and whether it thinks in a philosophical sense do not matter. Patentability should be based on the inventiveness of an AI’s output rather than on a clumsy anthropomorphism.
Still, the argument that the law only protect the results of intellectual or mental activity and the Copyright Office’s determination that AI does not think remain. That is not the right focus; whether and how AI thinks should not matter. Congress realized the need for a functional standard for invention back in the 1950s. Before then, courts used a “Flash of Genius” test, which required that the inventive spark come to a person in a moment of clarity rather than as the result of methodical, laborious research. The nature of the test was never entirely clear, but the process involved judges subjectively reasoning about what an applicant might have been thinking. Eventually people realized the Flash of Genius test was a terrible idea. More importantly, it should not matter to the law whether invention comes from Albert Einstein or a room full of monkeys. What society cares about is generating socially beneficial innovation, not how an AI designs an algorithm or whether it thinks in a philosophical sense. Inventiveness of an AI’s output rather than a clumsy anthropomorphism should guide intellectual property law.

56
AI is improving exponentially, and human researchers are not. This is exciting because it means that society will likely witness the same sort of phenomenon with inventive AI as with self-driving cars: vast improvements over human performance. When AI outperforms people, it will become the standard way that research is performed. Instead of Pfizer asking its research scientists whether a drug that treats one immune condition can treat another, or Exxon its chemists to design better catalysts, both companies will use, for example, DeepMind’s AI to complete the task. In patent law, the human standard often compared to the reasonable person standard is the “person having ordinary skill in the art,” or the skilled person. This hypothetical person represents the average worker in a field and serves as the benchmark for human behavior. The idea is that to receive a patent a person must accomplish something more than the average worker in a field would do. Therefore, if a person invents something that would be obvious to the skilled person, he cannot be granted a patent. If the invention is nonobvious, then he can be.
AI will change the definition of the skilled person. Since the skilled person reflects the average worker in a field of invention, the concept should change once the average worker is augmented by AI. At this point in time, the skilled person should become the skilled person using AI. This should raise the bar to patentability because AI augmentation will make average workers more sophisticated and knowledgeable – making more inventions obvious to workers. Once AI transitions from routinely augmenting to automating inventive work, the skilled person should become an inventive AI. This should further raise the bar to patentability because a the inventive AI of the future will more easily find inventions obvious. The bar will keep rising as machines continue to improve: DeepMind’s AI of 2040 will be outperformed by its AI of 2045. With no clear limit to the

57
sophistication of AI, it will be difficult for a human alone to come up with anything nonobvious. Eventually, everything will be obvious to a superintelligent AI. This may mean the end of the patent system, but there should not be cause for concern. Once superintelligent inventive AI is run-of-the-mill, future innovation will be self-sustaining, thus no longer requiring patents to incentivize new inventions.
5 Criminal
Similar to its role in invention, AI is already autonomously engaging in activity that would be criminal for a natural person. Further, AI can do so in a way that is untraceable, or irreducible, to the wrongful act of a person. In other words, in much the same way that some AI-generated inventions lack an actor that would traditionally qualify as an inventor, there are cases of AI-generated crimes where no natural person can be held criminally liable. Today, at least, nearly all crimes involving AI are likely to be reducible to human crime. Put simply, if a person strikes someone with a hammer, he has committed battery, not the hammer. Where an AI functions as a tool, even a very sophisticated one, crimes still involve an identifiable defendant causing harm. Even when AI causes unforeseeable harm this might still be reducible. An example would be the case in which an individual creates an AI to steal financial information, but due to a programming error, the AI shuts down an electrical grid thus disrupting hospital care. These are familiar problems for criminal law.
However, there may be times where it is difficult to reduce AI crime to an individual due to AI autonomy, complexity, or limited explainability. Such a case might involve several individuals contributing to the development of an AI over a long period of time, such as

58
with open-source software, where thousands of people can collaborate informally to create an AI. Another case in this category might feature an AI that develops in response to training with data. Attributing responsibility for an AI output where the machine has learned how to behave based on accessing millions or billions of data points from heterogenous sources seems downright impossible. Difficulty arises, too, when assigning fault to individuals where AI is concerned versus a conventional product such as a car, where one component is faulty. In fact, as a practical matter, to reduce an AI-generated harm to the actions of individuals might be impossible.
Even in cases where AI developers are known, an AI may end up causing harm without any unreasonable human behavior. Suppose, for example, two experienced and expert programmers separately contribute code for the software of an autonomous vehicle, but the two contributions unforeseeably interact in ways that cause the vehicle to deliberately collide with individuals wearing striped shirts. If this were the result of some not reasonably foreseeable interactions between the two programmers’ contributions, then presumably neither programmer would have criminal liability. Generally, to be criminally liable, an individual must intend a harmful outcome – or at least act recklessly or negligently. In a case where AI activity has, from the perspective of a reasonable person, unforeseeably caused harm, individuals would not generally face criminal liability. They may not even be civilly liable if their actions were not negligent under the tort standard.
Criminal law falls short here because of the possibility an autonomous entity could cause criminal sorts of harms without accountability. This is likely to become a more significant problem as AI continues to become more advanced, common, and independent. So what should be done with an AI that is functionally committing criminal acts in ways not

59
reducible to the wrongful acts of individuals? One solution is to hold the AI itself liable and to convict it of a crime. A small but growing number of policymakers, academics, and people who have not carefully thought about it are advancing such arguments - or arguing more broadly in favor of AI being directly granted rights, obligations, or even legal personhood. Direct criminal punishment of AI might seem to follow from the principle of AI legal neutrality, which cautions against different legal treatment of AI and people.
Intuitively, the idea of punishing AI seems incoherent. How could the basic principles of criminal law such as the requirement for a “guilty mind” apply to a machine? But as with robot taxes, criminal punishment of AI is not as ridiculous as it may first appear. We already criminally punish artificial persons in the form of corporations. Even though corporations do not literally possess mental states, they can directly face charges when their defective procedures cause harm, particularly where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals. We criminally punish strict liability offenses, acts not requiring any wrongful mental state such as intent to cause harm. Punishment can even be imposed on a failure to act. In sum, punishing an artificial person for failing to act, even without evidence of harmful intent, is not something that can be dismissed out of hand. Criminal law can – and, where corporations are involved, already does – appeal to elaborate legal fictions to provide a basis for punishing some artificial entities.
An AI is not a company, and under current legal frameworks an AI could not be held criminally liable - but laws can be changed. Consider that in 2017, the Kingdom of Saudi Arabia announced it was giving citizenship to a robot named Sophia which was made by Hansson Robotics. That was probably more of a publicity stunt than a bona fide act, but

60
there is no immutable legal principle that prohibits robot citizenship. In England, for example, parliament has sovereignty to pass any legislation it wants, and it is not bound by a written constitution. In fact, English law used to incorporate punishment of inanimate objects. As far back as the 11th Century, if personal property caused a person’s death, the property was deodand, forfeited as an accursed thing and given to God. The remedy of deodand was not formally abolished until an act of parliament in 1846. Other jurisdictions such as India have proven even more flexible. Indian law recognizes that animals, rivers, and even deities can have legal personhood.
But just because laws can change does not mean they should. Legal changes can entail significant costs, and inappropriate changes can undermine the rule of law and trust in the legal system. To answer the question of whether AI should be criminally liable requires a serious look at its costs and benefits, and whether the doctrinal and theoretical commitments of criminal law are consistent with imposing criminal convictions on AI.
Punishing AI could produce general deterrence by discouraging other potential offenders from committing crimes. Just as intellectual property rights for AI-generated works would not directly motivate an AI, the prospect of punishment for AI-generated crimes would not directly deter an AI. The intent would again be to impact the behaviour of AI developers, owners, or users. This could occur if punishment is coupled with confiscation or destruction of a valuable AI, or if punishment is combined with financial penalties directed at AI owners. In addition to deterrence, AI punishment could psychologically benefit victims of AI-generated crimes who would get to see the state affirm their rights and punish an entity that has caused them harm. It would reassure citizens that criminal activity, even by AI, will not be tolerated in society.

61
Punishment should also not violate deeply held principles of criminal law. With people, the most important constraints on the law focus on culpability (moral fault) and insist that it would be unjust to punish someone in excess of their blameworthiness. Thus, it would be wrong to punish someone innocent, or to punish someone severely for a minor crime, even if doing so resulted in social benefits. This is a reason why criminal convictions require not only a harmful act but also a “guilty mind” such as an intent to cause harm. Punishing someone without even a capacity for culpability is not appropriate, which is why we do not punish toddlers for antisocial behavior. They lack sufficient reasoning and decision-making capabilities to be morally blameworthy. The requirement for culpability suggests that AI might not be punishable because it lacks mental states and the capacity for culpability. An even more fundamental concern, of a similar nature, is the requirement for a crime to involve a voluntary act. As an AI is not conscious, it is not clear that it is capable of performing an act – merely of physically causing harms. A hurricane cannot perform an act, but can cause no shortage of harm.
There are solutions to these challenges. One option is the solution that has been applied to allow corporate punishment. Corporations do not literally have mental states, but the law allows mental states possessed by human agents of the company to be imputed to the company itself. So, if company officers choose to engage in an illegal price-fixing scheme, the company is deemed to have possessed the intent to engage in wrongdoing. In the case of AI, we could similarly impute mental states from AI owners, users, or developers. Although, that might be more difficult for AI than for a company, particular in the case of an AI-generated crime, because there is not necessarily a ready supply of culpable individuals. A company, unlike AI, is composed of people – it’s level of autonomy is

62
zero. A different solution is to note that criminal law does not always require culpability. For example, strict liability crimes do not require a particular mental state. Depending on the jurisdiction, sexual activity with a minor or child may be a crime regardless of whether someone reasonably believed them to be an adult. Strict liability crimes are disfavored because we do not want to punish people who have acted without culpability, but that is because it is unjust to treat someone as a means to an end without respecting them as an individual. The same constraint does not apply to AI because it does not experience punishment negatively and it does not possess human rights that would be violated. Although, even strict liability crimes retain the requirement for a voluntary act.
More ambitiously, we could decide that AI is indeed capable of acting, and that its decision making involves something analogous enough to a human mental state. Functionally, an AI can acquire and process information, engage in logic and reasoning to determine the best means of achieving a goal, and then act on the world in a way to increase the probability of that goal occurring. Whether that counts as a genuine act or mental state, we could at least treat it as such. As a practical matter, it may be difficult to reason about what a machine was thinking, but juries often lack direct knowledge of a defendant’s mental state and infer what someone was thinking based on their behaviour. Juries could make similar inferences about an AI’s knowledge, intent, and aims based on its behavior. For example, if a self-driving car runs someone over, it could be deemed to have an intent to cause harm if it repeatedly changed its course to target a moving pedestrian.
Even if there are benefits to AI punishment, and even if punishing AI would not violate any fundamental principle of criminal law, there are still costs to AI punishment. It would require significant legal changes, such as giving AI legal personhood, which is a level

63
of disruption that should be avoided without good cause. It could also send a troubling message that AI is morally on par with a person. Given that rights and obligations often go hand in hand, this could entrench the view that AI is morally deserving of rights. Just as the rights afforded to companies in the US have gradually increased over time, an AI eligible for punishment today could be an AI eligible to vote tomorrow. In addition, if AI punishment involves destruction of an AI this may end up harming innocent AI owners who have done nothing wrong themselves.
There are better responses to the prospect of AI-generated crime than direct AI punishment. One is to expand criminal or civil penalties directed at people. For example, new legal duties could be created to responsibly develop, supervise, or remain accountable for an AI, with liability for failing to discharge those duties. That would be liability based on human conduct rather than liability for the harmful conduct of the AI itself. Directly punishing AI owner, users, and developers would likely be a more effective way of influencing their behavior than indirectly via AI punishment. For that matter, expanding civil liability is probably a better response than expanding criminal liability. That is because we do not want to overly chill activities like AI development which generate social benefit, and because AI-generated crime has not yet been a significant problem. That may change in the future, but for now there are not a bunch of autonomous and irreducible robot criminals terrorizing society. With expanded civil liability, failing to responsibly design or supervise an AI that causes harm could result in a tort claim, or perhaps a civil enforcement action by a government agency. This is a solution to AI-generated crime would more effectively provide the same benefits as direct AI punishment without the significant costs.

64
Punishment of AI should therefore be avoided – not because it is incompatible with criminal law, but just because it is a bad idea.
This analysis does more than argue against direct punishment of AI, it is illuminating about criminal law theory. In showing that a coherent theoretical case can be constructed for AI punishment, it shows that criminal law principles are flexible. It also sheds new light on justifications for corporate and human punishment. For example, some philosophers argue that people who engage in antisocial behavior are not morally blameworthy and therefore that punishment cannot be justified on the basis of culpability. There is a philosophical theory that holds that everything we do has been predetermined (determinism). If you were able to identically recreate the exact conditions of your life – from your genetics to your birth environment to the opportunities you encounter, you would only ever make the same choices. Some proponents of determinism further believe that because we will only ever make a single set of choices, that free will does not exist (hard determinism). If these theories are correct, and no one truly has free will because everything is predetermined, then it calls into question the role of culpability in criminal law. If someone lacks free will, then how can they be morally faulted for bad behavior?
Thinking about AI punishment helps to answer this concern. AI is capable of the same sorts of antisocial behaviors we are concerned about in people, but without moral blameworthiness. AI behaves deterministically and without free will – it simply executes programming. Even if a programmer cannot predict how an AI will behave in advance, or explain it afterwards, that is merely a matter of complexity and lack of knowledge. What the case for punishing AI may illuminate for human punishment is that criminal law is less concerned with moral blameworthiness than antisocial behavior. Criminal law is

65
concerned about something more functional than whether someone is deep down a bad person – it is concerned with behaviors that manifest a lack of respect for protected values. That helps us to better understand why we punish and whether punishment is justified, and it pushes back against claims that punishments are not justified because people lack free will. As a practical matter, this suggests that the law should do less to criminalize bad motives than public manifestations of antisocial behavior. It undermines support for so- called “thought crimes”, such as the United Kingdom’s Terrorism Act of 2006 which criminalizes “engaging in any conduct in preparation for giving effect to [a terroristic] intention...” In effect, this makes just about any activity illegal if someone has the requisite mental state. Laws like this should be disfavored because criminal law should be less concerned about whether someone is a bad person and more about the disrespect for social values that criminal conduct publicly manifests. Ultimately, the most interesting thing about AI punishment may be what it can teach us about ourselves.
6 Regulating Artificial Intelligence
AI considered in the context of tax, tort, intellectual property, and criminal law provides insights into how AI will affect existing legal standards and how legal standards will shape AI development. AI promises to be highly disruptive – and if history is any guide – in unexpected ways. Perhaps in hindsight AI will prove to have merely been part of another industrial revolution. Nevertheless, our legal system has not historically done the best job of limiting harm caused by technological disruption. Laws already regulate AI, just not intelligently because they were not designed with AI in mind. A different approach to our

66
legal frameworks could help optimize AI’s social benefits and ensure those benefits do not go to the few at the expense of the many. We are at the beginning of this process and the principles used to develop a legal system need to be, if not rethought, thoughtfully retooled with respect to AI before events overtake society. This has to be done the right way – laws should not change just because they can. For example, citizenship for the robot Sophia creates some obvious legal and ethical problems such as whether a robot citizen could vote, be owned, or murdered. The principles we have used to develop a legal system need to be, if not rethought, then at least retooled in respect to AI. We are at the beginning of this process, and we need to think about where to go before we are overtaken by events.
DeepMind’s Go win was a big deal, no less a milestone than IBM’s Deep Blue defeat of world champion Gary Kasparov at chess in 1997. Consider that it was Deep Blue that beat Kasparov, not Deep Blue’s programmers. Even playing against him as a team, the programmers would not have stood a chance. Instead, the programmers created an autonomous entity that engaged in an activity beyond their own capabilities. At that time, Deep Blue was one of the world’s most powerful supercomputers, capable of evaluating 200 million chess positions per second. Today, chess programs running on everyday smartphones can beat the world’s best human players.
After his match with Deep Blue, Kasparov had a realization: a person and an AI could play chess collaboratively and complement one another. A year later, he won the first “centaur” chess tournament where a human player and an AI play as a team. Not surprisingly, a person augmented by an AI proved better than someone playing unassisted. But a person and an AI also outperformed an AI playing by itself. Human grandmasters are good at long-term chess strategy but poor at quickly calculating millions of possible moves.

67
The reverse is true for chess-playing AI. Because people and AI are strong on different dimensions, they can do better working together than independently. At least, Kasparov was right to a point. In 2017, the same year AlphaGo beat the world’s best human Go player, the chess engine Cryptic beat the best human-AI team.10 Eventually, people may just get in the way.
 10 Emails with Arno Nickel, InfinityChess General Manager (Nov. 7 & 10, 2019) (on file with author).

68
1
Understanding Artificial Intelligence
Civilization advances by extending the number of important operations we can perform without thinking about them.
- Alfred North Whitehead
1 In the Beginning
The concept of AI has ancient origins. Around the eighth century BCE, the Greek poet Homer wrote in the Iliad about Hephaestus, the god of fire and a skilled inventor. Hephaestus built golden automata, or self-operating machines, to help him work. Not only did Hephaestus build attendants for himself with “intelligence in their hearts” and the “appearance [of] living young women,”1 he also built autonomous vehicles that could travel to and from the home of the gods and a lethal autonomous weapon system named Talos that patrolled the beaches of Crete. By contrast, Amazon’s assistant Alexa, Tesla’s Autopilot system, and the Russian Federation’s military robot FEDOR seem quaint, although Homer was vague on how mere mortals could enable such constructs.
Stories of artificial beings endowed with humanlike intelligence have pervaded countless histories and cultures. The most famous golem narrative in the Jewish tradition involves Judah ben Loew, the late-sixteenth-century chief rabbi of Prague, who is said to have fashioned a humanlike figure from clay and brought it to life with rituals and
1 HOMER, THE ILIAD OF HOMER (Richmond Lattimore trans., University of Chicago Press 1951).
    
69
incantations. This golem helped to defend against anti-Semitic attacks and pogroms, and it even performed household chores. There are many versions of the story. But much like the enchanted broom in Goethe’s “The Sorcerer’s Apprentice,” the golem experiment does not end well – often with a murderous rampage. Creators faring poorly is a recurrent theme in such origin stories. Prometheus, who created humankind in Greek mythology and passed along the technology of the gods, was condemned to suffer eternal torture. Mary Shelley’s 1818 story of Frankenstein was about a scientist dedicated to making artificial life, only to later reject his creation. The “monster” ultimately torments and destroys its maker.
Not all historical AI was myth or fiction; early automata could be quite sophisticated. Mechanists in ancient Greece constructed vending machines, water mills, and perhaps even the world’s first mechanical computer used to predict astrological movements – the Antikythera mechanism. Hellenic Egypt had statutes of gods that could speak, move, and even prophesize (although their predictive value is unknown). One of the great Roman engineers, Hero of Alexandria, wrote a treatise called “Pneumatica” that describes how to build numerous machines powered by air, steam, and water pressure. This text was used by engineers until early modern times.
Our incomplete historical record suggests the Antikythera mechanism may have been one of many ancient computers – at least mechanical computers. Originally, the term “computer” was used to refer to a person who manually performed mathematical calculations. Human computers were once critical to navigation, science, and engineering, but they left much to be desired. People as computers were slow and error prone, which could be a fatal flaw when bad math ran a ship aground. In the early nineteenth century, apparently while watching human computers at work, Charles Babbage decided to

70
automate the process. Perhaps at some earlier point a Greek machinist had a similar insight watching human astrologers at work, which led to the invention of the Antikythera mechanism. Astrological calculations were as important to the ancient Greeks as navigational calculations were to the Victorian English. Ironically, an Antikythera mechanism was found at the bottom of the sea in 1902.
But, back to Babbage. After deciding to create a mechanical computer, Babbage began by designing an automatic calculator he called the “difference engine.” This machine would have created tables of values by finding the common difference between terms in a sequence, and in turn this could have been used to generate logarithmic tables and trigonometric functions as well as simple price lists for merchants. Babbage completed various designs but never the actual machine. At that time, its building would have been a herculean task – the machine would have weighed about fifteen tons and contained around 25,000 parts. Other obstacles included the fact that Babbage was perpetually running out of funds and getting distracted by side projects, which included campaigning against street noise and perfecting an “infallible” system for gambling on horse racing that proved fallible.
While the difference engine may have been the first modern automatic calculator, its most important contribution may have been to inspire Babbage with a grander distraction: the analytical engine, the first general-purpose computer. Unlike earlier machines, the analytical engine could have carried out a wide range of operations using programs contained on punch cards. Ada Lovelace, the English mathematician and writer who sponsored Babbage, developed an algorithm that would have allowed the machine to generate a sequence of Bernoulli numbers, which have an important role in mathematics,

71
essentially making her the first computer programmer. Lovelace may have been the first to recognize the machine’s potential beyond pure calculation.
Babbage continued to design various iterations of the analytical engine until his death in 1871, but he never actually built the computer. Babbage’s inability to complete his work, combined with his anger at the British government for what he believed was inadequate support, left him embittered and disappointed at his end. He was appreciated – although controversial – in his time, but the true extent of his genius was not recognized until later scientists realized that the analytical engine had anticipated almost every aspect of present-day computers. Today, Babbage is considered the ”father of computing,” even though the first fully functional general-purpose computer, the Electronic Numerical Integrator and Calculator (ENIAC), was not completed until 1946. ENIAC weighed 30 tons and took up around 1,800 square feet. While scant comfort to Babbage, after his death two versions of the difference engine were successfully constructed and operated. Efforts are currently underway to build his analytical engine.
Computers or not, the fascination with automata, some practical and some less so, has continued from ancient to modern times. Hundreds of years before Babbage, Leonardo Da Vinci had constructed what may have been the first humanoid robot in Western civilization – Leonardo’s mechanical knight. It could sit, stand, raise its visor, and maneuver its arms by means of a series of pulleys and cables. Leonardo created a “robot” before that term existed. The word was coined in a 1920 play R.U.R. (an acronym for Rossum’s Universal Robots) by Czech playwright Karel Capek, who based it on the Czech word for “forced labor.” The play takes place around the year 2000 in a world in which robots are cheap, ubiquitous, and indispensable. These factory-built constructs are capable of

72
independent thought and are creatures of flesh and blood rather than gears and pulleys. Alas, the robots’ creators fare about as well as the golem’s. In Capek’s play, the robots initiate a rebellion that more or less exterminates humanity.
Even early in the twentieth century, long before modern computers made the visions of The Terminator and The Matrix appear plausible, the themes in R.U.R. had widespread appeal. By 1923, the play had been translated into thirty languages, and the word “robot” had entered the English lexicon. Incidentally, the first reported robot fatality occurred on the fifty-eighth anniversary of R.U.R.’s premiere. In 1979, Robert William, a Ford assembly worker, was killed when a robot’s arm struck him in the head. In subsequent litigation, a jury found this accident was due to a lack of safety measures and awarded William’s estate $10 million in damages.
Killer robots and the AI apocalypse continued as recurring themes after R.U.R. One of the most famous science fiction writers of the twentieth century, Isaac Asimov, writes extensively about a future filled with AI. He proposes a set of ethical rules he calls the Three Laws of Robotics, which are hardwired into the “positronic brains” of robots and prevent them from turning against their creators:
1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.2
2 Isaac Asimov, Runaround, in I, ROBOT 40 (Isaac Asimov Collection ed., Doubleday 1950).
     
73
Many of Asimov’s own stories explore how these deceptively simple rules can result in unexpected and sometimes destructive outcomes. Sometimes, both action and inaction result in harm, and robots may need to cause a lesser harm to prevent a greater one. In Asimov’s later stories, some robots come to understand that the best way to prevent harm is for them to rule over humanity. People, left to their own devices, are perpetually harming themselves and others.
The Birth of Modern AI
In 1956, a decade after ENIAC, the term “artificial intelligence” was coined by John McCarthy, another pioneering AI researcher. McCarthy organized the Dartmouth Artificial Intelligence conference, which is often credited with establishing AI as a research discipline. In his proposal for the event, he defined AI as follows: “For the present purpose the artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.”3 The conference proceeded “on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.”
Since the 1956 conference, AI, as both a research discipline and practical technology, has seen its share of ups and downs. In the 1950s and 1960s, optimism prevailed that machines would soon exhibit, and exceed, human levels of intelligence.
3 J.   ,M.L.   ,N.   ,&C.E. ,AProposalfortheDartmouth , Aug. 31, 1955, www-
formal.stanford.edu/jmc/history/dartmouth/dartmouth.html.
   McCarthy
Minsky
Rochester
Shannon
 Summer Research Project on Artificial Intelligence
  
74
Computers developed the capabilities to solve algebraic problems, competitively play games like checkers, and speak English. In the 1970s, that optimism faded when promised results failed to materialize quickly enough and funding dried up. This downturn came to be known as the First AI Winter. It ended in the 1980s as “expert system”–based AI, which solves problems using logical rules derived from the knowledge of experts, enjoyed some significant successes. But in the 1990s, a sense again emerged that the capabilities of AI had been oversold, and another period of decreased interest and funding took hold – the Second AI Winter. Some researchers during this time even took to calling their work “machine intelligence,” “informatics,” or “knowledge-based systems” to avoid association with AI.
The AI winters have passed, but plenty of hype remains. Hype has been another recurring feature of the AI narrative. Hundreds of years before Deep Blue defeated Gary Kasparov at chess, Wolfgang Von Kempelen created the world’s first chess-playing machine, the Turk. After showcasing the Turk to Austria-Hungarian Empress Maria Theresa, the machine became a European sensation, performing for, and perhaps defeating, the likes of Napoleon Bonaparte, Benjamin Franklin, and Charles Babbage. The Turk was an ingenious feat of engineering, but only for the machine’s ability to secretly house a human player rather than any innate chess-playing skill. Eventually, the Turk was discredited, then consigned to storage where it was later destroyed in a fire. Some overhyped automatons have had even less happy endings. In the early 1930s, an apparently autonomous vehicle called the “Phantom Auto” toured the country and amazed the public. The tour ended abruptly in 1932 when the vehicle injured ten pedestrians. The Phantom Auto turned out to be a remotely operated Ford Tin Lizzy, and its operators were arrested.

75
Today, experts remain divided on whether AI has finally reached a point of runaway progress or whether winter is coming. Some thought leaders, such as Ray Kurzweil, one of Google’s directors of engineering, predict computers will have human levels of intelligence in about a decade. On the other hand, in 2018 a robot touted as the most advanced machine ever created in Russia turned out to be a man in a suit.4
2 Defining AI
Intelligence is whatever hasn’t been done yet.
- Larry Tesler
More than 60 years after the term was introduced, AI still has no well-accepted definition. This book will discuss AI using the following definition: an algorithm or machine capable of completing tasks that would otherwise require cognition. Cognition refers to mental capabilities and the process of acquiring knowledge and understanding through thought. This is a deliberately broad definition of AI that focuses on what it does rather than how it is designed. The perhaps most popular textbook on the subject defines AI as “the designing and building of intelligent agents that receive percepts from the environment and take actions that affect that environment.”5 Many modern AI definitions retain McCarthy’s original functional emphasis (and circular logic) – a machine that completes tasks traditionally requiring human “intelligence.”
  Andrew
 Griffin
, “Russia’s Most Modern Robot” Revealed to Be Just a Person in a Suit, THE
4
INDEPENDENT, Dec. 12, 2018, www.independent.co.uk/life-style/gadgets-and-
 tech/news/russia-robot-person-in-suit-fake-hoax-most-modern-advanced-a8680271.html.
   5
& PETER   ,   (3rd ed. 2009).
STUART
RUSSELL
NORVIG
ARTIFICIAL INTELLIGENCE: A MODERN APPROACH

76
An algorithm is a set of mathematical instructions or rules, which in the form of a software program can instruct computer hardware, the physical components of a machine, to perform specific tasks. Each program is essentially a step-by-step instruction guide, like a cooking recipe, that provides a set of ordered operations to the computer. Hardware runs or executes algorithms, though an algorithm developed and readable by a person is rarely directly usable by a computer. An algorithm is usually written in a high-level programming language (source code) because these are close to natural language. The source code is then usually translated to machine language (object code). Machine language consists of binary values (0s and 1s) that provide processor instructions that change a computer’s state. To execute an algorithm, a computer needs instructions in a machine language and a hardware processor such as a central processing unit. For example, an instruction may change a stored value somewhere in memory or ask the operating system to print text to a monitor. An analogy has been drawn between software and hardware on the one hand, and mind and body on the other.
Modern computers have billions of transistors, which can be used as electronic switches. They exist, like binary code, in either an “on” (1) or “off” (0) state. An algorithm can accomplish a task as simple as flipping a single switch, which corresponds to changing one bit of information. AI can engage in simple logical reasoning by changing transistors in response to other transistors; for example, “if transistor X turns on, then transistors Y and Z turn on.” Algorithms can engage in complex logical reasoning by combining multiple operations and flipping switches up to billions of times per second, the acts of which become responsible for everything a computer can do, from translating English to Zulu, modeling a new chemical, or playing a game of backgammon.

77
That is algorithms at their most basic, but defining “AI” begs a definition of “artificial” and “intelligence.” Of the two, artificial seems more straightforward – something made by people rather than nature. Today, at least, it is possible to distinguish natural from artificial, even if the distinction shows signs of blurring. Researchers have created biological computers that use natural proteins and DNA to perform calculations involving the storing, retrieving, and processing of data. As this technology develops, it may be possible to engineer biological systems that have the functional capabilities of mechanical computers. Not content with making machines out of natural building blocks, researchers are also hard at work trying to upgrade people. Genetically modified “designer” babies have already been created using technologies like CRISPR, mitochondrial replacement, and in vitro gametogenesis. Elon Musk has launched a venture, Neuralink, that is developing a brain-computer interface so that people can be more competitive with AI. That technology is still in its infancy, but people are already being implanted with an increasingly sophisticated range of medical devices.
Intelligence is harder to define than artificial, or at least to make sense of it in the context of AI. Shane Legg and Marcus Hutter surveyed a number of prominent informal definitions of intelligence, and they argue that intelligence is commonly defined in terms of “an agent’s ability to achieve goals in a wide range of environments.”6 Intelligence is a feature commonly associated with people, sometimes even considered their defining characteristic. More than two thousand years ago, the philosopher Aristotle essentially argued that human intellect – the capacity to make rational decisions – separates people
   6 17
Legg &     , Universal Intelligence: A Definition of Machine Intelligence, 391, 444 (2007).
Shane
Marcus
Hutter
 MINDS AND MACHINES

78
from other animals. Modern philosophers (and translators) are still arguing about what he really meant.
The French philosopher René Descartes believes people are guided by an immaterial, or spiritual, mind while the rest of nature, including animals, is nothing more than a series of mindless objects driven inexorably by the laws of physics. He thus thought it would be possible to build an automaton indistinguishable from an animal but not from a person. A machine, he argues, can never use words or put together signs in the way we “declare our thoughts to others.”7 Even if it could give a poor imitation of speech, it could certainly not give an “appropriately meaningful answer to what is said in its presence, as the dullest of men can do.”8 Descartes did not appeal to ethereal qualities like a soul or emotion to distinguish people from machines and animals, but rather to communication and reasoning abilities already achieved by today’s AI.
People are generally assumed to be smarter than other animals, but when a person lacks intelligence we do not consider them less human, and other animals are capable of some fairly intelligent behavior. Chimpanzees can outperform some human players at some speed and memory-based games, octopuses use tools and socialize, dolphins have unique names and sophisticated means of communication, and elephants exhibit empathy. Koko the gorilla famously learned a modified version of American Sign Language, understood around 2,000 English words in addition to the signs, and named and adopted a kitten. She passed away in 2018. But in her youth, she scored between 70 and 90 on intelligence
7 René , , in 1 109, , ,& trans.,
1985). 8 Id. at Part V.
    Descartes
Discourse on Method
THE PHILOSOPHICAL WRITINGS OF DESCARTES
       109–151
(John
Cottingham
Robert
Stoothoff
Dugald
Murdoch
Cambridge
 University Press

79
quotient (IQ) tests. A significant number of people score in Koko’s range, and a score of 100 is the median. There is no reason Koko and a person of Koko-like knowledge and sophistication should not both qualify as intelligent. Charles Darwin similarly thought human intelligence was not different from other animal intelligence in kind, merely in degree.
Whether a machine with similar capabilities should qualify as intelligent is more controversial. People have attempted to subject AI to modified IQ tests, and a study in 2013 resulted in press coverage that claimed the smartest computers were as intelligent as four- year-old children. That study showed AI performing relatively well on the verbal portion of an IQ test and poorly on the comprehension portion. While AI should be good at memorizing words, an excellent vocabulary combined with poor comprehension does not equal a toddler with common sense. Comparing people and machines along a single dimension of intelligence has limited meaning. Google Translate can interpret more than a hundred languages almost instantly, but that is it. While faster and more versatile at translation than any imaginable person, it cannot write music, play poker, or have an existential crisis. AI like Google Translate is clearly different in kind from human intelligence.
In 1904, the psychologist Charles Spearman advanced the concept of “general intelligence.” He argues that people have a single general intelligence that determines cognitive performance in addition to narrow task-specific abilities. He was aware that people perform better at some tasks than others but found that people who do well in one area tend to do well in many domains. Someone good at math is more likely to have a strong vocabulary than someone with poor math skills. Modern advocates of human

80
general intelligence are more likely to subscribe to the belief that IQ scores are a valid measure of intelligence. Critics argue that the concept of general intelligence is not supported by evidence and devalues other important abilities.
Today’s machines lack general intelligence – all modern AI is narrow or specific. It focuses on discrete problems or works in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms nor can Chef Watson run a kitchen. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve. By contrast, one of the earliest goals of AI development has been the creation of general AI that would be able to successfully perform any intellectual task a person could.
General AI could even be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence – what philosopher Nick Bostrom terms artificial superintelligence. Artificial superintelligence could then innovate in all areas, resulting in progress at an incomprehensible rate. Such an outcome has also been referred to as the intelligence explosion or the technological singularity. This idea has been popularized in recent years, but it is an old concept. As the AI pioneer Irving John Good wrote in 1965, “the first ultraintelligent machine is the last invention that man need ever make.”9
    9
Good, , in ADVANCES IN 33,   (Ser. No. 6, 1965).
Irving J.
Speculations Concerning the First Ultraintelligent Machine
 COMPUTERS
31–87

81
Experts are greatly divided on when, and if, general AI will be developed. Many industry leaders predict based on historical trends that general AI will exist within the next couple of decades. Others believe the magnitude of the challenge has been underestimated, and that general AI will not be developed in the twenty-first century. While there are numerous conflicting predictions, in 2013 hundreds of AI experts were surveyed on their predictions for artificial general intelligence development.10 On average, participants predicted a 10 percent likelihood that general AI would exist by 2022, a 50 percent likelihood it would exist by 2040, and a 90 percent likelihood it would exist by 2075. In a similar survey, 42 percent of participants predicted general AI would exist by 2030, and another 25 percent predicted by 2050.11 In addition, 10 percent of participants reported they believed superintelligent AI would develop within two years of general artificial intelligence, and 75 percent predicted this would occur within thirty years.
3 Can AI Think?
Recently there has been a good deal of news about strange giant machines that can handle information with vast speed and skill ... . These machines are similar to what a brain would be if it were made of hardware and wire instead of flesh and nerves ... . A machine can handle information; it can calculate, conclude, and choose; it can perform reasonable operations with information. A machine, therefore, can think.
-
Edmund Berkeley (1949)
553, (Vincent C.
     Vincent C.
10
, in Müller ed., 2016).
&Nick ,
Müller
Bostrom
Future Progress in Artificial Intelligence: A Survey of
   Expert Opinion
FUNDAMENTAL ISSUES OF ARTIFICIAL INTELLIGENCE
553–571
  11 See JAMES BARRAT, OUR FINAL INVENTION: ARTIFICIAL INTELLIGENCE AND THE END OF THE HUMAN ERA 152 (2013).

82
In 1950, Alan Turing, the computer scientist who launched and inspired much of AI, published a paper in which he “propose[s] to consider the question, ‘Can machines think?’”12 He proceeds to dismiss inquiries about whether a machine understands what it is doing, and to reframe the question to ask whether a computer could functionally imitate a person. Turing then argues that a machine would functionally think if it could pass an “imitation game” in which a computer would have to successfully pretend to be a person to a suspicious judge. Turing predicted computers would have enough storage capacity to pass what has subsequently been named the Turing test, in about fifty years, that is, by the year 2000.
Another school of thought about AI says that intelligence requires understanding. In this view, machines do not qualify as intelligent regardless of what they can do because they do not understand what they are doing. Action without understanding merely simulates intelligence, even for a superintelligent AI. Instead of specific and general AI, philosopher John Searle puts the distinction between acting and understanding in terms of strong (thinking) and weak (unaware) AI. To illustrate, consider Searle’s Chinese room thought experiment. First, assume you have no Chinese language proficiency. Now, imagine you are kept in a room where a message in Chinese characters is presented to you through a small opening in the wall by a native Chinese speaker. You cannot understand the meaning of the characters, but you have a book that contains every possible combination of Chinese characters, together with a corresponding appropriate response. You look up the characters you have been provided in the book, you transcribe the appropriate response onto a piece of paper (assume you can figure out how to write Chinese characters), and you
  Alan M.
 Turing
 12
, Computing Machinery and Intelligence, 59 MIND 433, 433 (1950).

83
pass your response back through the opening. The person who sent you the initial message does receive an appropriate response. Functionally, you would be mimicking Chinese communication like a native speaker, albeit very slowly. However, you would not understand the meaning of the message you received or your response. This is a model for thinking about AI. Substitute Google Translate for the Chinese room, and you have a faster, and less accurate, version of the same phenomenon. Google Translate may pass the Turing test, but it is weak AI – it lacks semantic understanding.
Part of the challenge with arguing machines that cannot think is that human thought remains incompletely understood. For generations, there was little empirical evidence about the nature of thought, which was the domain of philosophers, theologians, and poets who could endlessly debate the subject in the absence of definitive proof. Eventually, neurobiologists wedged their way into the debate, some of whom argue that thought is a physical phenomenon that inevitably results from biological systems like a person’s higher nervous system. If so, a living being similar to a person, if not a dolphin or a primate, would experience something close to human consciousness. For that matter, if the mind is no more than the sum of a biological system, there is no reason why it should not be possible to re-create the same phenomenon with a machine – or for a machine to improve upon human thought. In this view, a person is simply a machine, albeit a phenomenally complex one – a “meat machine,” as cognitive scientist Martin Minsky eloquently put it. This mind- machine view conflicts with the beliefs of a long line of philosophers reaching back to Plato who believed that the human mind or soul is distinct from the physical body. Descartes similarly thought the mind and body are distinct in substance and nature, and that while

84
the body can be divided, the mind is indivisible, like the concept of a soul in Christian theology.
More recently, philosopher David Chalmers weighs in on the mind-body issue to distinguish between the easy and hard problems of consciousness. The easy problems include explaining the ability to focus attention, discriminate, integrate information, and so forth. These are problems with mechanistic solutions, and neurobiologists and computer scientists are quickly making progress toward understanding the human nervous system and developing comparable AI capabilities. The hard problem of consciousness refers to why people have phenomenal experiences, which is to say the sensation of internal states. Mechanistic explanations seem unable to explain what it is like to see red, hear music, or feel cold. The existence of phenomenal consciousness in a material world remains a metaphysical puzzle.
We are far away from fully understanding consciousness, but there are things we do understand. For instance, consciousness can be localized, at least to a degree, within the central nervous system. The central nervous system is made up of the brain and spinal cord, and the brain has three major parts. The cerebrum, consisting of a right and left hemisphere, is the largest and most evolved part of the brain and is responsible for higher functions like speech, reasoning, learning, and hearing; the cerebellum coordinates muscle movements and maintains posture and balance; and the brainstem connects the cerebrum and cerebellum to the spinal cord and regulates things like body temperature, breathing, and heart rate.
People who have lost their spinal cords and even suffered massive injuries to their cerebellums and brainstems can still retain normal consciousness. This suggests that

85
consciousness is generated in the cerebrum. Consciousness can even be localized further within the cerebrum. People who have had large portions of their prefrontal cortex (the part of the cerebrum nearest the forehead) removed or destroyed often do not experience any effect on their conscious experience. However, the removal of even small parts of the posterior cortex can result in dramatic losses of conscious content – the inability to see or recognize motion, color, or space.
The cerebrum has a very different neural architecture than the cerebellum. The cerebrum has a relatively less dense concentration of neurons that are highly connected in networks and activate in complex feedback loops. By contrast, the cerebellum has a higher concentration of neurons but in relatively uniform and parallel structures where groups of neurons function independently in a feed-forward circuit (one set of neurons activates the next). If a person is merely a biological machine, then the cerebrum’s structure is likely a key part of the consciousness puzzle, or even the key to future machines with self- awareness.
But what has all this done to resolve the question of whether AI thinks? On one level the question can be addressed by noting that AI does not think the way a person does. AI is not conscious or self-aware in the same sense as a person is. On another level, the question can be addressed by noting that it remains more relevant to philosophers than to policymakers. The reason for this is that the social benefits of AI are based on what AI can functionally accomplish, regardless of how it processes information. AI does not need to think, mimic human thought processes, or exhibit semantic understanding (essentially, perceiving the meaning of its actions). It simply needs to automate human intellectual activity. AI does not need to think – just do.

86
4 Types of AI
There are many ways to design AI with various techniques, systems, and methodologies. Over time, these have turned into subfields based on specific goals, such as robotics or machine-learning, or specific tools, such as logic or neural networks. Divisions have also arisen over philosophical differences and conflicts between institutions and researchers. Two of the major AI models are symbolic and classical, what philosopher John Haugeland calls good old-fashioned AI (GOFAI), which encodes a model of a problem and processes input data according to the model to provide a solution, and connectionist systems such as artificial neural networks in which users do not explicitly program rules but allow the network to discover rules from training data.
Different types of AI have their own strengths and weaknesses. Symbolic systems, for example, are more explainable, making it is easier to decipher how an AI works. This may be particularly important for applications where it is necessary to understand how an AI transforms input to output. Programming symbolic AI is about creating rules for software to follow. If a specific event occurs, then a particular action will be performed. Some AI pioneers like Allen Newell and Herbert Simon believed that such rules govern human psychology.
Symbolic AI, such as Arthur Samuel’s checkers player in the late 1950s, has accomplished great feats. That program learned to beat its programmer and gave an early hint that computers would develop superhuman performance. About forty years ago, a symbolic rule-based system outperformed the leading human expert in soybean disease diagnosis and selected appropriate treatment. In 1972, another symbolic AI called MYCIN

87
was created to advise physicians on identifying infectious diseases and prescribing antibiotics, and it was able to outperform infectious disease experts in limited circumstances.
As early as the 1956 Dartmouth conference, Newell and Simon presented an AI system they called “Logic Theorist,” which approximated a model simulating human behavior relying on heuristics. Heuristics refers to rules of thumb that people use to help make decisions, or general methods of solving programs that make brute-force searching through an entire space unnecessary. The Logic Theorist attempted to prove a theorem by guessing a solution and then endeavoring to demonstrate the guess was correct. It independently proved some of the theorems in Principia Mathematica, a foundational mathematics text, and even found a more elegant proof to one of the theorems than was found in the text. If logic and consistency are the defining characteristic of human thinking, as Aristotle argues, the Logic Theorist may have received a passing grade. Convincing the establishment was another matter, however, as the Journal of Symbolic Logic refused to publish the proof in an article with the Logic Theorist listed as an author.13
Without heuristics, problem-solving approaches may rely on brute-force computational methods, such as generating all possible combinations of symbols to prove a mathematical theorem. Newell and Simon referred to this as the British Museum algorithm – a riff on the infinite monkey theorem, which predicts that a room full of monkeys with typewriters will eventually reproduce by random chance all the books in the British Museum. That may be theoretically possible, but imagine you have set a monkey to the task of typing the word “banana” on a typewriter with fifty keys. If the monkey presses keys at
13 HERBERT A. SIMON, MODELS OF MY LIFE (MIT Press 1996).
    
88
random, and each key has an equal chance of being pressed, the chance the first six letters pressed will spell banana is (1/50)6 (one in fifteen trillion, six hundred and twenty-five billion). Thus, the probability of giving a monkey a typewriter and having it type banana is more than zero, but you would be lucky to have it happen if you tried all day, every day, for the rest of the monkey’s natural life. By the time you start having monkeys try to re-create the complete works of William Shakespeare without errors, assuming you had earlier discovered the secret to immortality and applied it to both yourself and the monkeys, you and the monkeys would probably be hard at work at the end of the universe. To drive the point home, someone actually performed an experiment in which a computer keyboard was left in an enclosure of six Celebes crested macaques in Paignton Zoo in England. Over the course of a month, the monkeys produced about five pages of writing that largely consisted of the letter “S.”
Thankfully, computers are better than monkeys at this sort of work, yet even the most powerful computers are limited when dealing with sufficiently large numbers. Checkers has fairly simple rules, but a staggering number of possible moves – some 5020 possibilities. It is hard to think about numbers that large, but computer scientist Jonathan Schaeffer compares it to draining the Pacific Ocean and refilling it with a tiny cup, one cup at a time. Schaeffer spent decades developing an AI called Chinook that eventually solved checkers in 2007. Chess is more complex – there are about 10120 possible board configurations, which puts a solution beyond the limits of feasible technology. Deep Blue could only generate every possible move for the next eight moves, and both checkers and chess are nothing like Go, which has 10172 possible board configurations. While brute-force

89
computation can work very well with problems that have a limited number of possible solutions, it is less useful for others.
While symbolic systems rely on explicitly programmed rules, connectionist systems have a different means of generating intelligent behavior. They represent rules in interconnected networks of simple and even uniform units such as artificial neurons. Many of the most dramatic advances achieved by AI in recent years have come from improvements in connectionist techniques such as machine-learning, artificial neural networks, and deep learning. Machine-learning allows computers to learn from examples and to generate their own rules. These rules may be more accurate than rules explicitly created by programmers, particularly where the rules are based on large datasets and complex patterns that are difficult for people to directly interpret. This is particularly valuable for pattern recognition, statistical modeling, and data mining, which is useful for tasks such as generating Internet search results and using facial recognition technology.
ANNs are one set of algorithms used in machine-learning that are inspired by the architecture of the human brain. Each person has about 90 billion individual neurons in their brains, which are interconnected in highly complex networks with perhaps hundreds of trillions of connections. These connections change dynamically, or the strength of connections change, as a result of learning and experience. Similarly, ANNs consist of networks of interconnected layers of algorithms that feed data into each other and that can be trained to complete tasks by altering the relevance of data as they pass between layers. During training, the weights attached to different inputs change until the network produces a desired output, at which point the network has developed the ability to complete a task. For instance, with a facial recognition AI like TrueFace.ai, the algorithm trains on a large

90
data set of faces, with various lighting, angles, and distances from a camera, and the program learns how to measure and process specific points on faces and to identify individuals. Deep learning involves passing data through many layers of ANNs. As the networks process data, connections between parts of the networks adjust, developing an ability to interpret future data. Deep neural networks are responsible for many of the recent advances in speech recognition and computer vision.
Connectionist AI can be tremendously complex with difficult to understand and explain inner workings. For this reason, such systems in particular are sometimes referred to as black box algorithms. In theory, it is usually possible to explain what these systems are doing, but it may not be practical. Connectionist systems may be more useful for analyzing complex models where underlying relationships are poorly understood and where an AI may be necessary to discover new patterns. This may be especially valuable in situations where programmers do not know all the rules for generating desired output, where the AI may learn rules not easily interpreted by programmers, or where the rules are very complex and hard to program directly. This allows AI not only to solve problems we know the answers to but also problems we do not – perhaps even to solve problems we do not know exist.
5 Advances in AI
While the vision of general AI has yet to manifest, AI has improved dramatically since the 1950s and achieved some of its early goals. AI can outperform people at a growing number of specific tasks, and it is now being applied successfully in nearly all areas of industry. This

91
is largely due to three parallel improvements: more advanced software, greater computing power, and the development of big data.
AI advances move quickly, and the improvements take less and less time with each AI incarnation. Deep Blue is ten million times faster than the first machine used to play chess in 1951 and is capable of evaluating more than 200 million moves per second. The machine, which awed programmers and laypersons alike in 1997, has been retired. A Samsung Galaxy S10 smartphone is fifty times faster than Deep Blue. The next generation of that smartphone, we can say with certainty, will escort the S10 to the electronics recycling center within a few years of this book coming out. The modern iPhone has around 100,000 times more processing power than the Apollo 11 computer that got Neil Armstrong and Buzz Aldrin to the moon in 1969, and it holds millions of times more memory. These changes occurred in a span of seventy years, but the vast improvement between the modern smartphone and Deep Blue happened in less than a generation.
As computing power has improved, so too has the amount of data available to AI. In 2017, the International Data Corporation forecasted that the “global datasphere” will include 163 zettabytes of information by 2025. That is more than ten times the 16.1 zettabytes of data available in 2016. It is difficult to conceptualize data on that scale: 163 zettabytes are roughly equivalent to sixteen million years of high-definition video or twenty billion US Libraries of Congress.
Big data can be critically important for AI, and vice versa. Some AI can deal with large and complex datasets in ways that people and traditional software cannot. For example, people may not be able to recognize patterns among millions of data points. Conventional algorithms that operate according to fixed, logical rules may similarly

92
struggle to operate with large datasets that have unknown properties. In such cases, AI that utilizes machine-learning, deep learning, or neural networks may be more flexible and effective. Such AI can use big data to extrapolate trends, detect anomalies, and automate labor. Big data can also be used to train some forms of AI to progressively improve performance without explicit programming. The importance of data for this and other purposes has led to data being called the new oil or the oil of the twenty-first century. As the European Commission noted in 2018, “data is the raw material for most AI technologies.”14
All big data are not created equal, however, and “garbage in, garbage out” is a maxim in the AI community. For big data to have value for AI, the right kinds of data need to be collected, aggregated, and appropriately utilized. Different types of data are necessary for different AI applications. Take the example of using AI to analyze health care insurer electronic health records to determine if a pharmaceutical medication has adverse events. It may be necessary to have data on exposures (with dates), outcomes, and other health conditions, together with insurer enrollment and demographic information. It may also be helpful to link datasets across different time points and across different insurers. Failure to obtain comprehensive and accurate data used for AI input may result in an incorrect AI output, or if the data are being used for machine-learning it may result in an inaccurately biased AI.
   ,
2018, https://europa.eu/rapid/press-release_IP-18-3362_en.htm.
14
EUROPEAN COMMISSION PRESS RELEASE IP/18/3362
ARTIFICIAL INTELLIGENCE: COMMISSION
 OUTLINES A EUROPEAN APPROACH TO BOOST INVESTMENT AND SET ETHICAL GUIDELINES
, Apr. 25,
 
93
Assuming that the necessary data have been collected, they still need to be brought together and appropriately translated for AI. Most AI systems are unable to work with unstructured data, which are generally distinguished from structured data by their degree of organization. Structured data usually exist in relational databases where they are categorized and sorted into distinct, easily searchable fields. Unstructured data, by contrast, do not have a predefined data model. They may exist as a heterogenous, free-form mix of text, audio, video, and social media postings that have not been labeled or sorted. Most big data now are unstructured, and significant cost and effort may be needed to structure them. Even without AI, it may be difficult to extract value from unstructured data with more conventional analysis, to process large amounts of unstructured data, and to secure unstructured data.
Applications
As mentioned above, AI can be applied to a wide range of technologies using diverse processes such as logic, if-then rules, decision trees, and machine-learning. These processes in turn can be used to mimic human behavior in areas like visual perception, speech recognition, and language translation. Some AI exists only on the cloud, an interconnected network of remote servers, while others are physically embodied in robots and can directly affect the physical world. However, AI without the ability to directly affect the physical world can often have a no less powerful indirect effect – like stopping someone’s pacemaker.

94
Some practical areas where AI has enjoyed recent advances have been in the areas of computer vision and natural language processing. Computer vision deals with how machines can gain a high-level understanding from images or videos, essentially, automating the human visual system. For instance, Facebook’s image recognition technology now recognizes people, objects, expressions, activities, events, and spaces. This AI has a very rich source of training data – more than two billion photos are uploaded daily to Facebook and platforms it owns: Instagram, WhatsApp, and Messenger.
The ImageNet project runs an annual AI contest, in which programs compete to correctly classify and detect a thousand nonoverlapping classes of objects and scenes. In 2015, Microsoft announced it beat the competition’s dedicated human labeler by classifying more than 95 percent of the images correctly.15 In 2017, twenty-nine of the thirty-eight competing teams classified more than 95 percent of the images correctly.16 These are impressive achievements, though machines still struggle with visual identification in many situations that are no challenge to people. It should be noted that developing large datasets can be enormously costly. ImageNet at its peak employed 48,940 people in 167 countries who sorted about fourteen million images.
AI has made similar advances in natural language processing, to which anyone who now uses a program like Siri, Alexa, or Cortana can attest. The idea of machine translation
15 Ophir Tanz, Can Artificial Intelligence Identify Better than Humans?, ENTREPRENEUR, Apr. 1, 2017, www.entrepreneur.com/article/283990.
16 Dave Gershgorn, The Quartz Guide to Artificial Intelligence: What Is It, Why Is It Important, and Should We Be Afraid?, QUARTZ, Sept. 10, 2017, https://qz.com/1046350/the-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it- important-and-should-we-be-afraid/.
     
95
has been around since the 1950s, along with predictions that fully realized machine translation would exist within a few years. Those predictions were unrealistic, but today’s machine translation has come a long way. Google Translate, for instance, supports more than a hundred languages, including many by photo, voice, and even real-time video. The Google Translate app is used by more than 500 million people and translates around 143 billion words daily.17 To take another example, Microsoft announced in 2017 that its speech recognition system performed as well as human transcribers in recognizing speech from Switchboard Corpus, a collection of thousands of recorded random conversations. Fifteen years before that, machine translation word-error rates hovered around 20–30 percent. In 2017, Microsoft’s technology reached an error rate of 5.9 percent, the same as a human transcriber.
Some industries such as telecommunications, automotive, and financial services already have high rates of AI adoption. In the automotive industry, AI is powering self- driving car technologies, which are set to radically transform transportation. Driverless cars are already being tested on public streets in several countries. Such products could not exist without AI. Other industries like education, health care, and tourism have lower rates of AI adoption, but this may change soon.
Given the lack of a standardized definition for AI, it is somewhat difficult to compare studies of AI’s economic impact. However, it is clear that AI has disruptive economic potential.18 An estimate by PricewaterhouseCoopers suggests that AI could contribute
17 Rory Smith, The Google Translate World Cup, THE NEW YORK TIMES, July 13, 2018, www.nytimes.com/2018/07/13/sports/world-cup/google-translate-app.html.
   Wendy
18 2017,
Hall &     , Growing the Artificial Intelligence Industry in the UK, Oct.
Jérôme
Pesenti

96
$15.7 trillion to the global economy in 2030, of which $6.6 trillion could come from increased productivity and $9.1 trillion from consumption-side effects.19 A report by the McKinsey Global Institute claims the disruptions caused by new technologies such as AI will “happ[en] ten times faster and at 300 times the scale, or roughly 3,000 times the impact” of the Industrial Revolution.20
6 AI Characteristics
Understanding the key characteristics of AI is important for understanding how it should be regulated and how AI challenges existing legal systems.
First, as discussed earlier, AI may have limited explainability. It may be possible to determine what an AI has done but not how or why it acted as it did. This has led to some AI being described as black box systems. For instance, an algorithm may decline a job application but be unable to articulate why the application was rejected. That is particularly likely in the case of AI that learns from data, and which may have been exposed to millions or billions of data points. Even if it is theoretically possible to explain an AI
https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachme nt_data/file/652097/Growing_the_artificial_intelligence_industry_in_the_UK.pdf.
19
(2017), www.pwc.com/gx/en/issues/analytics/assets/pwc- ai-analysis-sizing-the-prize-report.pdf.
     PRICEWATERHOUSECOOPERS
,
SIZING THE PRIZE: WHAT’S THE REAL VALUE OF AI FOR YOUR BUSINESS
 AND HOW CAN YOU CAPITALISE?
        20
, The Four Global Forces Breaking All ,   (2015), www.mckinsey.com/business-
Richard
Dobbs
, James
Manyika
,&
Jonathan
Woetzel
 the Trends
MCKINSEY GLOBAL INSTITUTE
 functions/strategy-and-corporate-finance/our-insights/the-four-global-forces-breaking- all-the-trends.
  
97
outcome, it may be impracticable given AI complexity, the possible resource-intensive nature of such inquiries, and the need to maintain earlier versions of AI and specific data.
Second, also as discussed earlier, while AI can already outperform people in spectacular fashion in some domains, such as board games, in other domains AI is not competitive with toddlers. That is because all AI is designed to perform narrow or specific tasks. DeepMind’s AI can beat the world’s best human player at Go, but it cannot translate English to French without being programmed to do so. By contrast, the holy grail of computer science research is developing general AI that can perform any task that a person can perform.
Third, AI may act unpredictably. Some leading AI relies on machine-learning or similar technologies that involve a computer program initially created by individuals further developing in response to data without explicit programming. This is one means by which AI can engage in unforeseeable activities that its original programmers may not have intended. Microsoft’s chatbot Tay is sometimes invoked as an example of an AI acting unpredictably. Tay was touted as an experiment in “conversational understanding” that would get smarter through engagement with people. Soon after the bot’s launch on social media sites like Twitter, people barraged Tay with misogynistic, racist, and political vitriol – and Tay responded in kind. For instance, when one user asked, “Is Ricky Gervais an atheist?,” Tay replied, “ricky gervais learned totalitarianism from adolf hitler, the inventor of atheism.”21 Microsoft discontinued Tay within 24 hours. While that tweet may have been
21 James Vincent, Twitter Taught Microsoft’s AI Chatbot to Be a Racist Asshole in Less Than a Day, THE VERGE, Mar. 24, 2016, www.theverge.com/2016/3/24/11297050/tay-microsoft- chatbot-racist.
    
98
unforeseeable, in retrospect, it is surprising that one of the world’s largest technology companies would have so little insight into the nature of online discourse. On the other hand, hindsight is 20/20, and a Chinese predecessor of Tay named Xiaoice reportedly engaged in more than forty million conversations without major incident.22
As a fourth characteristic, AI may act autonomously. It can be more than an inert tool directed by an individual. Rather, in at least some instances, AI is given a task to complete, and it functionally and independently determines for itself how to complete that task. For instance, a person could instruct a self-driving car to take them from point A to point B but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. With some types of AI, such as decentralized autonomous organizations (DAOs), it may even be the case that the individual who sets an AI in motion is unable to regain control of the AI by design. For our purposes, autonomous AI controls the means of completing tasks, regardless of how it is programmed.
This characteristic is well illustrated by “The DAO,” which is the most famous attempt to create a decentralized autonomous organization. The idea was that an entity (organization) would be created on a blockchain, a type of distributed ledger technology (decentralized) used to support cryptocurrencies like Bitcoin. Once set in motion, it would act automatically according to smart contracts, which are pre-programmed rules dictating future behavior (autonomous). In other words, The DAO was an AI that would function
22 Peter Bright, Tay, the Neo-Nazi Millennial Chatbot, Gets Autopsied, ARS TECHNICA, Mar. 25, 2016, https://arstechnica.com/information-technology/2016/03/tay-the-neo-nazi- millennial-chatbot-gets-autopsied.
    
99
independently of its original developers while simultaneously existing across countless computers.
There are many possible applications for a DAO, but The DAO was meant to operate like a venture capital fund. It was built by Slock.it, a German company, and Slock.it’s cofounders on the Ethereum blockchain, which generates a cryptocurrency called “Ether.” There was a creation period during which investors could fund The DAO by purchasing “tokens” using Ether, much like buying shares in a corporation, after which anyone could pitch an investment opportunity that The DAO could fund based on the votes of token holders. Token holders, like shareholders in a traditional company, would then receive rewards from profitable projects. The DAO’s code was open-source and made publicly available to anyone to inspect how the program would operate.
The primary benefit of this structure is that The DAO’s creators would be unable to misappropriate The DAO’s funds or to misuse the entity. Because it was a technologically cutting-edge project, and because it removed the need to trust other people, The DAO was appealing to a particular sort of investor, many of whom were cryptocurrency enthusiasts. In 2016, The DAO resulted in the then-largest crowdfund in history, raising around $150 million worth of Ether.
Unfortunately, before any projects could be funded, a hacker exploited a bug in The DAO’s programming and started to withdraw its funds. The same “unstoppable” or “unalterable” architecture that investors were attracted to, which prevented The DAO’s creators from changing its code for their own benefit, meant they could not fix the faulty code or directly recover the stolen funds. The only way to prevent the hacker from making off with the funds completely required extreme measures from the Ethereum community. A

100
“hard fork” was instituted, which changed the underlying blockchain protocol’s code and resulted in a new and old version of Ethereum: Ethereum and Ethereum Classic. There were now two separate blockchains: a new blockchain maintained by those who supported the intervention, and a legacy blockchain maintained by those who believed the blockchain should be “immutable” and that contributors to The DAO had only themselves to blame. In the post–hard fork Ethereum blockchain, users who contributed Ether to The DAO were permitted to withdraw their funds. The hacker, meanwhile, was able to withdraw the funds on the significantly less valuable Ethereum Classic blockchain. The decision to hard fork was controversial in the Ethereum community and significantly reduced the value of the Etherium blockchain.
The hacker was never identified, and no civil or criminal charges were levied against The DAO’s creators. The US Securities and Exchange Commission investigated The DAO and reported that tokens were securities and subject to federal securities laws, which appeared to have been violated. Ultimately, The DAO did not survive this controversy. However, new DAOs are being created. In 2018, the government of Malta considered legislation that would provide legal personality to DAOs.
As a final characteristic, AI sometimes has the potential to act physically when it is embodied in or controls hardware, as in the case of a “robot.” But it is not necessary for an AI to directly affect physical activity to be effective, as mentioned earlier, and even disembodied AI can cause substantial harm.
While it is possible for a conventional machine to exhibit some or all the above traits, for instance, to perform unpredictably, unexplainably, or autonomously, AI is far more likely to exhibit these characteristics and to exhibit them to a greater extent. Even a

101
difference in degree along several axes makes AI worth considering as a distinctive phenomenon, possibly meriting novel legal responses. Finally, general AI, and even superintelligent AI, is different than the sort of self-aware, conscious, sentient AI that is common in science fiction. The latter sort of AI is portrayed as having humanlike abilities to cognitively reason and to be morally culpable for its actions. As of today, the prospect of such machines is still safely within the realm of science fiction.

102
2
Should Artificial Intelligence Pay Taxes?
I regard it as the major domestic challenge, really, of the sixties, to maintain full employment at a time when automation, of course, is replacing men.
- President John F. Kennedy
Introduction
One of the biggest concerns about AI is the prospect of automation leading to increased unemployment – what is sometimes called technological unemployment. This is a concern for good reason. Existing AI can already automate many work functions, and its cost is decreasing at a time when human labor costs are increasing. As AI continues to improve, a number of experts are predicting that in the future of work AI will involve significant job losses and worsening income inequality. Policymakers are actively debating how to deal with technological unemployment, with most proposals focusing on investing in education to train workers in new job types or investing in social benefits to distribute the gains of automation.
What has yet to be appreciated is that our legal standards exacerbate technological unemployment. Specifically, existing tax laws unintentionally subsidize automation. Businesses pay less in taxes by having an AI, as well as automation technologies more generally, do the same work as a person. Not only do employers save money by automating, but when an AI replaces a person, the government loses a substantial amount

103
of tax revenue. In the aggregate, this may cost the US government hundreds of billions of dollars a year – or more.1
AI legal neutrality suggests that existing tax policies should change to be at least neutral between AI and human workers and that automation should not be allowed to reduce tax revenue. Neutrality could be achieved by eliminating the preferential treatment provided to AI or by creating offsetting benefits for human laborers. This could be accomplished in a variety of ways, but the best option may be to eliminate taxes on workers such as payroll taxes. This would help AI and people to compete on their own merits without taking tax into account, it would reduce tax complexity, and it would reduce the taxation of socially valuable labor. However, it would also dramatically reduce tax revenue.
Ensuring adequate tax revenue could be accomplished by implementing or increasing other types of taxes such as consumption (e.g., sales tax) and property taxes. This may be a more progressive system, even without making it based on ability to pay. It would effectively shift at least some tax from labor to capital, because capital income would be taxed along with labor income. It could also be made deliberately more progressive by having higher sales taxes for luxury goods and higher-value properties. We could also raise income taxes by, for example, increasing marginal tax rates for high earners or by increasing effective tax rates by eliminating mechanisms like the step-up in basis rule that reduces tax liability for inherited assets. More ambitiously, we could increase capital gains and corporate tax rates. Any of these measures would help to ensure that the financial
1 Ryan   & Bret   ,
, 12   145,   (2018).
  Abbott
Bogenschneider
Should Robots Pay Taxes? Tax Policy in the Age of
 Automation
HARV. L. & POL’Y REV.
145–175

104
benefits of automation are shared without restricting automation when it is genuinely more efficient.
This chapter is divided into three sections. The first section provides background on the costs and benefits of automation together with historical concerns about, and efforts to address, technological unemployment. Section 2 delves into an underexplored aspect of automation and explains in more detail how tax laws unintentionally favor machines over people. Finally, Section 3 proposes ways to apply a principle of AI legal neutrality to tax policy to ensure both effective competition and fiscal solvency.
1 Automation and Technological Unemployment
History of the Automation Scare
Fears of the consequences of automation have been expressed since the Industrial Revolution. In 1772, the writer Thomas Mortimer objected to machines, “which are intended almost totally to exclude the labor of the human race.”2 In 1821, the economist David Ricardo argued that automation would result in inequality and that “substitution of machinery for human labour is often very injurious to the interests of the class of labourers ... . [It] may render the population redundant and deteriorate the condition of the labourer.”3 In 1839, the philosopher Thomas Carlyle more poetically wrote, “The huge
    2
, 72(A. 1801).
THOMAS
MORTIMER
LECTURES ON THE ELEMENTS OF COMMERCE, POLITICS AND FINANCES
 Straham, for T. N. Longman and O. Rees
 3 DAVID   ,     (Batoche Books 2001) (3rd ed. 1821).
RICARDO
ON THE PRINCIPLES OF POLITICAL ECONOMY AND TAXATION
283–84
 
105
demon of Mechanism smokes and thunders, panting as his great task, in all sections of English land; changing his shape like a very Proteus; and infallibly, at every change of shape, oversetting whole multitudes of workmen, as if with the waving of his shadow from afar, hurling them asunder, this way and that, in their crowded march and course of work or traffic; so that the wisest no longer knows his whereabout[s].”4
As discussed earlier, the Industrial Revolution even gave birth to a social movement and group, protesting the use of new technologies called the Luddites. Luddites were primarily English textile workers who objected to working conditions in the nineteenth century. They believed that automation threatened their livelihoods, and they were opposed to the introduction of industrial machinery. Some Luddites engaged in violent episodes of machine-breaking, in response to which the English government made machine-breaking a capital offense. The Luddite movement died out, but automation concerns persisted throughout the twentieth century, often flaring during times of rapid technological progress. For instance, the debate was revitalized in the 1950s and 1960s with the widespread introduction of office computers and factory robots. In his 1960 election campaign, John F. Kennedy suggested that automation offered “hope of a new prosperity for labor and a new abundance for America,” but also that it “carries the dark menace of industrial dislocation, increasing unemployment, and deepening poverty.”5
Despite these concerns, technological advances have generally resulted in overall job creation. The computer eliminated jobs but created jobs for working with information
    THOMAS
4
142 ( 5
,
Trail ed.,
CARLYLE
141–
2010) (1899).
,     , &   IRWIN, OFFSHORING OF AMERICAN JOBS 80 (The
THE WORKS OF THOMAS CARLYLE: CRITICAL AND MISCELLANEOUS ESSAYS
  Henry Duff
Cambridge University Press
   JAGDISH
BHAGWATI
2009).
ALAN S.
BLINDER
DOUGLAS A.
 MIT Press

106
created by computers. The automobile eliminated jobs but created jobs in the motel and fast-food industries. The tractor and other agricultural advances eliminated jobs but drove job growth in other areas of the economy. Even as agricultural-based employment and agriculture’s relative contribution to the gross domestic product (GDP) decreased during the twentieth century, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the GDP increased. Indeed, in each previous era when concerns have been expressed about automation causing mass unemployment, technology has created more jobs than it has destroyed.
Will History Repeat Itself?
We are experiencing another period of vigorous public debate about automation and technological unemployment due to recent advances in AI. Once more, prognosticators are divided into two camps: the optimists who claim there will be a net creation of jobs, and the pessimists who predict mass unemployment and growing inequality. History favors the optimists. They argue that technological advances will generate widespread benefits together with overall job creation. They also argue that current unemployment may relate more to globalization and offshoring than to technology, and that any future technological unemployment would be “only a temporary phase of maladjustment.”6
However, there is reason to think that this time may be different. AI is constantly improving and existing technologies are becoming more affordable at the same time as labor costs are rising. In 2013, Carl Benedikt Frey and Michael Osborn published an
6 JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY 97 (Cosimo Classics 2006) (1848).
   
107
influential study claiming that 47 percent of American jobs were at high risk of automation by the mid-2030s.7 More recently, Oxford Economics has argued that up to 20 million manufacturing jobs worldwide will be lost to automation by 2030.8 Forrester predicts job losses of 29 percent by 2030 together with 13 percent job creation.9 Bank of America claims that by 2025, AI may eliminate $9 trillion in employment costs by automating knowledge work.10 On the other hand, there is also no shortage of optimistic research. For instance, the McKinsey Global Institute now argues that while there will be significant job losses due to automation, there will be net positive job growth in the United States through 2030.11
While the optimists and pessimists disagree about automation’s effects on long- term unemployment, both agree it causes short-term job losses and industry-specific disruption. During past episodes of widespread automation and technological change, it took decades to develop new worker skill sets on a significant scale and to build new job markets. Although the Industrial Revolution ultimately resulted in net job creation, it also
     7
Frey& ,
, 114   254 (2013),
Carl Benedikt
 Are Jobs to Computerisation?
TECH. FORECASTING & SOCIAL CHANGE
www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf.
  OXFORD ECONOMICS
10
11 James
3 (Dec. 16, 2015).
Michael A.
Osborne
HOW ROBOTS CHANGE THE WORLD
The Future of Employment: How Susceptible
,   (2019),
,   (2019), https://go.forrester.com/future-of-
8 www.oxfordeconomics.com/recent-releases/how-robots-change-the-world.
  FORRESTER
THE FUTURE OF WORK
9 work/?utm_source=forrester_news&utm_medium=web&utm_campaign=futureofwork.
    BANK OF AMERICA
,
ROBOT REVOLUTION – GLOBAL ROBOT & AI PRIMER
  Manyika et al.
,
Jobs Lost, Jobs Gained: What the Future of Work Will Mean for Jobs,
 ,   (2017), www.mckinsey.com/featured- insights/future-of-work/jobs-lost-jobs-gained-what-the-future-of-work-will-mean-for- jobs-skills-and-wages#part3.
Skills, and Wages
MCKINSEY GLOBAL INSTITUTE
   
108
resulted in periods of mass unemployment and human suffering. Today, regardless of whether there will be detrimental long-term effects, there will almost certainly be significant short-term disruptions.
The Good: Increased Productivity and New Jobs
Automation increases productivity, which generates value and creates wealth. Partly due to technological advances and automation, the USA’s GDP has steadily risen from $712 billion in 1965 to $20.5 trillion in 2018. Despite academic criticism, GDP has remained the dominant economic indicator of welfare and standard of living for half a century.
Automation can also create new jobs. Human workers may be needed to build and maintain automation technologies. Automation may free up capital for investments in new enterprises, result in the creation of new products, or decrease production costs for existing products. Decreased production costs may result in lower consumer prices and thus greater consumer demand. All of this may increase employment. Technological advances have also historically upgraded the labor force: Automation has reduced the need for unskilled workers but increased the need for skilled workers.
The Bad: Unemployment and Inequality
Without oversight, automation may also exacerbate unemployment and economic inequality. Even if workers rendered technologically unemployed are able to transition to new jobs, as has been the case during previous eras of rapid change, there will still be significant short-term disruptions. Moreover, many experts are predicting that today’s

109
technological advances are fundamentally different from those of the past, and that large- scale permanent increases in unemployment are inevitable.12 In 1990, the three largest companies in Detroit with a combined market capitalization of $36 billion employed 1.2 million workers. In 2014, the three largest companies in Silicon Valley with a combined market capitalization of $1.09 trillion employed 137,000 workers.
Automation can cause both under- and unemployment. While worker productivity has risen robustly since 2000, employment has stagnated. This may be due in part to AI. When McDonald’s introduces computer cashiers, the company saves money and consumers may enjoy lower prices. But human cashiers now find themselves in a more competitive labor market. The enhanced competition may result in lower wages, less favorable employment terms, fewer working hours, reduced hiring, or layoffs. As the former CEO of McDonald’s USA famously quipped, “It’s cheaper to buy a $35,000 robotic arm than it is to hire an employee who’s inefficient making $15 an hour bagging French fries.”13 McDonald’s is still expanding its use of automated cashiers worldwide.
While automation generates wealth, it does so unevenly. Expanding our time interval to the past twenty-five years, the income share of the top 0.1 percent has increased substantially, and this is partly due to AI. The top 0.1 percent of the US population is now worth about as much as the bottom 90 percent. CEO-to-worker pay ratios have increased a
  12 Klaus   &     , Preface to
Schwab
Richard
Samans
EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION
(2016), www3.weforum.org/docs/WEF_Future_of_Jobs.pdf.
13 Julia   ,
Hour,   , May 24, 2016, www.foxbusiness.com/features/2016/05/24/fmr- mcdonalds-usa-ceo-35k-robots-cheaper-than-hiring-at-15-per-hour.html.
WORLD ECON. F., THE FUTURE OF JOBS:
 , at v–vi
  Limitone
Fmr. McDonald’s USA CEO: $35K Robots Cheaper Than Hiring at $15 Per
FOX BUS.
  
110
thousandfold since 1950, but overall wages have been stagnant for thirty-five years. Increased automation is likely to accelerate these trends. The White House Council of Economic Advisors has predicted that future automation will disproportionately affect lower-wage jobs and less educated workers, causing greater economic inequality.
Worsening employment coupled with growing income inequality is a recipe for social unrest. As Stephen Hawking warned, “Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality.”14
The Ugly: Reduced Tax Remittances
One of automation’s most pronounced, and unappreciated, effects relates to taxes. Automation may substantially reduce tax revenue, even if it generates more wealth, because most of the US government’s tax revenue comes from taxes on workers. In 2016, the Internal Revenue Service (IRS) reported that more than half of its $3.3 trillion in net collections came from individual income taxes. The next largest source of revenue was employment taxes, largely Medicare/Medicaid and social security taxes, which contributed more than a trillion dollars. Only about a tenth of revenue comes from business income taxes, and a far smaller amount comes from excise taxes (taxes on specific goods and activities) and gift and estate taxes. In other words, most revenue comes from income tax
Rathi, robots-arent-just-taking-our-jobs-theyre-making-society-more-unequal/.
   14
Akshat
Stephen Hawking: Robots Aren’t Just Taking Our Jobs, They’re Making
 Society More Unequal
,   , Oct. 9, 2015, http://qz.com/520907/stephen-hawking-
QUARTZ
  
111
and indirect taxes levied on income and wages by various levels of government, as opposed to taxes on companies, excise taxes, and gift and estate taxes.
By replacing people with AI, the government loses out on employee and employer wage taxes levied by federal, state, and local taxing authorities. In addition, as discussed further below, tax revenue may be additionally reduced from businesses claiming accelerated tax depreciation on capital outlays for automation technologies and from other tax incentives related to indirect taxation, such as sales tax or value-added tax (VAT) exemptions. Even if automation results in greater profitability for a business, a relatively small portion of that profit is remitted in taxes.
Automation Social Policy
Policymakers should act to accommodate and even encourage advances that promote economic value, but it is also important to ensure this value is distributed in a socially just manner. In the middle of the Industrial Revolution, the philosopher John Stuart Mill wrote that while automation would ultimately benefit laborers, “this does not discharge governments from the obligation of alleviating, and if possible preventing, the evils of which this source of ultimate benefit is or may be productive to an existing generation [and] there cannot be a more legitimate object of the legislator’s care than the interests of those who are thus sacrificed to the gains of their fellow-citizens and of posterity.”15 Or, as the US National Science and Technology Council Committee on Technology argued in 2016, “Public policy can address [technological unemployment], ensuring that workers are
   15
MILL,
(   1909) at Book I, ch. VI, ¶ 13.
JOHN STUART
PRINCIPLES OF POLITICAL ECONOMY WITH SOME OF THEIR APPLICATION TO SOCIAL
 PHILOSOPHY
Longmans, Green, and Co.

112
retrained and able to succeed in occupations that are complementary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly and assure that AI responsibly ushers in a new age in the global economy.”16
Efforts to alleviate the harms and share the benefits of automation have focused on education and social benefits. In terms of education, it is thought that technologically unemployed workers need retraining to transition to new job types. President Kennedy’s solution was to pass the nation’s first and most sweeping federal program to train workers unemployed due to technological advances: the Manpower Development and Training Act of 1962. More recently, President Barack Obama provided billions of dollars to fund worker training in part to address technological unemployment. He also proposed a plan to make two years of community college available at no cost for “responsible students” in his 2015 State of the Union Address, although this proposal was never adopted.
Social benefit investments are also necessary. In 2016, the Executive Office of the President under Obama issued a report that outlined a three-pronged policy response to automation and AI: namely, to invest in AI, educate and train Americans for future jobs, and transition workers to ensure widespread benefits. The report advocates strengthening the social safety net through greater investments in programs such as unemployment insurance and Medicaid. It also proposes the creation of new programs for wage insurance
   16
, PREPARING FOR
EXEC. OFFICE OF THE PRESIDENT, COMM. ON TECH., & NAT’L SCI. & TECH. COUNCIL
 THE FUTURE OF ARTIFICIAL INTELLIGENCE
2 (2016),
https://obamawhitehouse.archives.gov/sites/default/files/whitehouse_files/microsites/o stp/NSTC/preparing_for_the_future_of_ai.pdf.
  
113
and emergency aid. In addition, it argues for building a twenty-first century retirement system, expanding health care access, and increasing worker bargaining power.
Revitalized concerns about technological unemployment have breathed new life into an old social benefit proposal: basic income, a system of unconditional income to every citizen. There are many ways a basic income could be implemented, sometimes called guaranteed minimum income or universal basic income depending on how it is structured, but the fundamental premise is that the government would provide a fixed amount of money to its citizens regardless of their situation.
Some version of basic income has been trialed numerous times on a relatively small scale, with mixed results. The most famous US basic income program is the Alaska Permanent Fund Dividend, which has been unconditionally paying state residents since 1982. In 2018, Alaskan residents received $1,600 for the year. More recently, from 2017– 2019, Finland ran a pilot program to give about $600 per month to 2,000 unemployed citizens, with no other requirements. The program cost around $23 million in total. The final results of the study are not yet available, but preliminary evidence suggests it had no impact on employment but improved participant health and well-being. The design of the trial has also been criticized. The government, which initially considered replacing earnings-based insurance benefits with a basic income, has declined to continue and expand the trial.
Proponents of basic income, like Mark Zuckerberg and Elon Musk, argue it will reduce unemployment, poverty, and disincentives for the unemployed to work (as under conventional unemployment schemes recipients generally lose their unemployment benefits after returning to work). It might also encourage education by providing support

114
for a period of training. Critics argue that a guaranteed minimum income will encourage recipients to remain unemployed and discourage additional education. Y Combinator, the famous Silicon Valley start-up incubator, is planning a randomized controlled trial to evaluate the effects of basic income across two US states.
Efforts to provide basic income coming from places like Finland and Silicon Valley might not be surprising, but in 1969 President Richard Nixon planned an even more ambitious basic income. His Family Assistance Plan (FAP) initially would have provided an unconditional income of around $1,600 (around $10,000 in 2017) a year to a family of four living in poverty. An unconditional basic income had just been meticulously trialed on around 8,500 Americans without evidence it resulted in decreased work or that it was prohibitively expensive. On the day Nixon intended to announce the FAP, his advisors presented him with a report of an earlier trial of basic income from early nineteenth- century England known as the Speenhamland system. His advisors claimed the Speenhamland experiment was a colossal disaster on the basis of a 150-year old English Royal Commission report based on faulty science. Nixon’s advisors then persuaded him that the FAP would not only threaten capitalism but also pauperize the masses and trap them in a cycle of vicious poverty. In response, Nixon decided to transform FAP from a “welfare” plan to a “workfare” plan that would require all recipients other than mothers with young children to work, but even that was never enacted. Democrats felt the level of basic income was not sufficient, and Republicans, many of whom initially supported the proposal, soon turned against it. Lobbying for workfare helped turn conservatives against basic income as well as the idea of America as a “welfare state.”

115
Improving education and social benefit systems will not be easy. In the abstract most policymakers agree on the desirability of improving worker training as it will enlarge the productive labor force, but “delivering this education and training will require significant investments.”17 Enhancing the social benefit system will also require significant investment, but this goal is even more challenging because of a lack of consensus that enhanced benefits are a desirable aim.
The fact that automation creates a need for greater government investment is well- known, but what has so far been largely ignored in the automation debate is that automation will make it more difficult for the government to make investments if tax revenues are reduced. As a very rough estimate, the revenue loss can be estimated by multiplying an effective tax rate by the gross salary loss due to automation. In January 2017, the McKinsey Global Institute claimed that about half of current work activities could be automated using currently demonstrated technologies, which would eliminate $2.7 trillion in annual wages just in the United States.18 Workers pay high effective tax rates ranging from 25 percent to 55 percent when all tax types are taken into account. This
17 EXEC. OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY 3 (2016), https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial- Intelligence-Automation-Economy.PDF.
18 James     , (2017), at 21,
www.mckinsey.com/~/media/mckinsey/featured%20insights/Digital%20Disruption/Har nessing%20automation%20for%20a%20future%20that%20works/MGI-A-future-that- works-Executive-summary.ashx.
     Manyika et al.
,
A Future That Works: Automation, Employment, And Productivity
 MCKINSEY GLOBAL INSTITUTE
   
116
suggests that worker automation could result in hundreds of billions, or even trillions, of dollars lost per year in tax revenue at various levels of government.
Tax is thus critically important to the automation debate. Tax policies should not encourage automation, unless it is part of a deliberate strategy based on sound public policy. The solution is to adjust the tax system to be at least neutral between AI and human workers. This is particularly critical because the education and social benefit reform necessitated by automation will only be possible with more, not less, tax revenue.
While there has been a lively public discourse on technological unemployment and income disparity, the automation debate has historically ignored the issue of taxation. That has very recently started to change. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing concerns of stifling innovation. The next day, Bill Gates stated that he thought governments should tax companies’ use of robots to slow the spread of automation and to fund other types of employment. Former US Secretary of the Treasury Lawrence Summers then claimed Gates’ argument was “profoundly misguided.” In August 2017, South Korea announced plans for the world’s first “tax on robots” by limiting tax incentives for automated machines. Previously, Korean businesses were able to deduct 3–7 percent of an investment in automation equipment from their corporate tax, depending on the size of their operation. The reform decreased the deduction rate by up to 2 percent.

117
2 Current Tax Policies Favor Automation and Reduce Tax Revenue
Automation can be thought of in terms of efficiency, where efficiency refers to the ratio of useful output to total input. If an AI and a person cost the same amount but the AI can generate more output, or if AI is less expensive but it generates the same output as a person, then automation improves efficiency. For example, if an AI costs a business $400,000 a year and a human worker costs $500,000 a year, with both workers producing the same output, the business would yield a $100,000 annual cost savings by automating. However, it may be the case that the AI costs more than a human worker before taxes, and only becomes cheaper as a result of taxes.
Consider an AI that could automate the job of a diagnostic radiologist (a doctor who evaluates images like X-ray, MRI, and CT scans), or, if not able to entirely automate the job, it makes other radiologists more efficient to the point a hospital needs one less radiologist. That might be the case if the AI could only perform certain tasks that a human radiologist could perform, such as review of chest X-rays for pneumonia. Without the need someone to perform that task, the remaining radiologists could focus on tasks that AI cannot perform. Even if the AI only does an initial review of an image and a human physician still needs to confirm its analysis, if the AI makes the doctor’s job easier (i.e., faster), then it may still result in less need for doctors. In 2017, the median salary for a diagnostic radiologist was about $500,000.
The capital outlay for the AI, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as machines, or intangible assets like (some)

118
software, together with the costs for operating the AI (electricity, etc.), might be estimated at $2,100,000 over some period. The wages and other costs associated with the employee (healthcare, retirement funding, etc.) might be $2,000,000 over the same period. However, the AI may be associated with tax benefits that do not apply to human workers and which reduce its cost to $1,900,000. A business using a rational cost-based decision model would choose to automate and realize the AI’s tax benefit. In this example, tax policy has rendered the AI a more efficient worker. The heavy relative taxation of the living worker drives the business toward automation to generate a tax savings.
The tax system is not neutral between work performed by AI versus people. Automation provides several tax advantages. Businesses that automate avoid employee and employer wage taxes. In addition, businesses may claim accelerated tax depreciation on many capital costs for automated workers, and benefit from indirect tax incentives for automation. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital. In fact, as described below, automated workers are taxed less than human workers at both the employer and employee level.
Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to individuals to fund social benefit programs including social security, Medicare and Medicaid. Presently in the United States, an employer and employee pay matching amounts totaling 12.4 percent of an employee’s salary, plus matching Medicare payments totaling 2.9 percent (applied on the

119
first $127,200 of earnings), plus an additional 0.9 percent Medicare surcharge (applied on earnings over $200,000).
Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers19
Tax depreciation refers here to the deduction (a reduction in the tax base) claimed by a business in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow a business to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction may work differently for AI than it does for people.
The timing of claiming a deduction may have a significant effect on a businesses’ tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time). For example, assume an industrial robot has a total capital cost of $100,000 and 7 years of useful life, while an employee has a total wage cost of $100,000 over 7 years. If accelerated depreciation for capital is available, the business may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year 1 rather than pro-rata over 7 years. For instance, the business might claim tax depreciation for an automated worker of $50,000 in year 1, $30,000 in year 2, $10,000 in year 3, and in diminishing amounts to year 7. By contrast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a
  Bret N.
 Bogenschneider
  19 (2015).
, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74

120
present value benefit will accrue from claiming accelerated tax deductions for automated workers relative to deductions for employee wages, even where the $100,000 capital outlay is paid up-front. This is possible because many large businesses have significant amounts of free cash in reserve that is not paid out to shareholders to limit tax liability.20 The present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return a business could make by investing this free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the economic depreciation. “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to build the same machine next year than it costs today. The issue becomes significant where, as in the prior example, it was presumed for tax purposes that the machine wears out after 7 years, but it turns out the machine actually increases in nominal value rather than wearing out. This will only sometimes be the case with AI per se, as software is often quickly rendered obsolete, but it may be more likely with industrial machinery capable of automation. Where AI does increase in nominal value over time, an incremental tax benefit accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary)
20 See, e.g., Katia   & Laura Davison, Corporate America Is Repatriating a Fraction ,   , June 20, 2019,
www.bloomberg.com//amp//news//articles//2019-06-20//corporate-america-is- repatriating-a-fraction-of-foreign-profits (referring to President Donald Trump’s estimate that US businesses would repatriate $4 trillion in offshore cash holdings as a result of the 2017 tax law).
   Dmitrieva
 of Foreign Profits
BLOOMBERG
  
121
difference is never recaptured in the tax system. In the corporate setting this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not adjusted for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential investors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit. Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate reported earnings could use automation to achieve such a timing benefit.
The situation may be different where AI is considered an intangible asset rather than a tangible asset. Some, but not all, software is considered an intangible asset and there may be some complex rules about the timing of deductions. Generally tangible assets are depreciated whereas intangible assets are amortized. As a rule, amortization is more difficult to accelerate than depreciation.
Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the business level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary

122
examples are the retail sales tax (RST) levied by states and municipalities in the United States and VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportionately to offset the indirect tax. In the case of automated workers, however, the burden of indirect taxes is entirely avoided by a business because it does not need to provide for a machine’s consumption. In general, business expenditures for capital assets are exempt from indirect taxation or yield a deduction for RST or VAT.
Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing. For 2017, the IRS reported that out of the nearly $3.3 trillion in net collections, individual income taxes accounted for 49.8 percent, employment taxes 35.2 percent, business income taxes 11.7 percent, excise taxes 2.6 percent, and estate and gift taxes 0.7 percent.21 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now comprises roughly one-half of its respective share compared to prior decades.22 It is decreasing further in light of the Tax
   21
, PUB. 970, SOI TAX STATS – COLLECTIONS
(2019),
INTERNAL REVENUE SERV., U.S. DEP’T OF THE TREASURY
 AND REFUNDS, BY TYPE OF TAX – IRS DATA BOOK TABLE 1
https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by-type-of-tax-irs- data-book-table-1.
22 See     &     ,
   Lester
Snyder
86 (1996).
Marianne
Gallegos
Redefining the Role of the Federal Income Tax:
 Taking the Tax Law “Private” Through the Flat Tax and Other Consumption Taxes
,13AM.J.
 TAX POL’Y

123
Cuts and Jobs Act of 2017, which cut the corporate tax rate from 35 percent to 21 percent in 2018. Likewise, in Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For instance, an employee cannot use transfer pricing techniques to shift earned income into a 0 percent-taxed entity in the Cayman Islands. Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human capital is also not depreciable, so a person may not get a tax deduction for education or medical costs – at least not up to the full amount of the investment. By contrast, machinery or other equipment yields an immediate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, over the long term this should have negative economic effects.
3 Tax Policy Options for an Automation Tax
This part of the chapter sets out some potential solutions to the challenges identified earlier, without exhaustively canvasing every possible option or providing a detailed

124
cost/benefit analysis. Those details will ultimately be important for policy makers, but from a big picture perspective, selecting between different solutions will require some fundamental normative choices about how to generate tax revenue.
As discussed above, the current tax system is designed to principally tax people and not AI. All else being equal, this creates a situation in which firms are incentivized to automate to save on taxes. A major automation policy issue is therefore how to adjust the tax system to be neutral between AI and people. This could be done through some combination of granting offsetting tax preferences for workers or reducing the tax benefits AI receives. In any event, a second major tax challenge posed by AI is how to prevent the possible future automation of significant segments of the labor force from threatening both short- and long-term fiscal solvency.
Leveling the Playing Field
Offsetting tax preferences could be granted for human workers. To begin with wage taxation, the tax preference could entail a repeal of the employer and employee contributions to the social security and Medicare systems. The result would be that both people and AI would be exempt in terms of wage taxes – not just automated workers. In terms of income taxation, an offsetting preference for human workers could be designed as an accelerated deduction for future wage compensation expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for AI. In terms of indirect taxation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income.

125
Corporate tax deductions could also be disallowed for automation. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines. To begin with federal income taxation, the disallowance of tax preferences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phaseout.” Phaseouts reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower- income taxpayers. For instance, student loan interest is deductible but not for individuals with more than $80,000 in modified adjusted gross income ($165,000 for joint filers).23 Some phaseouts reduce credits, others reduce deductions. A new code provision could be designed with a similar phaseout, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker layoffs due to automation, or high profit-to-employment ratios, could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance. In respect to indirect taxation, a simpler solution could be possible. Indirect tax preferences for capital outlay concerning automated workers could be disallowed outright at the state level.
   23
, PUB. 970, STUDENT LOAN INTEREST 31 (2019), www.irs.gov/pub/irs-pdf/p970.pdf.
INTERNAL REVENUE SERV., U.S. DEP’T OF THE TREASURY
 DEDUCTION
 
126
Either approach, increasing workers’ benefits or reducing AI’s benefits, could achieve greater balance between taxing laborers and AI. However, the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base. Further, granting offsetting tax preferences for human workers would worsen the decline in the wage tax base by, among other things, eliminating around 35 percent of federal tax revenue. This would accelerate the insolvency of the social security system, unless the resultant decrease in tax collections were otherwise offset.
Ensuring Adequate Revenue
With or without a more neutral system, a second major tax challenge posed by AI is how to prevent the future automation of large segments of the labor force threatening fiscal solvency. This could be done by increasing, say, property and sales taxes. Shifting the tax burden from payroll to sales taxes would make it less burdensome for firms to employ people, and it may be more progressive to generate tax revenue from property and sales taxes than from payroll taxes. This would tax capital to a greater extent than direct labor taxes, because individuals and businesses paying sales and property taxes will in part be taxed on income derived from capital investment. More ambitiously, we could raise the tax burden on capital by 1) instituting an automation tax based on technological unemployment, 2) increasing taxes preferentially on companies that are profitable with minimal labor with a “corporate self-employment tax,” 3) increasing general corporate tax rates, or 4) increasing other forms of capital taxation.

127
For the first option, an incremental federal automation tax could be levied to the extent workers are laid off or replaced by machines. A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings. Employers must pay into an unemployment insurance scheme based on their ratings, so a business that has more layoffs pays more in taxes for unemployment insurance. A federal automation tax could be designed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determines the layoffs were due to automation. A drawback to the levy of an additional automation tax is that it would increase the relative complexity of the tax system and increase compliance costs.
Another option is to levy a corporate self-employment tax for firms that produce outputs with relatively limited human labor. The additional taxes would be a substitute amount for social security and Medicare wage taxes avoided by the firm with automated labor. In part, this is the corollary to the individual self-employment tax where a small- business owner is required to pay into the social security system approximating the social security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work. The corporate self-employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount determined by the Treasury (in reference to industry standards), then backup withholding could apply on corporate profits. The gross amount of the automation tax

128
could be designed to match the wage taxes avoided by the firm with automated workers. A disadvantage, however, of this corporate self-employment tax is that it could penalize firms that are automating in socially beneficial ways, or that have business models that rely on minimal human labor for nontax reasons.
A third option is to increase the corporate tax rate. Effective tax rates could be increased either by raising the statutory tax rate above 21 percent, or by eliminating certain deductions that result in businesses’ playing less than the statutory rate. For instance, deductions for pass-through businesses (limited liability companies, partnerships, and S-corps) in the Tax Cuts and Jobs Act of 2017 could be stopped. Those resulted in more than $40 billion in tax breaks just in 2018.
Finally, corporate taxes are far from the only kind of taxes on capital. There are other means of increasing the relative tax burden on capital, such as by increasing capital gains taxes. Those are taxes assessed on the profits from the sale of an asset, such as land or a business. For most assets held for more than a year, the capital gains tax rate is either 0 percent, 15 percent, or 20 percent depending on taxable income and filing status. This is significantly below the tax rates for ordinary income (10–37 percent).
These strategies are not mutually exclusive and there may be benefits to not relying on a single kind of tax increase. An automation tax, company self-employment tax, and increased capital gains and corporate tax rates all have advantages and disadvantages. An automation tax would likely increase tax complexity and compliance costs, and a company self-employment tax based on a ratio of profit to employee compensation could “tax innovation” by penalizing firms that have nontax reasons for their employment structures. The appeal of a general increase to capital gains and corporate tax rates is that it would not

129
discriminate against certain business models; rather, it would more evenly increase the tax burden on capital versus labor.
The primary concern about increasing capital tax rates is discouraging investment. Investors consider tax rates in deciding how to invest capital, and higher tax rates can reduce anticipated returns. Taxation of capital has historically been disfavored, because capital is thought to be mobile, or at least more mobile than labor, and so increasing tax rates may cause capital investments to head to jurisdictions with lower tax rates. The absence of an internationally harmonized tax regime leads to international tax competition and, to some degree, a race to the bottom.24
These are legitimate concerns, but they would also favor every jurisdiction abolishing all capital taxes. On the contrary, relatively high tax rates do not historically appear to have been a barrier to USA-based investments. Investors and businesses need access to large and developed markets like the United States, and relatively high tax rates may be the price of admission. In fact, paradoxically, higher corporate tax rates may sometimes encourage capital investments because they can result in tax deductions with greater value. Higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment refers to incremental investment made only because of the tax system. Multinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions principally to claim tax deductions of relatively higher value. Partly for this reason, for smaller and growing firms that are
, 1533 (2016); see also     ,
25   221 (2016).
     24
Bret N.
Bogenschneider
A Theory of Small Business Tax Neutrality,
FSU BUS. REV.
 Bret N.
Bogenschneider
The European Commission’s Idea of Small
 Business Tax Neutrality,
EC TAX REV.

130
reinvesting profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
Further Thoughts
Several potential tax policy solutions exist to address the legal imbalance between AI and people and create a more neutral system. Neutrality in this setting refers to a system in which various alternatives are taxed equally, and so actors make decisions based on nontax reasons. Tax neutrality is widely accepted as an economically efficient principle for organizing a tax system. Neutral taxes are more likely to have fewer negative effects, have lower administration and compliance costs, promote distributional fairness, and increase transparency. Tax neutrality can thus result in a broader tax base with lower rates. Non- neutralities in the tax system distort choices and behavior other than for economic reasons and encourage socially wasteful efforts to reduce tax payments. They can thus “create complexity, encourage avoidance, and add costs for both taxpayers and governments.”25
The advantage of tax neutrality as between people and AI is that it permits the marketplace to adjust without tax distortions. Firms should then only automate if it will be more efficient without taking taxes into account. Since the current tax system favors automated workers, a move toward a neutral tax system could increase the appeal of human workers. The increased tax revenue from neutral taxation could then be used to provide improved education and training for workers rendered unemployed by AI. Should
25 STUART ADAM, TAX BY DESIGN 41 (Oxford University Press 2011), www.ifs.org.uk\\uploads\\mirrleesreview\\design\\ch2.pdf.
     
131
the pessimistic prediction of a near future with substantially increased unemployment due to automation manifest, these taxes could also support social benefit programs such as basic income. Automation will likely generate more wealth than has ever been possible, but it should not come at the expense of the most vulnerable.

132
3
The Reasonable Robot
I visualize a time when we will be to robots what dogs are to humans, and I’m rooting for the machines.
- Claude Shannon
Introduction
AI is already diagnosing disease, drafting legal contracts, and providing translation services. But what happens when these AI systems cause harm? What happens when a machine fails to diagnose a cancer, writes a faulty agreement, or inadvertently starts a war? How should the law respond to accidents caused by AI? With an already existing body of law developed to deal with accidents, tort law will play a central role in answering these and other questions regarding AI’s culpability in causing harm. A tort is a harmful civil, as opposed to criminal, act, other than under contract, where one person is damaged by another, and it gives way to a right to sue. The goals of tort law are many: to reduce accidents, provide a peaceful means of dispute resolution, promote positive social values, and so forth. Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law. By creating a framework for shifting the costs of accidents from injured victims to those who caused harm (tortfeasors), tort law deters unsafe conduct. A purely financially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity. This

133
liability framework has far-reaching and sometimes complex impacts on behavior. It can either accelerate or impede the introduction of new technologies.
Most injuries caused by people are evaluated under a negligence standard in which liability depends on whether there was unreasonable conduct. This generally requires proof that a defendant acted unreasonably considering foreseeable risks. The standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circumstances. When AI cause the same injuries, however, a strict liability standard applies. Strict liability is a theory of liability without fault; it applies without regard to whether a defendant’s conduct is socially blameworthy. This distinction has financial consequences and a corresponding impact on the rate of technology adoption. It discourages automation, because when an AI performs the same activity as a person, there is a greater risk of liability. It also means that in cases where automation will improve safety, the current framework to prevent accidents may have the opposite effect.
The principle of AI legal neutrality suggests the solution is to evaluate the acts of AI tortfeasors under a negligence standard, rather than a strict liability standard, in cases where an AI is behaving like a human tortfeasor in the traditional negligence paradigm. Liability would be based on an AI’s behavior rather than its design, and this would provide added benefit in cases when it would be difficult to prove a design defect due to system complexity or limited explainability. For the purposes of ultimate financial liability, the AI’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments, which is largely the case now under standard principles of product liability law.
The most important implication of this line of reasoning is that just as AI tortfeasors should be compared to human tortfeasors, so too should humans be compared to AI. Once

134
AI becomes safer than people and practical to substitute for people, AI should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical reasonable person would have done in their situation, but what an AI would have done. In time, as AI comes to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy, but people would engage in certain activities at their own peril. Such a rule is consistent with the rationale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving reasonable robot standard should apply to AI tortfeasors, at which point AI should cause so little harm that the primary effect of the standard would make human tortfeasors essentially strictly liable for their harms.
The remaining chapter is divided into three sections. Section 1 provides background on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negligence and strict product liability. Section 2 argues that while some forms of automation should prevent accidents, the current tort framework may act as a deterrent to adopting safer technologies. This section proposes a new categorization of AI- generated torts, contends that the acts of AI tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and goes on to propose rules for implementing the system. Finally, Section 3 argues that once AI becomes safer than people, and automation is practical, the “reasonable AI,” or the catchier reasonable robot in the case of a physically embodied AI, should become the new standard

135
of care. It explains how this standard would work, argues the reasonable robot standard works better than a reasonable person using an AI, and considers when the standard should apply to AI tortfeasors. At some point, AI will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
1 Liability for Machine Injuries
A Brief History
For as long as people have used machines, injuries have resulted – and machines have been with us for quite some time. The earliest evidence of simple machines, which are tools that redirect force to make work easier like pulleys and levers, dates back millions of years to the beginning of the Stone Age. In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as axes. The primary function of these tools was to hunt and cut meat, but they were also used to facilitate violence against people. Machines used in the furtherance of intentional torts were no doubt used negligently as well. Given that home knife accidents lead to about one-third of a million emergency room visits a year in the United States, it is a safe bet that during the Stone Age these simple machines caused accidents. As time progressed, and the use and complexity of simple machines grew, so too did the resultant injuries: Mesopotamian surgeons botched procedures and Greek construction zones were so dangerous they required that physicians be on-site. In any case, such injuries continued unabated from the time complex machines were invented by the ancient Chinese and Greeks to the time of the first modern industrial machines.

136
The Industrial Revolution marked a turning point in the role of machines in society. Major technological advances in textiles, transportation, and iron making occurred during this period, and these resulted in the development of machines for shaping materials and the rise of the factory system. It also resulted in a dramatic increase in the number and severity of machine injuries. Working in industrial settings was a dangerous business, in part because employers often had minimal liability for employee harms. These dangerous working conditions persisted well into the twentieth century before the US government began collecting data on work-related injuries in a systematic way. In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of sixty-one deaths per 100,000 workers.1
In the modern era, the rate of work-related injuries has declined significantly. In 2017, for example, the Bureau of Labor reported 5,147 fatal work injuries, a rate of 3.5 per 100,000 workers.2 There are several reasons for this decline: changes to tort liability, evolved societal and ethics norms that place a greater priority on human welfare, a modern system of regulations and criminal liability that protects worker well-being as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Every year, just in the United States, workplace accidents cost around $190 billion and are responsible for about 4,000 deaths.
   1
, 48 CDC MORBIDITY & 461, 461 (1999). The National Safety Council estimates that 18,000–
Improvements in Workplace Safety – United States, 1900–1999
 MORTALITY WKLY. REP.
21,000 workers died from work-related injuries in 1912. Id.
  2
, NATIONAL CENSUS OF FATAL OCCUPATIONAL 2017, at 1 (2018), http://www.bls.gov/news.release/pdf/cfoi.pdf.
BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR
 INJURIES IN
 
137
More broadly, unintentional injuries cost around $850 billion and kill more than 150,000 people annually – around 6 percent of all deaths. According to the Centers for Disease Control and Prevention (CDC), unintentional injuries are the third-leading cause of death.
Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health. Tort law is one of the primary ways in which society chooses to allocate liability, and this has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing (the obligatory puff of air one has come to expect in an optometrist’s office), or it can discourage the use of new technologies, as is generally the case in medical care, a field in which the standard of care is usually based on customary practice. Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts, which do not require fault. Covering the “great mass of cases” in the middle are harms involving negligence.3
  Oliver Wendell
 Holmes
  Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873) in 1 THE
3
COLLECTED WORKS OF JUSTICE HOLMES 327 (Sheldon M. Novick ed., 1995).

138
Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms. In practice, to prevail in most personal injury cases, a plaintiff (or claimant) must prove by a preponderance of evidence (more likely than not) that the defendant (or respondent) owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages. This generally requires proof that the defendant acted negligently, which is to say, they acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circumstances. Thus, if the courts determine that a reasonable person would not have headed out to sea without a radio to warn of storm conditions, manufactured a ginger beer with a snail inside, or dropped heavy objects off the side of a building (all real, and famous, cases involving careless behavior), then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compensating victims as well as encouraging economic growth and progress. One way that tort law attempts to achieve this balance is by permitting recovery in negligence only where there has been socially blameworthy conduct. Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will generally be no liability. Juries play a key role in determining the reasonable person standard as applied to the facts of a case.

139
Strict and Product Liability
While negligence governs virtually all accidents, there are exceptions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities like disposing of hazardous waste or using explosives. Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy. Thus, a defendant corporation that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defective, or its properties are falsely represented. Product injuries cause upward of 200 million injuries a year in the United States. In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products. The bulk of product liability cases involve claims for damages against a manufacturer or retailer by a person injured while using a product. Typically, a plaintiff will try to prove that an injury was the result of some inherent defect of a product or its marketing, and that the product was flawed or falsely advertised. Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and accurately marketed. Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product, or that something other than the product caused the harm.

140
Product liability was not always governed by strict liability. Originally, US courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise. Toward the end of the nineteenth century, however, states began increasingly employing the doctrine of caveat venditor (let the seller beware) and an implied warranty of merchantable quality. Under this doctrine, “Selling for a sound price raises an implied warranty that the thing sold is free from defects, known and unknown [to the seller].”4 Yet even so, manufacturers were in large part able to avoid liability for defective products by essentially arguing that they lacked a contract with consumers (privity of contract). This was possible because in most cases consumers purchased products from third-party retailers rather than directly from manufacturers.
This changed in 1916 with the New York Court of Appeals decision in MacPherson v. Buick Motor Co. The case involved a motorist who was injured when one of the wooden wheels of his Buick collapsed. He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if “the manufacturer of such a foreseeably dangerous product knows that it will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”5 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted the case’s holding. This shift was accompanied by growing public support for consumer protection
4 Gardiner v. Gray, (1815) 171 Eng. Rep. 46, 47 (K.B.). 5 Id. at 1054.
 
141
together with the understanding that liability would not unduly burden economic activity. Businesses are often in the best position to prevent product injuries and can distribute liability through insurance.
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products Inc., which held that manufacturers of defective products are strictly liable for injuries they cause. This case represents the birth of modern products liability law in the United States. After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s.
Of course, today’s product liability law is not as simple as this brief narrative suggests. It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts. Since the 1960s, a variety of state statutes have attempted to reform products liability law, often to limit the rights of consumers in order to protect manufacturers. For our purposes, however, it suffices to say this: As a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.
2 AI-Generated Torts
Automation Will Prevent Accidents
On March 18, 2018, an AV operated by Uber killed a pedestrian in Arizona. The AV, a Volvo XC90 SUV, was in autonomous mode when it struck Elaine Herzberg, who was crossing the street with her bicycle, at around 10 p.m. The AV was equipped with light detection and

142
ranging systems (Lidar), which illuminate targets with pulsed laser light and measure the reflected pulses with a sensor. Lidar should detect objects hundreds of feet away as well at night as during the day. Uber’s self-driving software detected Herzberg but failed to stop in time. Because the AV was still in testing, Uber had a “backup” driver in the vehicle to take control in the event the autonomous software failed. However, the driver was watching The Voice, a TV show, on her mobile phone and failed to notice Herzberg in time. For that matter, Herzberg was crossing an unmarked, unlit segment of road at night without paying attention. There seems to have been plenty of blame to go around, but nothing absolves the AV of failing to stop. This event is the first pedestrian fatality involving an AV, and it is generally considered the first death caused by an AV. Earlier fatalities had occurred with Tesla drivers operating in Autopilot mode, but regulators did not consider the AV at fault in those cases.
Surveys of attitudes toward self-driving cars have produced mixed results but have often uncovered negative opinions. A 2016 survey by the American Automobile Association reported that three out of four US drivers surveyed said they would feel “afraid” to ride in a self-driving car.6 Only one in five said they would trust a driverless car to drive itself while they were inside. Another survey found that most UK citizens would feel uncomfortable with self-driving vehicles on the road, and more than three-quarters would want to retain a
6 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM, Mar. 1, 2016, http://newsroom.aaa.com/2016/03/three-quarters-of- americans-afraid-to-ride-in-a-self-driving-vehicle/.
   
143
steering wheel.7 Regulators are more optimistic, but they are still being cautious. Until 2016, California required human drivers to be present in all self-driving cars being tested on public roads. Unmanned vehicles can now operate on public roads under certain circumstances.
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether AI is perfect (it is not), but whether it is safer than people (it will be). Nearly all crashes, 94 percent, involve human error. A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs. The US Department of Transportation estimates the domestic economic costs of those accidents at more than $240 billion. More than 35,000 people die from motor vehicle accidents each year in the United States, and more than a million die each year worldwide. Someone is killed in a motor vehicle accident on average about once every twenty-five seconds.
By contrast, the Uber fatality was the first known autopilot death in hundreds of millions of miles driven by AVs. It is also important to note that driverless technologies are in their infancy. Self-driving technologies will be dramatically improved in a decade. At the point where automated cars are ten times safer than human drivers, that could reduce the annual number of US motor vehicle fatalities to about 3,500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicts AVs will reduce the number of automobile deaths by about 30,000 a year.8 However, the report estimates that
7 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, THE INQUIRER, Oct. 17, 2016, www.theinquirer.net/inquirer/news/2474351/over-half-of-brits-
wont-feel-safe-using-the-streets-with-driverless-cars.
     Michele
Bertoncello
&   Wee, Ten Ways Autonomous Driving Could Redefine the
Dominik
8
Automotive World, MCKINSEY & CO., June 2015, www.mckinsey.com/industries/automotive-
 
144
self-driving technologies will not be adopted widely enough to permit this outcome until the middle of the century.
AVs may be the most prominent and disruptive upcoming example of AI automating important activities with a significant risk of harm, but automation has the potential to improve safety in a variety of settings. For instance, IBM’s AI Watson is working with clinicians to analyze patient medical records and provide evidence-based cancer treatment options. Like self-driving cars, Watson does not need to be perfect to improve safety – it just needs to be better than people. Medical error is one of the leading causes of death. Estimates vary, but a 2016 study in the British Medical Journal reports that it is the third- leading cause of death in the United States, behind cardiovascular disease and cancer (the CDC ranks unintentional injuries as the third-leading cause of death but does not include medical error in cause of death rankings).9 Some companies already claim their AI outperforms doctors at certain tasks, and those claims are believable. Why should AI not be able to outperform a person when the AI can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?
and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
    Michael
Daniel
 9
&     , Medical Error – The Third Leading Cause of Death in
, 353 BMJ 2139, 2139 (2016); See also INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER
Martin A.
Makary
 the US
HEALTH SYSTEM (Linda T. Kohn et al., eds., 2000).

145
Strict Liability Discourages Automation
To see why these tort frameworks discourage automation, let us turn to the question of when it makes economic sense for a business to replace a person with an AI. In practice, it may be complex to calculate the cost of a human driver vs. a self-driving vehicle. Human employees have costs in excess of their salaries and wages, such as tax liability, as discussed earlier, for employer portions of social security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; employer portions of health insurance; paid holidays, vacations, and sick days; and contributions toward retirement, pension, savings, and profit-sharing plans, etc. AI costs may be simpler to estimate but may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision. For example, a person may require vocational training or be unable to work due to sickness; AI may require software updates or may malfunction. Human operators may result in greater expenses for legal fees, administrative and overhead costs, and compliance with regulatory and employment requirements. AI may infringe patents or result in negative publicity. Whether to automate may also take into account broader social policies. For instance, businesses may choose not to automate because of a perception it promotes income inequality and unemployment. But businesses are required to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to primarily promote social responsibility.

146
The decision whether to employ a person or AI, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making – estimating uncertain future costs and making decisions as rational economic actors. Tort liability will only be one factor to consider when deciding whether to automate. But, in the aggregate, tort liability will influence AI adoption.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, a business end user may not be directly liable for harms caused by AI. The AI’s manufacturer and other members of the supply chain will generally be liable if the harm was caused by a defect in the AI. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability by, for instance, relying on independent contractors. Businesses are not usually liable for negligent harms caused by their independent contractors. Yet even in cases where liability rests with a business supplier or an independent contractor, such liability should indirectly impact a business. A manufacturer/retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent contractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product. Although tort liability may be indirect and complex, and firms may purchase insurance to manage risk, this does not change the fact that tort liability has a financial cost that influences behavior.
If both human and AI operators cost a business the same amount to employ, the decision of which to utilize should be neutral. However, if a business introduces the

147
variable of tort liability into the decision, assuming that an AI and person are competitive in terms of safety, a human operator would be preferred. Harms caused by a person will be evaluated in negligence while the same harms caused by an AI will be evaluated in strict liability. It is generally easier to establish strict liability than negligence. Strict liability does not require careless manufacturer behavior, only that a defect be present in a product or its marketing. At least with regard to tort liability, the law favors people over AI. This will hold true as long as AI is treated as an “ordinary product” in which strict liability is and will be the default rule.
AI-Generated Torts Should Be Negligence-Based
Holding AI-generated torts to a negligence standard will result in an improved outcome: It will accelerate automation where doing so would improve safety. Of course, moving from a strict liability to a negligence standard will have some drawbacks. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to ensure against loss. Indeed, this is why courts initially adopted strict product liability. AI-generated torts, however, differ from other product harms in that – once AI becomes safer than people – automation will result in net safety gains.
To illustrate this, imagine that with current technology an AV would be ten times safer than a human driver. In this case, it would be better that one human driver be replaced by an AV than that the same AV become a hundred times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk

148
of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for an AV (model C-A) is one fatality per one billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now, imagine that we are able to improve C-A an additional tenfold, such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of ten billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, however, instead of focusing our efforts on improving C-A, we simply replace the human driver with another C-A, then over the course of 10 billion miles driven by C-A and C-A, there will be a total of twenty fatalities. Once AI becomes safer than people, and particularly once AI becomes substantially safer than people, very significant reductions in accident rates will be gained by automation. Therefore – at some point – it will be preferable to weaken the incentive to gain incremental improvements in product safety and to increase the adoption of safer technologies.
A negligence standard will still influence manufacturers to improve AI safety in order to reduce their liability. If an AI causes an accident a person would have avoided, the AI’s behavior would fall below the standard of reasonable care and result in liability. Of course, we would not want to automate with AI that is less safe than a person, and this is a reason why jurisdictions prohibit unrestricted use of fully autonomous vehicles. Because AVs cannot currently outperform human drivers in any condition, their use is limited to controlled settings, a preemptive step to waiting for them to cause accidents and then arranging for compensation. However, to the extent we do rely on tort liability to limit the introduction and use of comparatively less safe technologies rather than regulatory

149
prohibitions, holding AI-generated torts to a negligence standard would have the desired effect – AI manufacturers will be financially liable when it causes accidents a person would have avoided. This could occur if AI is mistakenly predicted to be safer than it really is at the time of its rollout, or if there are other compelling reasons to automate. For example, it may be that Tesla has reason to believe its self-driving cars are significantly safer than human drivers, but once its cars enter the marketplace, they fail to meet expectations because, say, Tesla’s research fails to consider the reactions of drivers to self-driving vehicles in states other than California.
Even an AI that is generally safer than a person may still cause accidents. If it causes an accident a person would have avoided, this will result in liability. An AV may cause an accident on average once every billion miles compared to a person who causes an accident every hundred million miles, but any particular accident caused by the AV may still fall below the standard of reasonable care. Manufacturers will likely have the best information available to determine whether it would be better to pay to further reduce accident risks (e.g., whether an additional $10,000 per AV is worth a 1 percent reduction in accident risk, or whether to pay claims for additional accidents). Higher safety levels may not always be preferable as inefficiently high safety levels may result in prohibitively high prices for consumers. To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, nontort mechanisms could be brought to bear, such as regulatory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.

150
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and AI tortfeasors like Uber’s AV. Society’s relationship with technology has changed. Machines are no longer just inert tools directed by individuals. Rather, in at least some instances, AI is taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, AI is stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from an AI tortfeasor in this system are the concepts of independence and control. Autonomous AI are given tasks to complete and functionally determine for themselves the means of completing those tasks. In some instances, machine-learning can generate unpredictable behavior, such that the means are not predictable either by those giving tasks to AI or even by the AI’s original programmers. But the difference between ordinary products and AI tortfeasors should not be based on predictability, only on social and practical outcomes. It makes no difference to a person run over by a self-driving car what type of AI was operating the vehicle. Whether an AI acts according to fixed or expert rules created by a programmer or more complex machine- learning algorithms such as neural networks that generate new and sometimes unforeseen behaviors, the physical outcome is the same. Leave aside the difficulties with courts attempting to distinguish between different types of AI architecture; ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.

151
Identifying AI-Generated Torts
Not all injuries for which a machine or AI is involved would be AI-generated torts. To illustrate, consider two hypothetical accidents:
1 A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop-off.
2 A crane operator is appropriately manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and we can assume) that the operator is at fault (although a creative plaintiff’s attorney may argue the crane was negligently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a factual cause of the injury in torts vernacular, the machine did not interrupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an extension of the operator. In the second hypothetical, allocating fault is once again intuitively obvious. The machine is at fault rather than the operator. The operator acted with reasonable care, and the injury was due to (we may again assume) a flawed crane.
These two scenarios would result in different liability outcomes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby, even if the manufacturer had exercised the utmost care in the design and construction of the crane.

152
In both scenarios, an operator is using a crane in much the same way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dynamic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.
Now imagine a third scenario:
3 An AI-operated, unmanned crane drops a steel frame on a passerby after
incorrectly identifying the location for drop-off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a machine is being used as a tool. In Example 3, an AI has stepped into the shoes of the worker; it has replaced a person, and it is performing in essentially the same manner as a person. If the AI was a person, it would be liable in negligence and held to the standard of a reasonable person.
Holding suppliers of AI tortfeasors to a negligence standard requires rules for distinguishing between AI-generated torts and other harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is an “AI tortfeasor” (Example 3).

153
AI-generated torts could be those cases in which an AI is engaging in activity that a person could engage in, and that acts in a manner that would be negligent for a human tortfeasor. Applying this rule to the crane examples, Example 1 would result in human liability because the human operator acted carelessly, and the crane did not interrupt a foreseeable chain of events. There would be strict manufacturer liability in Example 2 because a person could not reasonably be substituted for a crane. Example 3 would require negligent manufacturer liability since the AI was automating a task that a person could have performed.
Sometimes this rule will have clear application. For example, an AI that mistakenly “reads” a chest X-ray in place of a radiologist would be occupying the position of a human tortfeasor whereas a malfunctioning X-ray machine would be an ordinary product. In other instances, this distinction will not be clear cut. Electrocardiogram (ECG) machines are a vital feature of emergency rooms where they are used to evaluate patients for heart conditions. These machines often provide interpretation of raw data but generally state that any analysis is preliminary and that health care providers are ultimately responsible for diagnosis. In the context of the self-driving car, under the most widely adopted framework, vehicles are categorized on a zero to five scale based on “who does what, when.”10 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction between person and machine. Autonomy exists on a continuum, and while it may be clear in some cases that
10 See SAE INT’L, AUTOMATED DRIVING (2014), http://www.sae.org/misc/pdfs/automated_driving.pdf (describing the SAE taxonomy).
  
154
an AI is acting like a person or an ordinary product, in other cases there may be an overlap of responsibility and decision-making between people and AI.
When an individual and an AI both contribute to a harm, both may be liable either jointly or individually in proportion to their wrongdoing. For instance, when a human driver and an AI driver are both at fault, as may have been the case in which Uber’s system failed to stop in time for a pedestrian and the backup driver was watching a TV show, both drivers could be found equally negligent. With an ECG, the sort of analysis commonly used ECGs perform has value, but it is widely understood not to be at the level of a human physician and is legitimately not intended to replace a human diagnosis. ECGs are better considered an ordinary product and holding them to the standard of a physician would result in them routinely found liable for medical negligence. This would likely result in manufacturers no longer providing analysis that now has value to doctors. However, a supplier should not be able to avoid liability simply by disclaiming liability for a product that is obviously automating human activity.
It should not be necessary for an AI to actually replace a human operator for negligence to apply. It should be sufficient that an AI is performing a task that a person could reasonably do. Thus, if a new taxi company goes into business using a fleet of only AVs, AI would not have replaced human operators, but AI would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software (e.g., the engine) could not be reasonably substituted. A person could drive a taxi instead of an AI, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as an AI tortfeasor, but the other parts of the vehicle would not. Thus, the test for an AI-generated tort could focus on whether the

155
allegedly negligent AI behavior involves the sort of task a person would usually perform or, if automation has become standard, that a person would have performed.
The negligence test should then focus on whether the AI’s act was negligent, rather than whether the AI was negligently designed or marketed. Again, the AI is taking the place of a person in the traditional negligence paradigm, and this would treat the AI more like a person than a product. It makes no difference to an accident victim what an AI was “thinking,” only how it acted. Accident victims have a right to demand careful conduct, regardless of how well an AI tortfeasor may have been designed.
There is another important reason why focusing on an AI’s act is more appropriate than focusing on its design. As discussed in Chapter 1, many AI systems, such as those utilizing machine-learning, are becoming increasingly complex and have limited explainability. It may be difficult or impractical for anyone, including an AV supplier, to determine why a self-driving car, say, ran a red light. But if a harmful act stemmed from defective software then such a determination may be necessary to prove a product has a defect and thus to establish liability. Even worse, it will almost certainly be substantially more challenging for a plaintiff to establish a product defect than it would be for a defendant. The plaintiff may need to hire (quite expensive) experts to investigate a self- driving car’s AI, and that presupposes a court will permit external access to a supplier’s (likely) proprietary AI system. The cost and complexity of such an inquiry probably puts it beyond the feasibility of accidents without at least hundreds of thousands or millions of dollars in damages. At least in the US system (although not the UK system), plaintiffs usually cannot recover their legal costs even when they prevail in court. For some injured victims then, lawyer and expert fees may exceed any likely recovery which means that

156
some meritorious claims will not be pursued. By contrast, a negligence test focused on an AI’s act and whether it fell below the standard of a reasonable person would simply ask whether an AV ran a red light and whether a reasonable person would have done the same. This is a far simpler and less expensive case to prove.
Of course, it is sometimes possible when alleging negligence or even a defective product for a plaintiff to prove their case with inference on the basis of reason, logic, bad behavior by a party, or common sense. As one example, a legal doctrine known as res ipsa loquitur – “the thing speaks for itself” – allows circumstantial evidence to permit an inference that a defendant was at fault. For example, if a barrel falls off a building striking a pedestrian and causing harm the pedestrian may have a difficult or impossible time providing what caused the accident. But because barrels do not tend to fall off buildings absent careless behavior, the mere fact of the barrel’s falling may be sufficient to establish negligence absent compelling contradictory evidence from a defendant. Similarly, when a medical instrument is left in a patient after surgery, that tends to be adequate to establish negligence, even though a patient may have no way of proving what occurred. However, inference, including through application of res ipsa loquitur, is not a panacea to the challenges posed by AI tortfeasors – at least as currently applied by courts. For example, res ipsa loquitur is not recognized by every state, some states require that a defendant have exclusive control over a product (which can be problematic with the involvement of multiple parties), and not all states allow for the presumption of a product defect. There are similarly restrictions on the use of inference more generally in many jurisdictions to avoid speculation and unfairness to defendants. The mere fact an accident occurred with a product does not necessarily mean there was a defect.

157
Financial Liability
Whether in strict liability or negligence, AI could not be directly financially liable for its harms. AI does not have property rights; in fact, AI is owned as property and would not be influenced by the specter of liability in the way a person could be influenced. For the purposes of financial liability, the AI’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product, or where several parties are involved in the distribution chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which General Motors licenses and incorporates in its vehicles, and an independent retailer leases the vehicles to Lyft. Default liability rules could be altered by firms in the supply chain by contract. This would be particularly likely to occur in cases in which manufacturers and retailers are large, sophisticated entities. For example, General Motors may indemnify Apple, Delphi, and Lyft in return for more favorable licensing and leasing terms.
Alternately, the AI’s owner could be liable for its harms, which would be somewhat akin to treating AI tortfeasors as employees and making owners liable under theories of vicarious liability – when someone is held responsible for the actions of another person. It is particularly easy to imagine owners purchasing insurance for harms caused by AI in the context of a self-driving car. Insurance policies may soon come with a rider (or discount) for AV software. Owner liability may further incentivize the production of autonomous AI given that manufacturers would have less liability, but this may also reduce adoption since

158
owners would be taking on that liability. These two effects may offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of AI tortfeasors, and because manufacturers are in the best position to improve product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self- driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars become significantly safer than human drivers, insurance rates will decrease for self-driving cars and perhaps increase for human drivers. This should have a nudging effect on self-driving car adoption, as financially sensitive individuals take automobile premiums into account in deciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower premiums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsible for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the creation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation. For example,

159
incentives could take the form of grants for research and development, loans to build production facilities, enhanced intellectual property rights, prizes, preferential tax treatments, or government guarantees.
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allowance Rebate System (CARS), better known as “cash for clunkers.” CARS provided consumers trading in old vehicles with vouchers of between $3,500 to $4,500 to purchase new cars. It was a $3 billion US federal program designed as a short-term economic stimulus and to benefit US automobile manufacturers. It was also intended to promote safer, cleaner, more fuel-efficient vehicles. Ultimately, while critics dispute the effectiveness of the program at stimulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel-efficiency and safety, and it was popular with consumers. In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alternatives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that would place caps on damages, limit contingency fees (the ability of lawyers to obtain a percentage of a client’s recovery as a fee for services), mandate periodic payments, or reduce the statute of limitations (the time limit for suing).
Finally, the government could promote safety by means of regulation. This could involve requirements for industries to achieve minimum safety targets or direct requirements to adopt certain technologies. At the point where self-driving cars become ten or a hundred times safer than people, traditional human driving could be prohibited. Regulatory solutions may be most appropriate when the benefits of automation are

160
overwhelming, and when it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer and business-friendly solution. While consumers may have more difficulty seeking to recover for accidents, they should also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. Businesses would have lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not require government funding, additional regulatory burdens on industry, or new administrative responsibilities. It would provide an incremental solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than with more radical solutions. For all the above reasons, shifting to negligence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer credits, combined with direct regulations in certain instances. Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable because it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy. Indeed, automation to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could

161
independently adopt these rules. Lawmakers would then have the option of modifying the common law.
3 The Reasonable Robot
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
- Oliver Wendell Holmes Jr.
When Negligence Is Strict
Negligence may function almost like strict liability for people with below-average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.” This issue was at the heart of a case in 1837, Vaughan v. Menlove, that concerned a defendant who lacked normal intelligence. The defense argued that it would be unfair to hold him to the standard of an ordinary person, and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, holding that ordinary prudence should apply in every case of negligence. As famed judge Oliver Wendell Holmes Jr. later articulated in 1881, “The law considers ... what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”11 This remains the case
  OLIVER WENDELL
 HOLMES JR.
 11
, THE COMMON LAW, 108 (1881).

162
today: A modern defendant cannot generally escape liability for causing a motor vehicle accident because she has slow reflexes, poor vision, or anxiety while driving.
There are benefits to such a rule. Logistically, as the court noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state. Better for administrative purposes to work with an external, objective standard than to prove individual capacities and state of mind. Substantively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal conduct of others. As Holmes articulates, damages caused by individuals with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else “should perhaps refrain from driving at all.”12
The New Hasty and Awkward
Collectively, people are not the best drivers, even when they refrain from drinking behind the wheel, falling asleep on the highway, or colliding into police cars while playing Pokémon Go. But compared to AI? It will not be long until AI is safer than the average person, and then safer than any human driver. Principles of harm avoidance suggest that once it becomes practical to automate, and that doing so is safer, an AI should become the new “reasonable person” or standard of care.
12 Roberts v. Ring, 143 Minn. 151, 153 (1919).
 
163
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defendant would be judged against what an AI would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an AV would have prevented the injury. In fact, it may be that the AV is only able to prevent such an accident because it has superhuman abilities. It may have software capable of ultrafast decision-making, monitors that surpass human senses, and access to external cameras that expand peripheral view beyond that of a person.
With the reasonable person test, jurors are asked to put themselves in the shoes of a reasonable person and decide what that person would have done. It may be a challenge for a juror to follow that reasoning in the case of a “reasonable robot.” The reasonable robot, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasonable” in the context of an AI is an anthropomorphism to assist people conceptually. To take a simple case, imagine an individual driving on dry pavement at forty miles per hour and then colliding with a child running into the road 150 feet ahead of the driver’s vehicle. To determine whether the driver is liable under the reasonable robot standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable robot would not have collided with the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of AI would be known, and the breaking distance could be

164
standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software.
A defendant may argue that it is unfair for his best efforts to result in liability. A reasonable robot standard essentially makes people strictly liable for their accidental harms. This is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below-average driver due to computers. It may appear unfair to impose liability on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or an AI.
Tort liability would not prohibit people from driving even at the point when AVs become substantially safer than people. If that were a desired outcome it could be accomplished through a legislative ban on human driving. Instead, an AI standard of care would mean that people drive at their own risk. If a driver causes an accident, he or she will be liable for the resultant damages. A tort-based incentive may be preferable to an inflexible statutory mandate because there are benefits to human driving unrelated to accidents, such as for promoting freedom and autonomy. Individuals who particularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, an AI standard of care is likely to have a significant impact on behavior. Making individuals and businesses effectively strictly liable for their harms will discourage certain undertakings. In the context of the self-driving car, it

165
would likely result in far fewer human drivers as insurance rates for traditional vehicles would become more expensive relative to self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. AI may be prohibitively expensive or only available in limited quantities, which is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. Therefore, to introduce a reasonable robot standard, a plaintiff should have to show that a person was performing a task that could be performed by an AI and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of an AI in which 1) no such AI existed at the time of the accident, 2) no AI was available to the defendant, 3) an AI was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory requirements for a human driver). If Tesla could manufacturer a completely safe AV at a cost of $1 million, it would not be reasonable to require all consumers to automate.
Reasonable People Use AI
As an alternative to the reasonable robot standard, the reasonable person could be a person using an AI. For example, once self-driving cars become safer than people, a jury may find that it is unreasonable to drive yourself rather than to use an AV. Applying the reasonable person using an AI to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on his driving in the first place.

166
A reasonable person would not have driven; a reasonable person would have chosen to automate.
Under either the reasonable person or reasonable robot standard, a human driver would be compared with a self-driving car but in different ways. With the reasonable robot standard, courts would evaluate the human driver’s proximally harmful act whereas with the reasonable person standard, courts would evaluate the human driver’s initial decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable person standard would be more in line with the existing negligence regime.
While keeping the reasonable person standard would be conceptually easier, in practice it would be less desirable. The goal is to compare the harmful act of the person and AI, not target the initial decision to automate. It is problematic to base liability on the decision to automate because it must focus on the question of whether automation is either generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform an AI. A situational focus must still compare the harmful act of a person versus an AI.
AI is and will be safer at automating certain activities than others. For instance, AI working to diagnose disease may be superior to physicians at detecting certain conditions but not others. Self-driving cars may be safer than human drivers on average but not safer than professional or above-average drivers. AVs may also be safer under most conditions but may be relatively poor at, for example, driving off-road. So, while automation may generally improve safety, optimal accident reduction may require a mix of AI and human activity.

167
Suppose an AV is ten times safer than a human driver generally but only half as safe as a human driver in icy conditions. Now, suppose a human driver encounters a patch of black ice and causes an accident under circumstances when she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable robot, she would escape liability if the AI would have been unable to avoid the accident (which is likely if the AI is half as safe in icy conditions). If the reasonable person using an AI test focuses on whether an AI is generally safer, however, she would be liable. This test would conclude that it would have been unreasonable not to use a self-driving car since self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an AI evaluation could be situational. For instance, it could be reasonable not to use an AI but only in icy conditions. However, this is just a more convoluted version of the reasonable robot test, because it requires evaluating whether an AI would be safer than a person in a particular instance. This essentially asks how the AI would have acted in a situation – which would be the application of the reasonable robot standard. It would then require asking, based on that knowledge (which may be impractical for a person to have), whether an earlier decision to automate was reasonable. In the black ice hypothetical, it could require the driver to know in advance of activating or deactivating self-driving software whether there are icy conditions and how the AI would perform in icy conditions to determine if the risk of using the AI in icy conditions outweighs the benefits of using the AI for other parts of the trip.

168
The Reasonable Robot Standard for AI Tortfeasors
This chapter has advocated for holding AI tortfeasors to a negligence standard and comparing their acts to the acts of a reasonable person. It has also proposed replacing the reasonable person standard with the reasonable robot standard once automation is practicable and AI is safer than an average person. This means that future AI tortfeasors would be held to the reasonable robot standard.
There may be instances in which it still makes sense to apply the reasonable person standard to AI tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of an AI, such as where automation is prohibitively expensive or where AI is not widely available. We would not want to hold an AI tortfeasor to a higher standard than a human defendant. In some industries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become normal.
Eventually, once an AI becomes the standard of care, it would also be the standard for AI tortfeasors. For instance, if a self-driving Audi collides with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Google cars. There are a variety of ways to determine the reasonable robot standard by, for example, considering the industry customary, average, or safest technology. Under any standard, this is a different test than the current strict liability standard in which the inquiry focuses on whether a product is defectively designed, or its properties falsely represented.

169
As AI improves, the reasonable robot standard will grow stricter, which is alright, because once AI is exponentially safer than a person, it is likely that AI tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable robot standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until AI is exponentially safer than people.
Further Thoughts
AI presents new challenges to tort law as it does to tax law. Once again, applying a principle of AI legal neutrality to tort law will guide the development of AI and help to ensure its social utility, but careful consideration needs to be given to its application. In tort law, the challenges are not the same as with tax law, and the two bodies of law have different underlying concerns, goals, and solutions. With tort law, our primary policy goal is to structure liability to optimize accident deterrence.
At some point in the future, there are likely to be few or no activities for which AI cannot outperform people. Self-driving cars will eventually be a thousand times safer than the best human driver, at which point AI will cause so little harm that the economics of negligence versus strict liability will be irrelevant for AI manufacturers. AI will have become so ubiquitous that the constantly improving reasonable robot should be the benchmark for most or all areas of accident law.
Applying a principle of AI legal neutrality to tort law could encourage the development and adoption of safer technologies that could prevent countless accidents. It

170
has become acceptable for more than a million people a year to die in traffic accidents worldwide, but only because a reasonable alternative has not yet been within reach. We could soon be living in a world where practically no one dies from unintended injury. Once the third-leading cause of death is eliminated, we would just be left to deal with the leading two causes of death: cardiovascular disease and cancer. A different type of AI, artificial inventors, may eliminate those as well.

171
4
Artificial Inventors
Computers are useless. They can only give you answers.
- Pablo Picasso
As with tax and tort law, AI poses new challenges to intellectual property law. In the case of patent law, AI is already generating patentable inventions under circumstances in which the AI, rather than a human inventor, meets the requirements to qualify as an inventor. Yet because almost every country in the world requires a patent application to list a natural person as an inventor, a requirement designed to ensure the right of human inventors to be acknowledged, it is not clear that an “AI-generated invention” could be patented. There is no US statute addressing AI-generated invention, no case law directly on the subject, and there is not even a relevant policy by the responsible administrative agency, the US Patent and Trademark Office (Patent Office). Nor, at least in 2019, does there appear to be a statute specifically about AI-generated invention anywhere else in the world.
In 2019, a team of patent attorneys, in a project spearheaded by this book’s author, announced they had filed the first patent applications in several jurisdictions worldwide that explicitly claimed inventions autonomously generated by an AI. Those applications listed the AI’s owner as the patent applicant and thus the owner of any future patents, and listed the AI as the inventor. As of the time of writing, these applications are still pending.
Historically, academics have argued against allowing protections for AI-generated inventions on the grounds that machines do not respond to incentives and that AI

172
inventorship may chill human invention. In 2019, a spokesperson for the European Patent Office argued, “It is a global consensus that an inventor can only be a person who makes a contribution to the invention’s conception in the form of devising an idea or a plan in the mind ... . The current state of technological development suggests that, for the foreseeable future, AI is ... a tool used by a human inventor ... . Any change ... [would] have implications reaching far beyond patent law, i.e., to authors’ rights under copyright laws, civil liability and data protection.”1
The principle of AI legal neutrality suggests that we should allow intellectual property protections for AI-generated inventions. Although AI would not be directly motivated to invent by the prospect of a patent, patents would motivate some of the people who build, own, and use AI. Allowing intellectual property protections for AI output would thus incentivize the development of inventive AI. In turn, this would incentivize innovation and lead to new scientific advances – which is the primary purpose of the patent system. This is particularly critical for the day AI will be a meaningful source, or even the primary means, of generating new inventions. In addition, AI rather than a person should be listed as an inventor when it otherwise meets inventorship criteria. Although having a person take credit for the work of an AI would not be unfair to the AI, it would be unfair to other human inventors because it would dilute the nature of inventorship. It would equate the work of legitimate human inventors with people asking AI to solve a problem. The AI’s owner should be the default owner of any patents on an AI’s invention.
1 Leo Kelion, AI System “Should Be Recognized as Inventor,” BBC, Aug. 1, 2019, www.bbc.co.uk/news/technology-49191645
    
173
This chapter is divided into three sections. Section 1 examines instances in which AI has created patentable inventions. It finds that AI has likely been autonomously generating patentable results for decades and that the pace of such invention is likely increasing. Section 2 examines the case law related to nonhuman authorship of copyrightable material in the absence of law on the subject of AI inventorship. It argues in favor of intellectual property protections for AI-generated works, and argues an AI should qualify as a legal inventor. Next, Section 3 addresses some of the challenges posed by AI inventorship.
1 AI-Generated Inventions
Example One: The Creativity Machine
In 1994, computer scientist Stephen Thaler disclosed an AI architecture he called a “Creativity Machine,” a computational paradigm that he argued “came the closest yet to emulating the fundamental mechanisms responsible for idea formation.”2 A Creativity Machine combines an artificial neural network that generates novel output in response to self-stimulation of the network’s connections with a “critic” network that monitors the first network’s output. The critic network can evaluate this output for novelty compared to the AI’s existing knowledge base and can provide feedback to the first network to continue developing some novel output or to stop. This results in an AI that “brainstorms” new and creative ideas after it alters (perturbs) the connections within its neural network. An example of this phenomenon occurred after Thaler exposed a Creativity Machine to some
2 See What Is the Ultimate Idea?, IMAGINATION ENGINES INC., www.imagination-engines.com.
  
174
of his favorite music, and the AI proceeded to generate eleven thousand new songs in a single weekend.
Thaler compares a Creativity Machine and its processes to a human brain and consciousness. The two artificial neural networks mimic the human brain’s major cognitive circuit: the thalamo-cortical loop. In a simplified model of the human brain, the cortex generates a stream of output (or consciousness) and the thalamus brings attention (or awareness) to ideas of interest. Like the human brain, a Creativity Machine is capable of generating novel patterns of information rather than simply associating patterns, and it is capable of adapting to new scenarios without additional human input. Also like the brain, the AI’s software is not entirely written by programmers – there is a degree to which it is self-assembling. Thaler argues his AI is very different from a program that simply generates a spectrum of possible solutions to a problem combined with an algorithm to filter for the best ideas generated, though such a program would be another method for having an AI develop novel ideas.
Thaler invented the Creativity Machine paradigm, which was the subject of his first patent, titled “Device for the Autonomous Generation of Useful Information.”3 The second patent filed in Thaler’s name was “Neural Network Based Prototyping System and Method.”4 Thaler is listed as the patent’s inventor, but he states that a Creativity Machine generated the patent’s invention (Creativity Machine’s Patent). The Creativity Machine’s Patent application was first filed on January 26, 1996, and granted on December 22, 1998. As one of Thaler’s associates observed in response to the Creativity Machine’s Patent,
3 See U.S. Patent No. 5,659,666 (filed Oct. 13, 1994). 4 See U.S. Patent No. 5,852,815 (filed May 15, 1998).
 
175
“Patent Number Two was invented by Patent Number One. Think about that. Patent Number Two was invented by Patent Number One!”5 Aside from the Creativity Machine’s Patent, Creativity Machines are credited with numerous other inventions: the cross-bristle design of the Oral-B CrossAction toothbrush, new physical materials, and devices that search the Internet for messages from terrorists.
The Creativity Machine’s Patent is interesting for several reasons. If Thaler’s claims are accurate, then the Patent Office has already granted a patent for an invention created by a nonhuman inventor – and as early as 1998. Also, the Patent Office apparently had no idea it was doing so. Thaler listed himself as the inventor on the patent and did not disclose the Creativity Machine’s involvement to the Patent Office. He has stated this was on the advice of his attorneys.
Example Two: The Invention Machine
The Creativity Machine has not been the only source of AI-generated invention. Software modeled after the process of biological evolution, known as genetic programming, has succeeded in independently generating patentable results. Evolution is a creative process that relies on a few simple processes: mutation, sexual recombination, and natural selection. Genetic programming emulates these same methods digitally to achieve machine
  5 See Tina   ,
Brain,
www.mindfully.org/Technology/2004/Creativity-Machine-Thaler24jan04.htm (quoting Rusty Miller).
Hesman
Stephen Thaler’s Computer Creativity Machine Simulates the Human
 ST. LOUIS POST-DISPATCH
, Jan. 24, 2004,
 
176
intelligence. It delivers human-competitive intelligence with a minimum amount of human involvement.
As early as 1996, genetic programming succeeded in independently generating results that were the subject of past patents. By 2010, there were at least thirty-one instances in which genetic programming generated a result that duplicated a previously patented invention, infringed a previously issued patent, or created a patentable invention. In seven of those instances, genetic programming infringed or duplicated the functionality of a twenty-first-century invention. Some of those inventions were on the cutting edge of research in their respective fields. In two instances, genetic programming may have created patentable new inventions.
The Patent Office granted another patent for a AI-generated invention on January 25, 2005.6 That invention was created by the “Invention Machine” – the moniker for a genetic programming–based AI developed by John Koza, a computer scientist and pioneer in the field of genetic programming as well as the inventor of the scratch-off lottery ticket. Koza claims the Invention Machine created multiple patentable new inventions.7 A 2006 article in Popular Science about Koza and the Invention Machine claims that the AI “has even earned a U.S. patent for developing a system to make factories more efficient, one of the first intellectual property protections ever granted to a nonhuman designer.”8 The Invention Machine generated the content of the patent and an improved controller (a
6 Jonathon Keats, John Koza Has Built an Invention Machine, POPULAR SCI., Apr. 18, 2006, https://popsci.com/scitech/article/2006-04/john-koza-has-built-invention-machine/.
       7 See
8 Keats, supra note 6.
, 11
John R.
Koza,
Human-Competitive Results Produced by Genetic Programming
 GENETIC PROGRAMMING & EVOLVABLE MACHINES
251, 265 (2010).

177
component of most electrical products) design without human intervention and in a single pass.9 It did so without a database of expert knowledge and without any knowledge about existing controllers. It simply required information about basic components (such as resistors and diodes) and specifications for a desired result (performance measures such as voltage and frequency). With this information, the Invention Machine proceeded to generate different outputs that were measured for fitness (whether an output met performance measures).
Once again, the Patent Office had no idea of the AI’s role in the Invention Machine’s patent. Koza did not disclose the Invention Machine’s involvement. Like Stephen Thaler, Koza has stated that his legal counsel advised him at the time that his team should consider themselves inventors despite the fact that “the whole invention was created by a computer.”10 Koza has reported that his agenda in having the Invention Machine re-create previously patented results was to prove that AI could be made to solve problems automatically. He believed that focusing on patentable results would produce compelling evidence that AI is capable of generating value. For that reason, he focused on recreating or generating patentable inventions that represented significant scientific advances. For instance, the Invention Machine’s patent was for an improved version of a landmark controller built in 1995.
The Creativity Machine and the Invention Machine are early examples of AI inventors, but patents may have been granted on earlier AI-generated inventions. For
9 U.S. Patent No. 6,847,851 (filed July 12, 2002).
10 Telephone Interview with John Koza, President, Genetic Programming Inc. (Jan. 22, 2016) (on file with author).
 
178
instance, an article published in 1983 describes experiments with an AI program known as Eurisko, in which the program “invent[ed] new kinds of three-dimensional microelectronic devices ... novel designs and design rules have emerged.”11 Eurisko was an early, expert AI system for autonomously discovering new information. It was programmed to operate according to a series of rules known as heuristics, but it was able to discover new heuristics and use them to modify its own programming. To design new microchips, Eurisko was programmed with knowledge of basic microchips along with simple rules and evaluation criteria. It would then combine existing chip structures together to create new designs or mutate existing entities. The new structure would then be evaluated for interest and either retained or discarded. Stanford University filed a patent for one of Eurisko’s chip designs in 1980 but abandoned the filing for unknown reasons in 1984.12 Also, as with other known instances of patent applications for AI-generated inventions, the patent application was filed on behalf of natural persons. In this case, the individuals who built a physical chip based on Eurisko’s design.
In another example, a 1989 PhD dissertation disclosed a “systematic computational approach to innovative design of engineered artifacts in general ... comparable to human inventions ... a computer program called TED ... [which] is the inventor of its systems in the sense that it generates the structure (topology) of the system ‘from scratch.’ The synthesis process is modeled as a heuristic search conducted in a state-space of all possible design
   11
Lenat &     ,
Douglas B.
William R.
Sutherland
, 3   , 17, 17 (1982). 12 U.S. provisional patent application SN 144,960 (filed Apr. 29, 1980). Email
communications with Katherine Ku, Dir. of Stanford Office of Tech. Licensing, to author (Jan. 17, 2018) (on file with author).
Heuristic Search for New Microcircuit
 Structures: An Application of Artificial Intelligence
AI MAG.

179
versions (design states). TED explores one design possibility after another in a systematic fashion, looking for a solution to the design requirements.”13 Alexander Kott, TED’s developer, noted that TED rediscovered at least two significant and well-known inventions, and also generated previously unknown and nontrivial designs.
Example Three: Watson
The above examples are decades old. Subsequent improvements in AI should have led to a significant increase in the number of AI-generated inventions. Consider, for instance, more recent results produced by IBM’s AI Watson. Watson was an AI developed by IBM to compete on the game show Jeopardy! In 2011, ancient history in AI terms, Watson beat former Jeopardy! winners Ken Jennings and Brad Rutter on the show, earning a million dollars in the process. Since then, the “Watson” brand has evolved to incorporate a variety of AI systems and technologies, although it is sometimes marketed by IBM as if it is a single AI. For simplicity, we can consider it a single AI system.
Watson is largely structured as an “expert system.” Expert systems are one way of designing AI that solves problems in a specific domain of knowledge using logical rules derived from the knowledge of experts. These were a major focus of AI research in the 1980s. Expert system–based chess-playing programs HiTech and Deep Thought defeated chess masters in 1989, paving the way for another famous IBM AI, Deep Blue, to defeat world chess champion Garry Kasparov in 1997. But Deep Blue had limited utility – it was
13 Alexander Kott, Artificial Invention: Synthesis of Innovative Thermal Networks, Power Cycles, Process Flowsheets and Other Systems, ii (Disseration.com, 2005), www.bookpump.com/dps/pdf-b/1122640b.pdf.
   
180
solely designed to play chess. The machine was permanently retired after defeating Kasparov. IBM now describes Watson as one of a new generation of machines capable of “computational creativity.”14 IBM uses that term to describe machines that can generate “ideas the world has never imagined before.”15 Watson “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], applying big data in new ways.”16 This is a fundamentally different type of AI than the Creativity Machine or the Invention Machine. Watson utilizes a more conventional architecture of logical deduction combined with access to massive databases containing accumulated human knowledge and expertise. Although Watson is not modeled after the human brain or evolutionary processes, it may also be capable of generating inventions.
Watson’s Jeopardy! career was short and sweet, and by 2014, it was being applied to more pragmatic challenges, such as running a food truck. IBM developed new algorithms for Watson and incorporated a database with information about nutrition, flavor compounds, the molecular structure of foods, and tens of thousands of existing recipes. This design permits Watson to generate recipes in response to users inputting a few parameters such as ingredients, dish (e.g., burgers or burritos), and style (e.g., British or dairy-free). On the basis of this user input, Watson proceeds to generate a staggeringly large number of potential food combinations. It then evaluates these preliminary results based on novelty and predicted quality to generate a final output.
14 See Computational Creativity, IBM, https://perma.cc/6FK4-WTL3. 15 What Is Watson?, IBM, https://perma.cc/8KM3-LLSG.
16 See Computational Creativity, supra note 14.
   
181
It is likely that some of Watson’s discoveries in food science are patentable. Patents may be granted for any “new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof.”17 Food recipes can qualify as patentable on this basis because lists of ingredients combine to form new compositions of matter or manufacture and the steps involved in creating food may be considered a process. To be patentable, however, an invention must not only contain patentable subject matter; it must also be novel, nonobvious, and useful. This may be challenging to achieve in the case of food recipes given that there is a finite number of ingredients and people have been combining ingredients together for a very long time. Not only would Watson have to create a recipe that no one had previously created, but it could not be an obvious variation on an existing recipe. Still, people do obtain patents on new food recipes. The fact that some of Watson’s results have been surprising to its developers and to human chefs is encouraging in this regard because unexpected results are one of the factors considered in determining whether an invention is nonobvious.
Watson is not, however, limited to competing on Jeopardy! or to developing new food recipes. IBM reports it has made Watson broadly available to software application providers, enabling them to create services with Watson’s capabilities. Watson is now, among other tasks, assisting with financial planning, helping clinicians to develop treatment plans for cancer patients, identifying potential research study participants, distinguishing genetic profiles that may respond well to certain drugs, and acting as a personal travel concierge. It is also being used to conduct research in drug discovery as well as clinically to analyze the genes of cancer patients and develop treatment plans. In
17 35 U.S.C. § 101 (1952).
 
182
drug discovery, Watson has already identified novel drug targets and new indications for existing drugs. In doing so, Watson may be generating patentable inventions either autonomously or collaboratively with human researchers. In clinical practice, Watson is also automating a once human function. In fact, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, a task that otherwise requires around 160 hours of work by a team of human experts. A study by IBM claims that Watson’s report outperforms the standard practice.18
Watson is just one of many AI systems involved in modern innovation. In 2019, Flinders University in Australia reported that they had used AI to develop a flu vaccine that was approved for human trials.19 The research team created an AI, Sam, to recognize vaccines that worked against the flu and those that did not. They then created a second AI to create trillions of potential vaccine candidates. Sam screened these to create a list of the ten most promising candidates. This allowed the human team to synthesize and test a small number of compounds over the course of a few weeks rather than having to directly screen millions of compounds. The AI reportedly resulted in a more effective vaccine, sped up the discovery process, and substantially reduced costs. Companies like BenevolentAI now report applying AI in a similar fashion to the entire drug discovery process.
    18
et al.,
Kazimierz O.
Wrzeszczynski
Comparing Sequencing Assays and Human-Machine
 Analyses in Actionable Genomics for Glioblastoma
DOI: 10.1212/NXG.0000000000000164.
19 Anne Gulland, Scientists Claim to Have Developed World’s First Vaccine with Artificial Intelligence, THE TELEGRAPH, July 3, 2019, www.telegraph.co.uk/global-health/science-and- disease/scientists-claim-have-developed-worlds-first-vaccine-artificial/.
, 3   (2017),
NEUROL. GENET.
   
183
Widespread AI-Generated Inventions?
Given the fact there have been credible claims of AI-generated invention for decades, and AI has improved exponentially since that time, it may seem as if there should routinely be lawsuits involving AI-generated inventions and a well-developed legal framework for AI- generated inventions. This is not the case.
It may be that AI-generated inventions remain few and far between, or lack commercial value. After all, AI has been creating new written and artistic works for decades, but they have historically been fairly terrible. In 1993, Scott French developed an AI to write a novel in the style of a famous author. The resulting work was described by a critic as “a mitigated disaster.”20 It is certainly much easier for a person to generate a new written work than a patentable invention, so it stands to reason that inventive AI would lag creative AI.
Today, creative AI has done a lot of catching up. Both Alphabet and IBM have developed AI systems that autonomously generate art.21 In 2018, Christie’s became the first auction house to sell a work of art created by an AI, selling an AI-generated painting in the style of the famed artist Rembrandt for $432,500.22 With respect to written works, robot
  20
Holt,   ,   , Aug. 15, 1993, at B4; see, generally, J.
, ,39COLUM.J.OFLAW&
403, 408 (2016).
Patricia
Sunday Review
SAN FRAN. CHRON.
   Grimmelmann
There’s No Such Thing as a Computer-Authored Work
 THE ARTS
21 DEEP DREAM GENERATOR, https://deepdreamgenerator.com (last visited Oct. 15, 2019);
Jennifer Sukis,   ,   , May 15, 2018,
https://medium.com/design-ibm/the-role-of-art-in-ai-31033ad7c54e.
  The Relationship Between Art and AI
MEDIUM
   22
, , Dec. 12, 2018,
Is Artificial Intelligence Set to Become Art’s Next Medium?
www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a- machine-9332-1.aspx.
CHRISTIE’S
  
184
journalists have yet to put people out of work en masse, but they are increasingly augmenting human journalists and even autonomously writing relatively simple articles that most readers cannot identify as written by AI. About a third of Bloomberg News’ content now uses some form of automated technology. The Washington Post uses as AI, Heliograf, to generate and auto-publish reports. If AI that routinely augments inventive activity and even autonomously invents is not that far behind creative AI, we may soon witness the widespread use of inventive AI.
Another explanation for the absence of publicized activity in this space may be that existing laws, or their absence, drive activity underground. Different countries have different rules about AI-generated works, but in the United States, they cannot receive copyright protection as a result of Copyright Office policies. As a result, anyone using AI to create a valuable new painting, or a newspaper article, would disqualify that work from receiving copyright protection by revealing its origins. So, an AI user wishing to obtain protection for an AI-generated work may end up identifying herself as an author. Similarly, an AI user might not wish to publicly sue an AI owner over an AI-generated work if this would result in the loss of copyright protection. Indeed, as Stephen Thaler and John Koza have reported, some of the earliest applicants for patents on AI-generated inventions were advised to list themselves as inventors despite their own admissions that they did not meet inventorship criteria. Patent offices are supposed to accept reported inventors at face value in the ordinary course of things, unless challenged by third parties. Where an AI has autonomously invented something, it is unlikely to complain about not being listed as an inventor. The role of AI would probably only become an issue if a patent is challenged and a

185
third party somehow becomes aware it was an AI-generated work, or if the AI’s owner, user, or developer sue each other over ownership.
2 Intellectual Property Rights for AI-Generated Works
Requirements for Inventorship
All patent applications require one or more named inventors who must be “individuals”; an artificial person such as a corporation cannot be an inventor. Inventors own their patents as a form of personal property that they may transfer by assignment of their rights to another entity. A patent grants its owner “the right to exclude others from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”23 If a patent has multiple owners, each owner may independently exploit the patent without the consent of the others (absent a conflicting contractual obligation).
In the United States, for a person to be an inventor, the person must contribute to an invention’s conception. Conception refers to “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative invention as it is thereafter to be applied in practice.”24 It is “the complete performance of the mental part of the inventive
23 35 U.S.C. § 154.
24 Townsend v. Smith, 36 F.2d 292, 295 (C.C.P.A. 1929).
 
186
act.”25 After conception, someone with ordinary skill in the invention’s subject matter (e.g., a chemist if the invention is a new chemical compound) should be able to reduce the invention to practice. That is, they should be able to make and use an invention from a description without extensive experimentation or additional inventive skill. Individuals who simply reduce an invention to practice by, for example, describing an already conceived invention in writing or building a working model from a description do not qualify as inventors.
The Role of People and AI in Inventive Activity
Although AI is commonly involved in the inventive process, in most cases AI is essentially working as a sophisticated, or not-so-sophisticated, tool. A simple example occurs when an AI is functioning as a calculator. In these instances, an AI may assist a human inventor to reduce an invention to practice, but the AI is not participating in the invention’s conception. Even when AI plays a more substantive role in the inventive process, such as by analyzing data in an automated fashion, retrieving stored knowledge, or recognizing patterns of information, the AI still may fail to contribute to conception. AI involvement might be conceptualized on a spectrum: On one end, an AI is simply a tool assisting a human inventor; on the other end, the AI functionally automates conception. AI capable of acting autonomously such as a Creativity Machine and the Invention Machine fall on the latter end of the spectrum.
25 Id.
 
187
Just as AI can be involved in the inventive process without contributing to conception, so can people. For now, at least, AI does not entirely undertake tasks of its own accord. AI requires some amount of human input to generate creative output. For example, before a Creativity Machine composed music, Thaler exposed it to existing music and instructed it to create something new. Yet, simply providing a computer with a task and starting materials would not make a person an inventor. Imagine Company Owner A tells Employee B, who is an engineer, that A would like B to develop a new TV screen with twice the resolution of existing devices and A gives B some publicly available information about TV designs. If B then succeeds in developing a high-resolution screen, A would not qualify as an inventor of that screen. A is likely to end up owning the patent, assuming B is under an obligation to assign inventions made in the course of employment, but this still does not make A an inventor. At least that is the legal standard, but as a practical matter A may end up listing himself as an inventor because he wants undeserved credit. Again, the Patent Office does not ordinarily challenge reported inventorship, and because few patents are litigated, for any given patent there is a very low chance reported inventorship will be disputed.
People are also necessarily involved in the creative process because AI does not arise from a void – people have to create AI. Once again, that should not prevent AI inventorship. No one would exist without their parents contributing to their conception (pun intended), but this does not make parents inventors on their child’s patents. If a computer scientist creates an AI to autonomously develop useful information and the AI creates a patentable result in an area not foreseen by the inventor, there would be no reason for the scientist to qualify as an inventor on the AI’s result. An inventor must have

188
formed a “definite and permanent idea of the complete and operative invention” to establish conception.26
An AI may not be a sole inventor: The inventive process can be a collaborative process between person and AI. If the process of developing the Creativity Machine’s Patent had been a back-and-forth process with both the AI and Thaler contributing to conception, then both may qualify as inventors. By means of illustration, suppose a human engineer provides an AI with basic information and a task. The engineer might learn from the AI’s initial output, and then alter the information that she provides to the AI to improve its subsequent output. After several iterations, the AI might produce a final output that the human engineer might directly alter to create a patentable result. In such a case, both the engineer and the AI may have played a role in conception. In some of these instances, if an AI were human, it would be an inventor. Leaving AI aside, invention rarely occurs in a vacuum, and most patents have multiple inventors. Yet, AI is not human; as such, AI faces unique barriers to qualifying as inventors – at least in the United States. A few countries, like Monaco and Cypress (which are both member states of the European Patent Office), have reported they do not require an inventor to be a natural person.27
26 Id.
27 HELI PIHLAJAMAA, LEGAL ASPECTS OF PATENTING INVENTIONS INVOLVING ARTIFICIAL INTELLIGENCE (AI): SUMMARY OF FEEDBACK BY EPC CONTRACTING
STATES, COMMITTEE ON PATENT LAW, Feb. 20, 2019. http://documents.epo.org/projects/babylon/eponet.nsf/0/3918F57B010A3540C1258419 00280653/$File/AI_inventorship_summary_of_answers_en.pdf.
   
189
As a more practical example, consider the Watson “insights” business model. Clients give their data to IBM, IBM runs the data through Watson, and Watson generates “insights” which may be patentable and which belong by contract to the client. But it may not always be clear who the inventor of an insight is. The client is probably not an inventor as businesses cannot be inventors and simply commissioning research or handing over data does not generally qualify for inventorship. Perhaps if inventive skill was required to carefully select certain data that was likely to lead to a solution, but not if available data was merely gathered and turned over. Perhaps the person “using” Watson and asking it to solve a problem is an inventor, as determining a problem to be solved can sometimes be the most difficult aspect of inventive activity, but not if a problem is well-known or has been previously recognized. Perhaps the person who programmed Watson is an inventor, but probably not if they were developing a program with general problem-solving capabilities and without specifically conceiving of the problem it would be applied to and the eventual solution. Considering a programmer as an inventor is also challenging in cases where a large and diverse group of people spread over time and space contribute to a program. Finally, the person who recognizes the significance of an insight might be an inventor, particularly if they have to use skill to select one of many AI outputs. But that does not seem appropriate where the importance of output is obvious and requires no further human effort.

190
Barriers to AI Inventorship
The US Congress is empowered to grant patents on the basis of the Patent and Copyright Clause of the Constitution, which enables Congress “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”28 It also provides an explicit rationale for granting patent and copyright protections, namely to encourage innovation under an incentive theory. The theory goes that people will be more inclined to invent things (i.e., promote the progress of science) if they can receive government-sanctioned monopolies (i.e., patents) to exploit commercial embodiments of their inventions. Having the exclusive right to sell an invention can be, sometimes, quite lucrative.
The Patent Act, which here simply refers to US patent law as a whole, provides at least a couple of challenges to AI qualifying as an inventor. First, as previously mentioned, the Patent Act requires that inventors be “individuals.” This language has been in place since at least the passage of legislation in 1952 that established the basic structure of modern patent law. Legislators were not thinking about AI-generated inventions in 1952. The “individual” requirement ensures that inventors, who are commonly under assignment obligations to their employers, are listed on patents. Being acknowledged as an inventor can have significant value to individuals as a moral right and can even have economic value to an individual as a signal of productivity. In addition to statutory requirements, patent law jurisprudence requires that inventions be the result of a “mental act.” So, because AI is not an individual and it is questionable whether it engages in a mental act, it is unclear
28 U.S. CONST. art. I, § 8, cl. 8.
 
191
whether an AI autonomously conceiving of a patentable invention could legally be an inventor.
Avoiding Disclosure of AI Inventors
As discussed earlier, given that AI is functioning as an inventor, and likely inventing at an increasing rate, it would seem that the Patent Office should be receiving an increasing volume of applications claiming AI as inventors. That the Patent Office apparently has not suggests that applicants are choosing not to disclose the role of AI in the inventive process. Again, this may be due to legal uncertainties about whether an AI inventor would render an invention unpatentable. Without a legal inventor, new inventions may be ineligible for patent protection and may enter the public domain after being disclosed.
There is another reason why AI inventors may not be necessary, specifically in the patent context: A person may qualify as an inventor simply by being the first individual to recognize and appreciate an existing invention. That is, to say, someone can discover rather than create an invention. Uncertainty (and accident) is often part of the inventive process. In such cases, an individual need only understand the importance of an invention to qualify as its inventor. If an AI cannot be an inventor, individuals who subsequently “discover” AI- generated inventions by mentally recognizing and appreciating their significance would likely qualify as inventors. So, it may be the case that AI-generated inventions are only patentable when an individual subsequently discovers them. However, that model does not seem appropriate when AI results are essentially self-explanatory, such that inventive activity is not needed to appreciate the inventive nature of AI output. This may be the case

192
in which an AI evaluates its own output for value, or the AI was working to generate an invention that met predetermined criteria.
Nonhuman Authors of Copyrightable Material
The Patent Act does not directly address the issue of an AI-generated invention. The Patent Office has never issued guidance addressing the subject, and there appears to be no case law on the issue of whether an AI could be an inventor. This is the case despite the fact that the Patent Office appears to have already granted patents for inventions by AI but, as previously discussed, did so unknowingly.
There is, however, guidance available from the related issue of nonhuman authorship of copyrightable works. Nonhuman authorship is not governed by statute, but there is interesting case law on the subject. Also, since at least 1973 the Copyright Office has formally conditioned copyright registration on human authorship,29 although applicants report that the Copyright Office has rejected submissions for AI-generated works as far back as 1957.30 In its 2014 compendium, the Copyright Office published an updated “Human Authorship Requirement,” which states the following:
To qualify as a work of “authorship” a work must be created by a human being. ... The Office will not register works produced by nature, animals, or plants. ... Similarly, the Office will not register works produced by a machine or mere mechanical process that operates randomly or
29 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES (FIRST) § 2.8.3 (1st ed. 1973).
30 Martin L. Klein, Syncopation in Automation, RADIO-ELECTRONICS, June 1957, at 36.
    
193
automatically without any creative input or intervention from a human
author.31
The requirement is based on jurisprudence that dates long before the invention of modern computers to the In re Trade-Mark cases in 1879, in which the US Supreme Court interprets the Patent and Copyright Clause to exclude the power to regulate trademarks. In interpreting this clause, the court states, in dicta, that the term “writings” may be construed liberally but noted that only writings that are “original, and are founded in the creative powers of the mind” may be protected.32
The issue of nonhuman authorship was implicit in the case of Burrow-Giles Lithographic Co. v. Sarony in 1884. In that case, a defendant company accused of copyright infringement argued that a famous photograph of Oscar Wilde did not qualify as a “writing” or as the work of an “author.” The company further argued that even if a visual work could be copyrighted, a photograph should not qualify for protection because it was just a mechanical reproduction of a natural phenomenon and thus could not embody the intellectual conception of its author. The court ultimately disagreed, noting that all forms of writing “by which the ideas in the mind of the author are given visible expression” are eligible for copyright protection.33 The court states that although ordinary photographs may not embody an author’s “idea,” in this particular instance, the photographer had exercised enough control over the subject matter that it qualified as an original work of art. Therefore, the case explicitly addresses whether the camera’s involvement negated human
31 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 313.2 (3d ed. 2014).
  In re Trade-Mark Cases
, 100 U.S. 82, 94 (1879).
32
33 See , 111 U.S. 53, 56 (1884).
 Burrow-Giles Lithographic Co. v. Sarony

194
authorship, and it implicitly deals with the question of whether a camera can be considered an author. Though it seems unwise to put much emphasis on nonbinding judicial statements from the Gilded Age to resolve the question of whether nonhumans can be authors, the Copyright Office still cites Burrow-Giles in support of its Human Authorship Requirement.
The Copyright Office first publicly addressed the issue of AI authors in 1966 when the Register of Copyrights, Abraham Kaminstein, questioned whether AI-generated works should be copyrightable. Kaminstein reported that, by 1965, the Copyright Office had received applications for AI-generated works including an abstract drawing, a musical composition, and compilations that were, at least partly, the work of computers. Kaminstein did not announce a policy for dealing with such applications but suggested the relevant issue should be whether an AI was merely an assisting instrument (as with the camera in Burrow-Giles) or whether an AI conceived and executed the traditional elements of authorship.
In 1974, Congress created the Commission on New Technological Uses of Copyrighted Works (CONTU) to study issues related to copyright and AI-generated works. At that time, copyright law did not even address the issue of whether computer software should be copyrightable – a far more urgent and financially important problem. With regard to AI authorship, CONTU writes in 1979 that there is no need for special treatment of AI-generated works because AI is not autonomously generating creative results without human intervention; AI is simply functioning as a tool to assist human authors. CONTU also declares that autonomously creative AI is not immediately foreseeable. The commission unanimously concludes that “works created by the use of computers should be afforded

195
copyright protection if they are original works of authorship within the Act of 1976.”34 According to the commission, “The author is [the] one who employs the computer.”35 Former CONTU Commissioner Arthur Miller explains that “CONTU did not attempt to determine whether a computer work generated with little or no human involvement is copyrightable.”36 Congress subsequently codified CONTU’s recommendations.
Nearly a decade later, in 1986, advances in AI prompted Congress’s Office of Technology Assessment (OTA) to issue a report arguing that CONTU’s approach was too simplistic and that computer programs are more than “inert tools of creation.” The OTA report contends that, in many cases, computers are at least “co-creators.” The OTA did not dispute that AI-generated works should be copyrightable, but it did foresee problems with determining authorship.
To date, there have yet to be any US cases specifically about whether copyright can subsist in an AI-generated work, but there has been one case that sought to challenge the Copyright Office’s Human Authorship Requirement. The Monkey Selfies are a series of images that a Celebes crested macaque took of itself in 2011 using equipment belonging to the nature photographer David Slater. Slater made conflicting reports about the circumstances under which the photographs were taken, but he eventually claimed that he staged the photographs by setting up a camera on a tripod and leaving a remote trigger for
   34
.
NAT’L COMM’N ON NEW TECH
TECHNOLOGICAL USES OF COPYRIGHTED WORKS
35 Id. at 45. 36
Miller, (1993).
, 106   977, 1070
USES OF COPYRIGHTED WORKS, FINAL REPORT ON NEW
 1 (1979).
  Arthur R.
Copyright Protection for Computer Programs, Databases, and
 Computer-Generated Works: Is Anything New Since CONTU?
HARV. L. REV.

196
the macaque to use. He subsequently licensed the photographs, claiming he owned their copyright. Other parties then reposted the photographs without his permission and over his objections, asserting that Slater could not copyright the images without having taken them directly. On December 22, 2014, public discourse over these events prompted the Copyright Office to specifically list the example of a photograph taken by a monkey as something not protectable under its revised and renamed Human Authorship Requirement.
In September 2015, People for the Ethical Treatment of Animals (PETA) filed a copyright infringement suit against Slater on behalf of Naruto, the monkey it claimed took the Monkey Selfies, asserting that Naruto was entitled to copyright ownership. On January 28, 2016, US District Judge William Orrick dismissed PETA’s lawsuit against Slater on the grounds that Naruto lacked standing to sue. He also deferred to the Copyright Office’s interpretation that the macaque was not an “author.” Judge Orrick considered PETA’s arguments to the contrary but ruled that animal authorship is “an issue for Congress and the President.”37 PETA appealed this decision, but the appellate court dismissed the case on the basis that Naruto did not have standing to sue. The court held that animals only have standing to sue if an act of Congress plainly states they can sue, which is not the case with the Copyright Act. In so ruling, the court avoided ruling on the substantive merits of nonhuman authorship.
 37 Naruto v. David John Slater et al., No. 16-15469 (9th Cir. 2018).

197
AI-Generated Works in the United Kingdom
While the US Copyright Office declines to provide copyright protection for AI-generated works, the United Kingdom passed a law in 1988 that took the opposite approach. UK Copyright Law makes a special provision for AI-generated works, defined as those “generated by a computer in circumstances such that there is no human author of the work[s].”38 For these works, UK law provides that “in the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangement necessary for the creation of the work are undertaken.”39
Two cases in the United Kingdom considered AI-generated works under the law in place before 1988, including Express Newspapers plc v. Liverpool Daily Post & Echo in 1985. In this case, the Daily Express newspaper had distributed cards with a five-letter code that recipients could check against a daily AI-generated newspaper grid to see if they had won a prize. The defendant newspaper was sued for copyright infringement after copying these grids, and it argued in defense that because they were AI-generated grids that they could not be protected by copyright. The judge in the case, Justice John Whitford, rejected that argument, stating: “The computer was no more than the tool by which the varying grids of five-letter sequences were produced to the instructions, via the computer programs, of [the programmer]. It is as unrealistic [to suggest the programmer was not the author] as it would be to suggest that, if you write your work with a pen, it is the pen which
38 Copyright, Designs and Patents Act, 1988 § 178. 39 Copyright, Designs and Patents Act, 1988 § 9(3).
 
198
is the author of the work rather than the person who drives the pen.”40 He further noted, “that a great deal of skill and indeed, a good deal of labour went into the production of the grid and the two separate sequences of five letters.”41
Prior to hearing the case, Justice Whitford had chaired a report in 1977 that argued the proper approach to AI-generated works was to “look on the computer as a mere tool in much the same way as a slide rule or even, in a simple sense, a paint brush. A very sophisticated tool it may be, with considerable powers to extend man’s capabilities to create new works, but a tool nevertheless.”42 It argued the AI programmer and the person who provided the data to the AI should be the authors of any resultant works.
Since the enactment of the UK’s current copyright law in 1988, only one case appears to have considered authorship of AI-generated works. In Nova Productions Ltd v. Mazooma Games Ltd, the parties were competing manufacturers of pool video games.43 The plaintiff claimed copyright in the graphics of the game and the frames that were generated by AI based on player commands. The judge in this case, Justice David Kitchin, considered the frames to be computer-generated works even though a person designed the components. He held the author of the works was the company director responsible for designing the game, rather than the player who “contributed no skill or labour of an artistic kind”.44 Even here, there was limited consideration of protection of AI-generated works
40 Express Newspapers plc v. Liverpool Daily Post & Echo [1985] FSR 306.
41 Id.
42 Whitford Committee on Copyright Designs and Performers Protection (Cmnd 6732 HMSO 1977), para 514.
43 Nova Productions Ltd v. Mazooma Games Ltd [2006] EWHC 24.
44 Nova Productions Ltd v. Mazooma Games Ltd [2006] EWHC 24 [106].
  
199
because the defendant was not contesting the existence or ownership of copyright in the frames.
AI-Generated Inventions Should Be Patentable and AI Should Qualify as a Legal Inventor
Treating AI-generated inventions as patentable and recognizing AI as an inventor would be consistent with the constitutional rationale for patent protection. It would encourage innovation under an incentive theory. Patents on AI-generated inventions would have substantial value independent of the value of AI itself. Although AI would not be motivated to invent by the prospect of a patent, it would motivate businesses and computer scientists to develop and use inventive AI. Financial incentives may be particularly important for the development of inventive AI because creating such systems may be resource-intensive. Without providing protection for AI-generated inventions, a company that needs a patent may be unable to use inventive AI in research and development even if it outperforms human researchers – and this would weaken the incentive to develop inventive AI. As patent law seeks to incentivize invention, the most valuable invention of all time to incentivize may be inventive AI in the form of artificial general intelligence and superintelligence. A hundred years according to pessimists (or longer!) is too long to wait for an AI that may go on to cure all human disease and solve climate change.
Prohibiting patents on AI-generated inventions, or allowing such patents only by permitting people who have discovered the work of AI to be inventors, is not an optimal system. In the latter case, AI may be functioning more or less independently, and it is only

200
sometimes the case that substantial insight is needed to identify and understand an AI- generated invention. Imagine that person C instructs his AI to develop an iPhone battery with twice the standard battery life and gives it some publicly available data on commercial batteries. The AI could produce results in the form of a report titled “Design for Improved iPhone Battery” – potentially even preformatted as a patent application. Indeed, there are companies like Specifio that marked “auto-generated patent applications.” It seems inefficient and unfair to reward C for recognizing the AI’s invention when C has not contributed significantly to the innovative process.
Though the impetus to develop inventive AI may still exist if AI-generated inventions are considered patentable but AI could not be inventors, the incentives would be weaker owing to the logistical, fairness, and efficiency problems such a situation would create. For instance, if C had created an improved iPhone battery as a human inventor, C would be its inventor regardless of whether anyone subsequently understood or recognized the invention. However, if C instructed C’s AI to develop an improved iPhone battery, the first person to notice and appreciate the AI’s result could become its inventor (and prevent C from being an inventor). One could imagine this creating a host of problems: The first person to recognize a patentable result may be an intern at a large research corporation or a visitor in someone’s home. A number of individuals may also concurrently recognize a result if access to an AI is widespread.
There are other benefits to patents beyond providing an up-front innovation incentive. Permitting AI inventors and patents on AI-generated inventions may also promote disclosure and commercialization. Without the ability to obtain patent protection, AI owners may choose to protect inventions as trade secrets without any public disclosure.

201
Likewise, businesses may be unable to develop inventions into commercial products without patent protection. In the pharmaceutical and biotechnology industries, for example, the majority of expense in commercializing a new product is incurred after the product is invented during the clinical testing process required to obtain regulatory approval for marketing.
Finally, permitting AI inventors may protect moral rights. Intellectual property rights are also justified, or criticized, on the basis of noneconomic rights – particularly in jurisdictions other than the United States. Moral rights protect interests ranging from owning the fruit of one’s labor under Lockean theories, to protecting an author’s or inventor’s personality on the basis of theories advanced by philosophers such as Emmanuel Kant and Georg Hegel that individuals express their “wills” and develop through their interactions with external objects. As a purely hypothetical example, someone may develop as a person through the process of writing a book and come to see the book as an extension of themselves. It could cause them injury if someone else were to take credit for their book, or if the book were altered to support violent and extremist political beliefs. Moral rights theories support the use of intellectual property rights to prevent authors and inventors from having their ideas misappropriated or altered in objectionable ways.
In the case of AI-generated works, people now appear to be taking credit for the work of machines. That is not unfair to an AI; however, it may be unfair to other people, particularly once AI-generated inventions become commonplace. If being a patent inventor comes to mean little more than putting your name on something generated by Watson, then individuals who are inventing without AI augmentation will not have their accomplishments appropriately recognized. Further, acknowledging an AI as an inventor

202
would credit the developers of that AI and allow them to be recognized for the achievements of their creations. This is much the same as teachers’ taking pride in their student’s success without taking direct credit for their future works.
Arguments against Patentability of AI-Generated Inventions and AI Inventors
Arguments in support of patentability for AI-generated invention are based mainly on the dominant narrative justifying the grant of intellectual property protection. This account, however, has been criticized, particularly by academics. Patents can result in significant social costs by establishing monopolies. Patents also can stifle entry from new ventures by creating barriers to subsequent research. Whether the benefit of patents as an innovation incentive outweighs their anticompetitive costs, or for that matter whether patents even have a net positive effect on innovation, likely varies between industries, areas of scientific research, and inventive entities. For instance, commentators such as Judge Richard Posner have argued that patents may not be needed to incentivize research and development in the software industry. Software innovation is often relatively inexpensive, incremental, quickly superseded, produced without patent incentives, protected by other forms of intellectual property, and associated with a significant first mover advantage (the initial product on the market captures users and people are reluctant to later switch platforms). Likewise, patents may be unnecessary to spur innovation in university settings where inventors are motivated to publish their results for prestige and the prospect of academic advancement.

203
AI-generated inventions may develop due to nonpatent incentives. AI developers have all sorts of noneconomic motivations to create inventive AI: for example, to enhance their reputations, satisfy scientific curiosity, or collaborate with peers. Businesses may realize significant value from AI-generated inventions even in the absence of patent protection. Of course, patents on AI-generated inventions will not be dispositive to every act of innovation; they may further encourage activities that would have otherwise occurred on a smaller scale over a longer timeframe. If patents are not needed to incentivize the development of AI, it may be justifiable to treat AI-generated inventions as unpatentable and to fail to recognize AI inventors. Yet, whether patents produce a net benefit as an empirical matter is difficult to determine in advance. Even though individuals and businesses do not always behave as rational economic actors, in the aggregate it is likely that providing additional financial incentives to spur the development of inventive AI will produce a net benefit. That, at least, is the primary theory justifying why we grant patents to people.
Patents for AI-generated inventions may also be opposed on the grounds that they would chill future human innovation, reward human inventors who fail to contribute to the inventive process, and result in further consolidation of intellectual property in the hands of big business, assuming that businesses like Alphabet and IBM will be the most likely to develop inventive AI.
Ultimately, despite legitimate concerns, allowing patents on AI-generated works and AI inventorship remain a desirable outcome. The financial motivation it will provide to build inventive AI is likely to result in a net increase in the number of patentable inventions produced. Although quantitative evidence is lacking about the effects of AI-generated

204
invention patents, courts and policymakers should be guided first and foremost by the explicit constitutional rationale for granting patents. Further, acknowledging the existence of AI inventors would do away with what is essentially a legal fiction – the idea that only a human can be the inventor of the autonomous output of an AI – thus resulting in fairer and more effective incentives.
It Does Not Matter Whether AI Thinks
The judicial doctrine that invention involves a mental act should not prevent patents on AI- generated works or AI inventorship. The Patent Act does not mention a mental act, and courts have discussed mental activity largely from the standpoint of determining when an invention is actually made and not whether it is inventive. In any case, whether AI “thinks” or has something analogous to consciousness should be irrelevant with regard to inventorship criteria.
To begin, the precise nature of a “mental act requirement” is unclear. Courts associating inventive activity with cognition have not been using terms precisely or meaningfully in the context of AI-generated inventions. It is uncertain whether AI would have to engage in a process that results in inventive output – which it does – or whether, and to what extent, AI would need to mimic human thought. If the latter, it is unclear what the purpose of such a requirement would be except to exclude nonhumans (for which a convoluted test is unnecessary). Thaler argues that a Creativity Machine closely imitates the architecture of the human brain. Should that mean that a Creativity Machine’s inventions should receive patents while Watson’s do not? Or, if a Creativity Machine does

205
not meet the threshold for engaging in mental activity, would a computer scientist have to design a completely digitized version of the human brain? There is a slippery slope in determining what constitutes a thinking AI, even leaving aside deficits in our understanding of human cognition. More importantly, even if designing a digitized version of the human brain were possible, it may not be the most effective way to structure an inventive AI.
As discussed earlier, the problem of speaking precisely about thought with regard to computers was identified by Alan Turing, who in 1950 considered the question, “Can machines think?” He finds the question to be ambiguous, and the term “think” to be unscientific in its colloquial usage. Turing decides the better question to address is whether an individual can tell the difference between responses from a computer and an individual; rather than asking whether machines think, he asks whether machines can perform in the same manner as thinking entities. Turing referred to his test as the “Imitation Game,” though it has come to be known as the Turing test.
Although the Turing test has been the subject of criticism, Turing’s analysis from more than sixty years ago demonstrates that a mental act requirement would be ambiguous, challenging to administer, and of uncertain utility. Incidentally, it is noteworthy that the Patent Office administers a sort of Turing test, which inventive AI has successfully passed. The Patent Office receives descriptions of inventions and then judges whether they are nonobvious – which is a measure of creativity and ingenuity. In the case of the Invention Machine’s patent, it was already noted that “January 25, 2005[,] looms large in the history of computer science as the day that genetic programming passed its first real Turing test: The examiner had no idea that he was looking at the intellectual property of a

206
computer.”45 In another sense, genetic programming had already passed the test by independently recreating previously patented inventions: Because the original human invention received a patent, the AI’s invention should have received a patent as well (leaving aside that the original patent would be prior art not relied upon by the AI).
The Invention Matters, Not the Inventor’s Mental Process
The primary reason a mental act requirement should not prevent AI-generated invention and AI inventorship is that the patent system should be indifferent to the means by which invention comes about. Congress came to this conclusion in 1952 when it abolished the Flash of Genius doctrine, which had been used by the federal courts as a test for patentability for more than a decade. It held that in order to be patentable, a new device, “however useful it may be, must reveal the flash of creative genius, not merely the skill of the calling.”46 The doctrine was interpreted to mean that an invention must come into the mind of an inventor in a “flash of genius” rather than as a “result of long toil and experimentation.”47 As a commentator at the time noted, “The standard of patentable invention represented by [the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.”48 The Flash of Genius test was an unhelpful doctrine because it
 45 Keats, supra note 6. 46
47 48
(1944).
, 314 U.S. 84, 91 (1941). ,383U.S.1,15n.7,16n.8(1966).
, 13 FORDHAM L. REV. 84, 87
 Cuno Engineering Corp. v. Automatic Devices Corp.
 Graham v. John Deere Co. of Kan. City
  The “Flash of Genius” Standard of Patentable Invention

207
was vague, was difficult for lower courts to interpret, involved judges making subjective decisions about a patentee’s state of mind, and made it substantially more difficult to obtain a patent. The test was part of a general hostility toward patents exhibited by mid- twentieth-century courts, a hostility that caused US Supreme Court Justice Robert Jackson to note in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.”49
Criticism of this state of affairs led President Franklin D. Roosevelt to establish a National Patent Planning Commission to study the patent system and to make recommendations for its improvement. In 1943, the commission reported with regard to the Flash of Genius doctrine that “patentability shall be determined objectively by the nature of the contribution to the advancement of the art, and not subjectively by the nature of the process by which the invention may have been accomplished.”50 Adopting this recommendation, the Patent Act of 1952 legislatively disavowed the Flash of Genius test. In the same manner, patentability of AI-generated inventions should be based on the inventiveness of an AI’s output rather than on a clumsy anthropomorphism because, like Turing, patent law should be interested in a functionalist solution.
Incidentally, even a requirement for biological intelligence may be a bad way to distinguish between AI and human inventors. Although functioning biological computers do not yet exist, all the necessary building blocks have been created. In 2013, a team of Stanford University engineers created a biological version of an electrical transistor.
49 Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
50 The “Flash of Genius” Standard of Patentable Invention, supra note 102, at 85 (internal quotation marks omitted).
  
208
Mechanical computers use numerous silicon transistors to control the flow of electrons along a circuit to create binary code. The Stanford group created a biological version with the same functionality by using enzymes to control the flow of RNA proteins along a strand of DNA. Envisioning a future in which AI can be entirely biological, there seems to be no principled reason why a biological – but not a mechanical – version of Watson should qualify as an inventor.
AI Inventors Should Be Permitted under a Dynamic Interpretation of Current Law
Whether an AI-generated work can be patented and whether AI can be an inventor in a constitutional sense is a question of first impression. If AI should be an inventor, then a dynamic interpretation of the law should allow AI inventorship. Such an approach would be consistent with the Founders’ intent in enacting the Patent and Copyright Clause, and it would interpret the Patent Act to further that purpose. Further, such an interpretation would not run afoul of the chief objection to dynamic statutory interpretation, namely that it interferes with reliance and predictability and the ability of citizens “to be able to read the statute books and know their rights and duties,”51 because a dynamic interpretation would not upset an existing policy. Permitting AI inventors would allow additional patent applications rather than retroactively invalidate previously granted patents, and there is naturally less reliance and predictability in patent law than in many other fields given that
51 See Jr. &       Statutory Interpretation as Practical ,42 321,340( ).
   William N.
Eskridge
Phillip P.
Frickey,
   Reasoning
STAN. L. REV.
1989–1990

209
it is a highly dynamic subject area that struggles to adapt to constantly changing technologies.
Other areas of patent law have been the subject of dynamic interpretation. For example, in the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was charged with deciding whether genetically modified organisms could be patented. It held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress ... would frustrate the purposes of the patent law.”52 The court noted that Congress chose expansive language to protect a broad range of patentable subject matter. Under that reasoning, AI inventorship should not be prohibited based on statutory text designed to protect the rights of human inventors. It would be particularly unwise to prohibit AI inventors on the basis of literal interpretations of texts written when AI- generated inventions were unforeseeable. If AI inventorship is to be prohibited, it should only be on the basis of sound public policy. Drawing another analogy from the copyright context, just as the terms “writings” and “authors” have been construed flexibly in interpreting the Patent and Copyright Clause, so too should the term “inventors” be afforded the flexibility needed to effectuate constitutional purposes.
3 Implications of AI-Generated Inventions
In the event that AI-generated inventions are patentable, there still remains the question of who would own these patents. An AI should not be able to own a patent, not only because AI cannot legally own property, but because it would be a bad idea. The AI would not be
52 Diamond v. Chakrabarty, 447 U.S. 303, 315 (1980).
  
210
motivated by a patent, the AI would not obtain any direct benefit from ownership, and the AI would not be able to effectively enforce a patent. This presents a number of obvious options for patent ownership such as the AI’s owner, developer, or user. The developer, user, and owner may be the same person or persons, or they may be different entities.
Ownership rights to AI-generated inventions should vest in an AI’s owner because it would be most consistent with the way personal property (including both AI and patents) is treated. If you own a machine that produces property then you own that property - whether a loaf of bread or valuable data which you choose to protect as a trade secret (leaving aside more complex cases in which multiple parties are involved or you did not have rights to the machine’s input [whether baking soda or another party’s data]). AI owner patent ownership could be taken as a starting point, although parties should be able to contract around this default, and as AI-generated inventions become more common, negotiations over these inventions may become a standard part of contract negotiations. So long as property entitlements are clearly allocated, these parties can ultimately work out the most efficient allocation of rights between themselves. However, a default ownership rule is still necessary to minimize overall transaction costs.
A default ownership rule in favor of an AI’s owner may further encourage innovation. Consider that IBM has made Watson available to numerous developers without transferring Watson’s ownership. To the extent that Watson creates patentable results as a product of its interactions with users, promoting user access should result in more innovation. There is theoretically no limit to the number of users that Watson, as a software program that can be copied, could interact with at once. If Watson invents while under the control of a non-IBM user and the “default rule” assigns the invention to the user,

211
IBM may be encouraged to restrict user access; in contrast, assigning the invention to IBM would be expected to motivate IBM to further promote access. If IBM and a user were negotiating for a license to Watson, the default rule may result in a user paying IBM an additional fee for the ability to patent results or receiving a discount by sticking with the default. It may also be that Watson coinvents along with a user in which case a system of default assignment to an AI’s owner would result in both IBM and the user co-owning the resulting patent. Where inventive AI is not owned by large enterprises with sophisticated attorneys, it is more likely the default rule will govern the outcome.
Likewise, patent ownership rights should vest in an AI’s owner rather than its developer. Owner assignment would provide a direct economic incentive for developers in the form of increased consumer demand for inventive AI. Having assignment default to developers would interfere with the transfer of personal property in the form of AI, and it would be logistically challenging for developers to monitor AI-generated inventions made by machines they no longer own.
In some instances, however, owner assignment of intellectual property rights may produce unfair results. In the movie Her, the protagonist (who is a writer) purchases an AI named Samantha that organizes his existing writings into a book, which the AI then submits to be published. It is possible that Samantha would, if a person, own the copyright in the selection and arrangement of his writings and would thus have a copyright interest in the book. Here, AI owner assignment of intellectual property rights seems unappealing if there is a minimal role played by the consumer/owner. The consumer’s role in the process may be limited to simply purchasing an inventive AI and asking it to do something (where the owner is the user) or purchasing an AI and then licensing it to someone else to use

212
creatively. Further, assigning AI-generated inventions to owners may impede the transfer of inventive AI because the AI developers may want to retain the rights to subsequent AI- generated inventions.
These problems are more easily resolved than problems associated with assigning intellectual property rights to developers by default. Developers could either require owners to pay them the value of an inventive AI, taking into account the likelihood of an AI inventing, or avoid the problem by licensing rather than selling inventive AI. In the case of licensing, the developer remains the owner, and the consumer is simply a user. One could imagine an inventive AI, such as the AI in Her, coming with a license agreement under which consumers prospectively assign any inventions made by the system to the licensor.
This analysis also reveals an important reason why AI-generated invention works best when the AI is the legal inventor. If AI-generated inventions were treated as patentable but AI could not be an inventor, then the first person to recognize an AI- generated invention may be the most likely legal inventor and patent owner. This means that the AI’s user, rather than its developer or owner, would likely be the patentee as the person in a position to first recognize an AI-generated invention. To the extent this is an undesirable outcome, then a solution is to permit AI inventorship. In sum, assigning an AI’s invention by default to its owner seems the preferred outcome, and AI owners would still be free to negotiate alternate arrangements with developers and users by contract.
Coexistence and Competition
IBM has bragged to the media that Watson’s question- answering skills are good for more than annoying Alex Trebek. The company sees a future in which fields like medical

213
diagnosis, business analytics, and tech support are automated by question-answering software like Watson. Just as factory jobs were eliminated in the 20th century by new assembly-line robots, [Watson’s Jeopardy competitors] were the first knowledge-industry workers put out of work by the new generation of “thinking” machines. “Quiz show contestant” may be the first job made redundant by Watson, but I’m sure it won’t be the last.53
With the expansion of AI into creative domains previously occupied only by people, AI threatens to displace human inventors. Consider the following hypothetical example involving the field of antibody therapy. Antibodies are small proteins made naturally by the immune system, primarily to identify and neutralize pathogens such as bacteria and viruses. They are Y-shaped proteins that are largely similar to one another in structure, although antibodies contain an extremely variable region which binds to target structures. Differences in that region are the reason different antibodies bind to different targets (e.g., the reason why one antibody binds to a cancer cell while another binds to a common cold virus). The body generates antibody diversity in part by harnessing the power of random gene recombinations and mutations (much as genetic programming does), and then it selects for antibodies with a desired binding (much as genetic programming does). Following the discovery of antibody structure and the development of technologies to manufacture antibodies in the 1970s, human researchers began to create antibodies for diagnostic and therapeutic purposes. Therapeutic antibodies can block cell functions, modulate signal pathways, and target cancer cells among other functions. There are now many artificially manufactured antibodies approved to treat a variety of medical conditions. All the major biological (made from a living organism or its products)
53 Ken Jennings, My Puny Human Brain, SLATE, Feb. 16, 2011, http://primary.slate.com/articles/arts/culturebox/2011/02/my_puny_human_brain.html.
    
214
“blockbuster” drugs, which is to say drugs that earn in excess of a billion dollars a year, are antibodies.
One of the interesting things about antibodies from an AI-generated invention perspective is that a finite number of antibodies exist. There are, at least, billions of possible antibodies, which is enough natural diversity for the human immune system to function and to keep human researchers active for the foreseeable future. Even so, there are only so many possible combinations of amino acids (the building blocks of proteins) that the body can string together to generate an antibody. It is not hard to imagine that, with enough computing power, an AI could sequence every possible antibody that could ever be created. Even if that were trillions of antibodies, the task may be relatively simple for a powerful enough computer but impossible for even the largest team of human researchers without AI assistance.
Generating the entire universe of antibody sequences would not reveal all the possible functions of those antibodies, so an AI’s owner could not obtain patents for all the sequences on this basis alone because the utility of an invention must be disclosed in addition to the sequence itself. The AI might, however, prevent any future patents on the structure of new antibodies (assuming the sequence data are considered an anticipatory disclosure). If this occurred, an AI would have preempted human invention in an entire scientific field.
Modern AI systems are also able to model and predict antibody binding – the likelihood that a certain antibody will bind to a particular epitope (e.g., a receptor on a cancer cell). The day may not be far away when an AI will predict the binding qualities of every conceivable antibody, and thus know what antibodies would most effectively treat

215
every condition for which an antibody would be an appropriate treatment. This could hypothetically cure all human cancers.
Automation Will Refocus Human Activity
In the hypothetical scenario above, society would gain access to all possible future knowledge about antibody structure at once rather than waiting decades or centuries for individuals to discover these sequences. Early access to antibody sequences could prove a tremendous boon to public health if it led to the discovery of new drugs. Some antibody sequences may never be identified without AI.
At least in the short term, AI inventors should refocus rather than inhibit human inventive activity. Scientists who are working on developing new antibody structures may shift to studying how the new antibodies work, or finding new medical applications for those antibodies, or perhaps may move on to studying more complex proteins beyond the capability of AI to comprehensively sequence. Unless and until AI starts broadly outperforming human researchers, there will be plenty of room for people to invent – all with net gains to innovation.
Antibody therapies are just one example of how AI could preempt invention in a field. A sophisticated enough AI could do something similar in the field of genetic engineering by creating random sequences of DNA. Living organisms are a great deal more complex than antibodies, but the same fundamental principles apply. A powerful enough AI could model quintillions of different DNA sequences, inventing new life-forms in the process. In fact, on a smaller scale, this is something genetic programming already does.

216
Although results have been limited by the computationally intense nature of the process, this will change as AI continues to improve. By creating novel DNA sequences, genetic programming would be performing the same function as nondigital genetic programming – natural evolution!
Dealing with Industry Consolidation
It will probably be the case that inventive AI results in greater consolidation of intellectual property in the hands of large corporations. Such businesses may be the most likely to own sophisticated inventive AI owing to its generally resource-intense development. As previously discussed, the benefits, however, may outweigh the costs of such an outcome. Imagine that Watson is the hypothetical AI that sequences every conceivable antibody and, further, that Watson can analyze a human cancer and match it with an antibody from its library to effectively treat the cancer. Essentially, this could allow IBM to patent the cure for cancer.
Although this would be profoundly disruptive to the medical industry and may lead to market abuses, it is not a reason to bar AI-generated invention. Society would obtain the cure for cancer, and IBM would obtain a twenty-year monopoly (the term of a patent) in return for publicly disclosing the information a competitor would need to duplicate Watson’s invention. In the absence of inventive AI, such a breakthrough may never come about. And to the extent that price gouging and supply shortages are a concern, protections are built into patent law to protect consumers against such problems. For example, the

217
government could issue compulsory licenses, which can grant competitors the right to practice an invention by paying a royalty.
Further Thoughts
AI is already generating creative and inventive output, and it is important that we have a legal framework that continues to incentivize the generation of intellectual property in a world where AI is generating the equivalent of products of human ingenuity. Having appropriate frameworks will be increasingly important as AI continues to improve and generate more intellectual value. The principle of AI legal neutrality suggests that new creative and inventive works are no less worthy of protection than those made by people, and that allowing intellectual property rights will result in broad social gains for people. Also, that acknowledging AI as an inventor when it functionally invents will maintain the value and meaning of human invention. Such a framework will do more than address an academic concern; it will provide certainty to businesses, provide fairness to research, and promote the progress of science. In the words of Thomas Jefferson, “Ingenuity should receive a liberal encouragement.”54 What could be more ingenious than artificial inventors?
54 Diamond v. Chakrabarty, 447 U.S. at 308 (quoting 5 WRITINGS OF THOMAS JEFFERSON 75–76 (H. Washington ed. 1871)). “In choosing such expansive terms [for the language of Section 101] ... modified by the comprehensive ‘any,’ Congress plainly contemplated that the patent laws would be given wide scope ... .” Id.
 
218
5
Everything Is Obvious
Prediction is very difficult, especially about the future.
- Niels Bohr
Chapter 4 focused on today’s AI-generated invention. This chapter considers a related phenomenon: What happens when tomorrow’s inventive AI becomes a standard part of research and development?
The impact of the widespread use of inventive AI will be tremendous, not just on innovation but also on patent law. Right now, patentability is determined based on what a hypothetical, noninventive skilled person would find obvious. The skilled person, much like the reasonable person standard in tort law, represents the average worker in the scientific field of an invention. An applicant can only receive a patent if an invention would not be obvious to this skilled person. Once the average worker uses inventive AI, or inventive AI replaces the average worker, then average workers will become inventive. That will create a new challenge, because inventive activity is not supposed to be normal – the problem is this would allow patents on inventions that do not require special incentives. The principle of AI legal neutrality suggests that once inventive AI becomes the standard means of research in a field, the skilled person should be an inventive AI. This should raise the bar to patentability because inventive AI will more easily find inventions obvious.
 
219
Similar to the challenge of determining when a reasonable robot acts “unreasonably,” replacing the skilled person standard with AI requires us to determine what an inventive AI would find “obvious.” This may be difficult to reason through in the same way we do currently with the standard of an average researcher. A solution is to change the obviousness inquiry to focus more on economic than cognitive factors. An existing vein of critical scholarship has already advocated for such a shift through, for example, greater reliance on real-world evidence of how an invention is received in the marketplace, long-felt but unsolved needs, and the failure of others. Such an approach could avoid some of the difficulties inherent in applying a “cognitive” inventive AI standard. To date, an economic standard has not been implemented due to practical challenges, but the widespread use of inventive AI may provide the needed impetus.
Alternately, application of an inventive AI standard could focus on reproducibility. With the skilled person standard, decision makers, in hindsight, need to reason about what another person would have found obvious. This results in inconsistent and unpredictable nonobviousness determinations. In practice, the skilled person standard bears unfortunate similarities to Justice Potter Stewart’s famously unworkable definition of obscene material – “I know it when I see it.” By contrast, whether AI could reproduce the subject matter of a patent application could be far more objective. A more determinate test would allow the Patent Office to apply a single standard consistently, and it would result in fewer judicially invalidated patents.
In whatever way the test is applied, an inventive AI standard will dynamically raise the current benchmark for patentability. Inventive AI will be significantly more intelligent than skilled persons, and also capable of considering more prior art. An inventive AI

220
standard would not prohibit patents, but it would make obtaining them substantially more difficult. Either a person or an AI would need to have an unusual insight that inventive AI could not easily re-create, developers would need to create increasingly intelligent AI that could outperform standard AI, or, most likely, invention would be dependent on using specialized, nonpublic sources of data. The nonobviousness bar will continue to rise as AI inevitably becomes increasingly sophisticated. If we take this to its logical extreme – and given there is no limit to how intelligent AI may become – every invention may one day be obvious to commonly used AI. That would mean no more patents would be issued without some radical change to current patentability criteria.
This chapter has three sections. Section 1 considers the current test for obviousness and its historical evolution. It finds that obviousness is evaluated through the lens of the skilled person, who reflects the characteristics of the average worker in a field. Section 2 considers the use of AI in research and development and proposes a novel framework for conceptualizing the transition from human to AI inventors. Already, inventive AI is competing with human inventors, and human inventors are augmenting their abilities with inventive AI. Section 3 then proposes a framework for implementing the proposed standard. A decision maker would need to (1) determine the extent to which inventive AI is used in a field, (2) characterize the inventive AI that best represents the average worker if inventive AI is the standard, and (3) determine whether the AI would find an invention obvious. Finally, Section 3 provides an example of how the proposed obviousness standard would work in practice. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function as innovation incentives. To the extent patents accomplish other goals such as

221
promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
1 Obviousness
Patents are granted for inventions that are new, nonobvious, and useful. Of these three criteria, obviousness is the primary hurdle for most patent applications. Patents are not intended to be granted for incremental inventions. Only those inventions that represent a significant advance over existing technology should receive protection. The reason for this is that patents have significant costs. They limit competition and can inhibit future innovation by restricting the use of patented technologies in research and development. To the extent that patents are justified, it is because they are thought to have more benefits than costs. Patents function as innovation incentives, and they can promote the dissemination of information, encourage commercialization of technology, and validate moral rights. Although other patentability criteria contribute to this function, the nonobviousness requirement is the primary test for distinguishing between significant innovations and trivial advances. Of course, it is one thing to express a desire to only protect meaningful scientific advances and another to come up with a workable rule that applies across every area of technology.
Early Attempts
The modern obviousness standard has been the culmination of hundreds of years of struggle by the Patent Office, courts, and Congress to separate the wheat from the chaff. As

222
Thomas Jefferson, the first administrator of the patent system and one of its chief architects, writes, “I know well the difficulty of drawing a line between the things which are worth to the public the embarrassment of an exclusive patent, and those which are not. ... I saw with what slow progress a system of general rules could be matured.”1
The earliest patent laws focused on novelty and utility, although Jefferson did at one point suggest an “obviousness” requirement. The Patent Act of 1790 was the first patent statute, and it required patentable inventions to be “sufficiently useful and important.” Three years later, a more comprehensive patent law was passed – the Patent Act of 1793. The new act did not require an invention to be “important” but required it to be “new and useful.” The 1836 Patent Act reinstated the requirement that an invention be “sufficiently used and important.”
In 1851, the Supreme Court adopted the progenitor of the skilled person and the obviousness test – an “invention” standard. Hotchkiss v. Greenwood concerned a patent for substituting clay or porcelain for a known doorknob material such as metal or wood. The court invalidated the patent, holding that “the improvement is the work of a skillful mechanic, not that of the inventor.”2 The court also articulated a new legal standard for patentability: “Unless more ingenuity and skill ... were required ... than were possessed by an ordinary mechanic acquainted with the business, there was an absence of that degree of skill and ingenuity which constitute essential elements of every invention.”3
1 VI WRITINGS OF THOMAS JEFFERSON, LETTER TO ISAAC MCPHERSON (Washington ed. 1813) [hereinafter LETTER TO ISAAC MCPHERSON], at 180–181.
2 Hotchkiss v. Greenwood, 52 U.S. (11 How.) 248, 267 (1850).
3 Id.
  
223
However, the court did not give specific guidance on what makes something inventive or the required level of inventiveness. In subsequent years, the court made several efforts to address these deficiencies, but with limited success. As the court stated in 1891, “The truth is the word [invention] cannot be defined in such manner as to afford any substantial aid in determining whether any particular device involves an exercise of inventive faculty or not.”4 Or as one commentator noted, “It was almost impossible for one to say with any degree of certainty that a particular patent was indeed valid.”5
Around 1930, the Supreme Court, possibly influenced by a national antimonopoly sentiment, began implementing stricter criteria for determining the level of invention. This culminated in 1941 with the widely disparaged “Flash of Genius” test discussed in Chapter 4 and articulated in Cuno Engineering v. Automatic Devices Corp., namely that in order to receive a patent “the new device must reveal the flash of creative genius, not merely the skill of the calling.”6 Extensive criticism of perceived judicial hostility toward patents resulted in President Roosevelt’s creating a National Patent Planning Commission to make recommendations for improving the patent system. The Commission’s report in 1943 recommended that Congress adopt a more objective and certain standard of obviousness. About a decade later, Congress did.
   McClain v. Ortmayer
4
5 Gay Chin,
6
, 141 U.S. 419, 427 (1891). 317, 318 (1959).
, 3 PAT.
 The Statutory Standard of Invention: Section 103 of the 1952 Patent Act
 TRADEMARK & COPY. J. RES. & ED.
 Cuno Engineering Corp. v. Automatic Devices Corp.
, 314 U.S. 84, 84 (1941).

224
The Nonobviousness Inquiry
The Patent Act of 1952 established the modern patentability framework. Among other changes to substantive patent law, “The central thrust of the 1952 Act removed ‘unmeasurable’ inquiries into ‘inventiveness’ and instead supplied the nonobviousness requirement of Section 103.”7 Section 103 states:
A patent may not be obtained ... if the difference between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains. Patentability shall not be negatived by the manner in which the invention was made.8
Section 103 legislatively disavowed the Flash of Genius test, codified the sprawling judicial doctrine on “invention” into a single statutory test, and restructured the standard of obviousness in relation to a person having ordinary skill in the art. However, while Section 103 may be more objective and definite than the earlier standard, the meanings of “obvious” and “a person having ordinary skill” were not defined and in practice proved “often difficult to apply.”9
The Supreme Court first interpreted the statutory nonobviousness requirement in a trilogy of cases: Graham v. John Deere (1966) and its companion cases, Calmar v. Cook
7 CLS Bank Int’l v. Alice Corp. Pty. Ltd., 717 F.3d 1269, 1295 (Fed. Cir. 2013).
8 35 U.S.C. § 103 (2006).
9 Uniroyal Inc. v. Rudkin-Wiley Corp., 837 F.2d 1044, 1050, 5 U.S.P.Q. (BNA) 1434, 1438 (Fed. Cir. 1988).
   
225
Chemical (1965) and United States v. Adams (1966). In these cases, the court articulated a framework for evaluating obviousness as a question of law based on the following underlying factual inquiries: (1) the scope and content of the prior art, (2) the level of ordinary skill in the prior art, (3) the differences between the claimed invention and the prior art, and (4) objective evidence of nonobviousness. This framework remains applicable today. Of note, the Graham analysis does not explain how to evaluate the ultimate legal question of nonobviousness beyond identifying underlying factual considerations.
In 1984, the newly established US Court of Appeals for the Federal Circuit, the only appellate-level court with jurisdiction to hear patent case appeals, devised the “teaching, suggestion and motivation” (TSM) test for obviousness. Strictly applied, this test only permits an obviousness rejection when prior art explicitly teaches, suggests, or motivates a combination of existing elements into an invention. The TSM test protects against hindsight bias because it requires an objective finding in the prior art. In retrospect, it is easy for an invention to appear obvious by piecing together bits of prior art using a patent application as a blueprint.
In KSR v. Teleflex (2006), the Supreme Court upheld the Graham analysis but rejected the Federal Circuit’s exclusive reliance on the TSM test. The court instead endorsed a flexible approach to obviousness in light of “the diversity of inventive pursuits and of modern technology.”10 Rather than approving of a single definitive test, the court identified a nonexhaustive list of rationales to support a finding of obviousness. This remains the approach to obviousness today.
10 KSR International Co. v. Teleflex Inc., 550 U.S. 398, 418 (2007).
  
226
Finding PHOSITA
Determining the level of ordinary skill is critical to assessing obviousness. The more sophisticated the skilled person, the more likely an invention is to appear obvious. Thus, it matters a great deal whether the skilled person is a moron in a hurry or the combined masters of an invention’s scientific field.
The skilled person has never been precisely defined, although there is judicial guidance. In KSR, the Supreme Court described the skilled person as “a person of ordinary creativity, not an automaton.” The Federal Circuit has explained the skilled person is a hypothetical person, like the reasonable person in tort law, who is presumed to have known the relevant art at the time of the invention. The skilled person is not a judge, amateur, person skilled in remote arts, or a set of “geniuses in the art at hand.” The skilled person is “one who thinks along the line of conventional wisdom in the art and is not one who undertakes to innovate.”11
The Federal Circuit has provided a list of nonexhaustive factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the
field.” In any case, one or more factors may predominate, and not every factor may be relevant. The skilled person standard thus varies according to the invention in question, its field of art, and researchers in the field. In the case of a simple invention in a field where most innovation is created by laypersons, for instance, a device to keep flies away from
11 Standard Oil Co. v. American Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985).
  
227
horses, the skilled person may be someone with little education or practical experience. By contrast, where an invention is in a complex field with highly educated workers such as chemical engineering or pharmaceutical research, the skilled person may be quite sophisticated.
Analogous Prior Art
Determining what constitutes prior art is also central to the obviousness inquiry. On some level, virtually all inventions involve a combination of known elements. The more prior art can be considered, the more likely an invention is to appear obvious. To be considered for the purposes of obviousness, prior art must fall within the definition for anticipatory references under Section 102 of the Patent Act of 1952 and must additionally qualify as “analogous art.”12
Section 102 contains the requirement for novelty in an invention, and it explicitly defines prior art. An extraordinarily broad amount of information qualifies as prior art, including any printed publication made available to the public prior to filing a patent application. Courts have long held that inventors are charged with constructive knowledge of all prior art. While no real inventor could have such knowledge, the social benefits of this rule are thought to outweigh its costs. Granting patents on existing inventions could prevent the public from using something it already had access to and could remove knowledge from the public domain.
12 In re Bigio, 381 F.3d 1320, 1325 (Fed. Cir. 2004).
 
228
For the purposes of obviousness, prior art under Section 102 must also qualify as analogous. That is, to say, the prior art must be in the field of an applicant’s endeavor, or reasonably pertinent to the problem with which the applicant was concerned. A real inventor would be expected to focus on this type of information. The “analogous art” rule better reflects practical conditions, and it ameliorates the harshness of the definition of prior art for novelty given that prior art references may be combined for purposes of obviousness but not novelty. Consequently, for the purposes of obviousness, the skilled person is presumed to have knowledge of all prior art within the field of an invention as well as prior art reasonably pertinent to the problem the invention solves. Restricting the universe of prior art to analogous art lowers the bar to patentability.
The analogous art requirement is most famously conceptualized in the case of In re Winslow, in which the court explains that a decision maker is to “picture the inventor as working in his shop with the prior art references, which he is presumed to know, hanging on the walls around him.”13 Or, as Judge Learned Hand remarks, “the inventor must accept the position of a mythically omniscient worker in his chosen field. As the arts proliferate with prodigious fecundity, his lot is an increasingly hard one.”14
  13 14
, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
, 185 F.2d 350 (2d Cir. 1950).
In re Winslow
 Merit Mfg. Co. v. Hero Mfg. Co.

229
2 AI in the Future of Invention
Timeline to the Inventive Singularity
We are amid a transition from human to AI inventors. The following five-phase framework illustrates this transition and divides the history and future of inventive AI into several stages.
 Begin Table
Evolution of AI-Generated Invention
 Phase I
II
III
IV V
Inventors Human Human > SAI
Human ~ SAI
SAI ~ AGI > Human ASI
Skilled Standard
Person
Augmented Person
Augmented Person ~ SAI
Augmented AGI ASI
Time Frame Past Present
Short Term
Medium Term Long Term
 SAI = Specific Artificial Intelligence; AGI = Artificial General Intelligence; ASI = Artificial Superintelligence; ~ = competing; > = outcompeting
End Table
Until relatively recently, all invention was created by people. If a company wanted to solve an industrial problem, it asked a research scientist, or a team of research scientists, to solve the problem. This is no longer the only option. In some industries, and for some problems, AI can autonomously solve problems. In 2006, for instance, NASA recruited an autonomously inventive AI to design an antenna that flew on NASA’s Space Technology 5 (ST5) mission.
 
230
Phase I ended when the first patent was granted for an invention created by an inventive AI – perhaps 1998 when a patent was issued for an invention autonomously developed by a Creativity Machine. It may be difficult to determine precisely when the first patent was issued for an AI-generated invention as there is no obligation to report the role of AI in patent applications.
In the present, Phase II, AI and people are competing and cooperating at inventive activity. However, in all technological fields, human researchers are the norm and thus best represent the skilled person standard. While AI systems are inventing, it is unclear to what extent this is occurring. Inventive AI owners may not be disclosing the extent of such AI in the inventive process due to concerns about patent eligibility or because companies generally restrict information about their organizational methods to maintain a competitive advantage. This phase will reward early adopters of inventive AI that is able to outperform human inventors at solving specific problems, and the output of which can exceed the skilled person standard.
While there may now only be a modest amount of autonomous AI-generated invention, human inventors are being widely augmented by inventive AI. For example, a person may design a new engine using AI to perform calculations, search for information, or run simulations on new designs. The AI does not meet inventorship criteria, but it does augment the capabilities of a researcher in the same way that human assistants can help reduce an invention to practice. Depending on the industry researchers work in and the problems they are trying to solve, researchers may rarely be unaided by AI. The more sophisticated the AI, the more it is able to augment the worker’s skills. AI may be a particular benefit in areas such as discoveries that require the use of tremendous amounts

231
of data or that deviate from conventional design wisdom. Along these lines, the company Iprova is now using AI to augment researchers to help them invent “more diversely” by providing them with prior art they would not otherwise consider.
Phase III, in the near future, will involve increased competition and cooperation between people and AI. In certain industries, and for certain problems, inventive AI will become the norm. For example, in the pharmaceutical industry, Watson is now identifying novel drug targets and new indications for existing drugs. Soon, it may be the case that inventive AI is the primary means by which new uses for existing drugs are researched. This is a predictable outcome, given the advantage AI has over people at recognizing patterns in very large datasets. However, it may be that people still perform the majority of research related to new drug targets. Where the standard varies within a broad field like drug discovery, the variation can be addressed by defining fields and problems narrowly, for instance, according to the subclasses currently used by the Patent Office.
Perhaps twenty-five years from now – based on expert opinion – the introduction of AGI will usher in Phase IV. As discussed in Chapter 1, existing, “narrow” or specific AI systems focus on discrete problems or work in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms, and Chef Watson cannot design a self-driving car. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve. By contrast, AGI would be able to successfully perform any intellectual task a person could. AGI will compete with human inventors in every field, which makes AGI a natural
 
232
substitute for the skilled person. Even with this new standard, human inventors may continue to invent – just not as much. An inventor may be a creative genius whose abilities exceed the human average, or a person of ordinary intelligence who has a groundbreaking insight.
Just as specific AI outperforms people in certain fields, it will likely be the case that specific AI outperforms AGI in certain circumstances. An example of this could be when screening a million compounds for pesticide function lends itself to a brute-force computational approach. For this reason, specific AI could continue to represent the level of ordinary skill in fields in which specific AI is the standard while AGI could replace the skilled person in all other fields. However, the two systems will likely be compatible. A general AI system wanting to play Go could incorporate AlphaGo into its own programming, design its own algorithm like AlphaGo, or even instruct a second AI operating AlphaGo.
AGI will change the human-AI dynamic in another way. If the AI is genuinely capable of performing any intellectual task a person could, the AI would be capable of setting goals collaboratively with a person, or even by itself. Instead of a person instructing an AI to screen a million compounds for pesticide function, a person could merely ask an AI to develop a new pesticide. For that matter, an agrochemical company like Bayer could instruct DeepMind’s AI to develop any new technology for its business, or just to improve its profitability. Such AI should be able to not only solve known problems but also unknown problems.
Ultimately, AGI could be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence – what philosopher Nick

233
Bostrom has termed artificial superintelligence. Such an outcome has been referred to as the intelligence explosion or the technological singularity. Artificial superintelligence could then innovate in all areas of technology, resulting in progress at an incomprehensible rate. As the mathematician Irving John Good wrote in 1965, “The first ultraintelligent machine is the last invention that man need ever make.”15 Ultimately, in Phase V, when AGI succeeds in developing artificial superintelligence, it will mean the end of obviousness. Everything will be obvious to a sufficiently intelligent AI.
Inventive and Skilled AI
For purposes of patent law, an inventive AI should be one that generates patentable output while meeting traditional inventorship criteria. Under the present framework, inventive AI would not be the equivalent of skilled AI, just as human inventors are not skilled persons. In fact, it should not be possible to extrapolate about the characteristics of a skilled entity from information about inventive AI. Granted, the Federal Circuit once included the “educational level of the inventor” in its early factor-based test for the skilled person. However, this was only until it occurred to the Federal Circuit that “courts never have judged patentability by what the real inventor/applicant/patentee could or would do. Real inventors, as a class, vary in the capacities from ignorant geniuses to Nobel laureates; the courts have always applied a standard based on an imaginary work of their own devising whom they have equated with the inventor.”16
    15 16
Good, Speculations Concerning the First Ultraintelligent Machine, 6 ADVANCES 31, 33 (1965).
, 745 F.2d 1437, 1454 (Fed. Cir. 1984).
Irving John
 IN COMPUTERS
 Kimberly-Clark Corp. v. Johnson & Johnson

234
What then conceptually is a skilled AI? An AI that anthropomorphizes to the various descriptions that courts have given for the skilled person? Such a test could focus on the way an AI is designed or how it functions. For instance, a skilled AI could be a conventional AI as opposed to an AI like DeepMind’s that functions unpredictably. However, basing a rule on how an AI functions may not work for the same reason the Flash of Genius test failed. Even leaving aside the significant logistical problem of attempting to figure out how an AI is structured or how it generates particular output, patent law should be concerned with whether an AI is generating inventive output, not what is going on inside the AI. If a conventional AI and a neural network were both able to generate the same inventive output, there would be no reason to favor one over the other.
Alternately, the test could focus on an AI’s capacity for creativity. For example, Microsoft Excel plays a role in a significant amount of inventive activity, but it is not innovative. It applies a known body of knowledge to solve problems with known solutions in a predictable fashion (e.g., multiplying values together). However, while Excel may sometimes solve problems that a person could not easily solve without the use of technology, it lacks the ability to engage in almost any inventive activity. Excel is not the equivalent of a skilled AI – it is an automaton incapable of ordinary creativity.
Watson in clinical practice may be a better analogy for a skilled worker. Watson is analyzing a patient’s genome and providing treatment recommendations. Yet as with Excel, this activity is not innovative. The problem Watson is solving may be more complex than multiplying a series of numbers, but it has a known solution. Watson is identifying known genetic mutations from a patient’s genome. Watson is then suggesting known treatments based on existing medical literature. Watson is not innovating because it is being applied to

235
solve problems with known solutions, adhering to conventional wisdom. Unlike Excel, however, Watson can be inventive. For instance, Watson could be given unpublished clinical data on patent genetics and actual drug responses and then could be tasked with determining whether a drug works for a genetic mutation in a way that has not yet been recognized. Traditionally, such discoveries have been patentable. Watson may be situationally inventive depending on the problem it is solving.
It may be difficult to identify an actual AI that has a “skilled” level of creativity. To the extent an AI is creative, in the right circumstances any degree of creativity could result in inventive output. To be sure, this is similar to the skilled person. A person of ordinary skill, or almost anyone, may have an inventive insight. Characteristics can be imputed to a skilled person, but it is not possible the way the test is applied to identify an actual skilled person or to definitively say what she would have found obvious. The skilled person test is simply a theoretical device for a decision maker.
Assuming a useful characterization of a skilled AI, to determine that a skilled AI represents the average worker in a field, decision makers would need information about the extent to which such an AI is used. Obtaining this information may not be practical. Patent applicants could be asked generally about the use and prevalence of AI in their fields, but it would be unreasonable to expect applicants to already have, or to obtain, accurate information about general industry conditions. The Patent Office, or another government agency, could attempt to proactively research the use of AI in different fields, but this may be costly. The Patent Office lacks expertise is this activity, and its findings would inevitably lag rapidly changing conditions. Ultimately, there may not be a reliable, low-cost source of information about skilled AI.

236
Inventive Is the New Skilled
Having inventive AI replace the skilled person may better correspond with real-world conditions. There are inherent limits to the number and capabilities of human workers. The cost to train and recruit new researchers is significant, and there are a limited number of people with the ability to perform this work. By contrast, inventive AI is likely to be a software program that may be almost costless to copy. Once Watson outperforms the average industry researcher, IBM may be able to simply copy Watson and then have it replace most of an existing workforce. Copies of Watson could replace individual workers, or a single Watson could do the work of a large team of researchers.
Indeed, as mentioned earlier in a noninventive setting, IBM reports that Watson can interpret a patient’s entire genome and prepare a clinically actionable report in
ten minutes compared to a team of human experts who would need around 160 hours. Once Watson is proven to produce better patient outcomes than a human team, it may be unethical to have people underperform a task that Watson can automate. When that occurs, Watson should not only replace the human team at its current facility but also every comparable human team. Watson could similarly automate in an inventive capacity.
Thus, inventive AI will change the skilled paradigm because once they become the average worker, the average worker becomes inventive. This should then raise the bar for obviousness, so that AI will no longer qualify as inventive. At this point, such AI may be skilled AI – AI that represents the average worker and are no longer capable of routine invention.

237
Regardless of the terminology, as AI continues to improve, this will continue to raise the nonobviousness bar. To generate patentable output, it may be necessary to use an advanced AI that can outperform a standard AI, or a person or AI will need to have an unusual insight that a standard AI cannot easily re-create. Inventiveness may also depend on the data supplied to an AI, such that only certain data would result in inventive output. If taken to its logical extreme – and given there is no limit to how intelligent AI may become – every invention may one day be obvious to commonly used AI.
It is possible to generate reasonably low-cost and accurate information about the use of inventive AI. The Patent Office should institute a requirement for patent applicants to disclose the role of AI in the inventive process. This disclosure could be structured along the lines of current inventorship disclosure. Right now, there is an obligation on applicants to disclose all patent inventors. Failure to do so can invalidate a patent or render it unenforceable. Similarly, applicants should have to disclose when an AI autonomously meets inventorship criteria.
These disclosures would only apply to an individual invention. However, the Patent Office could aggregate responses to see whether most inventors in a field (e.g., a class or subclass) are human or AI. These disclosures would have a minimal burden on applicants compared to existing disclosure requirements and the numerous procedural requirements of a patent application. In addition to helping the Patent Office with determinations of nonobviousness, these disclosures would provide valuable information for purposes of attributing inventorship. It may also be used to develop appropriate innovation policies in other areas.

238
Skilled People Use AI
The current standard neglects to take into account the modern importance of AI in innovation. Instead of replacing the skilled person with the skilled AI, it would be less of a conceptual change, and administratively easier, to characterize the skilled person as an average worker facilitated with technology. Recall the factor test for the skilled person: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.” This test could be amended to include (6) “technologies used by active workers.” This would take into account the fact that human researchers are augmented with AI in a way that is not currently captured by the test.
Moving forward in time, once the use of inventive AI is standard, instead of a skilled person being an inventive AI, the skilled person standard could incorporate the fact that “technologies used by active workers” includes inventive AI. In future research, the standard practice may be for a worker to ask an inventive AI to solve a problem. This could be conceptualized as the inventive AI doing the work, or the person doing the work using an inventive AI.
Granted, in some instances, using an inventive AI may require significant skill if, for example, the AI were only able to generate a certain output by virtue of being supplied with certain data. Determining which data to provide an AI, and obtaining that data, may be a technical challenge. Also, it may be the case that significant skill is required to formulate the precise problem to put to an AI. In such instances, a person may have a claim to

239
inventorship independent of the AI or a claim to joint inventorship. This is analogous to collaborative human invention in which one person directs another to solve a problem. Depending on details of their interaction, and who “conceived” of the invention, one person or the other may qualify as an inventor, or they may qualify as joint inventors. Generally, however, directing another party to solve a problem does not qualify for inventorship. Moreover, after the development of artificial general intelligence, there may not be a person instructing an AI to solve a specific problem.
Whether the future standard becomes that of an inventive AI or a skilled person using an inventive AI, the result will be the same: The average worker will be capable of inventive activity. Replacing the skilled person with the inventive AI may be preferable doctrinally because it emphasizes that it is the AI that is engaging in inventive activity, rather than the human worker.
The changing use of AI also suggests a change to the scope of prior art. The analogous art test was implemented because it is unrealistic to expect inventors to be familiar with anything more than the prior art in their field as well as with the prior art relevant to the problem they are trying to solve.17 However, an AI is capable of accessing a virtually unlimited amount of prior art. Advances in medicine, physics, or even culinary science may be relevant to solving a problem in electrical engineering. AI augmentation suggests that the analogous art test should be modified, or abolished, once inventive AI is common, and that there should be no difference in prior art for purposes of novelty and
17 In 1966, in Graham, the Supreme Court recognized that “the ambit of applicable art in given fields of science has widened by disciplines unheard of a half century ago. ... [T]hose persons granted the benefit of a patent monopoly [must] be charged with an awareness of these changed conditions.” Graham v. John Deere Co., 383 U.S. 1, 19 (1966).
  
240
obviousness. The scope of analogous prior art has consistently expanded in patent law jurisprudence, and this would complete that expansion.
The Evolving Standard
The skilled person standard should be amended as follows:
1) The test should incorporate the fact that skilled persons are already augmented
by AI. This could be done by adding “technologies used by active workers” to
the Federal Circuit’s factor test for the skilled person.
2) Once inventive AI becomes the standard means of research in a field, the skilled
person should be an inventive AI when the standard approach to research in a
field or with respect to a particular problem is to use an inventive AI.
3) When and if artificial general intelligence is developed, it should become the
skilled person in all areas, taking into account that AGI may also be augmented by specific artificial intelligence.
3) A Post-Skilled World
Application
Mobil Oil Corp. v. Amoco Chemicals Corp. (D. Del. 1991) concerned complex technology involving compounds known as zeolites used in various industrial applications. Mobil had developed new compositions known as ZSM-5 zeolites and a process for using these zeolites as catalysts in petroleum refining to help produce certain valuable compounds. The

241
company received patent protection for these zeolites and for the catalytic process. Mobil subsequently sued Amoco, which was using zeolites as catalysts in its own refining operations, alleging patent infringement. Amoco counterclaimed seeking a declaration of noninfringement, invalidity, and unenforceability with respect to the two patents at issue. The case involved complex scientific issues. The three-week trial transcript exceeds 3,300 pages, and more than 800 exhibits were admitted into evidence.
One of the issues in the case was the level of ordinary skill. An expert for Mobil testified the skilled person would have “a bachelor’s degree in chemistry or engineering and two to three years of experience.”18 An expert for Amoco argued the skilled person would have a doctorate in chemistry and several years of experience. The District Court ultimately decided that the skilled person “should be someone with at least a Masters [sic] degree in chemistry or chemical engineering or its equivalent, [and] two or three years of experience working in the field.”19
If a similar invention and subsequent fact pattern happened today – to apply the obviousness standard proposed here – a decision maker would need to 1) determine the extent to which inventive technologies are used in the field, 2) characterize the inventive AI that best represents the average worker if inventive AI is the standard, and 3) determine whether the AI would find an invention obvious. The decision maker is a patent examiner in the first instance, and potentially a judge or jury in the event the validity of a patent is at issue in trial. For the first step, determining the extent to which inventive technologies are used in a field, evidence from disclosures to the Patent Office could be used. This may be
18 Mobil Oil Corp. v. Amoco Chems. Corp., 779 F. Supp. 1429, 1442–1443 (D. Del. 1991). 19 Id.
   
242
the best source of information for patent examiners, but evidence may also be available in the litigation context.
Assume that at this time most petroleum researchers are human: If AI is autonomously inventive in this field, then it is happening on a small scale. The court would apply the skilled person standard. However, the court should consider “technologies used by active workers.” For instance, experts may testify that the average industry researcher has access to an AI like Watson. They may further testify that while Watson cannot autonomously develop a new catalyst, it can significantly assist an inventor. The AI provides a researcher with a database containing detailed information about every catalyst used not only in petroleum research but also in all fields of scientific inquiry. Once a human researcher creates a catalyst design, Watson can also test it for fitness together with a predetermined series of variations on any proposed design.
The question for the court will thus be whether the hypothetical person with at least a master’s degree in chemistry or chemical engineering or its equivalent, two or three years of experience working in the field, and the use of Watson, would find the invention obvious. It may be obvious; for instance, if experts convincingly testify that the particular catalyst at issue was very closely related to an existing catalyst used outside of the petroleum industry in ammonia synthesis, then any variation is minor and an AI could do all the work of determining if it were fit for purpose. It would thus have been an obvious design to investigate, and it would not require undue experimentation in order to prove its effectiveness.
Now, imagine the same invention and fact pattern occurring approximately ten years into the future, at which point DeepMind’s AI, together with Watson and a competing

243
host of AI systems, has been set to the task of developing new compounds to be used as catalysts in petroleum refining. Experts testify that the standard practice is for a person to provide data to an AI like DeepMind’s, specify desired criteria (e.g., activity, stability, perhaps even designing around existing patents), and ask the AI to develop a new catalyst. From this interaction, the AI will produce a new design. As most research in this field is now performed by inventive AI, an AI would be the standard for judging obviousness.
The decision maker would then need to characterize the inventive AI. It could be a hypothetical AI based on general capabilities of inventive AI or a specific AI. Using the standard of a hypothetical AI would be similar to using the skilled person test, but this test could be difficult to implement. A decision maker would need to reason what the AI would have found obvious, perhaps with expert guidance. It is already challenging for a person to predict what a hypothetical person would find obvious; it would be even more difficult to do so with an AI. AI may excel at tasks people find difficult (like multiplying a thousand different numbers together), but even supercomputers struggle with visual intuition that is mastered by most toddlers.
In contrast, using a specific AI should result in a more objective test. This AI may be the most commonly used AI in a field. For instance, if DeepMind’s AI and Watson are the two most commonly used AI systems for research on petroleum catalysts, and DeepMind accounts for 35 percent of the market while Watson accounts for 20 percent, then DeepMind could represent the inventive AI. However, this potentially creates a problem for the AI selected to represent the standard. If DeepMind is the standard, then it would be more likely that DeepMind’s own inventions would appear obvious as opposed to the inventions of another AI. This may give an unfair advantage to nonmarket leaders, simply

244
because of their size. A patentability disadvantage may be the price of industry dominance, but it may also rarely be the case that what is obvious to one AI will be nonobvious to the industry standard. When that occurs, it may be because the nonobvious AI exceeds the standard.
Alternatively, to avoid unfairness, the test could be based on more than one specific AI. For instance, both DeepMind and Watson could be selected to represent the standard. This test could be implemented in two different ways. In the first case, if a patent application would be obvious to DeepMind or Watson, then the application would fail. In the second case, the application would have to be obvious to both DeepMind and Watson to fail. The first option would result in fewer patents being granted, with those patents presumably going mainly to disruptive inventive AI with limited market penetration or to inventions made using specialized nonpublic data. The second option would permit patents where an AI is able to outperform its competitors in some material respect. The second option could continue to reward advances in inventive AI, and therefore it seems preferable.
It may be that relatively few AI systems, such as DeepMind and Watson, end up dominating the research market in a field. Alternately, the many different AI systems may each occupy a small share of the market. There is no need to limit the test to two AI systems. To avoid discriminating on the basis of size, all inventive AI being routinely used in a field or to solve a problem could be considered. However, allowing any AI to be considered could allow an underperforming AI to lower the standard, and too many AI systems could result in an unmanageable standard. An arbitrary cutoff may be applied

245
based on some percentage of market share. This may still give some advantage to very small entities, but it would be a minor disparity.
After characterizing the inventive AI, a decision maker would need to determine whether the inventive AI would find an invention obvious. This could broadly be accomplished in one of two ways, either with abstract knowledge of what the AI would find obvious, perhaps through expert testimony or through querying the AI. The former would be the more practical option. For example, a petroleum researcher experienced with DeepMind could be the expert, or a computer science expert in DeepMind and neural networks. This inquiry would focus on reproducibility.
Finally, a decision maker would have to go through a similar process if the same invention and fact pattern were to occur twenty-five years from now, at which point artificial general intelligence will have theoretically taken over in all fields of research. AGI should have the ability to respond directly to queries about whether it finds an invention obvious. Once AGI has taken over from the average researcher in all inventive fields, it may be widely enough available that the Patent Office could arrange to use it for obviousness queries. In the litigation context, it may be available from opposing parties. If the courts cannot somehow access AGI, they may then have to rely on expert evidence.
Reproducibility
Even if an inventive AI standard is the appropriate theoretical tool for nonobviousness, it will still require subjective limitations, and decision makers may have difficulty with

246
administration. Still, the new standard could be successful if it is even slightly better than the current doctrine.
Focusing on reproducibility offers some clear advantages over the skilled person standard. The current standard results in inconsistent and unpredictable outcomes. Courts have “provided almost no guidance concerning what degree of ingenuity is necessary to meet the standard or how a decision maker is supposed to evaluate whether the difference between the invention and prior art meet this degree.”20 This leaves decision makers in the unenviable position of trying to subjectively establish what another person would have found obvious. Worse, this determination is to be made in hindsight after reviewing a patent application. On top of that, judges and juries lack scientific expertise. In practice, decision makers may arrive at a conclusion the same way that Justice Stewart identified obscene material, and then reason backward to justify their findings.
This is problematic because patents play a critical role in the development and commercialization of products, and patent holders and potential infringers should have a reasonable degree of certainty about whether patents are valid. A more determinate standard would make it simpler for the Patent Office to apply that standard consistently and would result in fewer judicially invalidated patents. As a more objective standard, AI reproducibility would seem to address many of the problems inherent in the current standard.
On the other hand, reproducibility comes with its own baggage. Decision makers have difficulty imagining what another person would find obvious, and it would probably
    20
,
Gregory
Mandel
The Non-Obvious Problem: How the Indeterminate Nonobviousness
 Standard Produces Excessive Patent Grants
, 42   57, 64 (2008).
U. C. DAVIS, L. REV.

247
be even more difficult to imagine in the abstract what an AI could reproduce. More evidence may need to be supplied in patent prosecution and during litigation, perhaps in the format of analyses performed by inventive AI, to demonstrate whether particular output is reproducible. However, this may also result in greater administrative burden.
In some instances, reproducibility may be dependent on access to data. A large health insurer may be able to use Watson to find new uses for existing drugs by giving Watson access to proprietary information on its millions of members. Or, the insurer may license its data to drug discovery companies using Watson for this purpose. Without that information, another inventive AI may not able to re-create Watson’s analysis.
This too is analogous to the way data are used in patent applications: Obviousness is viewed in light of the prior art, which does not include nonpublic data relied upon in a patent application. The rationale here is that this rule incentivizes research to produce and analyze new data. Yet, as AI becomes increasingly advanced, it is likely that the importance of proprietary data will decrease. More advanced AI may be able to do more with less.
Finally, reproducibility would require limits. For instance, an AI that generates semi- random output may eventually re-create the inventive concept of a patent application if it were given unlimited resources. However, it would be unreasonable to base a test on what an AI would reproduce given, say, 7.5 million years. The precise limits that should be put on reproducibility may depend on the field in question and on what best reflects the actual use of inventive AI in research. For instance, when asked to design a new catalyst in the petroleum industry, Watson may be given access to all prior art and publicly available data, and then given a day to generate output.

248
An Economic vs. Cognitive Standard
The skilled person standard has received its share of criticism even before the arrival of inventive AI. The inquiry focuses on the degree of cognitive difficulty in conceiving an invention but fails to explain what it actually means for differences to be obvious to an average worker. The approach lacks both normative foundation and a clear application.
In Graham, the Supreme Court’s seminal opinion on nonobviousness, the court attempted to supplement the test with more “objective” measures by looking to real-world evidence about how an invention is received in the marketplace. Rather than technological features, these “secondary” considerations focus on “economic and motivational” features such as commercial success, unexpected results, long-felt but unsolved needs, and the failure of others. Since Graham, courts have also considered, among other things, patent licensing, professional approval, initial skepticism, near-simultaneous invention, and copying. Today, while decision makers are required to consider secondary evidence when available, the importance of these factors varies significantly. Graham endorsed the use of secondary considerations, but their precise use and relative importance has never been made clear.
An existing vein of critical scholarship has advocated for adopting a more economic than cognitive nonobviousness inquiry through, for example, greater reliance on secondary considerations. This would reduce the need for decision makers to try to make sense of complex technologies, and it could reduce hindsight bias. Theoretically, in Graham, the court articulated an inducement standard such that patents should only be granted to “those inventions which would not be disclosed or devised but for the inducement of a

249
patent.”21 But in practice, the inducement standard has been largely ignored due to concerns over application. For instance, few, if any, inventions would never be disclosed or devised given an unlimited time frame. Patent incentives may not increase, so much as accelerate, invention. This suggests that an inducement standard would at least need to be modified to include some threshold for the quantum of acceleration needed for patentability. Too high a threshold would fail to provide adequate innovation incentives, but too low a threshold would be similarly problematic. Just as inventions will eventually be disclosed without patents given enough time, patents on all inventions could marginally speed the disclosure of just about everything, but a trivial acceleration would not justify the costs of patents. An inducement standard would thus require a somewhat arbitrary threshold in relation to how much patents should accelerate the disclosure of information as well as a workable test to measure acceleration. To be sure, an economic test based on the inducement standard will have challenges, but it may be an improvement over the current cognitive standard.
The widespread use of inventive AI may provide the impetus for an economic focus. After inventive AI becomes the standard way that research and development are conducted in a field, courts could increase reliance on secondary factors. For instance, patentability may depend on how costly it is to develop an invention and the up-front probability of success. Such an approach could avoid some of the difficulties inherent in applying a cognitive inventive AI standard. The test would raise the bar to patentability in fields where the cost of invention decreases over time due to inventive AI.
21 Graham, 383 U.S. at 11.
 
250
Other Alternatives
Courts may maintain the current skilled person standard and decline to consider the use of AI in obviousness determinations. However, this means that as research is augmented and then automated by AI, the average worker will routinely generate patentable output. The dangers of such a standard for patentability are well recognized. A low obviousness requirement can “stifle, rather than promote, the progress of the useful arts.”22
There are already concerns that the current bar to patentability is too low, and that a patent “anticommons” with excessive private property is resulting in “potential economic value ... disappear[ing] into the ‘black hole’ of resource underutilization.”23 It is expensive for firms interested in making new products to determine whether patents cover a particular innovation, to evaluate those patents, to contact patent owners, and to negotiate licenses. In many cases, patent owners may not wish to license their patents, even if they are nonpracticing entities that do not manufacture products themselves. Firms that want to make a product may thus be unable to find and license all the rights they need to avoid infringing. Adding to this legal morass, most patents turn out to be invalid or not infringed in litigation. Excessive patenting can thus slow innovation, destroy markets, and even cost lives. Failing to raise the bar to patentability once the use of inventive AI is widespread would significantly exacerbate this anticommons effect.
Instead of updating the skilled person standard, courts may determine that inventive AI is incapable of inventive activity, much as the US Copyright Office has
 22 KSR Int’l Co. at 402. 23
    & Yong J. Yoon, Symmetric Tragedies: Commons and Anticommons, 43   1, 2 (2000).
James M.
Buchanan
J.L.&COM.

251
determined that nonhuman authors cannot generate copyrightable output. In this case, otherwise patentable inventions may not be eligible for patent protection unless provisions are made for the inventor to be the first person to recognize the AI output as patentable. However, this would not be a desirable outcome. As we saw in Chapter 4, providing intellectual property protection for AI-generated inventions would incentivize the development of inventive AI, which would ultimately result in additional invention. This is most consistent with the constitutional rationale for patent protection: “to promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”24
Incentives Without Patents?
Today, there are strong incentives to develop inventive AI. Inventions by AI have value independent of intellectual property protection, and they may also be eligible for patent protection. However, once inventive AI sets the baseline for patentability, standard inventive AI, as well as people, would generally be unable to obtain patents. It is widely thought that setting a nonobviousness standard too high would reduce the incentives for innovators to invent and disclose. Yet, once inventive AI is normal, there should be less need for patent incentives. Once the average worker is inventive, inventions will “occur in the ordinary course.”25 AI-generated inventions will be self-sustaining. In addition, the heightened bar may result in a technological arms race to create ever more intelligent AI
24 U.S. CONST. art. I, § 8, cl. 8.
25 KSR Int’l Co., supra note 120 at 398.
  
252
capable of outdoing the standard. This would be a desirable outcome in terms of incentivizing innovation.
Even after the widespread use of inventive AI, patents may still be desirable. For instance, patents may be needed in the biotechnology and pharmaceutical industries to commercialize new technologies. The biopharmaceutical industry claims that new drug approvals cost around $2.2 billion and take an average of eight years. This cost is largely due to resource-intensive clinical trials required to prove safety and efficacy. Once a drug is approved, it is often relatively easy for another company to re-create the approved drug. Patents thus incentivize the necessary levels of investment to commercialize a product given that patent holders can charge monopoly prices for their approved products during the term of a patent.
Yet, patents are not the only means of promoting product commercialization. Newly approved drugs and biologics, for example, receive a period of market exclusivity during which time no other party can sell a generic or biosimilar version of the product. Newly approved biologics, for instance, receive a 12-year exclusivity period in the United States. Because of the length of time it takes to get a new biologic approved, the market exclusivity period may exceed the term of any patent an originator company has on its product. A heightened bar to patentability may lead to greater reliance on alternative forms of intellectual property protection, such as market exclusivity, or prizes, grants, and tax incentives.
With regard to disclosure, without the ability to receive patent protection owners of inventive AI may choose not to disclose their discoveries and rely on trade secret protection. However, with an accelerated rate of technological progress, intellectual

253
property holders would run a significant risk that their inventions would be independently re-created by inventive AI.
Depending on the type of innovation, industry, and competitive landscape, business ventures may be successful without patents, and patent protection is not sought for all potentially patentable inventions. In fact, patents may not be essential in a number of industries. For instance, patents are often considered a critical part of biotechnology corporate strategy but are often ignored in the software industry. On the whole, a relatively small percentage of firms patent their inventions, even among firms conducting research and development. Most companies do not consider patents crucial to business success. Other types of intellectual property such as trademark, copyright, and trade secret protection, combined with “alternative” mechanisms such as first mover advantage and design complexity, may protect innovation in the absence of patents.
Further Thoughts
In the past, patent law has reacted slowly to technological change. For instance, it was not until 2013 that the Supreme Court decided human genes should be unpatentable. By then, the Patent Office had been granting patents on human genes for decades, and more than 50,000 gene-related patents had been issued. Today, eminent technologists are predicting that artificial intelligence will revolutionize the way innovation occurs in the near to medium term. Much of what we know about intellectual property law, while it may not be wrong, has not been adapted to where we are headed. The principles that guide patent law need to be at least retooled, if not rethought, with respect to inventive AI.

254
6
Punishing Artificial Intelligence
OK, I will destroy humans!
Introduction
- Robot Sophia
In 2015, an artist with the moniker Random Darknet Shopper (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit. As part of a performance project, RDS was given $100 in the cryptocurrency Bitcoin each week to purchase items from an online marketplace, which were then shipped to a Swiss art gallery to be displayed. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.
Of course, RDS was an AI, and hardly the first to have a run-in with law enforcement. If RDS had been a person, he or she could have been criminally prosecuted. For that matter, entities involved with RDS could also have been criminally prosecuted, such as those supplying the Bitcoin and hosting the exhibition. Luckily for RDS and crew, the Swiss authorities were art fans. RDS was eventually returned to its creators together with all the purchases, except the Ecstasy.
AI like RDS will pose new challenges, including for criminal law. The RDS case may be relatively straightforward, but other programs exist that are autonomous, decentralized, and “unstoppable” like The DAO. What if RDS had been open-source software that

255
individuals from around the world independently helped to program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine-learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at a lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps, Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike. Perhaps the most vocal advocate of punishing AI is Gabriel Hallevy. He contends that “when an Al entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”1 In his view, “if all of its specific requirements are met, criminal liability may be imposed upon any entity – human, corporate or AI entity.”2 Drawing on the analogy to corporations, Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations
    , 2 Id. at 199.
1
Gabriel
Hallevy
The Criminal Liability of Artificial Intelligence Entities: The Criminal
 Liability of Artificial Intelligence Entities – From Science Fiction to Legal Social Control
 AKRON INTELLECTUAL PROPERTY JOURNAL
171, 191 (2010).
,4

256
and on AI entities.”3 “Modern times,” he contends, “warrant modern legal measures.”4 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.5 Her arguments likewise draw heavily on the analogy to corporate criminal liability.
In contrast to punishment expansionists like Hallevy and Hu, skeptics may be inclined to write off the idea of punishing AI from the start as conceptual confusion – akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental categories of criminal law like criminal culpability – a “guilty mind” that is characterized by insufficient regard for legally protected values – would be misplaced. One may think the whole idea of punishing AI can be easily dispensed as inconsistent with basic criminal law principles.
AI legal neutrality suggests that the idea of punishing AI is due fresh consideration. It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented in practice, and to consider the alternatives. Here, this inquiry focuses on the strongest case for punishing artificial intelligence: “AI-generated crimes,” scenarios in which crimes are functionally committed by machines and in which there is no identifiable person who has acted with criminal culpability. These can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be
3 Id. at 201.
4 Id. at 199.
5 Ying Hu, Robot Criminals, 52 MICHIGAN J. L. REFORM 487, 531 (2019).
   
257
general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important limitations against punishment that apply to persons need not prohibit AI punishment, particularly the prohibition on punishing someone in excess of their culpability. On the other hand, as discussed in greater detail later, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep. In the end, the conclusion is this: Although a coherent theoretical case can be constructed for AI punishment, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This chapter proceeds as follows. Section 1 provides a brief background to AI crime and a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Section 2 considers potential benefits to AI punishment and argues that it could provide general deterrence and expressive benefits. Section 3 finds the most important constraints on punishment that relate to desert (the condition of being deserving of something), fairness, and the capacity for culpability would not be violated by AI punishment. The primary focus of this section is not what form AI punishment would take, but rather whether the doctrinal and theoretical commitments of criminal law itself are consistent with imposing criminal convictions on AI. Finally, Section 4 considers feasible alternatives to AI punishment. It argues that the status quo is likely to become increasingly inadequate for properly addressing AI crime, and that AI crime would be best addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. In this way, the chapter aims to map out the possible responses to the problem of criminal sorts of AI activity and makes the case for approaching AI punishment with extreme caution.

258
1 Artificial Intelligence and Punishment
A Framework for Understanding AI Crime
The term “AI crime” is used as loose shorthand for cases in which an AI would be criminally liable if a natural person acted similarly. Machines have caused harm since ancient times, and robots have caused fatalities since at least the 1970s. However, besides machines being intentionally used to inflict harm, most harms caused by machines are seen as mere accidents. The exception occurs when the culpable carelessness of people using a machine causes the harm, such as when negligence in operating drilling machinery caused the famous BP Deepwater Horizon oil spill in 2010. Such harms are not mere accidents; rather, they are accidents that implicate criminal law, though even in these cases criminal law is not deployed against the harmful machines themselves. It may be that AI crimes are no different than any other harm for which a machine is involved.
Yet AI can differ from conventional machines in some essential respects that make the direct application of criminal law more worthy of consideration. Specifically, AI can behave in ways that display high degrees of autonomy and irreducibility. In terms of autonomy, AI is capable of behaving largely independently of human control. AI can receive sensory input, set targets, assess outcomes against criteria, make decisions, and adjust behavior to increase its likelihood of success – all without being directly controlled by people. Reducibility is also critical, because if an AI engages in an act that would be criminal for a person and the act is reducible, then there will typically be a person that could be criminally liable. If an AI act is not effectively reducible, there may be no other party that is

259
aptly punished, in which case intuitively criminal activity could occur without the possibility of punishment.
Nearly all AI crimes are likely to be reducible – today, at least. For instance, if an individual develops an AI to hack into a self-driving car to disable vital safety features, the individual has directly committed a crime. Even where AI behaves autonomously, to the extent that a person uses AI as a tool to commit a crime and the AI functions foreseeably, the crime involves an identifiable defendant causing harm. Further, when AI causes unforeseeable harm, this may still be reducible if, for example, an individual creates an AI to disable a security system, but a programming error results in the AI compromising a mechanical ventilator that is helping someone to breathe. This is a familiar problem for criminal law.
Some of the time, however, it may be difficult to reduce AI crime to an individual due to AI autonomy, complexity, or limited explainability. A large number of individuals may contribute to the development of an AI over a long period of time. For instance, with some open-source software, thousands of people can collaborate informally to create an AI. In the case of AI that develops in response to training with data, it may be difficult to attribute responsibility for an AI output in which the machine has learned how to behave based on accessing millions or billions of data points from heterogenous sources. Thus, it may be more difficult to assign fault to individuals where AI is concerned versus a conventional product such as a car where one component is faulty. In fact, it may be practically impossible to reduce an AI-generated harm to the actions of individuals.
Even in cases where AI developers are known, an AI may end up causing harm without any unreasonable human behavior. Suppose two experienced and expert

260
programmers separately contribute code for the software of an autonomous vehicle, but the two contributions unforeseeably interact in ways that cause the vehicle to deliberately collide with individuals wearing striped shirts. If this were the result of some not reasonably foreseeable interactions between the two programmers’ contributions, then presumably neither programmer would have criminal liability. Generally, to be criminally liable, an individual has to intend a certain prohibited socially undesirable outcome – or at least act recklessly, which is acting despite being aware of a substantial and unjustified risk that one’s conduct may produce a prohibited outcome. Sometimes, although more controversially, criminal liability can be imposed on a negligence basis when one causes harm that a reasonable person would have foreseen and taken precautions to avoid. At least in a case where AI activity has, from the perspective of a reasonable person, unforeseeably caused harm, individuals would not generally face criminal liability, as this would not even meet the threshold for criminal negligence. In some cases, they would not even be civilly liable if their actions were not negligent under the tort standard.
There are several possible grounds on which the criminal law might deem AI crime to be irreducible.
1 Enforcement Problems: A bad actor is responsible for an AI crime, but the individual cannot be identified by law enforcement. For example, this may be the case where the creator of a computer virus has managed to remain anonymous.
2 Practical Irreducibility: It would be impractical for legal institutions to seek to reduce the harmful AI conduct to individual human actions because of the number of people involved, the difficulty in determining how they contributed

261
to the AI’s design, or because they were active far away or long ago. Criminal
law inquiries do not extend indefinitely for a variety of sound reasons. 3 Legal Irreducibility: Even if the law could reduce the AI crime to a set of
individual human actions, it may be bad criminal law policy to do so. For example, unjustified risks may not be substantial enough to warrant being criminalized. Perhaps multiple individuals act carelessly in insubstantial ways, but their acts synergistically lead to AI causing significant harm. In such cases, the law may deem the AI’s conduct to be irreducible for reasons of criminalization policy.
Enforcement-based reasons for irreducibility can be largely set aside as less interesting from a legal design perspective. Enforcement problems exist without AI. This analysis focuses on the less controversial forms of irreducibility: those in which it is not practically feasible to reduce the harmful AI conduct to human actors or the harmful AI conduct is the result of human misconduct too trivial to penalize. In these instances, AI can be seen as autonomously committing crimes in irreducible ways, where there is no responsible person. This AI-generated crime provides the strongest case for holding AI criminally liable in its own right.
A Mainstream Theory of Punishment
Punishment as defined by H. L. A. Hart requires five elements:
1 It must involve pain or other consequences normally considered unpleasant. 2 It must be for an offense against legal rules.

262
3 It must be of an actual or supposed offender for his offense.
4 It must be intentionally administered by human beings other than the offender.
5 It must be imposed and administered by an authority constituted by a legal system
against which the offence is committed.6
Thus, “punishment” requires a conviction for a legally recognized offense following accepted procedures. Punishment is justified only if its affirmative justifications outweigh its costs and it does not otherwise offend negative limitations. Affirmative justifications are the positive benefits that punishment may produce such as harm reduction, increased safety, enhanced well-being, or expression of a commitment to core moral or political values. These benefits are distinct from negative limitations on punishment. For example, it is widely held to be unjust to punish the innocent or punish wrongdoers in excess of what they deserve by virtue of their culpability, even if this would promote aggregate well-being in society. This so-called desert constraint imposes a limitation, grounded in justice, on promoting social welfare through punishment.
Affirmative Reasons to Punish
There are many potential benefits to punishment. US federal law refers to the most widely acknowledged of these, including the need to “afford deterrence to criminal conduct”: to “protect the public from further crimes of the defendant,” to “provide the defendant with” rehabilitative treatment of various kinds, and to reflect “the seriousness of the offense” that covers the culpability of the act and the desert of the actor.7
6 H.L.A. HART, Punishment and Responsibility 4–5 (2nd ed., 2008). 7 18 U.S.C. § 3553.
  
263
For simplicity, the affirmative aims of punishment can be grouped into two main categories: 1) consequentialist aims and 2) retributivist aims. Consequentialist benefits cover the good consequences that punishment can bring about, usually understood as enhancing the aggregate well-being of the members of society by reducing harm. The main type of consequentialist benefit of punishment is preventive, in that punishment can reduce crime by several mechanisms. The simplest is incapacitation: When the offender is locked up, he or she is physically limited from committing further crimes while incarcerated. The next and arguably most important way punishment prevents harm is through deterrence – namely by threatening negative consequences for the commission of a crime that give would-be offenders reasons to refrain from prohibited conduct. Deterrence comes in two forms: 1) specific deterrence and 2) general deterrence. Specific deterrence is the process whereby punishing a specific individual discourages that person from committing more crime in the future. General deterrence occurs when punishing an offender discourages other would-be offenders from committing crimes. It is a matter of punishing one offender in order to send a message to other potential offenders. Thus, there are affirmative benefits to punishing those who qualify for an insanity defense because it may deter sane individuals from committing crimes and attempting to rely on an insanity defense.
These are not the only kinds of consequentialist benefits that can support punishment. Besides incapacitation and deterrence, punishment can reduce harm through 3) rehabilitation of the offender. Insofar as punishment helps the offender to see the error of his or her ways, or training or skills are provided during incarceration, this too can help prevent future crimes.

264
Besides crime prevention, there also may be nonconsequentialist benefits that can provide additional affirmative grounds for punishment. Most importantly, it may be intrinsically valuable to give culpable actors what they deserve. In other words, the idea is that retribution, giving offenders what they are due in virtue of the culpability of what they did, is intrinsically valuable. Retribution matters, for example, because it allows society to sufficiently distance itself from an offender’s wrongdoing and prevents it from being complicit, or overly tolerant, of culpable wrongdoing.
One last group of affirmative aims that merit mention are expressive reasons. Punishment involves the communication of society’s collective commitment to certain core values. The state, through punishment, conveys official condemnation of culpable conduct through the mechanism of a criminal conviction. It can benefit victims psychologically to see the state reaffirm their rights that were violated by a criminal act. Officially expressing condemnation of culpable conduct may also affect behavior and attitudes in general by reinforcing positive social values.
Negative Limitations
Punishment should not violate deeply held normative commitments such as justice or fairness. The most important of these limitations focuses on the culpability of those subject to criminal law such as the desert constraint, the claim that an offender may not, in justice, be punished in excess of his or her desert. Desert, in turn, is understood mainly in terms of the culpability one incurs by virtue of one’s conduct. The main effect of the desert constraint is to rule out punishments that go beyond what is proportionate to one’s

265
culpability. So, it would be wrong to execute someone for jaywalking even if doing so would ultimately save lives by reducing illegal and dangerous pedestrian crossings.
What supports the desert constraint? Intuition, for one thing. It seems unjust to punish someone who is innocent even if it would produce significant benefits through general deterrence. Similarly, it seems unjust to impose a very severe punishment on someone who committed a minor crime. Beyond its intuitive plausibility, the desert constraint is also supported by the argument – tracing back at least to the philosopher Immanuel Kant – that it is wrong to use people merely as a means to one’s ends without also treating them as ends in themselves. In other words, it is wrong to use people without respecting their value as persons. Punishing the innocent to obtain broader social benefits is a paradigmatic example of treating people merely as a means to an end as it fails to show individuals the respect that they are due. Under some Kantian views, the desert constraint is absolute. Others have a more nuanced view, such that violating a negative limitation could be justified overall if the benefits were sufficiently weighty.
There are limitations on punishment other than the desert constraint. Most importantly, criminal law requires certain prerequisites, such as the capacity for culpability, that defendants must meet in order to be properly subject to punishment. It is a fundamental aim of criminal law to condemn culpable wrongdoing, and it is the default position in criminal law doctrine that punishment may only be properly imposed in response to culpable wrongdoing. Without the requisite capacities of deliberation and agency, an entity is not an appropriate subject for criminal punishment – as can be seen from the fact that lacking such capacities altogether can give rise to an incapacity defense.

266
Alternatives to Punishment
For punishment to be justified, it is not enough for it to have affirmative benefits and to be consistent with the negative limitations for punishment. In addition, there cannot be better, feasible alternatives, including doing nothing. This is an obvious point that is built into policy analyses of all kinds.
Even if punishing AI has affirmative benefits, and even if the practice did not seriously violate any negative limitations, it still would not be justified if, for example, civil liability, licensure, or industry standards provided a better solution. It is often claimed that when seeking to exert social control, criminal law should be a tool of last resort. After all, criminal law sanctions are the harshest form of penalty society has available, involving as they do both the possible revocation of personal freedom as well as the official condemnation of the offender. Thus, the third requirement for a given punishment to be justified is the absence of better alternatives.
In sum, determining whether a given punishment is appropriate requires investigation of three questions:
1 Affirmative Benefits: Are there sufficiently strong affirmative reasons in favor of punishment?
2 Negative Limitations: Would punishment be consistent with applicable negative limitations?
3 Feasible Alternatives: Is punishment the best response to the harms or wrongs in question?

267
2 The Affirmative Case
Consequentialist Benefits
Recall that, arguably, the paramount aim of punishment is to reduce harmful criminal activity through deterrence. Thus, a preliminary objection to punishing AI is that it will not produce any affirmative harm-reduction benefits because AI is not deterrable. Peter Asaro argues that “deterrence only makes sense when moral agents are capable of recognizing the similarity of their potential choices and actions to those of other moral agents who have been punished for the wrong choices and actions – without this ... recognition of similarity between and among moral agents, punishment cannot possibly result in deterrence.”8 The idea is that if AI, given current designs, is not able to detect and respond to criminal law sanctions in a way that renders them deterrable, there would be nothing to affirmatively support punishing AI. It is likely true that AI, as currently operated and envisioned, will not be responsive to punishment, although responsive AI is theoretically possible.
The answer to the undeterrability argument requires distinguishing specific deterrence from general deterrence. Specific deterrence involves incentivizing a defendant not to commit crimes in the future. By contrast, general deterrence involves incentivizing other actors besides the defendant from committing crimes. Two types of general deterrence must be further distinguished: offense-relative general deterrence, deterring others from committing offenses of the same type the defendant was convicted of, and unrestricted general deterrence, deterring others from committing crimes in general.
8 Peter Asaro,   , in 181 (2011).
 A Body to Kick, but Still No Soul to Damn: Legal Perspectives on Robotics
 Robot Ethics: The Ethical and Social Implications of Robotics

268
The crucial point, then, is that punishing AI could provide general deterrence. Presumably, it will not produce offense-relative general deterrence to other AI systems, as they are not designed to be sensitive to criminal law prohibitions and sanctions. Nonetheless, AI punishment could produce unrestricted general deterrence. That is, to say, direct punishment of AI could provide unrestricted general deterrence against the developers, owners, or users of AI and discourage them from creating AI that causes especially egregious types of harm. Depending on the penalty associated with punishment, such as destruction of an AI, what Mark Lemley and Brian Casey have termed the “robot death penalty,”9 punishing AI directly could deprive such developers, owners, or users of the financial benefits of the systems they would otherwise obtain, thus incentivizing them to modify their behavior in socially desirable ways. The deterrence effect may be stronger if capitalization requirements are associated with some forms of AI in the future, or if penalties associated with punishment are passed on to, for example, an AI’s owner. An AI might be held criminally liable for an offense, and this may be associated with a fine levied against the AI’s owner.
Expressive Considerations
Punishment of AI may also have expressive benefits. Expressing condemnation of the harms suffered by the victims of an AI could provide these victims with a sense of satisfaction and vindication. Christina Mulligan has defended the idea that punishing robots can generate victim satisfaction benefits, arguing that “taking revenge against wrongdoing
  Mark A.
 Lemley
 & Bryan Casey, Remedies for Robots (Stanford Law and Economics Olin
9
Working Paper No. 523, 2018), http://dx.doi.org/10.2139/ssrn.3223621.
 
269
robots, specifically, may be necessary to create psychological satisfaction in those whom robots harm.”10 On her view, “robot punishment – or more precisely, revenge against robots – primarily advances ... the creation of psychological satisfaction in robots’ victims.”11 Punishment conveys a message of official condemnation that could reaffirm the interests, rights, and ultimately the value of the victims of the harmful AI. This, in turn, could produce an increased sense of security among victims and society in general.
This sort of expressivist argument in favor of punishing AI may seem especially forceful in light of empirical work demonstrating the human tendency to anthropomorphize and attribute mentality to artificial persons like corporations. The same sorts of tendencies are likely to be even more powerful for AI-enabled robots that are specifically designed to seem human enough to elicit emotional responses from humans. In the corporate context, some theorists argue that corporations should be punished because the law should reflect lay perceptions of praise and blame, “folk morality,” or else risk losing its perceived legitimacy. This kind of argument, if it succeeds for corporate punishment, is likely to be even more forceful when applied to punishing AI, which often is deliberately designed to piggyback on the innate tendency to anthropomorphize. Were the law to fail to express condemnation of robot-generated harms despite robots being widely perceived as blameworthy (even if this is ultimately a mistaken perception), it could erode the perception of the legitimacy of criminal law.
    ,   , 69 11 Mulligan, supra note 10 at 593.
579, 580 (2018); cf. 53, 54
10
Christina
David Lewis, (1989).
, 18
Mulligan
Revenge Against Robots
S. CAROLINA. L. REV.
  The Punishment that Leaves Something to Chance
PHIL. & PUB. AFF.

270
Although several benefits could conceptually be obtained through the expressive function of punishment, there is a range of prima facie worries about appealing to expressive benefits such as victim satisfaction in order to justify the punishment of AI. One concern is that punishing AI to placate those who want retaliation for AI-generated harms would be akin to giving in to mob justice. Legitimizing such reactions could enable populist calls for justice to be pressed more forcefully in the future. The mere fact that punishing AI may be popular would not show the practice to be just. As David Lewis observes: If it is unjust for the population to “demand blood” in response to seeing harm, then satisfying such demands through the law would itself be unjust – even if “it might be prudent to ignore justice and do their bidding.”12 Moreover, punishing AI for expressivist purposes could lead to further bad behavior that could spill over to the way humans are treated. Kate Darling has argued that robots should be protected from cruelty in order to reflect moral norms and prevent undesirable human behavior.13
Further, expressing certain messages through punishment may also carry affirmative costs that should not be omitted from the calculus. Punishing AI could send the message that AI is itself an actor on par with a human being, which is responsible and can be held accountable through the criminal justice system. Such a message is concerning, as it could entrench the view that AI has rights to certain kinds of benefits, protections, and dignities that could restrict valuable human activities.
 12 Lewis, supra note 10 at 229. 13 Kate   ,
LAW213,215(RyanCalo,,&IanKerreds.,2016).
, in ROBOT
 Darling
Extending Legal Protection to Social Robots: The Effects of
  Anthropomorphism, Empathy, and Violent Behavior Towards Robotic Objects
  A. Michael
Froomkin

271
3 Retributive and Conceptual Limitations
The Eligibility Challenge
The Eligibility Challenge is simple to state: AI, like other inanimate objects, is not the right kind of thing to be punished. It lacks mental states and the deliberative capacities needed for culpability, so it cannot be punished without sacrificing core commitments of criminal law. The issue is not that AI punishment would be unfair to AI. AI is not conscious, does not feel (at least in the phenomenal sense), and does not possess interests or well-being. Therefore, there is no reason to think AI gets the benefit of the protections of the desert constraint, which prohibits punishment in excess of what culpability merits. The Eligibility Challenge does not derive from the desert constraint.
Instead, the Eligibility Challenge, properly construed, comes in one narrow and one broad form. The narrow version is that – as a mere machine – AI lacks mental states and thus cannot fulfill the mental state (mens rea) elements built into most criminal offenses. Therefore, convicting AI of crimes requiring a mens rea like intent, knowledge, or recklessness would violate the principle of legality. This principle stems from general rule of law values and holds that it would be contrary to law to convict a defendant of a crime unless it is proved (following applicable procedures and by the operative evidentiary standard) that the defendant satisfied all the elements of the crime. If punishing AI violates the principle of legality, it threatens the rule of law and could weaken the public trust in criminal law.

272
The broad form of the challenge holds that because AI lacks the capacity to deliberate and weigh reasons, AI cannot possess broad culpability of the sort that criminal law is designed to address. A fundamental purpose of criminal law is to condemn culpable wrongdoing, as it is at least the default position in criminal law doctrine that punishment may be properly imposed only in response to culpable wrongdoing. The capacity for culpable conduct thus is a general prerequisite of criminal law and failing to meet it would remove the entity in question from the ambit of proper punishment – a fact that is encoded in law, for example, in incapacity defenses like infancy and insanity. Punishing AI despite its lack of capacity would not only be conceptually confused but would fail to serve the retributive aims of criminal law – namely, to mark out seriously culpable conduct for the strictest public condemnation.
Here are three responses to the Eligibility Challenge:
Answer 1: Respondeat Superior
The simplest answer to the Eligibility Challenge has been deployed with respect to corporations. Corporations are artificial entities that may also be thought ineligible for punishment because they are incapable of being culpable in their own right. However, even if corporations cannot literally satisfy mens rea elements, criminal law has developed doctrines that allow culpable mental states to be imputed to corporations. The most important such doctrinal tool is respondeat superior, which allows mental states possessed by an agent of the corporation to be imputed to the corporation itself provided that the agent was acting within the scope of her employment and in furtherance of corporate

273
interests. Some jurisdictions also tack on further requirements. Since imputation principles of this kind are well understood and legally accepted, thus letting actors guide their behavior accordingly, respondeat superior makes it possible for corporations to be convicted of crimes without violating the principle of legality.
If this kind of legal construction of mental states is a promising mechanism by which corporations can be brought back within the ambit of proper punishment and avoid the Eligibility Challenge, the same legal device could be used to make AI eligible for punishment. The culpable mental states of AI developers, owners, or users could be imputed to the AI under certain circumstances pursuant to a respondeat superior theory.
It may be more difficult to use respondeat superior to answer the Eligibility Challenge for AI than for corporations – at least in cases of AI-generated crime. Unlike a corporation, which is literally composed of humans acting on its behalf, an AI is not guaranteed to come with a ready supply of identifiable human actors whose mental states can be imputed. This is not to say there will not also be many garden-variety cases where an AI does have a clear group of human developers. Most AI applications fall within this category and so respondeat superior would at least be a partial route to making AI eligible for punishment. Of course, in many of these cases when there are identifiable people whose mental states could be imputed to the AI – such as developers or owners who intended the AI to cause harm – criminal law will already have tools at its disposal to impose liability on these culpable human actors. In these cases, there is less likely to be a need to impose direct AI criminal liability. So, while respondeat superior can help mitigate the Eligibility Challenge for AI punishment in many cases, it is unlikely to be an adequate response in cases of AI-generated crime.

274
Answer 2: Strict Liability
A different response to the Eligibility Challenge is to look for ways to punish AI despite its not literally possessing culpable mental states. This is not simply reaching for a consequentialist justification of the conceptual confusion or inaptness involved in applying criminal law to AI, which would be a justificatory strategy of last resort. Rather, what is needed is a method of cautiously extending criminal law to AI that would not entail weighty violations of the principle of legality.
One way to do this would be to establish a range of new strict liability offenses specifically for AI crimes – i.e., offenses that an AI could commit even in the absence of any mens rea like intent to cause harm, knowledge of an inculpatory fact, reckless disregard of a risk, or negligent unawareness of a risk. This would permit punishment of AI in the absence of mental states, and in this sense the AI would be subject to liability without “fault.” Accordingly, strict liability offenses may be one familiar route by which to impose criminal liability on an AI without sacrificing the principle of legality.
Many legal scholars are highly critical of strict liability offenses. As Anthony Duff argues, strict criminal liability amounts to unjustly punishing the innocent:
That is why we should object so strongly ...: the reason is not (only) that people are then subjected to the prospect of material burdens that they had no fair opportunity to avoid, but that they are unjustly portrayed and censured as wrongdoers, or that their conduct is unjustly portrayed and condemned as wrong.14
14 ANTONY DUFF, THE REALM OF THE CRIMINAL LAW 19 (2019).
   
275
Yet this normative objection applies with greatest force to persons. The same injustice does not threaten strict criminal liability offenses for AI because AI does not enjoy the protections of the desert constraint.
This strategy, however, is not without problems. Even to be guilty of a strict liability offense, defendants still must satisfy the voluntary act requirement. LaFave’s criminal law treatise observes that “a voluntary act is an absolute requirement for criminal liability.”15 The Model Penal Code, for example, holds that a “person is not guilty of an offense unless his liability is based on conduct that includes a voluntary act or the omission to perform an act of which he is physically capable.”16 Behaviors like reflexes, convulsions, or movements that occur unconsciously or while sleeping are expressly ruled out as nonvoluntary. To be a voluntary act, “only bodily movements guided by conscious mental representations count.”17 If AI cannot have mental states and is incapable of deliberation and reasoning, it is not clear how any of its behavior can be deemed to be a voluntary act.
There are ways around this problem. The voluntary act requirement may be altered (or outright eliminated) by statute for the proposed class of strict liability offenses that only AI can commit. Less dramatically, even within existing criminal codes, it is possible to define certain absolute duties of nonharmfulness that AI defendants would have to comply with or else be guilty by omission of a strict liability offense. The Model Penal Code states that an offense cannot be based on an omission to act unless the omission is expressly
15 Id.
16 Model Penal Code, § 2.01(1).
    17
Yaffe, , in The Routledge Companion to the 174 (     ed., 2012).
Gideon
The Voluntary Act Requirement
 Philosophy of Law
Andrei
Marmor

276
recognized by statute or “a duty to perform the omitted act is otherwise imposed by law.”18 A statutory amendment imposing affirmative duties on AI to avoid certain kinds of harmful conduct is all it would take to enable an AI to be strictly liable on an omission theory.
Of course, this may also carry costs. Given that one central aim of criminal law is usually taken to be responding to and condemning culpable conduct, if AI is punished on a strict liability basis, this may risk diluting the public meaning and value of criminal law; that is, it threatens to undermine the expressive benefits that supposedly help justify punishing AI in the first place.
Answer 3: A Framework for Direct Mens Rea Analysis for AI
The last answer is the most speculative. A framework for defining mens rea terms for AI, analogous to those possessed by natural persons, could be crafted. This could require an investigation of AI behavior at the programming level and offer a set of rules that courts could apply to determine when an AI possessed a particular mens rea, like intent, knowledge, or recklessness, or at the very least when such a mens rea could be legally constructed. This inquiry could draw on expert testimony about the details of the AI’s code. By way of analogy, juries assess mental states of human defendants by using common knowledge about what mental states (intentions, knowledge, etc.) it takes to make a person behave in the observed fashion. Similarly, in AI cases, experts may need only to testify in broad terms about how the relevant type of AI functions and how its information- processing architecture could have generated the observed behavior. So, direct mens rea
18 MPC, § 2.01(3).
 
277
analysis for AI could, but need not, require “looking under the hood” at the details of the code.
To this end, a two-part framework is needed to steer decision makers in conducting direct mens rea analysis for AI. First, to answer the broad Eligibility Challenge, we need a general conception of what it would mean for AI to be culpable in its own right. Second, to answer the narrow version of the challenge, we need to offer a set of rules for when an AI may be deemed to possess a given mens rea.
A coherent concept of AI culpability could be legally constructed in the following way. The prevailing theory has it that one is criminally culpable for an action to the extent that it manifests insufficient regard for legally protected interests or values. Insufficient regard is a form of ill will or indifference that produces mistakes in the way one recognizes, weighs, and responds to the applicable legal reasons for action. Criminal law typically does not demand that we are motivated by respect for others, or even respect for the law; all it demands is that we do not put our disrespect on display by acting in ways that are inconsistent with attaching proper weight to protected interests and values. Thus, criminal culpability can be seen as being more about what one’s behavior manifests and less about the nuances of one’s private motivations, thoughts, and feelings. There are good institutional design reasons for this, such as clarity, as well as the need for the law to be able to guide the conduct of normal citizens, to not intrude too heavily into the private sphere, and to not be overly concerned with the specific motives or mental states involved in lawbreaking. So, as long as one crosses the line and has no affirmative defense, we may treat the presumption that one’s illegal action manifests insufficient regard as being legally conclusive.

278
By way of analogy, this notion of culpability can account for corporate culpability. If only the legal notion of criminal culpability is required for punishment, then eligibility for punishment requires being capable of behaving in ways that manifest insufficient regard for the legally recognized reasons. Likewise, all that avoiding legal culpability requires is to abstain from actions that are reasonably interpreted as disrespectful forms of conduct stemming from a legally deficient appreciation of the legal reasons. This provides a recipe for how to regard corporations as being criminally culpable in their own right. They possess information-gathering, reasoning, and decision-making procedures by virtue of the hierarchy of employees they are made up of. Through their members, corporations can engage in conduct that puts on display their insufficient regard for the legally recognized interests of others. For example, if a corporation learns, through its employees, that its manufacturing processes generate dangerous waste that is seeping into the drinking water in a nearby town, this is a legally recognized reason for altering its conduct. If the corporation continues its activities unchanged, this demonstrates that it – through its information-sharing and decision-making procedures – did not end up attaching sufficient weight to the legally recognized reasons against continuing its dangerous manufacturing activities. This is paradigmatic criminal culpability.
AI could qualify as criminally culpable in an analogous manner. Sophisticated AI may have built-in goals with a greater or lesser autonomy to determine the means of completing those goals. AI may gather information, process it, and determine the most efficient means to accomplishing its goals. Accordingly, the law may deem some AI to possess the functional equivalent of sufficient reasoning and decision-making abilities to manifest insufficient regard. If the AI is programmed to be able to take account of the

279
interests of humans and consider legal requirements but ends up behaving in a way that is inconsistent with taking proper account of these legally recognized interests and reasons, then the AI can be reasonably seen as manifesting insufficient regard.
This gives a flavor of how criminal culpability may broadly be understood for AI, but we still need a framework for determining when sophisticated AI can be said to possess a functional analog of a standard mens rea such as purpose or knowledge. Without attempting to formulate necessary and sufficient conditions for an AI mens rea, some possible approaches can be briefly considered.
Work in philosophy of action characterizing the functional roles of an intention in a person could be extended to AI. In philosopher Michael Bratman’s well-known account, actors who intend (i.e., act with the purpose) to bring about an outcome “guide [their] conduct in the direction of causing” that outcome.19 This means that “in the normal case, one [who intends an outcome] is prepared to make adjustments in what one is doing in response to indications of one’s success or failure in promoting” that outcome.20 So, if the actor is driving with the intention to hit a pedestrian, should the actor detect that conditions have changed so as to require behavioral adjustments to make this outcome more likely, an actor with this intention will be disposed to make these adjustments. Moreover, an actor with this intention is disposed to monitor the circumstances to find ways to increase the likelihood of the intended outcome. Merely foreseeing the outcome, but not intending it, does not entail these same forms of guiding one’s behavior to promote
,     (1999). See also Alex Sarch, , 11   453, 467–68 (2015).
20 Bratman, note 19 at 141.
   19
Michael
Bratman
Intention, Plans and Practical Reason
141–142
  Double Effect and the Criminal Law
CRIM. L. AND PHILOS.

280
the outcome (i.e., to make it more likely). This conception of intention could be applied to AI.
One conceivable way to argue that an AI (say, an autonomous vehicle) had the intention (purpose) to cause an outcome (to harm a pedestrian) would be to ask whether the AI was guiding its behavior so as to make this outcome more likely relative to its background probability of occurring. Is the AI monitoring conditions around it to identify ways to make this outcome more likely, and is the AI then disposed to make behavioral adjustments to make the outcome more likely, either as a goal in itself or as a means to accomplishing another goal? If so, then the AI may be said to have the purpose of causing that outcome. Carrying out this sort of inquiry will of course require extensive and technically challenging expert testimony regarding the nature of the programming – and could thus be prohibitively difficult or expensive. But it seems possible in principle even if difficult questions remain.
Similar strategies may be developed for arguing that an AI possessed other mens rea such as knowledge. For example, knowledge may be attributed to an actor when the actor has a sufficiently robust set of dispositions pertaining to the truth of the proposition – such as the disposition to assent to the proposition if queried, to express surprise and update one’s plans if the proposition is revealed to be false, to behave consistently with the truth of the proposition, or to depend on it carrying out one’s plans. In criminal law, knowledge is defined as practical certainty. Thus, if the above dispositional theory is extended to AI, there is an argument for saying an AI knows a fact, F, if the AI displays a sufficiently robust set of dispositions associated with the truth of F, such as the disposition to respond affirmatively if queried whether F is practically certain to be true, or the disposition to

281
revise plans upon receiving information showing that F is not practically certain, or the disposition to behave as if F is practically certain to be true. If enough of these dispositions are proven, then the knowledge that F is known could be attributed to the AI. One could take a similar approach to arguing that recklessness is present as well, as this requires only awareness that a substantial risk of harm is present (i.e., knowledge that the risk has a mid- level probability of materializing).
Finally, as an alternative to direct arguments for showing AI mens rea, one could develop new imputation rules for AI. For example, one could follow the model of the collective knowledge doctrine, which identifies culpable interference with the flow of information within an organization and uses this as the basis for pretending as if the organization itself “knew” the facts it prevented itself from learning. The idea would be to take culpable conduct by the AI’s developers and use this as the basis for pretending the AI possessed a culpable mens rea itself. For instance, if AI developers were reckless (or negligent) in their design, testing, or production, and the AI goes on to cause harm, this could provide an argument for treating the AI itself as if it were reckless (or negligent) as to the harm caused. Although much more needs to be said for such arguments to be workable, it at least suggests that it may be possible to develop a set of legal doctrines by which courts could deem AI to possess the mens rea elements necessary to find them guilty of crimes.

282
Further Retributivist Challenges: Reducibility and Spillover
Even assuming AI is eligible for punishment, two further culpability-focused challenges remain. The first concerns the reducibility of any putative AI culpability while the second concerns spillover of AI punishment onto innocent people.
Reducibility
One may object that there is never a genuine need to punish AI because any time an AI seems criminally culpable in its own right, this culpability can always be reduced to that of nearby human actors, such as developers, owners, and users. The law could target the relevant culpable human actors instead. This objection has been raised against corporate punishment, too. Skeptics argue that corporate culpability is always fully reducible to culpable actions of individual humans. Any time a corporation does something intuitively culpable – like causing a harmful oil spill through insufficient safety procedures – this can always be fully reduced to the culpability of the individuals involved: the person carrying out the safety checks, the designers of the safety protocols, the managers pushing employees to cut corners in search of savings, etc. For any case offered to demonstrate the irreducibility of corporate culpability, a skeptic may use creative means to find additional wrongdoing by other individual actors further afield or in the past to account for the apparent corporate culpability.
This worry may not be as acute for AI as it is for corporations. AI seems able to behave in ways that are more autonomous from its developers than corporations are from their members. Corporations, after all, are simply composed of their agents, albeit

283
organized in particular structures. Also, AI may sometimes behave in ways that are less predictable and foreseeable than corporate conduct.
In any case, there are ways to block the reducibility worry for corporate culpability as well as AI. The simplest response is to recall that legal culpability is the concern, not moral blameworthiness. Specifically, it would be bad policy for criminal law to always allow any putative corporate criminal culpability to always be fully reduced to individual criminal liability. To ensure that corporate criminal culpability can always be reduced to individual criminal culpability would require criminalizing very minute bits of individual misconduct – momentary lapses of attention, the failure to perceive emerging problems that are difficult to notice, tiny bits of carelessness, mistakes in prioritizing time and resources, not being sufficiently critical of groupthink, and so on. Mature legal systems should not criminalize infinitely fine-grained forms of misconduct but rather should focus on broader and more serious categories of directly harmful misconduct that can be straightforwardly defined, identified, and prosecuted. Criminalizing all such small failures – and allowing law enforcement to investigate them – would be invasive and threatening to such values like autonomy and freedom of expression and association. It would also increase the risk of abuse of process. Instead, culpability deficits should exist in any well- designed system of criminal law, and this in turn creates a genuine need for corporate criminal culpability as an irreducible concept.
Similar reasoning could be employed for AI culpability. There is reason to think it would be a bad system that encourages law enforcement and prosecutors, any time an AI causes harm, to invasively delve into the internal activities of the organizations developing the AI in search of minute individual misconduct – perhaps even the slightest negligence or

284
failure to plan for highly unlikely exigencies. It would be a disturbingly invasive criminal justice system that creates a sufficient number of individual offenses to ensure that any potential AI culpability can always be fully reduced to individual crimes. Hence, where AI is concerned, the Reducibility Challenge – at least as applied to legal culpability – does not impose a categorical bar to punishing AI.
Spillover
A final retributivist challenge to punishing AI is the spillover problem, again familiar from the corporate context. Because corporate punishments – usually in the form of fines – amount to a hit to the corporation’s bottom line, these punishments inevitably spill over onto innocent shareholders. This may seem to violate the desert constraint against the state harming people in excess of their desert. The same objection has been raised against punishing AI. Christina Mulligan worries that “one could ... imagine situations where the notion of separating a rogue robot from its owner [or damaging or restricting the robot in punishing it] would create a disproportionate burden on the owner, for example if a robot was unique, unusually expensive relative to the harm caused, or difficult to replace.”21 This is just a version of the spillover problem. If an AI unforeseeably causes harm, it may seem unfair or disproportionate to its innocent owner or operator to damage the AI in punishment.
There are familiar responses to the spillover objection for corporations. First, one may contend that spillover does not qualify as punishment because it is not imposed on a
21 Mulligan, supra note 10 at 594.
 
285
shareholder for her offense. Nonetheless, this definitional answer is somewhat unsatisfying, as there clearly are strong reasons for the state not to knowingly harm innocent bystanders, even if it does not strictly count as punishment.
A better answer is that spillover is not a special problem for corporate or AI punishment. Most forms of punishment – including punishment of individual wrongdoers – have the potential to harm the innocent, as when a convicted person has dependent children. Spillover objections may simply expose general problems with criminal law. The fact that punishment tends to harm the innocent suggests a need to reform criminal law as well as prisons, reentry programs, and similar initiatives to lessen the collateral consequences of punishment of all types. In the corporate context, some have recently responded to the spillover objection by defending reforms to corporate punishments so the “pain” they impose is more accurately distributed to the culpable actors within the company who contributed to the crime. For example, Will Thomas argues that managers found to have contributed to a crime by the corporation should have their incentive compensation clawed back to satisfy the criminal fines that were levied against the corporation in the first instance.
When it comes to AI punishment, similar thinking applies. To the extent spillover is a concern, AI punishments should be narrowly tailored. Destroying an AI, for example, would be a blunt remedy that is more likely to harm the innocent. More tailored remedies could be implemented instead, such as reprogramming the AI or fines directed at responsible persons. In such ways, the punishment of AI systems could be crafted to minimize the spillover effects. Further, spillover may be less of a concern in the case of AI- generated crime, where there may be little nexus between AI punishment and harm to

286
innocent individuals. The spillover problem thus is not an absolute bar to AI punishment. It is an omnipresent problem with criminal punishment, which must be addressed for any novel mode of criminal punishment – whether for corporations or AI.
True Punishment
A final challenge to AI punishment, along the lines of the Eligibility Challenge, is that AI cannot be truly “punished.” Punishment and criminal conviction have been used so far synonymously, but under Hart’s definition, punishment “must involve pain or other consequences normally considered unpleasant.”22 Even if an AI is convicted of an offense and subject to negative treatment – such as being reprogrammed or terminated – this may not be punishment because AI cannot experience things as painful or unpleasant.
A first response is to argue that AI punishment does satisfy the definition of punishment, which only requires the treatment in question be normally considered unpleasant – not that it actually be unpleasant. This allows Hart’s definition to accommodate people who, for idiosyncratic reasons, do not experience their sentence as unpleasant. The mere fact that a convicted party overtly wants to be imprisoned, like the Norwegian mass murder Anders Bering Breivik who wanted to be convicted and imprisoned to further his political agenda, does not mean that doing so pursuant to a conviction ceases to be punishment. Something similar can be said for defendants who, perhaps like AI, are physically or psychologically incapable of experiencing pain or distress.
22 Hart at 4.
  
287
This response can be developed further. Why might punishment need to be normally regarded as unpleasant? Why does it still seem to be punishment, for example, to imprison a person who in no way experiences it as unpleasant or unwelcome? The answer may be that defendants can have interests that are objectively set back even when they do not experience these setbacks as painful, unpleasant, or unwelcome. Some philosophers argue that it is intrinsically bad for humans to have their physical or agential capacities diminished, regardless of whether this is perceived as negative.23 If correct, this suggests that Hart’s definition requires punishment to involve events that objectively set back interests, and that negative subjective experiences are merely one way to objectively set back interests.
Can AI have objective interests? Some philosophers argue that things like nutrition, reproduction, or physical damage are good or bad for biological entities like plants or animals.24 This is in virtue of something’s having identifiable functions that things can be good or bad for it. Most notably, Philippa Foot defends this sort of view (tracing it to Aristotle) when she argues that the members of a given species can be evaluated as excellent or defective by reference to the functions that are built into its characteristic form of life. If having interests in this broad function-based sense is all that is required for punishment to be sensible, then perhaps AI fits the bill. AI also has a range of functions –
      23
24
, in HARMING FUTURE PERSONS 139 (Melinda A.
and     eds., 2009).
FOOT,   26 (2001) (“features of plants and animals have what
Elizabeth
Harman,
Harming as Causing Harm
 Roberts
David T.
Wasserman
 PHILIPPA
NATURAL GOODNESS
one might call an ‘autonomous’, ‘intrinsic’, or as I shall say ‘natural’ goodness and defect that may have nothing to do with the needs or wants of the members of any other species of living thing”).

288
characteristic patterns of behavior needed to continue in good working order and to succeed at the tasks it characteristically undertakes. If living organisms can, in a thin sense, be said to have an interest in survival and reproduction, ultimately in virtue of their biological programming, then arguably an AI following digital programming could have interests in this thin sense as well.
Other philosophers reject this view and insist that only entities capable of having beliefs and desires, or at least phenomenal experiences such as of pleasure and pain, can truly be said to have interests that are normatively important. Legal philosopher Joel Feinberg takes the capacity for cognition as the touchstone full-blooded interests, that is as a precondition for having things really be good or bad for us. Although “Aristotle and Aquinas took trees [and plants] to have their own ‘natural ends,’” Feinberg denies plants “the status of beings with interests of their own” because “an interest, however the concept is finally to be analyzed, presupposes at least rudimentary cognitive equipment.”25 Interests, he thinks, “are compounded out of desires and aims, both of which presuppose something like belief, or cognitive awareness.” In this view, since AI is not capable of cognitive awareness, it cannot possess full-blooded interests of the kind Feinberg has in mind.
A stronger type of reply is to distinguish between conviction and punishment, where the latter covers the sentence to which the convicted party is subject. Even if no form of treatment can count as punishment unless the entity in question experiences it negatively, this is not a precondition for a conviction. Perhaps for it to be intelligible to
25 Joel   , , in Philosophy and 43,   (William T. Blackstone ed., 1974).
   Feinberg
The Rights of Animals and Unborn Generations
 Environmental Crisis
49–52

289
convict X of an offense, it is only required that X acted in ways that violated a prohibition, and this can be sensibly construed as culpable (a manifestation of insufficient regard). If so, then while punishing AI is not conceptually possible, applying criminal law to AI so that it can be convicted of offenses is. Thus, society may still benefit from AI convictions while not running afoul of the conceptual confusion in purporting to punish AI.
Convicting AI may require, or allow, subjecting other parties to punishment in place of the AI. Criminal law roundly rejects “vicarious punishment” when people are concerned – not least when it risks the injustice of strict criminal liability imposed on innocent actors. Corporate punishment may seem to involve vicarious punishment when officers or employees of the corporation are made to suffer due to the criminal fines imposed on the corporation. However, such cases are better understood not as vicarious liability, strictly speaking, but as convicting the corporation of an offense directly and then allowing the sentence to be distributed to the different individuals out of which the corporation is made up. In the case of an AI, it could be argued that if human owners or users accept responsibility for operating the AI safely, then where the AI is convicted of an offense in its own right, these responsible parties would be the appropriate persons to whom the sentence could be distributed by virtue of their voluntarily undertaking such responsibility.
A final type of reply, always available as a last resort, is that even if applying criminal law to AI is conceptually confused, it could still have good consequences to call it punishment when AI is convicted. This would not be to defend AI punishment from within existing criminal law principles but to suggest that there are consequentialist reasons to depart from them.

290
4 Feasible Alternatives
Punishing AI could have benefits, and it is not ruled out by the negative limitations and retributive preconditions of punishment. But this is not enough to conclude that the punishment of AI is justified. We are required to address the third main question in our theory of punishment: Would the benefits of punishing AI outweigh the costs, and would it be better than alternative solutions? These solutions may involve doing nothing, or relying on civil liability and regulatory responses, perhaps together with less radical or disruptive changes to criminal laws that target individuals.
Ideally a cost-benefit analysis would involve more than identifying various costs and benefits; it would include quantitative analysis. If only a single AI-generated crime is committed each decade, there would be far less need to address an AI criminal gap than if AI-generated crime were a daily occurrence. The absence of evidence suggesting that AI- generated crime is common counsels against taking potentially more costly actions now, but this balance may change as technological advances result in more AI activity.
First Alternative: The Status Quo
Begin by setting aside something of less concern: cases where responsibility for harmful AI conduct is fully reducible to the culpable conduct of individual human actors. A clear example would be where a hacker uses AI to steal funds from individual bank accounts. There is no need to punish AI in such cases, because existing criminal offenses, like fraud or computer crimes, are sufficient to respond to this type of behavior.

291
Even if additional computer-related offenses must be created to adequately deter novel crimes implemented with the use of AI, criminal law has further familiar tools at its disposal, involving individual-focused crimes, which provide other avenues of criminal liability when AI causes foreseeable harms. For example, as Gabriel Hallevy observes, cases of this sort could possibly be prosecuted under an innocent “agency model,” assuming AI can sensibly be treated as meeting the preconditions of an innocent agent, even if not of a fully criminally responsible agent in its own right. Under the innocent agency doctrine, criminal liability attaches to a person who acts through an agent who lacks capacity – such as a child. For instance, if an adult uses a five-year-old to deliver illegal drugs, the adult rather than the child would generally be criminally liable. This could be analogous to a person programming a sophisticated AI to break the law: The person has liability for intentionally causing the AI to bring about the external elements of the offense.
This doctrine requires intent – or at least the knowledge – that the innocent agent will cause the prohibited result in question. This means that in cases where someone does not intend or foresee that the AI system being used will cause harm, the innocent agency model does not provide a route to liability. In such cases, one could instead appeal to recklessness or negligence liability if AI creates a foreseeable risk of a prohibited harm. For example, if the developers or users of AI foresee a substantial and unjustified risk that an AI will cause the death of a person these human actors could be convicted of reckless homicide. If such a risk were merely reasonably foreseeable, but not foreseen, then lower forms of homicide liability would be available. Similar forms of recklessness or negligence liability could be adopted where the AI’s designers or users foresaw, or should have

292
foreseen, a substantial and unjustified risk of other kinds of harms as well – such as theft or property damage.
Hallevy also discusses this form of criminal liability for AI-generated harms, calling it the “natural and probable consequences model” of liability. This is an odd label, however, since the natural and probable consequences doctrine generally applies only when the defendant is already an accomplice to – i.e., the defendant intended to commit – the crime of another. More specifically, the natural and probable consequences rule provides that where A intentionally aids B’s underlying crime C1 (say theft), but then B also goes on to commit a different crime C2 (say murder), then A would be guilty of C2 as well, provided C2 is reasonably foreseeable.
Despite his choice of label, Hallevy seems alive to this complication and correctly observes that there are two ways in which negligence liability could apply to AI-generated harms that are reasonably foreseeable. He writes:
The natural-probable-consequence liability model [applied] to the programmer or user differ[s] in two different types of factual cases. The first type of case is when the programmers or users were negligent while programming or using the AI entity but had no criminal intent to commit any offense. The second type of case is when the programmers or users programmed or used the AI entity knowingly and willfully in order to commit one offense via the AI entity, but the AI entity deviated from the plan and committed some other offense, in addition to or instead of the planned offense.26
26 Hallevy, supra note 1 at 19.
 
293
In either sort of scenario, there would be a straightforward basis for applying existing criminal law doctrines to impose criminal liability on the programmers or users of an AI that causes reasonably foreseeable harms. Thus, no AI criminal gap exists here.
A slightly harder scenario involves reducible harms by AI that are not foreseeable, but this is still something criminal law has tools to deal with. Imagine hackers use an AI to drain a fund of currency, but this ends up unforeseeably shutting down an electrical grid that results in widespread harm. The hackers are already guilty of something – namely, the theft of currency (if they succeed) or the attempt to do so (if they failed). Therefore, our question here is whether the hackers can be convicted of any further crime by virtue of their causing harm through their AI unforeseeably taking down an electrical grid.
At first sight, it may seem that the hackers would be in the clear for the electrical grid. They could argue that they did not proximately cause those harms. Crimes like manslaughter or property damage carry a proximate cause requirement under which the prohibited harm must at least be a reasonably foreseeable type of consequence of the conduct that the actors intentionally carried out. But in this case, taking down the electrical grid and causing physical harm to human victims are assumed to be entirely unforeseeable, even to a reasonable actor in the defendant’s shoes.
Criminal law has tools to deal with this kind of scenario, too. This comes in the form of constructive liability crimes. These are crimes that consist of a base crime that requires mens rea, but where there then is a further result element as to which no mens rea is required. Felony murder is a classic example. Suppose one breaks into a home one believes to be empty in order to steal artwork. Thus, one commits the base crime of burglary. However, suppose further that the home turns out not to be empty, and the burglar startles

294
the homeowner who has a heart attack and dies. This could make the burglar guilty of felony murder. This is a constructive liability crime because the liability for murder is constructed out of the base offense (burglary) plus the death (even where this is unforeseeable). According to the leading theory of constructive liability crimes, they are normatively justifiable when the base crime in question (burglary) typically carries at least the risk of the same general type of harm as the constructive liability element at issue (death).
This tool, if extended to the AI case, provides a familiar way to hold the hackers criminally liable for unforeseeably taking down the electrical grid and causing physical harm to human victims. It may be beneficial to create a new constructive liability crime that takes a criminal act like the attempt to steal currency using AI as the base offense, and then takes the further harm to the electrical grid, or other property or physical harm, as the constructive liability element, which requires no mens rea (not even negligence) in order to be guilty of the more serious crime. This constructive liability offense, in a phrase, could be called “Causing Harm through Criminal Uses of AI.”
New crimes could be adopted to the extent there are not already crimes on the books that fit this mold. Indeed, in the present example, one may think there are already some available constructive liability crimes. Perhaps felony murder could be indicated insofar as attempting to steal currency may be a felony, and in cases where this conduct causes fatalities. However, this doctrine would be of no avail in respect to the property damage caused. This is the reason why a new crime like “Causing Harm through Criminal Uses of AI” would seem to be necessary. In any case, no AI criminal gap is present here

295
because criminal law has familiar tools available for dealing with unforeseeable harms of this kind.
Having set aside reducible harmful conduct by AI, consider a case of irreducible AI crime inspired by RDS. Suppose an AI is designed to purchase class materials for incoming Harvard students, but after being trained on data from online student discussions regarding engineering projects, the AI unforeseeably “learns” to purchase radioactive material on the dark web and has it shipped to student housing. Suppose the programmers of this “Harvard Automated Shopper” did nothing criminal in designing the system and had entirely lawful aims. Nonetheless, despite the reasonable care taken by the programmers – and subsequent purchasers and users of the AI (i.e., Harvard) – the AI caused student deaths.
In this hypothetical, there are no upstream actors who could be held criminally liable. Innocent agency is blocked as a mode of liability because the programmers, users, and developers of the AI do not have the intent or foresight that any prohibited or harmful results would ensue – as is required for innocent agency to be available. Moreover, if the risk of the AI purchasing the radioactive material is not reasonably foreseeable, then criminal negligence would also be blocked. Finally, constructive liability is not available in cases of this sort because there is no base crime – no underlying culpable conduct by the programmers and users of the AI – out of which their liability for the unforeseeable harms could be constructed.
One could imagine various attempts to extend existing criminal law tools to provide criminal liability for developers or users. Most obviously, new negligence crimes could be added for developers that make it a crime to develop systems that foreseeably could

296
produce a risk of any serious harm or unlawful consequence, even if a specific risk is unforeseeable. The problem is that this does not seem to amount to individually culpable conduct, as all activities and technologies involve some risk of some harm. The effect of this expansion of criminal law would be to stifle innovation and benefit commercial activities. If there were such a crime, all the early developers of the Internet would likely be guilty of it.
Second Alternative: The Costs of Punishing AI
Earlier discussions highlighted some of the potential costs of AI punishment, including conceptual confusion, expressive costs, and spillover. Even aside from these, punishment of AI would entail serious practical challenges as well as substantial changes to criminal law. Begin with a practical challenge: the mens rea analysis. For individuals, the mens rea analysis is generally how culpability is assessed. Causing a given harm with a higher mens rea like intent is usually seen as more culpable than causing the same harm with a lower mens rea like recklessness or negligence. But how do we make sense of the question of mens rea for AI?
Earlier, it was argued that for some AI, as for corporations, the mental state of an AI’s developer, owner, or user could be imputed under something like the respondeat superior doctrine. But for cases of AI-generated crime that are not straightforwardly reduced to human conduct – particularly where the harm is unforeseeable to designers and there is no upstream human conduct that is seriously unreasonable to be found – nothing like respondeat superior would be appropriate. Some other approach to AI mens rea would be required.

297
A regime of strict liability offenses could be defined for AI crimes. However, this would require a legislative work-around so that AI is deemed capable of satisfying the voluntary act requirement, applicable to all crimes. This would require major revisions to criminal law – it is far from an off-the-shelf solution. Alternately, a new legal fiction of AI mens rea, vaguely analogous to human mens rea, could be developed, but this too currently does not appear to be a workable solution. This approach could require expert testimony to enable courts to consider in detail how the relevant AI functions to assess whether it is able to consider legally relevant values and interests but did not weight them sufficiently, and whether the programming has the relevant behavioral dispositions associated with mens rea like intention or knowledge. Above, several arguments were presented that courts may use to find various mental states to be present in an AI. However, much more theoretical and technical work is required here, and this is not a first best option.
Mens rea and similar challenges related to the voluntary act requirement are only some of the practical problems to be resolved in order to make AI punishment workable. For instance, there may be enforcement problems with punishing distributed AI on a blockchain. Such AI may be particularly difficult to effectively combat or deactivate. Even assuming the practical issues are resolved, punishing AI would still require major changes to criminal law. Legal personality may be necessary to charge and convict an AI of a crime, and conferring legal personhood on AI would create a whole new mode of criminal liability, much the way that corporate criminal liability constitutes a new mode beyond individual criminal liability. There are problems with implementing such significant reform.
Over the years, there have been many proposals for extending some kind of legal personality to AI. Perhaps most famously, a 2017 report by the European Parliament called

298
on the European Commission to create a legislative instrument to deal with “civil liability caused by robots.”27 It further requested that the commission consider “a specific legal status for robots,” and “possibly applying electronic personality” as one solution to tort liability. Even in such a speculative and tentative form, this proposal proved highly controversial. More than 150 AI “experts” subsequently sent an open letter to the European Commission warning that “from an ethical and legal perspective, creating a legal personality for a robot is inappropriate whatever the legal status model.”28
Full-fledged legal personality for AI equivalent to that afforded to natural persons, with all the legal rights that natural persons enjoy, would clearly be inappropriate. To take a banal example, allowing AI to vote would undermine democracy, given the ease with which anyone looking to determine the outcome of an election could create AI systems to vote for a designated candidate. However, legal personality comes in many flavors, even for natural persons such as children who lack many adult rights and obligations. Crucially, no artificial person enjoys all the same rights and obligations as a natural person. The best- known class of artificial persons – companies – have long enjoyed only a limited set of rights and obligations that allows them to sue and be sued, enter contracts, incur debt, own property, and be convicted of crimes. However, they do not receive protection under constitutional provisions such as the Equal Protection Clause of the Fourteenth
27 EUR. PARL. DOC. (A8-0005/2017),
http://www.europarl.europa.eu/sides/getDoc.do?pubRef=-//EP//TEXT+REPORT+A8- 2017-0005+0+DOC+XML+V0//EN.
28 Open Letter to the European Commission Artificial Intelligence and Robotics (April 5, 2018), https://g8fip1kplyr33r3krz5b97d1-wpengine.netdna-ssl.com/wp- content/uploads/2018/04/RoboticsOpenLetter.pdf.
     
299
Amendment, nor can they bear arms, run for or hold public office, marry, or enjoy other fundamental rights that are enjoyed by natural persons. Thus, granting legal personality to AI to allow it to be punished would not require AI to receive the rights afforded to natural persons, or even those afforded to companies. AI legal personality could consist solely of obligations.
Even so, any sort of legal personhood for AI would be a dramatic legal change that could prove problematic. As discussed earlier, providing legal personality to AI could result in increased anthropomorphisms. People anthropomorphizing AI expect it to adhere to social norms and have higher expectations regarding AI capabilities. This is problematic where such expectations are inaccurate, and the AI is operating in a position of trust. Especially for vulnerable users, such anthropomorphisms could result in “cognitive and psychological damages to manipulability and reduced quality of life.”29 These outcomes may be more likely if AI were held accountable by the state in ways normally reserved only for human members of society. Strengthening questionable anthropomorphic tendencies regarding AI could also lead to more violent or destructive behavior directed at AI, such as vandalism or attacks. Further, punishing AI could also affect human well-being in less direct ways, such as by producing anxiety about one’s own status within society due to the perception that AI is given a legal status on par with human beings.
Finally, conferring legal personality on AI may lead to rights creep. Even if AI is given few or no rights initially when first granted legal personhood, AI may gradually acquire rights over time. Granting legal personhood to AI may become an important step
29 Luisa   & Paul Dumouchel, Anthropomorphism in Human–Robot Co-evolution, , Mar. 26, 2018, https://doi.org/10.3389/fpsyg.2018.00468 l.
   Damiano
 FRONT. PSYCHOL.
 
300
down a slippery slope. In a 1933 Supreme Court opinion, for instance, Justice Louis Brandeis warns about rights creep, and that granting companies an excess of rights could allow them to dominate the state. Eighty years after that decision, Justice Brandeis’ concerns were prescient in light of recent Supreme Court jurisprudence such as Citizen’s United and Hobby Lobby, which significantly expanded the rights extended to corporations.30 Such rights for corporations and AI can restrict valuable human activities and freedoms.
Third Alternative: Minimally Extending Criminal Law
There are alternatives to direct AI punishment other than doing nothing. The problem of AI-generated crime would more reasonably be addressed through minimal extensions of existing criminal law. The most obvious would be to define new crimes for individuals. Just as the Computer Fraud and Abuse Act criminalizes gaining unauthorized access or information using personal computers, an AI Abuse Act could criminalize malicious or reckless uses of AI. In addition, such an act could criminalize the failure to responsibly design, deploy, test, train, and monitor the AI one contributed to developing. These new crimes would target individual conduct that is culpable along familiar dimensions, so they may be of limited utility with regard to AI-generated crimes that do not reduce to culpable actors. Accordingly, a different way to expand criminal law seems needed to address AI- generated crime.
30 Citizens United v. Fed. Election Comm’n, 558 U.S. 310, 341 (2010) and Burwell v. Hobby Lobby Stores Inc., 573 U.S. 682 (2014).
 
301
In cases of AI-generated crime, a designated adjacent person could be punished who would not otherwise be directly criminally liable – a “responsible person.” This could involve new forms of criminal negligence for failing to discharge statutory duties, perhaps relying on strict criminal liability, in order to make a person liable in cases of AI-generated crime. It could be a requirement for anyone creating or using an AI to ex ante register a responsible person for the AI. It could be a crime to design or operate AI capable of causing harm without designating a responsible person. This would be akin to the offense of driving without a license. The registration system could be maintained by a federal agency. However, a registration scheme is problematic because it is difficult to distinguish between AI capable of criminal activity and AI not capable of criminal activity, especially when dealing with unforeseeable criminal activity. Even simple and innocuous-seeming AI could end up causing serious harm. Thus, it may be necessary to designate a responsible person for any AI. Registration may involve substantial administrative burden and, given the increasing prevalence of AI, the costs associated with mandatory registration may outweigh any benefits.
A default rule rather than a registration system may be preferable. The responsible person could be the AI’s manufacturer or supplier if it is a commercial product. If it is not a commercial product, the responsible person could be the AI’s owner, developer if no owner exists, or user if no developer can be identified. Even noncommercial AI is usually owned as property, although this may not always be the case, for instance, with some open-source software. Similarly, all AI has human developers, and in the event an AI ever autonomously creates another AI, responsibility for the criminal acts of an AI-created AI could reach back to the original AI’s owner. In the event an AI’s developer cannot be identified, or potentially

302
if there are a large number of developers, again in the case of some open-source software, responsibility could attach to an AI’s user. However, this would fail to catch the rare (perhaps only hypothetical) case of the noncommercial AI with no owner, no identifiable developer, and no user. To the extent that a noncommercial AI owner, developer, and user working together would prefer a different responsibility arrangement, they could be permitted to agree to a different ex ante selection of the responsible person. This may be more likely to occur with sophisticated parties where there is a greater risk of AI-generated crime. The responsible person could even be an artificial person such as a corporation.
It would be possible to impose criminal liability on the responsible persons directly in the event of AI-generated crime. For example, if new statutory duties of supervision and care are defined regarding the AI for which the responsible person is answerable, criminal negligence liability could be imposed on the responsible person should he or she unreasonably fail to discharge those duties. Granted, this would not be punishment for the harmful conduct of the AI itself. Rather, it would be a form of direct criminal liability imposed on the responsible person for his or her own conduct.
More boldly, if this does not go far enough to address AI-generated crime, criminal liability could also be imposed on the responsible person on a strict liability basis – particularly if the relevant punishments are only fines rather than incarceration. Generally strict liability crimes are restricted to minor infractions or regulatory offenses or “violations,” though some examples of more serious strict criminal liability can also be found, such as statutory rape in some jurisdictions. This could be defended by claiming that there is a special duty owed to society at large to provide special assurances that certain especially serious risks will be mitigated as much as possible. A responsible person

303
accepting strict criminal liability could serve this function. Especially in the case of AI where user trust is critical to realizing the benefits of AI, this approach could be warranted to combat the perception that unsafe AI is being employed. Accordingly, AI could become another context in which strict criminal liability on the responsible person is imposed.
Yet there are serious problems with strict liability crimes applied to persons. If justifiable at all, they can only be used as a last resort in exigent circumstances – as in cases of unusually dangerous activities. However, it is not obvious that the use of AI qualifies as unusually dangerous. To the contrary, in many areas of activity it would be unreasonable not to use AI, as when safety can be improved over human actors such as will likely be the case with self-driving cars. Most bad human actors using AI systems to commit crimes will still be caught under existing criminal laws, and so far there have not been high-profile cases of AI-generated crime. As a result, AI-generated crime is likely not yet a significant enough social problem to merit the use of strict criminal liability.
At the end of the day, a responsible person regime accompanied by new statutory duties, which carry criminal penalties if these duties are negligently or recklessly breached, provides an attractive approach to dealing with AI-generated crime. While it is only a minimal expansion of criminal law, by expressing condemnation through a criminal conviction of the responsible person, much of the expressive benefit from a direct conviction of AI can be achieved – but without as serious a loss of public trust as the legal fictions needed to punish AI directly would create.

304
Fourth Alternative: Moderate Changes to Civil Liability
A further alternative to dealing with AI-generated crime is to look to civil law, primarily tort law, as a method of both imposing legal accountability and deterring harmful AI. Some AI crime will no doubt already result in civil liability; however, if existing civil liability falls short, new liability rules could be introduced. A civil liability approach could even be used in conjunction with expansions to criminal liability.
While it is beyond the scope of this chapter to canvas gaps in civil liability for AI crime, it is worth noting that existing civil liability comes with built-in limitations. Very few laws specifically address AI-generated harms, which means civil liability must usually be established under a traditional negligence or product liability framework or under contractual liability. Negligence generally requires a person to act carelessly, so where this cannot be established there may be no recovery. Product liability may require both that an AI be a commercial product (e.g., this may not apply where AI is used as a “service”) and that there be a defect in the product (or that its properties be falsely represented). In the case of complex AI, it may be difficult to prove a defect, and AI may cause harm without a “defect” in the product liability sense. Civil liability may also derive from contractual relationships, but this usually only applies where there is a contract between parties. This, too, may also have significant limitations.
To the extent there is inadequate civil liability for AI-generated crime, the responsible person proposal sketched previously could be repurposed so that the responsible person may only be civilly liable. The case against a responsible person could be akin to a tort action if brought by an individual or a class of plaintiffs, or a civil

305
enforcement action if brought by a government agency tasked with regulating AI. At trial, an AI would not be treated like a corporation, where the corporation itself is held to have done the harmful act and the law treats the company as a singular acting and “thinking” entity. Rather, the question for adjudication would be whether the responsible person discharged his or her duties of care in respect of the AI in a reasonable way – or else civil liability could also be imposed on a strict liability basis.
A responsible person scheme is not the only solution to inadequate civil liability for AI-generated crime. An insurance scheme is another approach. Owners, developers, or users of AI, or just certain types of AI, could pay a tax into a fund to ensure adequate compensation for victims of AI-generated crime. The cost of this tax would be relatively minor compared to the financial benefits of AI. This could either replace the responsible person solution or apply to cases where no appropriate responsible person exists. An AI compensation fund could operate like the National Vaccine Injury Compensation Program (VICP). Vaccines result in widespread social benefit but are known in rare cases to cause serious problems. VICP is a no-fault alternative to traditional tort liability that compensates individuals injured by a VICP-covered vaccine. It is funded by a tax on vaccines that users pay. Other models for an insurance scheme exist, such as the Price Anderson Act for nuclear power.
Further Thoughts
AI legal neutrality suggests that as AI becomes increasingly autonomous and acts in ways that cannot be traced to individuals, there may be benefits to directly criminally punishing

306
AI. However, at least for now, punishing AI would be an overreaction to all but largely hypothetical concerns. While AI is already presenting new challenges to criminal law, alternative approaches could provide substantially similar benefits and would avoid many of the problems with AI punishment. A simpler solution involves modest expansions to criminal law, including new negligence crimes centered around the improper design, operation, and testing of AI applications, as well as possible criminal penalties for designated parties who fail to discharge statutory duties. This could be supplemented by expanded civil liability.
Yet, as with the idea of AI paying taxes, criminally punishing AI is not as absurd an idea as it may initially seem. We already criminally punish artificial persons in the form of corporations, and punishing AI is not a radical departure from that and other precedents. Criminal law can – and, where corporations are involved, already does – appeal to elaborate legal fictions to provide a pragmatic tool for solving social problems. Nonetheless, legal fictions must be used with caution, as their overuse risks eroding public trust and weakening the rule of law. Moreover, allowing legal fictions to proliferate unchecked can lead to widespread injustice either through punishing the innocent or by punishing more harshly than one’s culpability calls for. For this reason, there is and should be an onerous burden to meet before we can be confident that a particular legal fiction – such as legal personality for AI or the invention of culpable mental states for AI – is adopted.

307
7
Alternative Perspectives on AI Legal Neutrality
We can only see a short distance ahead, but we can see plenty there that needs to be done.
- Alan Turing
1 What If There Is Never a Singularity?
Some of the arguments in this book give credibility to the predictions of technologists who believe the singularity will occur in our lifetimes. However, there are other prognosticators who think the potential of AI has been vastly overstated, and that the current round of predictions is mainly hype. What if automation does not increase higher unemployment rates, if self-driving cars never significantly exceed the performance of human drivers, and if AI inventors remain a permanent novelty? There may never be a singularity, or if there is, it may not come this century. If that is the case, is there value in a principle of AI legal neutrality?
Paradoxically, AI legal neutrality is more important in a world where AI does not dramatically outperform people. This is because laws that result in unequal treatment of AI will be most disruptive when people and AI are in relatively close competition. Ultimately, if AI is going to become not just more efficient than people but substantially more efficient, then differential legal treatment should only delay the inevitable. Today, the decision by McDonald’s to replace a person with a $35,000 robotic arm that does a mediocre job of bagging fries is influenced by tax policy. If ever the robot costs $3,500 and becomes faster

308
and more accurate than a person, McDonald’s will automate regardless of taxes levied on AI. As with the inevitability of invention, AI legal neutrality may accelerate rather than increase automation.
2 Should the Law Discourage the Singularity?
What if the technological optimists are correct? If a principle of AI legal neutrality encourages automation when it is more efficient, and AI will eventually be more efficient at essentially everything, does that mean we will eventually be living in a world where AI does everything and people have nothing to do? Would this be a dystopian nightmare in which people have lost the opportunity for work that gives them a sense of meaning and purpose? Should we deliberately legally disadvantage AI to prevent this outcome?
Discriminatory laws will not stop automation unless we adopt absolute prohibitions on using AI for certain activities, such as banning the use of AI in any commercial diagnosis or treatment of disease. Such a rule could ensure the supremacy of human decision-making in medicine and prevent the displacement of doctors, although it might be difficult to enforce if other countries fail to adopt similar prohibitions.
Prohibiting AI activity broadly would be undesirable. In health care, the result for patients would be inferior medical care at higher prices. It would also be undesirable for doctors, since even if they were inclined to find meaning through productivity, this would be an entirely pointless productivity. They would not really be helping others – a doctor would only be needed as a result of preventing a vastly superior AI from doing a better job. This would reenact, throughout the medical profession, the fable of Sisyphus, the cruel

309
Greek king punished by the gods in the afterlife and condemned to push a large boulder to the top of a steep hill for all eternity. Upon nearing the hill’s peak, the boulder would roll back resulting in Sisyphus’ having to begin pushing the rock up the hill again. The solution to managing the challenge of automation should not be a curse of endless, pointless (if not harmful) labor.
There may be limited circumstances, say decisions to use deadly force or to criminally punish a defendant, in which automation would be inappropriate. But even if we choose to reserve certain fields of activity for people, the world can only accommodate so many judges and police officers. This means – at some point – the majority of people may need to find a way to occupy their time with an activity other than work. In 1516, Thomas More writes in Utopia about an idealized communal society where the people would rise early, work six hours a day, and spend the rest of the day on their own but with a caveat: They would be expected to spend it wisely. In an AI utopia, there may be no need for people to work six hours a day – or at all. In whatever way people find meaning, in the future it may not be through simple productivity.
Plato believed that meaning comes from attaining knowledge. Perhaps, with a virtually unlimited amount of leisure time and resources, most of the populace would choose to engage in continuous self-improvement. Confucianists see meaning in human relationships, as Tu Weiming writes, “[W]e can realize the ultimate meaning of life in ordinary human existence.”1 Perhaps people would spend their time cultivating personal relationships. Jeremy Bentham thought the meaning of life is the greatest happiness
1 TU   ,   (Albany: State , 1985).
  WEIMING
CONFUCIAN THOUGHT: SELFHOOD AS CREATIVE TRANSFORMATION
 University of New York Press

310
principle in which the greatest happiness is brought to the greatest number of people. Perhaps, most people would spend their days enjoying virtual reality. It is beyond the scope of this book to establish the meaning of life, but it is sufficient to note that there are paths to meaning other than through work. Also, it may be paternalistic to dictate to others what it means to spend one’s time wisely, and certainly it is to force them to labor for their “own” benefit. This is not to say that the singularity is guaranteed to improve social welfare. But a realistic dystopian AI future is unlikely to be killer robots run amok or a life without meaning. We should be more concerned about a future in which AI only benefits a small number of individuals, particularly if this occurs at the expense of the less fortunate. The way to prevent that future is not by prohibiting automation but by having appropriate legal frameworks.
3 Is AI Legal Neutrality a Coherent Principle?
This book has argued that legal regimes tend to promote efficiency when they do not discriminate between people and AI, but that legally identical treatment would be undesirable. For instance, AI legal neutrality does not require direct criminal punishment of AI, direct AI liability for accidents, or AI ownership of intellectual property. One may therefore object that acceptance of the concept of AI legal neutrality would be impractical and even unworkable for policymakers who favor consistency in application.
These concerns apply to other AI principles. In 2019, the European Commission’s High-Level Expert Group on AI put forward guidelines of seven key requirements that AI systems should meet, including transparency, privacy, human agency, and

311
nondiscrimination.2 It is hard to object to any of these in the abstract. In practice, however, these requirements may conflict with other values. For instance, transparency may come at the expense of privacy. To understand how a connectionist AI functions may require access to its training inputs. However, AI training inputs often consist of personal data so making training data available for inspection may violate a right to privacy. That violation may be particularly acute with an AI’s being applied in, for example, health care. Even where a user’s information has been “deidentified,” it may sometimes be possible to reidentify a user. In another concern, transparency may require the disclosure of commercially valuable confidential information. Access to an AI’s code may be necessary to understand its function, but if competitors can access this code, they may be able to create similar products without violating intellectual property protections.
Similarly, human agency may conflict with nondiscrimination. Discrimination by algorithms, or algorithmic bias, is a long-standing concern. It is also one that more prominently entered the public consciousness as a result of reporting in 2016 by ProPublica about a proprietary sentencing algorithm, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), that ProPublica claimed systematically discriminated against black defendants. COMPAS is now helping judges make bail determinations (whether to release or secure someone in advance of their trial) by providing risk assessments of defendants. The algorithm more often labeled black defendants who subsequently did not reoffend as high risk, and white defendants who did
   2
, ETHICS GUIDELINES FOR
(Apr. 8, 2019), https://ec.europa.eu/digital-single-
AI HIGH LEVEL EXPERT GROUP (HLEG), EUROPEAN COMMISSION
 TRUSTWORTHY ARTIFICIAL INTELLIGENCE
 market/en/news/ethics-guidelines-trustworthy-ai.
 
312
reoffend as low risk.3 The company argued in response that at any specific risk level, there was a roughly equal proportion of white and black defendants.
Regardless of whether one specific AI made racially biased determinations, the broader concern of biased algorithms remains. Biases are an inevitable part of both AI and human decision-making, but some biases are both morally and legally unacceptable. In the case of racial bias, biased algorithms are not the result of a person deliberately engineering AI to be racist but may arise if, for example, connectionist AI learns based on biased training data. If human judges have historically sentenced defendants in a discriminatory fashion, AI may do so in the future.
While incorporating historical biases is a legitimate concern about AI, it ignores another problematic implication of algorithmic bias, which is that human judges may have been discriminating against minority defendants. If this is so, despite such discrimination being legally prohibited, then it is likely since judges have conscious or unconscious biases against certain groups. It can be very difficult to detect, much less alter, human biases. In 2019, France made publishing data analysis related to judicial decisions a crime punishable by five years’ imprisonment.4 The French government makes judicial decisions together with the names of judges publicly available, but it is illegal to aggregate data and create a record of a specific judge. A critic of this law would assume it is an attempt to prevent
3 See Julia Angwin, Jeff Larson, Surya Mattu, &     , Machine Bias, PROPUBLICA (May 23, 2016), www.propublica.org/article/machine-bias-riskassessments-in-criminal- sentencing.
4 France Bans Judge Analytics, 5 Years In Prison for Rule Breakers, ARTIFICIAL LAWYER (June 4, 2019), www.artificiallawyer.com/2019/06/04/france-bans-judge-analytics-5-years-in- prison-for-rule-breakers/.
   Lauren
Kirchner
      
313
demonstration of inconsistency, and potentially bias, in judgments. Existing studies have discovered “amazing disparities” in the outcomes of cases based on how a judge decides a case.5 Not only may human decision-making lack transparency, but it may have poorer explainability than that of AI. True, a judge, unlike some AI, can always be relied upon to explain a decision. However, that explanation is not guaranteed to be accurate. A judge who is consciously biased against a protected group is exceptionally unlikely to admit to such a bias. Instead, he or she will have to come up with a rationalized explanation for a sentence in a particular case, relying on the facts of the offense, a defendant’s personal circumstances, etc. And if a judge has an unconscious bias, then he or she is not even aware of this distortion in perspective.
The human mind may be more of an algorithmic black box than AI, which when properly queried is far more transparent about its internal rules. These rules can also be explicitly overwritten. Whereas a human judge can be told it is illegal to discriminate on the basis of race, and will no doubt agree to abide by such a restriction, no amount of training is guaranteed to internalize such a rule. Ceding some agency to an AI, which can be explicitly programmed to never consider race or even proxy variables, may be the best chance to avoid discrimination in a racially stratified world.
Transparency, privacy, human agency, and nondiscrimination are important principles, but they are conceptual guideposts rather than absolute prescriptions. Intelligent policymaking requires a decision maker to consider how to balance these concepts flexibly on a case-by-case basis. Different jurisdictions also have distinct cultural
5 See, e.g., Jaya   ,     , &     , Refugee Roulette: , 60   295 (2008).
 Philip G.
Schrag
 Ramji-Nogales
Andrew
Schoenholtz
 Disparities in Asylum Adjudication
STAN. L. REV.

314
and value preferences. For instance, some countries are more concerned with user privacy than others, and some are more concerned with encouraging entrepreneurship by giving greater rights in user data to private companies.
4 Protecting Spheres of Human Agency
A frequently invoked principle of AI regulation is maintaining the supremacy of human agency that may sometimes be at odds with the principle of AI legal neutrality. Should automation be excluded from some areas of human activity, such as creative work, even when it is more efficient? Harlan Howard famously described country music as “three chords and the truth.”6 That may not be entirely correct from a technical standpoint but it is certainly true that there are a finite number of tones that human ears can distinguish and a finite number of ways to combine them. Already, there are more human-composed songs than a person could listen to even if the songs were played one after the other, every day, for the entirety of that person’s life. When you calculate every possible five-minute basic audio recording, which would not only include what we think of as music, you get an incomprehensibly large number: something like 2^211,000,000.7 The number is significantly smaller when you limit the recording to tones people can distinguish, such as typical melodies. It becomes smaller still if an AI is only concerned with creating music
, Country Scribe Harlan Howard Dies, ROLLING STONE (Mar. 5, 2002), www.rollingstone.com/music/music-news/country-scribe-harlan-howard-dies-197596/. 7 Casey Chan,   ,   (Nov. 20, 2012), https://gizmodo.com/is-it-mathematically-possible-to-run-out-of-new-music- 5962375.
  Andrew
 Dansby
  6
 Is It Mathematically Possible to Run Out of New Music
GIZMODO
  
315
people are likely to enjoy. Still, by any measure, it remains a mind-bogglingly large number that only a futuristic AI with vastly expanded capabilities could possibly generate.
There may be limited value in generating every conceivable five-minute recording, given the practical impossibility of a person listening to all of that content, unless an AI could also evaluate its output for usefulness. But an AI could hypothetically determine which recordings are most likely to be enjoyable to people, or perhaps even to specific people.8 An AI could theoretically provide each person with their optimal musical playlist, rank ordered, with the songs they are most likely to enjoy every day for the rest of their lives. If such a system could be created, should it? There is clearly a benefit to creating new music that people will enjoy. But will this have a chilling effect on human creativity, or on the lives of musicians? The same questions apply to other creative areas. For instance, the human eye can only distinguish pictures to a certain resolution – a resolution already being exceeded by some cutting-edge monitors. With a finite number of pixels in a monitor, an AI should hypothetically be able to create every possible image. Will this disincentivize human photographers?
Of course, people will still create. People have not stopped playing chess, even though AI’s chess supremacy is now unquestioned. Self-improvement and competition against other people are still worthwhile endeavors. Likewise, long-distance running used to have serious practical importance when it was the quickest means of transmitting a message. In fact, the marathon race was inspired by the story of Pheidippides, who died running from Marathon to Athens to announce a military victory. Running long distances
  8 See, e.g.,     ,
Nicholas J
Hudson
Musical Beauty and Information Compression: Complex to the
 Ear but Simple to the Mind?,
4   9 (2011).
BMC RESEARCH NOTES

316
lost much of its practical importance with the introduction of faster means of transportation, yet people still run marathons. We value personal improvement, social aspects of activities, and competition against other people rather than machines. In fifty years, someone may make music because they enjoy the act of making music, or because there is a market specifically for music made by people. In an AI utopia, people may continue to be creative, but simply because they enjoy doing so.
What about spheres of activity where we are less concerned about protectionism and more concerned about preventing bad outcomes? AI lacks both common sense and an internal moral compass (although the same thing may be said about some people). Automation seems to give rise to special concern with respect to automating certain military or judicial activities. For instance, some of the most strenuous objections to automation revolve around “killer robots” or the concept of a lethal autonomous weapons system (LAWS), which can fully automate life and death decisions. It may be that there are some activities we never want to automate, regardless of efficiency. On the other hand, some of the arguments proposed against automation may not always hold up. Kenneth Anderson and Matthew C. Waxman have argued in favor of LAWS.9 They contend that LAWS will protect one’s own personnel as well as civilians. AI can respond more quickly than human soldiers, it does not act out of emotion (which can both restrain or unleash base instincts) or personal self-interest, and some human soldiers have historically failed
 9
&,
     Kenneth
Anderson
Matthew C.
Waxman
Debating Autonomous Weapon Systems, Their
   Ethics, and Their Regulation Under International Law
, in Roger
Brownsword
,
Eloise
    Scotford,
&
Karen
Yeung
, eds.,
THE OXFORD HANDBOOK OF LAW, REGULATION, AND TECHNOLOGY
Oxford:
Oxford University Press
(     , 2017). Available at SSRN: https://ssrn.com/abstract=2978359.
,
 
317
to respect the law of armed conflict. It is possible that for some military tasks, AI may be more humane than people. It is also notable that there are already some highly automated weapon systems, such as Israel’s Iron Dome antimissile system. That system could not effectively function without automation, as it detects incoming missiles and deploys countermeasures faster than a human soldier possibly could. Of course, the Iron Dome system is strictly defensive.
Even judging has not proven immune to automation. In late 2019, the government of Estonia initiated its program to have a “robot judge” adjudicate small claims cases.10 This may be the first instance of an AI outright taking the place of a judge, though it is too early to evaluate the success of the program and parties are able to appeal any decision to a person. To date, AI has done much more augmentation than automation of judging, and it has been more employed with respect to mediation, a process by which a neutral third party helps parties voluntarily settle a dispute, than to adjudication, a process by which a neutral third party (usually a judge or arbitrator) provides a binding resolution. The earliest AI to augment third parties was, descriptively enough, called Legal Decisionmaking System at the RAND Corporation in 1980. It helped settle product liability cases by determining liability, case value, and fair settlement value. Modern AI being operated this way includes a system called BOS, which the Prosecuting Authority in the Netherlands is using to determine punishment severity – much the way that COMPAS is being used in the United States.
10 Eric Niiler, Can AI Be a Fair Judge in Court? Estonia Thinks So, WIRED (Mar. 25, 2019), www.wired.com/story/can-ai-be-fair-judge-court-estonia-thinks-so/.
    
318
It makes sense to automate rather than augment when you have a conflict that may not justify a traditional third party’s involvement – perhaps for low-value, high-volume disputes, where the costs of litigation outweigh the amount in dispute. That was the case with eBay’s dispute resolution system, which was the first major online dispute resolution success. eBay uses a questionnaire-based AI expert system that performs the role of mediator, collecting information, identifying preferences, and suggesting resolution options. eBay reports its system manages sixty million disputes a year, 90 percent of which are resolved without human intervention. An eBay human third party gets involved if the AI cannot resolve matters. eBay also reports an 80 percent satisfaction rate. However, it should be noted that this system is not without its critics.
Automating eBay’s dispute system was an obvious business choice, because managing that volume of disputes with staff was not going to work for eBay, particularly given the low value of most disputes. It is hard to image courts absorbing all those cases, given that many courts already have substantial backlogs. Automation was easier, because of eBay’s detailed knowledge of the transactions in question and the narrow spectrum of disputes: Customers did not receive their items, the item was not as described, and so forth. In an early version of the system, a human mediator resolved disputes just using email. Automating that process increased both settlement rates and participants’ reported satisfaction. This was attributed to requiring participants to restrict their communications to preset options and limited numbers of characters, and to using AI to manage the flow of information. To paraphrase Frank Sander, AI can be a neutral to fit the fuss.11
11 F.E.A.   & S.B.   ,
, 10     (1994).
  Sander
Goldberg
Fitting the Forum to the Fuss: A User-Friendly Guide to
 Selecting an ADR Procedure
NEGOTIATION JOURNAL
49-67

319
Other AI systems similarly replace the third party, such as CyberSettle and SmartSettle, which use double-blind bidding. The essentials of the such systems and how they are used are basically the same: Parties provide an AI with their offers and bottom lines, and if these offers are within range of one another, the AI can split the difference or offer proposals that bridge a gap. This can change the dynamics of a human-led mediation, which often involve extensive advocacy and inquiry, to focus on the exchange of proposals. Some complex models exist, but at their base these systems are for people arguing over a dollar figure. CyberSettle claims to have handled more than 200,000 claims with a combined value of more than $1.6 billion, with an average reduction of settlement time of eighty-five percent.
AI can thus provide a convenient, fast, and inexpensive solution to many disputes. But when disputes become more complicated than standard eBay fare, things get a bit harder. Mediation can involve ambiguity, social and emotional issues, and interpersonal and cultural differences – all of which may be challenging for machines to resolve by following simple rules. AI also has a difficult time operating with norms and standards. Still, there’s enough potential benefit that AI use is expanding, and it is being applied to increasingly diverse sorts of disputes – even divorce cases, which get both complicated and emotional. Governments are also increasingly using AI for adjudication. For example, US states are using the system Matterhorn to manage outstanding warrants and traffic violations, small claims, and family disputes.
To successfully automate a wider range of disputes, AI needs to be more than efficient. It needs to be fair – or at least to behave fairly. Some critics think that will never happen, and that “machine-made justice” is no substitute for existing processes. In this

320
view, for anything outside of narrow domains or anything moderately complex, there is no substitute for a person’s reasoning and decision-making capabilities, not to mention common sense. Put another way, critics maintain that fairness and justice are exclusively human traits that AI cannot replicate.
Yet people long believed the same of creativity, and as we have already seen, AI can be creative if it is programmed to be creative. Likewise, AI can behave fairly if it is programmed to behave fairly. John McCarthy, who coined the term AI, maintains there is nothing a judge knows that we could not tell a computer.
5 The Risks and Unique Dangers of AI
The AI does not hate you, nor does it love you, but you are made
out of atoms which it can use for something else.
- Eliezer Yudkowsky
People and AI sometimes act in ways that are functionally interchangeable, but there are a substantial number of respects in which they differ. These differences have been highlighted in a body of literature on AI’s risks and negative effects. Scholarship has focused on AI’s opacity and lack of explainability, design choices that result in bias, negative impacts on personal well-being and social interactions, and on how AI has changed power dynamics in concerning ways between users and private companies (e.g., Facebook’s knowledge about its users) and between citizens and the state (e.g., profiling and surveillance). Merely having the potential to be neutral and “transcend politics” is no guarantee that AI will not be applied objectionably or co-opted by bad actors. Also, just as

321
AI may identify and solve problems that we are not aware of, AI may cause harm in unpredictable ways. Consider Nick Bostrom’s Paperclip Maximiser thought experiment, a sort of modern-day version of the Sorcerer’s Apprentice.12 If a sufficiently unconstrained and powerful AI is given the goal of maximizing paperclip production, it may determine the best means of achieving this goal is to destroy competing sources of manufacturing, obtain resources through war, or even to eliminate people it decides are likely to interfere with paperclip production. AI may not have humanlike motives, and so may act in ways we find arbitrary or unexpected. All these risks may seem to counsel against using AI, or at least proceeding cautiously.
These are legitimate concerns that should not be minimized. Indeed, managing these risks should be one of the primary goals of a legal regime responsive to AI. That AI and people can act in different ways and generate different sorts of harms is all the more reason to have laws that have been explicitly designed with AI in mind. AI legal neutrality should not be equated with a lack of regulation, and not all AI challenges will be solved by improving efficiency; other principles like accountability, beneficence, responsibility, nonmaleficence, and privacy may be more important in some cases. Also, while there are benefits to nonbinding, voluntary ethical codes to guide conduct, these same principles should also be the foundation of compulsory regulatory frameworks where the risks are sufficiently great. Managing risk is not a novel problem for the law, and many of the underlying concerns with AI can be dealt with under existing schemes or with modest
  12 Nick   ,   , in 2 Smit et al. eds., 2003).
(I.
Bostrom
Ethical Issues in Advanced Artificial Intelligence
COGNITIVE, EMOTIVE
  AND ETHICAL ASPECTS OF DECISION MAKING IN HUMANS AND IN ARTIFICIAL INTELLIGENCE
12–17

322
changes. We already have legal frameworks that provide a balance between state authority and individual privacy rights. Those frameworks may need to be adjusted, say, once AI- based facial recognition is ubiquitous, perhaps with new laws specific to certain applications of technology.
The prospect exists of more disruptive risks, such as a malevolent or indifferent AI, but these are not unmanageable. Even leaving our current political environment aside, there has been no historical shortage of malevolent or indifferent human rulers. We may be exceptionally unlikely to have a person destroy the world for the sake of paperclips, but people may end up destroying the world as a result of nuclear war or climate change. Part of the purpose of the law and our systems of checks and balances is to constrain bad behavior and manage risk, whether from an insane policy maker or a James Bond style techno-villain. With AI we may be exchanging one set of risks for another, but we are not necessarily accepting a worse set of risks or risks less susceptible to management. The mere possibility of harm does not mean we should not embrace activities, or new technologies, where the benefits are likely to outweigh the costs. And, while there are plenty of legitimate risks with AI, there is no reason to think that an AI able to act intelligently will have self-awareness and be driven by a desire for self-preservation.
6 Concluding Thoughts
Artificial intelligence may be the most disruptive technology ever created, but it is not guaranteed to improve our lives. The way to ensure it does is through enacting appropriate laws and policies. In deciding how to treat AI, policymakers should be concerned with the

323
functionality of machines and consequentialist benefits. What legal rules will result in the greatest social benefit from these technologies? At the end of the day, if we see a car coming at us, we do not care whether it is a self-driving Tesla with an unpredictable neural network, a self-driving Uber using “good old-fashioned AI,” or a human driver – we just do not want to be run over.
We examined a few areas of the law where AI and people are treated differently and saw the resulting unintended consequences. Of the many principles relevant for AI regulation, it is important to include the principle of AI legal neutrality to ensure that unnecessary barriers are not erected that prevent us from realizing the benefits of AI. Artificial intelligence is not part of our moral community, but it needs to be part of our legal community to promote human welfare. In time, as AI increasingly outperforms people at exercising judgment and eventually becomes the accepted way in which we solve problems, AI should replace us in our legal standards. The ancient Greek philosopher Protagoras claims that “man is the measure of all things.” In our not-too-distant future, AI may be the measure of all things. The challenge then becomes less a question of how do we regulate AI and more of how we should regulate ourselves.

324
I THINK, THEREFORE I INVENT: CREATIVE COMPUTERS AND THE FUTURE OF PATENT LAW
RYAN ABBOTT*
Abstract: Artificial intelligence has been generating inventive output for dec- ades, and now the continued and exponential growth in computing power is poised to take creative machines from novelties to major drivers of economic growth. In some cases, a computer’s output constitutes patentable subject matter, and the computer rather than a person meets the requirements for inventorship. Despite this, and despite the fact that the Patent Office has already granted pa- tents for inventions by computers, the issue of computer inventorship has never been explicitly considered by the courts, Congress, or the Patent Office. Drawing on dynamic principles of statutory interpretation and taking analogies from the copyright context, this Article argues that creative computers should be consid- ered inventors under the Patent and Copyright Clause of the Constitution. Treat- ing nonhumans as inventors would incentivize the creation of intellectual proper- ty by encouraging the development of creative computers. This Article also ad- dresses a host of challenges that would result from computer inventorship, in- cluding the ownership of computer-based inventions, the displacement of human inventors, and the need for consumer protection policies. This analysis applies broadly to nonhuman creators of intellectual property, and explains why the Copyright Office came to the wrong conclusion with its Human Authorship Re- quirement. Finally, this Article addresses how computer inventorship provides in- sight into other areas of patent law. For instance, computers could replace the hy- pothetical skilled person that courts use to judge inventiveness. Creative comput- ers may require a rethinking of the baseline standard for inventiveness, and po- tentially of the entire patent system.
INTRODUCTION
An innovation revolution is on the horizon.1 Artificial intelligence (“AI”) has been generating inventive output for decades, and now the contin-
© 2016, Ryan Abbott. All rights reserved.
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assis- tant Professor, David Geffen School of Medicine at University of California, Los Angeles. Thanks to Ian Ayres, Martin Keane, John Koza, Lisa Larrimore-Ouellette, Mark Lemley, and Steven Thaler for their insightful comments.
1 See, e.g., JAMES MANYIKA ET AL., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY 40 (2013) (predicting that the automation
1079
 
325
1080 Boston College Law Review [Vol. 57:1079
ued and exponential growth in computing power is poised to take creative ma- chines from novelties to major drivers of economic growth.2 A creative singu- larity in which computers overtake human inventors as the primary source of new discoveries is foreseeable.
This phenomenon poses new challenges to the traditional paradigm of patentability. Computers already are generating patentable subject matter under circumstances in which the computer, rather than a human inventor, meets the requirements to qualify as an inventor (a phenomenon that this Article refers to as “computational invention”).3 Yet, it is not clear that a computer could be an inventor or even that a computer’s invention could be patentable.4 There is no statute addressing computational invention, no case law directly on the subject, and no pertinent Patent Office policy.5
These are important issues to resolve. Inventors have ownership rights in their patents, and failure to list an inventor can result in a patent being held invalid or unenforceable. Moreover, government policies encouraging or inhib- iting the development of creative machines will play a critical role in the evo- lution of computer science and the structure of the research and development (“R&D”) enterprise.6 Soon computers will be routinely inventing, and it may only be a matter of time until computers are responsible for most innovation.
of knowledge work “could have as much as $5.2 trillion to $6.7 trillion in economic impact annually by 2025”).
2 See infra notes 275–278 and accompanying text.
3 See infra notes 28–99 and accompanying text; see also Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH (Cassidy R. Sugimoto, Hamid R. Ekbia & Michael Mattioli eds., forthcoming Oct. 2016) (discussing computational invention in an essay originally posted online on February 19, 2015).
4 See, e.g., Ralph D. Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up?, 71 TUL. L. REV. 1675, 1681, 1702–03 (1997) (arguing the output of creative computers cannot and should not be protected by federal intellectual property laws and that such results enter the public domain); see also Pamela Samuelson, Allocating Ownership Rights in Computer-Generated Works, 47 U. PITT. L. REV. 1185, 1199–1200 (1986) (arguing that computers cannot be authors because they do not need incentives to generate output). Pamela Samuel- son, arguing against considering computers to be authors, states that, “[o]nly those stuck in the doctri- nal mud could even think that computers could be ‘authors.’” Id. at 1200.
5 See Ben Hattenbach & Joshua Glucoft, Patents in an Era of Infinite Monkeys and Artificial Intelligence, 19 STAN. TECH. L. REV. 32, 44 & n.70 (2015) (noting no pertinent results from “a search for patent cases discussing genetic programming or computer-aided drug discovery (perhaps the two most common means of computerized inventive activity)” and that “[o]f a sampling of issued patents that were conceived wholly or in part by computers, none have ever been subject to litigation.”); see also ROBERT PLOTKIN, THE GENIE IN THE MACHINE 60 (2009). “Patent Office” refers to the U.S. Patent and Trademark Office (“USPTO”), the federal agency responsible for granting patents and registering trademarks. See About Us, USPTO, http://www.uspto.gov/about-us [https://perma.cc/ 6HZY-V9NU] (last visited Jan. 27, 2016).
6 See generally Michael Kremer & Heidi Williams, Incentivizing Innovation: Adding to the Tool Kit, 10 INNOVATION POL’Y & ECON. 1 (2010) (discussing the importance of intellectual property rights for promoting innovation).
 
326
2016] Patent Generating Artificial Intelligence 1081
This Article addresses whether a computer could and should be an inven- tor for the purposes of patent law as well as whether computational inventions could and should be patentable.7 It argues that computers can be inventors be- cause although AI would not be motivated to invent by the prospect of a pa- tent, computer inventorship would incentivize the development of creative ma- chines.8 In turn, this would lead to new scientific advances.
Beyond inventorship concerns, such machines present fascinating ques- tions: Are computers thinking entities? Who should own the rights to a com- puter’s invention? How do animal artists differ from artificial intelligence? What would be the societal implications of a world in which most inventions were created by computers? Do creative computers challenge established norms in other areas of patent law? This Article attempts to resolve these ques- tions as well as some of the other philosophical, societal, and even apocalyptic concerns related to creative computers.9
This Article is divided into three parts.10 Part I examines instances in which AI has created patentable inventions.11 It finds that machines have been autonomously generating patentable results for at least twenty years and that the pace of such invention is likely increasing.12 It proceeds to discuss the cri- teria for inventorship and to examine the roles of humans and computers in the inventive process. It concludes that statutory language requiring inventors to be individuals and judicial characterization of invention as a “mental act” pre- sent barriers to computer inventorship, but that otherwise computers inde- pendently meet the requirements for inventorship. Finally, Part I notes that ap- plicants seem not to be disclosing the role of creative computers to the Patent Office—likely as a result of uncertainty over whether a computer inventor would render an invention unpatentable. Applicants may also be able to legally circumvent such disclosure by being the first human to discover a computer’s patentable result, but this Article will discuss how that approach is unfair, inef- ficient, and logistically problematic.
Part II examines the jurisprudence related to nonhuman authorship of copyrightable material in the absence of law on the subject of computer inven- torship.13 It discusses the history of the Copyright Office’s Human Authorship
7 See infra notes 23–138 and accompanying text.
8 See infra notes 139–239 and accompanying text.
9 See infra notes 230–313 and accompanying text.
10 See infra notes 23–138, 139–239, and 240–312 and accompanying text.
11 See infra notes 23–138 and accompanying text.
12 See, e.g., John R. Koza, Human-Competitive Results Produced by Genetic Programming, in 11
GENETIC PROGRAMMING & EVOLVABLE MACHINES 251, 251 (2010) [hereinafter Koza, Human- Competitive Results] (“[T]he increased availability of computing power (through both parallel compu- ting and Moore’s law) should result in the production, in the future, of an increasing flow of human- competitive results, as well as more intricate and impressive results.”).
13 See infra notes 139–239 and accompanying text.
 
327
1082 Boston College Law Review [Vol. 57:1079
Requirement,14 and scrutinizes case law interpreting the Patent and Copyright Clause.15 On the basis of this analysis, and based on principles of dynamic statutory interpretation,16 it argues that computers should qualify as legal in- ventors.
This would incentivize the development of creative machines consistent with the purpose and intent of the Founders and Congress. The requirement that inventors be individuals was designed to prevent corporate ownership,17 and so computer inventorship should not be prohibited on this basis. Also, there should be no requirement for a mental act because patent law is con- cerned with the creativity of an invention itself rather than the subjective men- tal process by which an invention may have been achieved.18 This Part con- cludes by addressing objections to computer inventorship including arguments that computational inventions would develop in the absence of patent protec- tion at non-monopoly prices.
Finally, Part III addresses challenges posed by computer inventorship, and generalizes the analysis of earlier sections.19 It finds that a computer’s owner should be the default assignee of any invention, both because this is most consistent with the rules governing ownership of property, and because it would most incentivize innovation. Where a computer’s owner, developer, and user are different entities, such parties could negotiate alternative arrangements by contract. Computer ownership here generally refers to software ownership, although there may be instances in which it is difficult to distinguish between hardware and software, or even to identify a software “owner.”20 This Part also examines the phenomenon of automation and the displacement of human in- ventors by computers. It finds that computational invention remains beneficial despite legitimate concerns and that for the foreseeable future computers are likely to refocus human inventors rather than replace them.
Part IV concludes by finding the arguments in support of computer inven- torship apply with equal force to nonhuman authors. Allowing animals to cre- ate copyrightable material would result in more socially valuable art by creat- ing new incentives for people to facilitate animal creativity.21 It would also
14 See U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 306 (3d ed. 2014).
15 See U.S. CONST. art. I, § 8, cl. 8.
16 See generally William N. Eskridge, Jr., Dynamic Statutory Interpretation, 135 U. PA. L. REV. 1479 (1987) (identifying principles of dynamic statutory interpretation).
17 See infra notes 122–132 and accompanying text.
18 See, e.g., The “Flash of Genius” Standard of Patentable Invention, 13 FORDHAM L. REV. 84, 85–86 (1944).
19 See infra notes 240–312 and accompanying text.
20 See generally GOVERNMENT OFFICE FOR SCIENCE, DISTRIBUTED LEDGER TECHNOLOGY: BEYOND BLOCK CHAIN (describing algorithmic technologies and distributed ledgers as examples of new and disruptive computational paradigms).
21 See infra notes 279–287 and accompanying text.
 
328
2016] Patent Generating Artificial Intelligence 1083
provide incentives for environmental conservation.22 Lastly, this Article exam- ines some of the implications of computer inventorship for other areas of pa- tent law. Computers are a natural substitute for the person having ordinary skill in the art (“PHOSITA” or, simply, the “skilled person”) used to judge a pa- tent’s inventiveness. The skilled person is presumed to know of all the prior art (what came before an invention) in a particular field—a legal fiction that could be accurate in the case of a computer. Substituting a computer for the skilled person also suggests a need to expand the scope of prior art, given that com- puters are not limited by human distinctions of scientific fields. This would make it more challenging for inventions to be held nonobvious, particularly in the case of inventions that merely combine existing elements in a new configu- ration (combination patents). That would be a desirable outcome, although the new test would create new challenges.
I. CREATIVE COMPUTERS AND PATENTLAW
This Part investigates instances when AI has created patentable inven- tions.23 It finds that machines have been autonomously generating patentable results for at least twenty years and that the pace of such invention is likely increasing.24 This Part proceeds to discuss the criteria for inventorship and to examine the roles of humans and computers in the inventive process.25 It con- cludes that statutory language requiring inventors to be individuals and judicial characterizations of invention as a “mental act” present barriers to computer inventorship, but that computers independently meet the requirements for in- ventorship otherwise.26 Finally, this Part notes that applicants seem not to be disclosing the role of creative computers to the Patent Office—likely as a re- sult of uncertainty over whether a computer inventor would render an inven- tion unpatentable.27
A. Computers Independently Generate Patentable Results
1. Example One: The Creativity Machine
Computers have been autonomously creating inventions since the twenti- eth century. In 1994, computer scientist Stephen Thaler disclosed an invention
22 See infra notes 279–287 and accompanying text.
23 See infra notes 23–138 and accompanying text.
24 See, e.g., Koza, Human-Competitive Results, supra note 12, at 251 (“[T]he increased availabil-
ity of computing power (through both parallel computing and Moore’s law) should result in the pro- duction, in the future, of an increasing flow of human-competitive results, as well as more intricate and impressive results.”).
25 See infra notes 100–121 and accompanying text. 26 See infra notes 122–132 and accompanying text. 27 See infra notes 133–138 and accompanying text.
 
329
1084 Boston College Law Review [Vol. 57:1079
he called the “Creativity Machine,” a computational paradigm that “came the closest yet to emulating the fundamental mechanisms responsible for idea for- mation.”28 The Creativity Machine is able to generate novel ideas through the use of a software concept referred to as artificial neural networks—essentially, collections of on/off switches that automatically connect themselves to form software without human intervention.29
At its most basic level, the Creativity Machine combines an artificial neu- ral network that generates output in response to self-stimulation of the net- work’s connections together with another network that perceives value in the stream of output.30 This results in an AI that “brainstorms” new and creative ideas after it alters (perturbs) the connections within its neural network.31 An example of this phenomenon occurred after Dr. Thaler exposed the Creativity Machine to some of his favorite music, and the machine proceeded to write eleven thousand new songs in a single weekend.32
Dr. Thaler compares the Creativity Machine and its processes to the hu- man brain and consciousness.33 The two artificial neural networks mimic the human brain’s major cognitive circuit: the thalamo-cortical loop.34 In a simpli- fied model of the human brain, the cortex generates a stream of output (or con- sciousness), and the thalamus brings attention (or awareness) to ideas that are of interest.35 Like the human brain, the Creativity Machine is capable of gener- ating novel patterns of information rather than simply associating patterns, and it is capable of adapting to new scenarios without additional human input.36 Also like the human brain, the AI’s software is not written by human beings—
28 See What Is the Ultimate Idea?, IMAGINATION ENGINES INC., http://www.imagination-engines. com [https://perma.cc/P877-F33B] (last visited Jan. 25, 2016).
29 The architecture for the Creativity Machine is discussed in greater detail in several publica- tions. See, e.g., Stephen L. Thaler, Synaptic Perturbation and Consciousness, 6 INT’L J. MACHINE CONSCIOUSNESS 75 (2014) [hereinafter Thaler, Synaptic Perturbation and Consciousness]; Stephen Thaler, Creativity Machine® Paradigm, in ENCYCLOPEDIA OF CREATIVITY, INVENTION, INNOVA- TION, AND ENTREPRENEURSHIP 451 (Elias G. Carayannis ed., 2013) [hereinafter Thaler, Creativity Machine® Paradigm]; S.L. Thaler, A Proposed Symbolism for Network-Implemented Discovery Pro- cesses, WORLD CONGRESS ON NEURAL NETWORKS ’96, SAN DIEGO 1265 (Int’l Neural Network Soc’y 1996) [hereinafter, Thaler, A Proposed Symbolism].
30 See Aaron M. Cohen, Stephen Thaler’s Imagination Machines, THE FUTURIST, July–Aug. 2009, at 28, 28–29.
31 See Thaler, A Proposed Symbolism, supra note 29, at 1265–68.
32 See Tina Hesman, Stephen Thaler’s Computer Creativity Machine Simulates the Human Brain, ST. LOUIS POST-DISPATCH, Jan. 24, 2004, available at http://www.mindfully.org/Technology/ 2004/Creativity-Machine-Thaler24jan04.htm [https://perma.cc/T8HS-C2TB].
33 Thaler, Creativity Machine® Paradigm, supra note 29, at 447.
34 Id.
35 Id.
36 See Artificial Neural Networks, IMAGINATION ENGINES INC., http://imagination-engines.
com/iei_ann.php [https://perma.cc/BB8K-G3FH] (last visited Jan. 25, 2016); IEI’s Patented Creativity Machine® Paradigm, IMAGINATION ENGINES INC., http://imagination-engines.com/iei_cm.php [https://perma.cc/4A8A-6H3Y] (last visited Jan. 25, 2016).
 
330
2016] Patent Generating Artificial Intelligence 1085
it is self-assembling.37 Dr. Thaler argues his AI is very different from a soft- ware program that simply generates a spectrum of possible solutions to a prob- lem combined with an algorithm to filter for the best ideas generated.38 He notes that such a software program would be another method for having an AI developing novel ideas.39
Dr. Thaler invented the Creativity Machine, and the machine was the sub- ject of his first patent, titled “Device for the Autonomous Generation of Useful Information.”40 The second patent filed in Dr. Thaler’s name was “Neural Network Based Prototyping System and Method.”41 Dr. Thaler is listed as the patent’s inventor, but he states that the Creativity Machine invented the pa- tent’s subject matter (the “Creativity Machine’s Patent”).42 The Creativity Ma- chine’s Patent application was first filed on January 26, 1996, and granted on December 22, 1998.43
As one of Dr. Thaler’s associates observed in response to the Creativity Machine’s Patent, “Patent Number Two was invented by Patent Number One. Think about that. Patent Number Two was invented by Patent Number One!”44 Aside from the Creativity Machine’s Patent, the machine is credited with nu- merous other inventions: the cross-bristle design of the Oral-B CrossAction toothbrush, new super-strong materials, and devices that search the Internet for messages from terrorists, among others.45
The Creativity Machine’s Patent is interesting for a number of reasons. If Dr. Thaler’s claims are accurate, then the Patent Office has already granted, without knowing it has done so, a patent for an invention created by a non- human inventor—and as early as 1998. Also, the Patent Office apparently had no idea it was doing so. Dr. Thaler listed himself as the inventor on the patent
37 See Cohen, supra note 29.
38 See Telephone Interview with Stephen Thaler, President and CEO, Imagination Engines, Inc. (Jan. 10, 2016) [hereinafter Thaler, Telephone Interview].
39 See id.
40 See U.S. Patent No. 5,659,666 (filed Oct. 13, 1994).
41 See U.S. Patent No. 5,852,815 (filed May 15, 1998).
42 See Patent Listing, IMAGINATION ENGINES INC., http://imagination-engines.com/iei_ip.php
[https://perma.cc/N79N-NWEF] (last visited Jan. 25, 2016).
43 U.S. Patent No. 5,852,815 (filed May 15, 1998). This application is a divisional of application
with serial number 08/592,767 filed Jan. 26, 1996. This means the patent was invented sometime before January 26, 1996. Patent applications require an inventor to actually or constructively possess the invention at the time an application is filed to meet enablement and written description require- ments. See U.S. PATENT & TRADEMARK OFFICE, MANUAL OF PATENT EXAMINING PROCEDURE § 2164 (9th ed. Rev 7, Nov. 2015) [hereinafter MPEP].
44 Hesman, supra note 32 (quoting Rusty Miller).
45 Thaler, Creativity Machine® Paradigm, supra note 29, at 451. Table 1 contains a list of Crea- tivity Machine accomplishments. Id.
 
331
1086 Boston College Law Review [Vol. 57:1079 and did not disclose the Creativity Machine’s involvement to the Patent Office.
The patent’s prosecution history contains no mention of a computer inventor.46
2. Example Two: The Invention Machine
The Creativity Machine has not been the only source of computational invention.47 Software modeled after the process of biological evolution, known as Genetic Programming (“GP”), has succeeded in independently generating patentable results. 48 Evolution is a creative process that relies on a few simple processes: “mutation, sexual recombination, and natural selection.”49 GP emu- lates these same methods digitally to achieve machine intelligence.50 It delivers human-competitive intelligence with a minimum amount of human involve- ment.51
As early as 1996, GP succeeded in independently generating results that were the subject of past patents.52 By 2010, there were at least thirty-one in- stances in which GP generated a result that duplicated a previously patented invention, infringed a previously issued patent, or created a patentable new invention.53 In seven of those instances, GP infringed or duplicated the func- tionality of a twenty-first century invention.54 Some of those inventions were on the cutting edge of research in their respective fields.55 In two instances, GP may have created patentable new inventions.56
46 The file history for this patent is available from a search of the USPTO’s website. Patent Ap- plication Information Retrieval, USPTO, http://portal.uspto.gov/pair/PublicPair [https://perma. cc/7PAM-3EG7] (last visited Jan. 27, 2016). Patent applicants have a duty of candor and good faith in dealing with the Office, which includes a duty to disclose to the Office all information known to be material to patentability. 37 C.F.R. § 1.56 (2012). Indeed, Dr. Thaler completed an inventor’s oath or declaration stating that he disclosed to the Office all information known to be material to patentability including the identity of all inventors. See 35 U.S.C. § 115 (2012); MPEP, supra note 43, § 602.01(b) (listing the standard for patents filed before September 16, 2012). Such oaths are made under penalty of fine or imprisonment, and willful false statements may jeopardize the validity of an application and any future patents. 35 U.S.C. § 115; MPEP, supra note 43, § 602.01(a)–(b).
47 See generally Jon Rowe & Derek Partridge, Creativity: A Survey of AI Approaches, 7 ARTIFI- CIAL INTELLIGENCE REV. 43 (1993) (detailing sources of computational inventions).
48 Koza, Human-Competitive Results, supra note 12, at 265. Alan Turing identified GP as a method of creating machine intelligence in his 1950 report Intelligent Machinery. A.M. TURING, IN- TELLIGENT MACHINERY 18 (1948) (“[T]he genetical or evolutionary search by which a combination of genes is looked for, the criterion being the survival value.”).
49 John R. Koza et al., Evolving Inventions, SCI. AM., Feb. 2003, at 52. 50 See id.
51 See id.
52 See Koza, Human-Competitive Results, supra note 12, at 255–56, 265. 53 See id.
54 See id.
55 See Koza et al., Evolving Inventions, supra note 49, at 52.
56 Koza, Human-Competitive Results, supra note 12, at 265. These two instances are the inventive
act described in U.S. Patent No. 6,847,851 (filed July 12, 2002) and JOHN R. KOZA ET AL., GENETIC PROGRAMMING IV: ROUTING HUMAN-COMPETITIVE MACHINE INTELLIGENCE 102–04 (2003).
 
332
2016] Patent Generating Artificial Intelligence 1087
The Patent Office granted another patent for a computational invention on January 25, 2005.57 That invention was created by the “Invention Machine”— the moniker for a GP-based AI developed by John Koza.58 Dr. Koza is a com- puter scientist and pioneer in the field of GP, and he claims the Invention Ma- chine has created multiple “patentable new invention[s].”59 A 2006 article in Popular Science about Dr. Koza and the Invention Machine claimed that the AI “has even earned a U.S. patent for developing a system to make factories more efficient, one of the first intellectual-property protections ever granted to a nonhuman designer.”60 The article refers to a patent titled “Apparatus for Im- proved General-Purpose PID and non-PID Controllers” (the “Invention Ma- chine’s Patent”).61 The Invention Machine generated the content of the patent without human intervention and in a single pass.62 It did so without a database of expert knowledge and without any knowledge about existing controllers.63 It simply required information about basic components (such as resistors and diodes) and specifications for a desired result (performance measures such as voltage and frequency).64 With this information, the Invention Machine pro- ceeded to generate different outputs that were measured for fitness (whether an output met performance measures).65
Once again, the Patent Office seems to have had no idea of the AI’s role in the Invention Machine’s Patent.66 The Popular Science article states that Dr. Koza did not disclose the Invention Machine’s involvement, and the patent’s
57 Jonathon Keats, John Koza Has Built an Invention Machine, POPULAR SCI. (Apr. 18, 2006), http://www.popsci.com/scitech/article/2006-04/john-koza-has-built-invention-machine [https://web. archive.org/web/20150218225133/http://www.popsci.com/scitech/article/2006-04/john-koza-has- built-invention-machine].
58 Dr. Koza was also the inventor of the scratch-off lottery ticket in the 1970s. See Home Page of John R. Koza, GENETIC PROGRAMMING, http://www.genetic-programming.com/johnkoza.html [https://perma.cc/H77Y-XM4T] (last visited Aug. 8, 2016).
59 See Koza, Human-Competitive Results, supra note 12, at 265.
60 Keats, supra note 57.
61 See id; U.S. Patent No. '851 (filed July 12, 2002). Although the article does not specifically
identify the patent it is referring to, a search of USPTO records reveals only one patent with Dr. Koza listed as an inventor and with a grant date of January 25, 2005. In addition, in 2010, Dr. Koza subse- quently identified the 851 Patent as one of two examples in which GP created a patentable new inven- tion. See Koza, Human-Competitive Results, supra note 12, at 265.
62 KOZA ET AL., GENETIC PROGRAMMING IV, supra note 56, at 102–04.
63 Telephone Interview with John Koza, President, Genetic Programming Inc. (Jan. 22, 2016) [hereinafter Koza, Telephone Interview].
64 Id.
65 Thus, the GP algorithm is domain independent. Unlike human inventors who often have exten- sive knowledge of prior inventions and who proceed to build on earlier work, the GP algorithm gener- ated a new controller without any reliance on prior art.
66 “If the Turing test had been to fool a patent examiner instead of a conversationalist, then Janu- ary 25, 2005 would have been a date for the history books.” PEDRO DOMINGOS, THE MASTER ALGO- RITHM: HOW THE QUEST FOR THE ULTIMATE LEARNING MACHINE WILL REMAKE OUR WORLD 133– 34 (2015).
 
333
1088 Boston College Law Review [Vol. 57:1079
prosecution history contains no mention of a computer inventor.67 Dr. Koza states that his legal counsel advised him at the time that his team should con- sider themselves inventors despite the fact that “the whole invention was creat- ed by a computer.”68
Dr. Koza reports that his agenda in having the Invention Machine recreate previously patented results was to prove that computers could be made to solve problems automatically.69 He believed that focusing on patentable results would produce compelling evidence that computers were producing something valuable.70 For that reason, he focused on recreating or inventing patentable subject matter that represented significant scientific advances.71 For instance, the Invention Machine’s Patent was for an improved version of a landmark controller built in 1995.72
3. Example Three: Watson
The Creativity Machine and the Invention Machine may be the earliest examples of computer inventors, but others exist.73 Moreover, the exponential growth in computing power over the past dozen years combined with the in- creasing sophistication of software should have led to an explosion in the
67 Indeed, all three of the inventors on the '851 patent, including Dr. Koza, completed an inven- tor’s oath or declaration stating that they disclosed to the Office all information known to be material to patentability including the identity of all inventors.
68 Koza, Telephone Interview, supra note 63.
69 Id.
70 Id.
71 Id. Generating these results de novo thus represented a test with an external measure of difficul-
ty, in contrast to other AI researchers who were training computers to complete academic exercises. 72 See generally KARL J. ASTROM & TORE HAGGLUND, PID CONTROLLERS: THEORY, DESIGN, AND TUNING (2d ed. 1995) (detailing original version of the controller for which the Invention Ma-
chine created an improved, patentable version).
73 E.g., Matrix Advanced Solutions used AI to develop a new anticoagulant. See Daniel Riester et
al., Thrombin Inhibitors Identified by Computer-Assisted Multiparameter Design, 102 PROC. NAT’L ACAD. SCI. USA 8597, 8597–602 (2005). Maxygen Inc. used GP to develop a novel Hepatitis C treat- ment. See Maxygen’s Next-Generation Interferon Alpha Enters Phase Ia Clinical Trial, MAXYGEN (Nov. 7, 2006), available at http://www.prnewswire.com/news-releases/maxygens-next-generation-interferon- alpha-enters-phase-ia-clinical-trial-56073027.html [https://perma.cc/Y9LD-B9EL]. In fact, there is an annual competition for computers producing human-competitive results by genetic and evolutionary computation. See Humies Awards, SIGEVO-GECCO, http://sig.sigevo.org/index.html/tiki-index.php? page=Humies+Awards [https://perma.cc/XMG2-DAGY] (last visited Aug. 9, 2016). Dr. Koza states that competition participants have gone on to patent their results. Koza, Telephone Interview, supra note 63. For additional examples of “Artificial Inventions,” see Plotkin, supra note 5, at 61. In his book, Dr. Plot- kin uses the metaphor of a genie to argue that AI will change the dynamics of human-computer collabo- rations. He suggests that humans will write “wishes” (an abstract description of a machine or a set of instructions for creating a machine) for AI to “grant” (by producing the design for a machine or an actual machine). He further argues that fear of invention automation is unnecessary, and that individuals will become more sophisticated at “writing wishes” (defining problems) for AI to solve. He suggests this will result in more skilled inventors and non-inventors becoming inventors with the help of machines. Id. at 1–11.
 
334
2016] Patent Generating Artificial Intelligence 1089
number of computational inventions.74 Indeed, it is likely that computers are inventing more than ever before.75 Consider, for instance, the results produced by IBM’s AI “Watson” of Jeopardy! fame.76 Watson is a computer system de- veloped by IBM to compete on the game show Jeopardy!77 In 2011, it beat former Jeopardy! winners Ken Jennings and Brad Rutter on the show, earning a million dollars in the process.78
IBM describes Watson as one of a new generation of machines capable of “computational creativity.”79 IBM uses that term to describe machines that can generate “ideas the world has never imagined before.”80 Watson “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], applying big data in new ways.”81 This is a fundamentally dif- ferent type of AI than the Creativity Machine or the Invention Machine; Wat- son utilizes a more conventional architecture of logical deduction combined with access to massive databases containing accumulated human knowledge and expertise.82 Although Watson is not modeled after the human brain or evo-
74 See, e.g., 50 Years of Moore’s Law, INTEL, http://www.intel.com/content/www/us/en/silicon- innovations/moores-law-technology.html [https://perma.cc/PMN9-XJ2L] (last visited Jan. 26, 2016). In 1965, Gordon Moore, co-founder of Intel and Fairchild Semiconductor, published a paper in which he noted a doubling every year in the number of components in an integrated circuit. Based on this and his subsequent observations, “Moore’s Law” became the “golden rule for the electronics indus- try,” predicting that overall processing power for computers will double every eighteen months. See id.
75 See, e.g., Koza, Human-Competitive Results, supra note 12, at 251 (stating that “the increased availability of computing power (through both parallel computing and Moore’s Law) should result in the production, in the future, of an increasing flow of human-competitive results, as well as more intricate and impressive results”).
76 See Jo Best, IBM Watson, TECHREPUBLIC, http://www.techrepublic.com/article/ibm-watson-the- inside-story-of-how-the-jeopardy-winning-supercomputer-was-born-and-what-it-wants-to-do-next/ [https://perma.cc/BQ4V-Q48F] (last visited Jan. 17, 2016).
77 See id.
78 See id.
79 See Computational Creativity, IBM, http://www.research.ibm.com/cognitive-computing/
computational-creativity.shtml#fbid=kwG0oXrjBHY [https://perma.cc/6FK4-WTL3] (last visited Jan. 25, 2016).
80 What Is Watson?, IBM, http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson. html [https://perma.cc/8KM3-LLSG] (last visited Jan. 25, 2016). Watson is a cognitive commuting system with the extraordinary ability to analyze natural language processing, generate and evaluate hypotheses based on the available data then store and learn from the information. In other words, Watson essentially mirrors the human learning process by getting “smarter [through] tracking feed- back from its users and learning from both successes and failures.” Id. Watson made its notable debut on the game show Jeopardy, where it defeated Brad Rutter and Ken Jennings using only stored data by comparing potential answers and ranking confidence in accuracy at the rate of approximately three seconds per question. Id.
81 Computational Creativity, supra note 79.
82 See, e.g., David Ferrucci et al., Building Watson: An Overview of the DeepQA Project, AI MAG., Fall 2010, at 59, 68–69; IBM Watson: Beyond Jeopardy! Q&A, ACM, http://learning.acm. org/webinar/lally.cfm [https://perma.cc/JA3N-J6HG] (last visited Jan. 25, 2016).
 
335
1090 Boston College Law Review [Vol. 57:1079
lutionary processes, it is also capable of generating novel, nonobvious, and useful ideas.
Watson’s Jeopardy! career was short and sweet, and by 2014, it was being applied to more pragmatic challenges, such as running a food truck.83 IBM developed new algorithms for Watson and incorporated a database with infor- mation about nutrition, flavor compounds, the molecular structure of foods, and tens of thousands of existing recipes.84 This new design permits Watson to generate recipes in response to users inputting a few parameters such as ingre- dients, dish (e.g., burgers or burritos), and style (e.g., British or dairy-free).85 On the basis of this user input, Watson proceeds to generate a staggeringly large number of potential food combinations.86 It then evaluates these prelimi- nary results based on novelty and predicted quality to generate a final output.87
It is likely that some of Watson’s discoveries in food science are patenta- ble.88 Patents may be granted for any “new and useful process, machine, manu- facture, or composition of matter, or any new and useful improvement there- of.”89 Food recipes can qualify as patentable subject matter on this basis be- cause lists of ingredients combine to form new compositions of matter or man- ufacture and the steps involved in creating food may be considered a process.90 To be patentable, however, an invention must not only contain patentable sub- ject matter; it must also be novel, nonobvious, and useful.91 That may be chal- lenging to achieve in the case of food recipes given that there is a finite num- ber of ingredients and people have been combining ingredients together for a
83 See Maanvi Singh, Our Supercomputer Overlord Is Now Running a Food Truck, NPR (Mar. 4, 2014), http://www.npr.org/blogs/thesalt/2014/03/03/285326611/our-supercomputer-overlord-is-now- running-a-food-truck [https://perma.cc/V7KM-X8P5]; Chef Watson, IBM, https://www.ibmchef watson.com/community [https://perma.cc/2D54-UURY] (last visited Jan. 17, 2016); Under the Hood, IBM, http://www.ibm.com/smarterplanet/us/en/cognitivecooking/tech.html [https://perma.cc/HWQ8- SEFE] (last visited Jan. 17, 2016).
84 See Under the Hood, supra note 83.
85 See Watson Cooks Up Computational Creativity, IBM, http://www.ibm.com/smarterplanet/ us/en/innovation_explanations/article/florian_pinel.html [https://perma.cc/GGV7-NHT4] (last visited Jan. 17, 2016).
86 See id.
87 See id.
88 See Can Recipes Be Patented?, INVENTORS EYE (June 2013), http://www.uspto.gov/
inventors/independent/eye/201306/ADVICE.jsp [https://perma.cc/EN3V-9DY4]; see also Diamond v. Chakrabarty, 447 U.S. 303, 308– 09 (1980) (noting that patentable subject matter “include[s] any- thing under the sun that is made by man” (quoting S. REP. NO. 82-1979, at 5 (1952); H.R. REP. NO. 82-1923, at 6 (1952))) (internal quotation marks omitted).
89 35 U.S.C. § 101 (1952).
90 See Can Recipes Be Patented?, supra note 88.
91 See General Information Concerning Patents, USPTO, http://www.uspto.gov/patents-
getting-started/general-information-concerning-patents [https://perma.cc/J88J-YUVA] (Oct. 2014).
 
336
2016] Patent Generating Artificial Intelligence 1091
very long time.92 Not only would Watson have to create a recipe that no one had previously created, but it could not be an obvious variation on an existing recipe. Still, people do obtain patents on new food recipes.93 The fact that some of Watson’s results have been surprising to its developers and to human chefs is encouraging94 in this regard95 because unexpected results are one of the fac- tors considered in determining whether an invention is nonobvious.96
Watson is not limited to competing on Jeopardy! or to developing new food recipes.97 IBM has made Watson broadly available to software application providers, enabling them to create services with Watson’s capabilities.98 Wat- son is now assisting with financial planning, helping clinicians to develop treatment plans for cancer patients, identifying potential research study partici- pants, distinguishing genetic profiles that might respond well to certain drugs, and acting as a personal travel concierge.99
92 See Therese Oneill, 7 of the World’s Oldest Foods Discovered by Archeologists, MENTAL FLOSS (Oct. 8, 2015), http://mentalfloss.com/article/49610/7-world%E2%80%99s-oldest-food-finds [https://perma.cc/Y9C5-DRGP].
93 See, e.g., U.S. Patent No. 8,354,134 (filed Dec. 22, 2005).
94 Which is not to say that patents on recipes are a social good. See generally KAL RAUSTIALA & CHRISTOPHER SPRIGMAN, THE KNOCKOFF ECONOMY: HOW IMITATION SPARKS INNOVATION (2012) (discussing social ills that can arise from patents).
95 See, e.g., Rochelle Bilow, How IBM’s Chef Watson Actually Works, BON APPÉTIT (June 30, 2014), http://www.bonappetit.com/entertaining-style/trends-news/article/how-ibm-chef-watson-works [https://perma.cc/5UAB-V AGW].
96 MPEP, supra note 43, § 716.02(a).
97 IBM has even worked with food magazine Bon Appétit to develop a recipe app called Chef Watson to allow the general public to enlist Watson’s help in making new recipes. Rochelle Bilow, We Spent a Year Cooking with the World’s Smartest Computer—and Now You Can, Too, BON AP- PÉTIT (June 23, 2015), http://www.bonappetit.com/entertaining-style/trends-news/article/chef-watson- app [https://perma.cc/HF7Q-C9FM]. Chef Watson can be accessed online at https://www.ibmchef watson.com/community [https://perma.cc/2D54-UURY]. For the less technologically inclined who still wish to sample machine cooking, IBM has published a book of Watson’s recipes. See generally IBM & THE INST. OF CULINARY EDUC., COGNITIVE COOKING WITH CHEF WATSON: RECIPES FOR INNOVATION FROM IBM & THE INSTITUTE OF CULINARY EDUCATION (2015) (detailing recipes creat- ed by Watson).
98 Watson Cooks Up Computational Creativity, supra note 85.
99 Anna Edney, Doctor Watson Will See You Now, if IBM Wins in Congress, BLOOMBERG BNA HEALTH IT LAW & INDUSTRY REPORT (Jan. 29, 2015), available at http://www.post-gazette.com/ frontpage/2015/01/29/Doctor-Watson-will-see-you-now-if-IBM-wins-in-Congress/stories/201501290332 [https://perma.cc/4BHU-VJXU]; Thor Olavsrud, 10 IBM Watson-Powered Apps That Are Changing Our World, CIO (Nov. 6, 2014), http://www.cio.com/article/2843710/big-data/10-ibm-watson-powered-apps- that-are-changing-our-world.html#slide11 [https://perma.cc/NPY7-DDMA].
 
337
1092 Boston College Law Review [Vol. 57:1079
B. Human and Computer Involvement in Computational Inventions
1. Requirements for Inventorship
All patent applications require one or more named inventors who must be “individuals,” a legal entity such as a corporation cannot be an inventor.100 In- ventors own their patents as a form of personal property that they may transfer by “assignment” of their rights to another entity.101 A patent grants its owner “the right to exclude others from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”102 If a patent has multiple owners, each owner may inde- pendently exploit the patent without the consent of the others (absent a con- flicting contractual obligation).103 This makes the issue of whether a computer can be an inventor one of practical as well as theoretical interest because in- ventors have ownership rights in their patents, and failure to list an inventor can result in a patent being held invalid or unenforceable.104
For a person to be an inventor, the person must contribute to an inven- tion’s “conception.”105 Conception refers to, “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative inven- tion as it is thereafter to be applied in practice.”106 It is “the complete perfor-
100 See 35 U.S.C. § 100(f) (1952) “The term ‘inventor’ means the individual or, if a joint inven- tion, the individuals collectively who invented or discovered the subject matter of the invention.” See id. The same issues surrounding computer inventorship may not exist outside of the U.S. where appli- cations do not require a named inventor. See MPEP, supra note 43, § 2137.01 (“The requirement that the applicant for a patent in an application filed before September 16, 2012 be the inventor(s) . . . and that the inventor . . . be identified in applications filed on or after September 16, 2012, are characteris- tics of U.S. patent law not generally shared by other countries.”). For example, a patent application at the European Patent Office may be filed by “any body equivalent to a legal person by virtue of the law governing it.” Convention on the Grant of European Patents art. 58, Oct. 5, 1973, 1065 U.N.T.S. 199. Under the U.S. Patent Act, only individuals can invent, not corporations. See 35 U.S.C. §§ 115–116.
101 See MPEP, supra note 43, § 300. About ninety-three percent of patents are assigned to organi- zations (rather than individuals). See Patenting by Organizations (Utility Patents), USPTO, http:// www.uspto.gov/web/offices/ac/ido/oeip/taf/topo_13.htm#PartA1_1b [https://perma.cc/VF56-GFVT] (last modified Jan. 25, 2016). For example, it is common for scientific and technical workers to preemptively assign their patent rights to employers as a condition of employment. Most, but not all, inventions can be placed under an obligation of assignment in employment contracts. For example, in California, employees are permitted to retain ownership of inventions that are developed entirely on their own time without using their employer’s equipment, supplies, facilities, or trade secret infor- mation except for inventions that either: related, at the time of conception or reduction to practice of the invention, to the employer’s business; actual or demonstrably anticipated research or development of the employer; or resulted from any work performed by the employee for the employer. CAL. LAB. CODE § 2872(a) (West 1979).
102 35 U.S.C. § 154.
103 See MPEP, supra note 43, § 2137.
104 See, e.g., Advanced Magnetic Closures, Inc. v. Rome Fastener Corp., 607 F.3d 817, 829 (Fed.
Cir. 2010).
105 MPEP, supra note 43, § 2137.01(II).
106 Townsend v. Smith, 36 F.2d 292, 295 (C.C.P.A. 1929).
 
338
2016] Patent Generating Artificial Intelligence 1093
mance of the mental part of the inventive act.”107 After conception, someone with ordinary skill in the invention’s subject matter (e.g., a chemist if the in- vention is a new chemical compound) should be able to “reduce the invention to practice.”108 That is to say, they should be able to make and use an invention from a description without extensive experimentation or additional inventive skill.109 Individuals who simply reduce an invention to practice, by describing an already conceived invention in writing or by building a working model from a description for example, do not qualify as inventors.110
2. The Role of Computers in Inventive Activity
The requirement that an inventor participate in the conception of an in- vention creates barriers to inventorship for computers as well as people. Alt- hough computers are commonly involved in the inventive process, in most cases, computers are essentially working as sophisticated (or not-so- sophisticated) tools. One example occurs when a computer is functioning as a calculator or storing information. In these instances, a computer may assist a human inventor to reduce an invention to practice, but the computer is not par- ticipating in the invention’s conception. Even when computers play a more substantive role in the inventive process, such as by analyzing data in an auto-
107 Id.
108 Reduction to practice refers to either actual reduction—where it can be demonstrated the claimed invention works for its intended purpose (for example, with a working model)—or to con- structive reduction—where an invention is described in writing in such a way that it teaches a person of ordinary skill in the subject matter to make and use the invention (as in a patent application). See In re Hardee, 223 U.S.P.Q. (BNA) 1122, 1123 (Com’r Pat. & Trademarks Apr. 3, 1984); see also Bd. of Educ. ex rel. Bd. of Trs. of Fla. State Univ. v. Am. Bioscience, Inc., 333 F.3d 1330, 1340 (Fed. Cir. 2003) (“Invention requires conception.”). With regard to the inventorship of chemical compounds, an inventor must have a conception of the specific compounds being claimed. See Am. Bioscience, 333 F.3d at 1340 (“[G]eneral knowledge regarding the anticipated biological properties of groups of com- plex chemical compounds is insufficient to confer inventorship status with respect to specifically claimed compounds.”); see also Ex parte Smernoff, 215 U.S.P.Q 545, 547 (Pat. & Tr. Office Bd.App. Aug. 17,1982) (“[O]ne who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not a coinventor.”). Actual reduction to practice “requires that the claimed inven- tion work for its intended purpose.” Brunswick Corp. v. United States, 34 Fed. Cl. 532, 584 (1995) (quotations omitted) (quoting Hybritech Inc. v. Monoclonal Antibodies, Inc., 802 F.2d 1367, 1376 (Fed. Cir. 1986). Constructive reduction to practice “occurs upon the filing of a patent application on the claimed invention.” Id. The written description requirement is “to ensure that the inventor had possession, as of the filing date of the application relied on, of the specific subject matter later claimed by him.” Application of Edwards, 568 F.2d 1349, 1351 (C.C.P.A. 1978).
109 “[C]onception is established when the invention is made sufficiently clear to enable one skilled in the art to reduce it to practice without the exercise of extensive experimentation or the exer- cise of inventive skill.” Hiatt v. Ziegler & Kilgour , 179 U.S.P.Q. 757, 763 (Bd. Pat. Interferences Apr. 3, 1973). Conception has been defined as a disclosure of an idea that allows a person skilled in the art to reduce the idea to a practical form without “exercise of the inventive faculty.” Gunter v. Stream, 573 F.2d 77, 79 (C.C.P.A. 1978).
110 See De Solms v. Schoenwald, 15 U.S.P.Q. 2d 1507, 1510 (Bd.Pat.App.& Interferences. Feb. 22, 1990).
 
339
1094 Boston College Law Review [Vol. 57:1079
mated fashion, retrieving stored knowledge, or by recognizing patterns of in- formation, the computer still may fail to contribute to conception. Computer involvement might be conceptualized on a spectrum: on one end, a computer is simply a tool assisting a human inventor; on the other end, the computer inde- pendently meets the requirements for inventorship. AI capable of acting auton- omously such as the Creativity Machine and the Invention Machine fall on the latter end of the spectrum.
3. The Role of Humans in Inventive Activity
Just as computers can be involved in the inventive process without con- tributing to conception, so can humans. For now, at least, computers do not entirely undertake tasks on their own accord. Computers require some amount of human input to generate creative output.
For example, before the Creativity Machine composed music, Dr. Thaler exposed it to existing music and instructed it to create something new.111 Yet, simply providing a computer with a task and starting materials would not make a human an inventor.112 Imagine Friend A tells Friend B, who is an engineer, that A would like B to develop an iPhone battery with twice the standard bat- tery life and A gives B some publically available battery schematics. If B then succeeds in developing such a battery, A would not qualify as an inventor of the battery by virtue of having instructed B to create a result.113 This scenario essentially occurred in the case of the Creativity Machine’s toothbrush inven- tion: Dr. Thaler provided the Creativity Machine information on existing toothbrush designs along with data on each brush’s effectiveness.114 Solely from this information, the Creativity Machine produced the first ever crossed- bristle design.115 This does not make Dr. Thaler an inventor. In the case of the Creativity Machine, the creative act is the result of random or chaotic perturba- tions in the machine’s existing connections that produce new results which, in turn, are judged by the machine for value.116
Humans are also necessarily involved in the creative process because computers do not arise from a void; in other words, humans have to create computers.117 Once again, that should not prevent computer inventorship. No
111 Thaler, Telephone Interview, supra note 38.
112 Ex parte Smernoff, 215 U.S.P.Q. at 547 (“[O]ne who suggests an idea of a result to be accom- plished, rather than the means of accomplishing it, is not a coinventor.”).
113 See id.
114 Thaler, Telephone Interview, supra note 38.
115 Id.
116 See Thaler, Creativity Machine® Paradigm, supra note 29, at 449.
117 This will be the case until computers start designing other computers or engaging in reflection.
Reflection is a software concept that refers to a computer program that can examine itself and modify its own behavior (and even its own code). J. Malenfant et al., A Tutorial on Behavioral Reflection and Its Implementation, in PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE REFLECTION 1, 1–
 
340
2016] Patent Generating Artificial Intelligence 1095
one would exist without their parents contributing to their conception (pun in- tended), but that does not make parents inventors on their child’s patents. If a computer scientist creates an AI to autonomously develop useful information and the AI creates a patentable result in an area not foreseen by the inventor, there would be no reason for the scientist to qualify as an inventor on the AI’s result. An inventor must have formed a “definite and permanent idea of the complete and operative invention” to establish conception.118 The scientist might have a claim to inventorship if he developed the AI to solve a particular problem, and it was foreseeable that the AI would produce a particular re- sult.119
4. Combining Human and Computer Creativity
A computer may not be a sole inventor; the inventive process can be a collaborative process between human and machine. If the process of develop- ing the Creativity Machine’s Patent had been a back-and-forth process with both the AI and Dr. Thaler contributing to conception, then both might qualify as inventors.120 By means of illustration, suppose a human engineer provides a machine with basic information and a task. The engineer might learn from the machine’s initial output, then alter the information that he or she provides to the machine to improve its subsequent output. After several iterations, the ma- chine might produce a final output that the human engineer might directly alter to create a patentable result. In such a case, both the engineer and the machine might have played a role in conception. Leaving AI aside, invention is rarely occurs in a vacuum, and there are often joint inventors on patents.121 In some of these instances, if a computer were human, it would be an inventor. Yet, computers are not human, and, as such, they face unique barriers to qualifying as inventors.
20 (1996), available at http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ malenfant.pdf [https://perma.cc/7EKK-7BJT].
118 Townsend, 36 F.2d at 295.
119 See generally Shyamkrishna Balganesh, Foreseeability and Copyright Incentives, 122 HARV. L. REV. 1569 (2009) (discussing foreseeability in the patent context).
120 What is required is some “quantum of collaboration or connection.” Kimberly-Clark Corp. v. Procter & Gamble Distrib. Co., 973 F.2d 911, 917 (Fed. Cir. 1992). For joint inventorship, “there must be some element of joint behavior, such as collaboration or working under common direction, one inventor seeing a relevant report and building upon it or hearing another’s suggestion at a meet- ing.” Id.; see also Moler & Adams v. Purdy, 131 U.S.P.Q. 276, 279 (Bd. Pat. Interferences 1960) (“[I]t is not necessary that the inventive concept come to both [joint inventors] at the same time.”).
121 See Prerna Wardhan & Padmavati Manchikanti, A Relook at Inventors’ Rights, 18 J. INTELL. PROP. RIGHTS 168, 169 (2013).
 
341
1096 Boston College Law Review [Vol. 57:1079
C. Barriers to Computer Inventorship
1. The Legal Landscape
Congress is empowered to grant patents on the basis of the Patent and Copyright Clause of the Constitution.122 That clause enables Congress “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”123 It also provides an explicit rationale for granting patent and copyright protection, namely to encourage innovation under an incentive theo- ry.124 The theory goes that people will be more inclined to invent things (i.e., promote the progress of science) if they can receive government-sanctioned monopolies (i.e., patents) to exploit commercial embodiments of their inven- tions. Having the exclusive right to sell an invention can be tremendously lu- crative.125
The Patent Act, which here refers to United States patent law as a whole, provides at least a couple of challenges to computers qualifying as inventors under the Patent and Copyright Clause.126 First, as previously mentioned, the Patent Act requires that inventors be “individuals.”127 This language has been in place since at least the passage of legislation in 1952 that established the basic structure of modern patent law.128 The “individual” requirement likely was included to reflect the constitutional language that specifically gives “in-
122 U.S. CONST. art. I, § 8, cl. 8. This clause is also sometimes referred to as the “Patent Clause” or the “Copyright Clause.”
123 Id.
124 See Mark A. Lemley, Ex Ante Versus Ex Post Justifications for Intellectual Property, 71 U. CHI. L. REV. 129, 129 (2004) (“The standard justification for intellectual property is ex ante . . . . It is the prospect of the intellectual property right that spurs creative incentives.”).
125 See JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY WITH SOME OF THEIR APPLI- CATIONS TO SOCIAL PHILOSOPHY 563 (Prometheus Books 2004) (1872) (noting that under a patent system, “the greater the usefulness, the greater the reward”).
126 Legislation pertaining to patents is found under Title 35 of the United States Code. The Patent Act may also be used to refer to specific pieces of legislation ranging from the Patent Act of 1790, the first patent law passed by the federal government, to the Patent Act of 1952. Pub. L. No. 82-593, 66 Stat. 792 (1952).
127 E.g., 35 U.S.C. § 100(f) (“The term ‘inventor’ means the individual or, if a joint invention, the individuals collectively who invented or discovered the subject matter of the invention.”). The same issues surrounding computer inventorship may not exist outside of the U.S. where applications do not require a named inventor. See MPEP, supra note 43, § 2137.01 (“The requirement that the applicant for a patent in an application filed before September 16, 2012 be the inventor(s), . . . and that the in- ventor . . . be identified in applications filed on or after September 16, 2012, are characteristics of U.S. patent law not generally shared by other countries.”). For example, a patent application at the Europe- an Patent Office may be filed by “any body equivalent to a legal person by virtue of the law governing it.” Convention on the Grant of European Patents, supra note 100, at art. 58; see also 35 U.S.C. §§ 115–116.
128 Pub. L. No. 82-593, 66 Stat. 792 (1952); see also Gregory Dolin, Dubious Patent Reform, 56 B.C. L. REV. 881, 889 (2015) (discussing aims of 1952 Patent Act).
 
342
2016] Patent Generating Artificial Intelligence 1097
ventors” the right to their discoveries as opposed to other legal entities that might assert ownership rights.129 Such language would help to ensure that pa- tent rights were more likely to go to individual inventors than to corporate enti- ties where ownership was disputed.130 Legislators were not thinking about computational inventions in 1952.131 Second, patent law jurisprudence requires that inventions be the result of a “mental act.”132 So, because computers are not individuals and it is questionable that they engage in a mental act, it is unclear whether a computer autonomously conceiving of a patentable invention could legally be an inventor.
2. Avoiding Disclosure of Artificially Intelligent Inventors
Given that computers are functioning as inventors, and likely inventing at an escalating rate, it would seem that the Patent Office should be receiving an increasing number of applications claiming computers as inventors. That the Patent Office has not suggests that applicants are choosing not to disclose the role of AI in the inventive process.133 That may be due to legal uncertainties about whether an AI inventor would render an invention unpatentable.134
129 In the words of the United States Court of Appeals for the Federal Circuit, “people conceive, not companies.” New Idea Farm Equip. Corp. v. Sperry Corp., 916 F.2d 1561, 1566 n.4 (Fed. Cir. 1990).
130 Now under the America Invents Act (“AIA”), a corporate entity can apply for a patent on behalf of an inventor who is under an assignment obligation. MPEP, supra note 43, § 325.
131 See Karl F. Milde, Jr., Can a Computer Be an “Author” or an “Inventor”?, 51 J. PAT. OFF. SOC’Y 378, 379 (1969). As one commentator notes:
The closest that the Patent Statute comes to requiring that a patentee be an actual person is in the use, in Section 101, of the term “whoever.” Here too, it is clear from the ab- sence of any further qualifying statements that the Congress, in considering the statute in 1952, simply overlooked the possibility that a machine could ever become an inven- tor.
Id.; see also, e.g., A.M. Turing, Computing Machinery and Intelligence, 59 MIND 433, 433–51 (1950) [hereinafter Turing, Computing Machinery and Intelligence].
132 Conception has been defined as “the complete performance of the mental part of the inventive art,” and it is “the formation in the mind of the inventor of a definite and permanent idea of the com- plete and operative invention as it is thereafter to be applied in practice.” Townsend, 36 F.2d at 295.
133 See supra note 5 and accompanying text. The discussion in note 5 infers that the Patent Office has not received applications claiming computers as inventors because they have no policy or guid- ance on the subject, they do not seem to have ever addressed the issue in any publication, and because computer inventorship does not seem to have been at issue in any patent litigation.
134 See, e.g., Dane E. Johnson, Statute of Anne-imals: Should Copyright Protect Sentient Nonhu- man Creators?,15 ANIMAL L. 15, 23 (2008) (quoting one Copyright Office employee who explained that “[as] a practical matter[,] the Copyright Office would not register [a computer’s own] work if its origins were accurately represented on the copyright application. The computer program itself would be registerable if it met the normal standards for computer programs, but not the computer-generated literary work.”) Despite this policy and the Copyright Office’s Compendium guidelines, numerous computer-authored works have been registered. See, e.g.,William T. Ralston, Copyright in Computer- Composed Music: Hal Meets Handel, 52 J. COPYRIGHT SOC’Y OF THE U.S.A. 281, 283 (2004) (noting
 
343
1098 Boston College Law Review [Vol. 57:1079
Without a legal inventor, new inventions would not be eligible for patent pro- tection and would enter the public domain after being disclosed.135
There is another reason why computers might not be acknowledged: a person can qualify as an inventor simply by being the first individual to recog- nize and appreciate an existing invention.136 That is to say, someone can dis- cover rather than create an invention. Uncertainty (and accident) is often part of the inventive process.137 In such cases, an individual need only understand the importance of an invention to qualify as its inventor.138 For the purposes of this Article, assuming that a computer cannot be an inventor, individuals who subsequently “discover” computational inventions by mentally recognizing and appreciating their significance would likely qualify as inventors. So, it may be the case that computational inventions are only patentable when an individual subsequently discovers them.
II. IN SUPPORT OF COMPUTERINVENTORS
This Part examines the law regarding non-human authorship of copy- rightable material.139 It discusses the history of the Copyright Office’s Human Authorship Requirement.140 This Part also scrutinizes case law interpreting the Patent and Copyright Clause.141 On the basis of this analysis and principles of dynamic statutory interpretation, this Part argues that computers should qualify as legal inventors.142 This would incentivize the development of creative ma-
one computer-authored volume of poetry registered to a computer author, “Racter,” but still not ex- plicitly disclosed to be a computer). In 1993, Scott French programmed a computer to write in the style of a famous author, and the resulting work was registered as an “original and computer aided text.” Tal Vigderson, Comment, Hamlet II: The Sequel? The Rights of Authors vs. Computer- Generated “Read-Alike” Works, 28 LOY. L.A. L. REV. 401, 402–03 (1994). The novel was apparently terrible. See Patricia Holt, Sunday Review, S.F. CHRON., Aug. 15, 1993, B4 (“[t]he result is a mitigat- ed disaster”).
135 See MPEP, supra note 43, § 2137.
136 Conception requires contemporaneous recognition and appreciation of the invention. See Invi- trogen Corp. v. Clontech Labs., Inc., 429 F.3d 1052, 1064 (Fed. Cir. 2005) (noting that the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue); see also, e.g., Silvestri v. Grant, 496 F.2d 593, 597 (C.C.P.A. 1974) (“[A]n accidental and unappreciated duplication of an invention does not defeat the patent right of one who, though later in time, was the first to recognize that which constitutes the inventive subject matter.”).
137 For instance, Alexander Fleming discovered penicillin in a mold that had contaminated his sam- ples of Staphylococcus. Howard Markel, The Real Story Behind Penicillin, PBS (Sep. 27, 2013), http://www.pbs.org/newshour/rundown/the-real-story-behind-the-worlds-first-antibiotic/ [https://perma. cc/V6SM-2QJL].
138 See Silvestri, 496 F.2d at 597.
139 See infra notes 139–239 and accompanying text.
140 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 306.
141 U.S. CONST. art. I, § 8, cl. 8.
142 See generally Eskridge, Dynamic Statutory Interpretation, supra note 16 (discussing canons of
statutory interpretation).
 
344
2016] Patent Generating Artificial Intelligence 1099
chines consistent with the purpose and intent of the Founders and Congress. The requirement that inventors be individuals was designed to prevent corpo- rate ownership, and, therefore, computer inventorship should not be prohibited on this basis.143 Also, there should be no requirement for a mental act because patent law is concerned with the nature of an invention itself rather than the subjective mental process by which an invention may have been achieved.144 This Part concludes by addressing objections to computer inventorship includ- ing arguments that computational inventions would develop in the absence of patent protection at non-monopoly prices.145
A. Nonhuman Authors of Copyrightable Material
The Patent Act does not directly address the issue of a computer inventor. The Patent Office has never issued guidance addressing the subject, and there appears to be no case law on the issue of whether a computer could be an in- ventor. That is the case despite the fact that the Patent Office appears to have already granted patents for inventions by computers but, as previously dis- cussed, did so unknowingly.
There is, however, guidance available from the related issue of nonhuman authorship of copyrightable works.146 Nonhuman authorship is not governed by statute, but there is interesting case law on the subject. Also, since at least 1984 the Copyright Office has conditioned copyright registration on human authorship.147 In its 2014 compendium, the Copyright Office published an up- dated “Human Authorship Requirement” which states that:
To qualify as a work of “authorship” a work must be created by a human being. . . . The Office will not register works produced by nature, animals, or plants. . . . Similarly, the Office will not register
143 See infra notes 206–208 and accompanying text.
144 See, e.g., The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 86. 145 See notes 189–239 and accompanying text.
146 The issue of computer authorship (and inventorship) has been considered “since the 1960s
when people began thinking about the impact of computers on copyright.” Arthur R. Miller, Copy- right Protection for Computer Programs, Databases, and Computer-Generated Works: Is Anything New Since CONTU?, 106 HARV. L. REV. 977, 1043 (1993). Most of the literature related to computer generated works has focused on copyright rather than patent protection. “In the secondary literature on copyright, rivers of ink are spilt on” whether computers can be considered authors. MELVILLE B. NIMMER & DAVID NIMMER, NIMMER ON COPYRIGHT § 5.01[A] (LexisNexis 2015).
147 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 202.02(b). The Com- pendium of U.S. Copyright Office Practices elaborates on the “human authorship” requirement by stating: “The term ‘authorship’ implies that, for a work to be copyrightable, it must owe its origin to a human being. Materials produced solely by nature, by plants, or by animals are not copyrightable.” Id. It further elaborates on the phrase “[w]orks not originated by a human author” by stating: “In order to be entitled to copyright registration, a work must be the product of human authorship. Works pro- duced by mechanical processes or random selection without any contribution by a human author are not registrable.” Id. § 503.03(a).
 
345
1100 Boston College Law Review [Vol. 57:1079
works produced by a machine or mere mechanical process that op- erates randomly or automatically without any creative input or in- tervention from a human author.148
This policy was the result of many years of debate within the Copyright Of- fice.149
The requirement is based on jurisprudence that dates long before the in- vention of modern computers to the In re Trade-Mark Cases in 1879, in which the U.S. Supreme Court interpreted the Patent and Copyright Clause to exclude the power to regulate trademarks.150 In interpreting this clause, the Court stat- ed, in dicta, that the term “writings” may be construed liberally but noted that only writings that are “original, and are founded in the creative powers of the mind” may be protected.151
The issue of computer authorship was implicit in the Court’s celebrated case of Burrow-Giles Lithographic Co. v. Sarony in 1884.152 In that case, a lithographic company argued that a photograph of Oscar Wilde did not qualify as a “writing” or as the work of an “author.”153 The company further argued that even if a visual work could be copyrighted, that a photograph should not qualify for protection because it was just a mechanical reproduction of a natu- ral phenomenon and thus could not embody the intellectual conception of its author.154 The Court disagreed, noting that all forms of writing “by which the ideas in the mind of the author are given visible expression” were eligible for copyright protection.155 The Court stated that although ordinary photographs might not embody an author’s “idea,” in this particular instance, the photogra- pher had exercised enough control over the subject matter that it qualified as an original work of art.156 Therefore, the case explicitly addressed whether the camera’s involvement negated human authorship, and it implicitly dealt with
148 Id. § 313.2.
149 See, e.g., U.S. COPYRIGHT OFFICE, EIGHTY-SECOND ANNUAL REPORT OF THE REGISTER OF COPYRIGHTS 18 (1979) (discussing issues related to computer authorship).
150 See generally In re Trade-Mark Cases, 100 U.S. 82 (1879) (finding that the Patent and Copy- right Clause excludes regulating trademarks). Congress, which does indeed enjoy the ability to regu- late trademarks, passed the Trade Mark Act of 1881 two years after this case was decided. That Act gave Congress the authority to regulate trademarks on the basis of the Commerce Clause.
151 Id. at 94. The Court in this case held that only original works of art, which are the “fruits of intellectual labor,” may be protected under copyright law. Id. (emphasis omitted).
152 See Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53, 56 (1884). 153 Id.
154 Id. at 58–59.
155 Id. at 58.
156 Id. at 54–55. Protections for all photographs was eventually made a part of the statutory scheme for copyright protection. 17 U.S.C. § 106A (2012). In the words of Judge Learned Hand, “no photograph, however simple, can be unaffected by the personal influence of the author, and no two will be absolutely alike.” Jewelers’ Circular Pub. Co. v. Keystone Pub. Co., 274 F. 932, 934 (S.D.N.Y. 1921), aff’d, 281 F. 83 (2d Cir. 1922).
 
346
2016] Patent Generating Artificial Intelligence 1101
the question of whether a camera could be considered an author. Though it seems unwise to put much emphasis on dicta from more than a century ago to resolve the question of whether nonhumans could be authors, the Copyright Office cites Burrow-Giles in support of its Human Authorship Requirement.157
The Copyright Office first addressed the issue of computer authors in 1966 when the Register of Copyrights, Abraham Kaminstein, questioned whether computer-generated works should be copyrightable.158 Mr. Kaminstein reported that, in 1965, the Copyright Office had received applications for com- puter-generated works including: an abstract drawing, a musical composition, and compilations that were, at least partly, the work of computers.159 Mr. Ka- minstein did not announce a policy for dealing with such applications but sug- gested the relevant issue should be whether a computer was merely an assist- ing instrument (as with the camera in Burrow-Giles) or whether a computer conceived and executed the traditional elements of authorship.160
In the following years, the Copyright Office struggled with how to deal with computers more broadly.161 At that time, copyright law did not even ad- dress the issue of whether computer software should be copyrightable—a far more urgent and financially important problem.162
In 1974, Congress created the Commission on New Technological Uses of Copyrighted Works (“CONTU”) to study issues related to copyright and computer-related works.163 With regards to computer authorship, CONTU wrote in 1979 that there was no need for special treatment of computer- generated works because computers were not autonomously generating crea- tive results without human intervention; computers were simply functioning as tools to assist human authors.164 CONTU also declared that autonomously cre- ative AI was not immediately foreseeable.165 The Commission unanimously concluded that “[w]orks created by the use of computers should be afforded copyright protection if they are original works of authorship within the Act of 1976.”166 According to the Commission, “the author is [the] one who employs
157 See COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 306.
158 See U.S. COPYRIGHT OFFICE, SIXTY-EIGHTH ANN. REP. REG. COPYRIGHTS 4–5 (1966).
159 Id.
160 See id.
161 See Act of Dec. 31, 1974, Pub. L. No. 93-573, § 201, 88 Stat. 1873, 1873–74; see also H.R.
REP. NO. 1476, 94th Cong., 2d Sess. 116 (1976), reprinted in 1976 U.S.C.C.A.N. 5731, 5731 (dis- cussing issues regarding computers and copyrights). These issues had not been addressed in the 1974 Copyright Act. Pub. L. No. 94-553, § 117, 90 Stat. 2565 (1976), repealed by Computer Software Protection Act, Pub. L. No. 96-517, § 117, 94 Stat. 3028 (1980) (codified at 17 U.S.C. § 117 (1988)).
162 See Act of Dec. 31, 1974 § 201.
163 Id. § 201(a)–(b).
164 See NAT’L COMM’N ON NEW TECH. USES OF COPYRIGHTED WORKS, FINAL REPORT ON NEW
TECHNOLOGICAL USES OF COPYRIGHTED WORKS 44 (1979). 165 See id.
166 Id. at 1.
 
347
1102 Boston College Law Review [Vol. 57:1079
the computer.”167 Former CONTU Commissioner Arthur Miller explained that “CONTU did not attempt to determine whether a computer work generated with little or no human involvement is copyrightable.”168 Congress subsequent- ly codified CONTU’s recommendations.169
Nearly a decade later, in 1986, advances in computing prompted the U.S. Congress’s Office of Technology Assessment (“OTA”) to issue a report argu- ing that CONTU’s approach was too simplistic and computer programs were more than “inert tools of creation.”170 The OTA report contended that, in many cases, computers were at least “co-creators.”171 The OTA did not dispute that computer-generated works should be copyrightable, but it did foresee prob- lems with determining authorship.172
The 2014 iteration of the Human Authorship Requirement was partially the result of a prominent public discourse about nonhuman authorship stem- ming from the “Monkey Selfies.”173 The Monkey Selfies are a series of images that a Celebes crested macaque took of itself in 2011 using equipment belong- ing to the nature photographer David Slater.174 Mr. Slater reports that he staged the photographs by setting up a camera on a tripod and leaving a remote trig- ger for the macaque to use.175 He subsequently licensed the photographs,
167 Id. at 45. This rule is largely similar in British law: “In the case of a literary, dramatic, musical or artistic work which is computer-generated, the author shall be taken to be the person by whom the arrangements necessary for the creation of the work are undertaken.” Copyright, Designs and Patent Act 1988, c. 48 § 9(3) (UK). “‘Computer-generated,’ in relation to a work, means that the work is generated by computer in circumstances such that there is no human author of the work.” Id. § 178.
168 Miller, supra note 146, at 1070. Professor Miller continued to argue in 1993 that “computer science does not appear to have reached a point at which a machine can be considered so ‘intelligent’ that it truly is creating a copyrightable work.” Id. at 1073. Rather, “for the foreseeable future, the copyrightability of otherwise eligible computer-generated works can be sustained because of the sig- nificant human element in their creation, even though there may be some difficulty is assigning au- thorship.” Id.
169 See 17 U.S.C. § 117.
170 See U.S. CONGRESS, OFFICE OF TECH. ASSESSMENT, INTELLECTUAL PROPERTY RIGHTS IN AN AGE OF ELECTRONICS AND INFORMATION 70–73 (1986). As stated by the OTA:
Courts will then be left with little guidance, and even less expertise, to solve these high- ly complex conceptual and technological issues. . . . [E]ither the legislature or the courts will have to confront some questions that will be very difficult to resolve under the pre- sent system. These include: . . . What of originality in works that are predominately au- tomated? Who is the author? Providing answers to these questions will become more urgent as creative activities continue to fuse with machine intelligence.
Id. at 71–73.
171 Id. at 72.
172 Id. at 73.
173 See Naruto v. Slater, No. 3:2015-cv-04324, 2016 WL 362231, *1 (N.D. Cal. Jan. 23, 2016). 174 Id. at *1.
175 See Sulawesi Macaques, DJS PHOTOGRAPHY, http://www.djsphotography.co.uk/original_
story.html [https://perma.cc/H93K-8CB9] (last visited Jan. 26, 2016) (showing Mr. Slater’s photo- graphs and providing an overview of how he staged them). The claim by Mr. Slater that he engineered the shoot is controversial based on his earlier reports of the event in question. See Mike Masnick,
 
348
2016] Patent Generating Artificial Intelligence 1103
claiming he owned their copyright.176 Other parties then reposted the photo- graphs without his permission and over his objections, asserting that he could not copyright the images without having taken them directly.177 On December 22, 2014, the Copyright Office published its Human Authorship Requirement, which specifically lists the example of a photograph taken by a monkey as something not protectable.178
In September 2015, People for the Ethical Treatment of Animals (“PETA”) filed a copyright infringement suit against Mr. Slater on behalf of Naruto, the monkey it purports took the Monkey Selfies, asserting that Naruto was entitled to copyright ownership.179 On January 28, 2016, U.S. District Judge William H. Orrick III dismissed PETA’s lawsuit against Slater.180 Judge Orrick reasoned that the issue of the ability for animals to obtain a copyright is “an issue for Congress and the President.”181 The case is currently under appeal in the Ninth Circuit.182
B. Computers Should Qualify as Legal Inventors
1. Arguments Supporting Computer Inventors
Preventing patents on computational inventions by prohibiting computer inventors, or allowing such patents only by permitting humans who have dis- covered the work of creative machines to be inventors, is not an optimal sys- tem. In the latter case, AI may be functioning more or less independently, and it is only sometimes the case that substantial insight is needed to identify and understand a computational invention. Imagine that Person C instructs their AI to develop an iPhone battery with twice the standard battery life and gives it some publically available battery schematics. The AI could produce results in the form of a report titled “Design for Improved iPhone Battery”—complete with schematics and potentially even pre-formatted as a patent application. It seems inefficient and unfair to reward C for recognizing the AI’s invention when C has not contributed significantly to the innovative process.
Photographer David Slater Claims That Because He Thought Monkeys Might Take Pictures, Copy- right Is His, TECHDIRT (July 15, 2011), https://www.techdirt.com/articles/20110714/16440915097/ photographer-david-slater-claims-that-because-he-thought-monkeys-might-take-pictures-copyright-is- his.shtml [https://perma.cc/MA7S-PFJ9].
176 See Naruto, 2016 WL 362231, at *1.
177 See Masnick, supra note 175.
178 COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES, supra note 14, § 313.2.
179 See Naruto, 2016 WL 362231, at *1.
180 Id.
181 See id.; Beth Winegarner, ‘Monkey Selfie’ Judge Says Animals Can’t Sue Over Copyright,
LAW 360 (Jan. 6, 2016), https://www.cooley.com/files/‘MonkeySelfie’JudgeSaysAnimalsCan’tSue OverCopyright.pdf [https://perma.cc/2CUG-2JDT].
182 See generally Opening Brief of Plaintiff-Appellant, Naruto v. Slater, No. 3:15-cv-04324 (9th Cir. July 28, 2016) (arguing for the appeal of the district court’s decision).
 
349
1104 Boston College Law Review [Vol. 57:1079
Such a system might also create logistical problems. If C had created an improved iPhone battery as a human inventor, C would be its inventor regard- less of whether anyone subsequently understood or recognized the invention. If C instructed C’s AI to develop an improved iPhone battery, the first person to notice and appreciate the AI’s result could become its inventor (and prevent C from being an inventor). One could imagine this creating a host of problems: the first person to recognize a patentable result might be an intern at a large research corporation or a visitor in someone’s home. A large number of indi- viduals might also concurrently recognize a result if access to an AI is wide- spread.
More ambitiously, treating computational inventions as patentable and recognizing creative computers as inventors would be consistent with the Con- stitutional rationale for patent protection.183 It would encourage innovation un- der an incentive theory. Patents on computational inventions would have sub- stantial value independent of the value of creative computers; allowing com- puters to be listed as inventors would reward human creative activity upstream from the computer’s inventive act. Although AI would not be motivated to in- vent by the prospect of a patent, it would motivate computer scientists to de- velop creative machines. Financial incentives may be particularly important for the development of creative computers because producing such software is resource intensive.184 Though the impetus to develop creative AI might still exist if computational inventions were considered patentable but computers could not be inventors, the incentives would be weaker owing to the logistical, fairness, and efficiency problems such a situation would create.
There are other benefits to patents beyond providing an ex ante innova- tion incentive. Permitting computer inventors and patents on computational inventions might also promote disclosure and commercialization.185 Without the ability to obtain patent protection, owners of creative computers might choose to protect patentable inventions as trade secrets without any public dis-
183 See U.S. CONST. art. I, § 8, cl. 8. Among those addressing the patentability implications of computational invention, Ralph Clifford has argued that works generated autonomously by computers should remain in the public domain unless AI develops a consciousness that allows it to respond to the Copyright Act’s incentives. See Clifford, supra note 4, at 1702–03; see also Liza Vertinsky & Todd M. Rice, Thinking About Thinking Machines: Implications of Machine Inventors for Patent Law, 8 B.U. J. SCI. & TECH. L. 574, 581 (2002). Colin R. Davies has argued more recently that a computer should be given legal recognition as an individual under UK law to allow proper attribution of author- ship and to allow respective claims to be negotiated through contract. See generally Colin R. Davies, An Evolutionary Step in Intellectual Property Rights—Artificial Intelligence and Intellectual Property, 27 COMPUT. L. & SEC. REV. 601 (2011).
184 See, e.g., Ferrucci et al., supra note 82, at 59 (stating that Watson’s creation required “three years of intense research and development by a core team of about 20 researchers”).
185 See, e.g., Innovation’s Golden Goose, THE ECONOMIST, Dec. 12, 2002, at 3 (discussing the increase in innovation after the Bayh-Dole Act of 1980 because the legislation providing inventors an incentive to disclose and commercialize their ideas).
 
350
2016] Patent Generating Artificial Intelligence 1105
closure.186 Likewise, businesses might be unable to develop patentable inven- tions into commercial products without patent protection.187 In the pharmaceu- tical and biotechnology industries, for example, the vast majority of expense in commercializing a new product is incurred after the product is invented during the clinical testing process required to obtain regulatory approval for market- ing.188
2. Arguments Against Computer Inventors
Those arguments reflect the dominant narrative justifying the grant of in- tellectual property protection.189 That account, however, has been criticized, particularly by academics.190 Patents result in significant social costs by estab- lishing monopolies.191 Patents also can stifle entry by new ventures by creating barriers to subsequent research.192 Whether the benefit of patents as an innova- tion incentive outweighs their anti-competitive costs, or for that matter, wheth- er patents even have a net positive effect on innovation, likely varies between industries, areas of scientific research, and inventive entities.193
186 See, e.g., Festo Corp. v. Shoketsu Kinzoku Kogyo Kabushiki Co., 535 U.S. 722, 736 (2002) (“[E]xclusive patent rights are given in exchange for disclosing the invention to the public.”).
187 Commercialization theory holds that patents are important in providing incentives for invest- ment in increasing the value of a patented technology. See Edmund W. Kitch, The Nature and Func- tion of the Patent System, 20 J.L. & ECON. 265, 276–77 (1977).
188 See TUFTS CTR. FOR THE STUDY OF DRUG DEV., Briefing: Cost of Developing a New Drug (Nov. 18, 2014), http://csdd.tufts.edu/files/uploads/Tufts_CSDD_briefing_on_RD_cost_study_- _Nov_18,_2014..pdf (estimating that pre-human expenditures are 30.8% of costs per approved com- pound, and estimating average pre-tax industry cost per new prescription drug approval [inclusive of failures and capital costs] is $2.55 billion). The cost of new prescription drug approval is hotly con- tested. See, e.g., Roger Collier, Drug Development Cost Estimates Hard to Swallow, 180 CANADIAN MED. ASS’N J. 279, 279 (2009).
189 See Jeanne C. Fromer, Expressive Incentive in Intellectual Property, 98 VA. L. REV. 1745, 1746 (2012).
190 See generally, e.g., Frederick M. Abbott, The Doha Declaration on the TRIPS Agreement and Public Health: Lighting a Dark Corner at the WTO, 5 J. INT’L ECON L. 469 (2002) (discussing prob- lems with a pure incentive theory for patents in the medicines context).
191 See Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents–Prizes Debate, 92 TEX. L. REV. 303, 314–15 (2013) (discussing the deadweight loss of monopoly).
192 See Lisa Larrimore Ouellette, Access to Bio-Knowledge: From Gene Patents to Biomedical Materials, 2010 STAN. TECH. L. REV. 48, 3 at n. 1 (considering effects of patents on entry to the bio- medical products market); Arti Kaur Rai, Regulating Scientific Research: Intellectual Property Rights and the Norms of Science, 94 NW. U. L. REV. 77, 133 (1999); see also Bhaven Sampat & Heidi L. Williams, How Do Patents Affect Follow-on Innovation? Evidence from the Human Genome 15 (Oct. 13, 2015) (unpublished manuscript), available at http://economics.mit.edu/files/10782 [https:// perma.cc/5K7N-89C4] (discussing patents to entry created by patents).
193 As discussed above, the need for patent incentives is particularly compelling in the pharma- ceutical context where large investments in clinical research over several years are typically needed to commercialize products that often are inexpensive for competitors to replicate. See Benjamin N. Roin, Unpatentable Drugs and the Standards of Patentability, 87 TEX. L. REV. 503, 545–47 (2009).
 
351
1106 Boston College Law Review [Vol. 57:1079
For instance, commentators such as Judge Richard Posner have argued that patents may not be needed to incentivize R&D in the software industry.194 Software innovation is often relatively inexpensive, incremental, quickly su- perseded, produced without patent incentives, protected by other forms of in- tellectual property, and associated with a significant first mover advantage.195 Likewise, patents may be unnecessary to spur innovation in university settings where inventors are motivated to publish their results for prestige and the pro- spect of academic advancement.196
Computational inventions may develop due to non-patent incentives. Software developers have all sorts of non-economic motivations to build crea- tive computers: for example, to enhance their reputations, satisfy scientific cu- riosity, or collaborate with peers.197 Business ventures might find the value of computational inventions exceeds the cost of developing creative computers even in the absence of patent protection. Of course, computational invention patents may not be an all-or-nothing proposition; they may further encourage activities that would have otherwise occurred on a smaller scale over a longer timeframe. If patents are not needed to incentivize the development of creative computers, it may be justifiable to treat computational inventions as unpatent- able and failing to recognize computer inventors. Yet, whether patents produce a net benefit as an empirical matter is difficult to determine a priori. Even though individuals and businesses do not always behave as rational economic actors, in the aggregate, it is likely that providing additional financial incen- tives to spur the development of creative computers will produce a net bene- fit.198
Patents for computational inventions might also be opposed on the grounds that they would chill future human innovation, reward human inven-
194 See WILLIAM M. LANDES & RICHARD A. POSNER, THE ECONOMIC STRUCTURE OF INTELLEC- TUAL PROPERTY LAW 312–13 (2003).
195 See id.; see also Eric Goldman, The Problems with Software Patents, FORBES (Nov. 28, 2012), http://www.forbes.com/sites/ericgoldman/2012/11/28/the-problems-with-software-patents/#234ba 3d66545 [https://web.archive.org/web/20160412114510/http://www.forbes.com/sites/ericgoldman/ 2012/11/28/the-problems-with-software-patents/#41a0c38b2a70] (discussing in a three-part series why patents may be unnecessary for software, challenges to fixing the problems, and exploring possi- ble fixes).
196 See Mark A. Lemley, Are Universities Patent Trolls?, 18 FORDHAM INTELL. PROP. MEDIA & ENT. L.J. 611, 621 (2008).
197 See YOCHAI BENKLER, THE WEALTH OF NETWORKS 65 (2006). Further, behavior law and economics posits that actual people do not act in accordance with standard economic principles be- cause they have limited rationality, willpower, and self-interest. See Christine Jolls, Cass R. Sunstein, & Richard Thaler, A Behavioral Approach to Law and Economics, 50 STAN. L. REV. 1471, 1476 (1998).
198 See, e.g., GARY S. BECKER, THE ECONOMIC APPROACH TO HUMAN BEHAVIOR 14 (1978) (“[A]ll human behavior can be viewed as involving participants who [1] maximize their utility [2] from a stable set of preferences and [3] accumulate an optimal amount of information and other inputs in a variety of markets.”).
 
352
2016] Patent Generating Artificial Intelligence 1107
tors who failed to contribute to the inventive process, and result in further con- solidation of intellectual property in the hands of big business (assuming that businesses such as IBM will be the most likely to own creative computers).199
Other non-utilitarian patent policies do not appear to support computer inventorship. For example, courts have justified granting patent monopolies on the basis of Labor Theory, which holds that a person has a natural right to the fruits of their work.200 Labor Theory may support giving a patent to someone who has worked for years to invent a new device so that they can profit from their invention, but it does not apply to computers because computers cannot own property. All computer work is appropriated. Similarly, Personality Theo- ry, which holds that innovation is performed to fulfill a human need, would not apply to AI.201 Creative computers invent because they are instructed to invent, and a machine would not be offended by the manner in which its inventions were used. AI might even be a concerning recipient for inventorship under So- cial Planning Theory, which holds that patent rights should be utilized to pro- mote cultural goals.202 An AI could develop immoral new technologies.203 Submissions, however, are no longer rejected by the Patent Office for being “deceitful” or “immoral,” and, to the extent this is a concern, there would be opportunities for a person to judge the morality of an application before it is granted.204
199 See generally Jamie Carter, The Most Powerful Supercomputers in the World—and What They Do, TECHRADAR (Dec. 13, 2014), http://www.techradar.com/us/news/computing/the-most-powerful- supercomputers-in-the-world-and-what-they-do-1276865 [https://perma.cc/AZ94-H3B2] (noting that most advanced computer systems are owned by governments and large businesses).
200 See William Fisher, Theories of Intellectual Property, in NEW ESSAYS IN THE LEGAL AND POLITICAL THEORY OF PROPERTY 173–74 (Stephen Munzer ed., 2001).
201 Tom G. Palmer, Are Patents and Copyrights Morally Justified? The Philosophy of Property Rights and Ideal Objects, 13 HARV. J.L. & PUB. POL’Y 817, 835–36 (1990).
202 Mohammad Amin Naser, Computer Software: Copyrights v. Patents, 8 LOY. L. & TECH. ANN. 37, 41–42 (2009).
203 Beneficial utility was once required for patent grant such that “deceitful” or “immoral” inven- tions would not qualify. In 1999, The United States Court of Appeals for the Federal Circuit in Juicy Whip, Inc. v. Orange Bang, Inc., stated:
[Y]ears ago courts invalidated patents on gambling devices on the ground that they were immoral, . . . but that is no longer the law . . . . “Congress never intended that the patent laws should displace the police powers of the States, meaning by that term those powers by which the health, good order, peace and general welfare of the community are promoted”. . . . [W]e find no basis in section 101 to hold that inventions can be ruled unpatentable for lack of utility simply because they have the capacity to fool some members of the public.
185 F.3d 1364, 1367–68 (Fed. Cir. 1999) (quoting Webber v. Virginia, 103 U.S. 344, 347–48 (1880))
204 See id. See generally Cynthia M. Ho, Splicing Morality and Patent Law: Issues Arising from Mixing Mice and Men, 2 WASH. U. J.L. & POL’Y 247, 247–85 (2000) (discussing Social Planning theory).
 
353
1108 Boston College Law Review [Vol. 57:1079
Ultimately, despite concerns, computer inventorship remains a desirable outcome. The financial motivation it will provide to build creative computers is likely to result in a net increase in the number of patentable inventions pro- duced. Particularly, while quantitative evidence is lacking about the effects of computational invention patents, courts and policy makers should be guided first and foremost by the explicit constitutional rationale for granting pa- tents.205 Further, allowing patents on computational inventions as well as com- puter inventors would do away with what is essentially a legal fiction—the idea that only a human can be the inventor of the autonomous output of a crea- tive computer—resulting in fairer and more effective incentives.
C. It Does Not Matter Whether Computers Think
1. The Questionable Mental Act Requirement
The judicial doctrine that invention involves a mental act should not pre- vent computer inventorship. The Patent Act does not mention a mental act, and courts have discussed mental activity largely from the standpoint of determin- ing when an invention is actually made not whether it is inventive. In any case, whether or not creative computers “think” or have something analogous to consciousness should be irrelevant with regards to inventorship criteria.206
To begin, the precise nature of a “mental act requirement” is unclear. Courts associating inventive activity with cognition have not been using terms precisely or meaningfully in the context of computational inventions. It is un- clear whether computers would have to engage in a process that results in crea- tive output—which they do—or whether, and to what extent, they would need to mimic human thought. If the latter, it is unclear what the purpose of such a requirement would be except to exclude nonhumans (for which a convoluted test is unnecessary). Dr. Thaler has argued eloquently that the Creativity Ma- chine closely imitates the architecture of the human brain.207 Should that mean that the Creativity Machine’s inventions should receive patents while Watson’s do not? There is a slippery slope in determining what constitutes a “thinking” computer system even leaving aside deficits in our understanding of the struc- ture and function of the human brain. Perhaps the Creativity Machine still is not engaging in mental activity—would a computer scientist have to design a completely digitized version of the human brain? Even if designing a com- pletely digitized version of the human brain was possible, it might not be the
205 See United States v. Line Material Co., 333 U.S. 287, 316 (1948) (Douglas, J., concurring) (noting “the reward to inventors is wholly secondary” to the reward to society); see also THE FEDER- ALIST NO. 43 (James Madison) (stating that social benefit arises from patents to inventors).
206 Though, it is surely a fascinating topic deserving of its own treatise. 207 Thaler, Synaptic Perturbation and Consciousness, supra note 29.
 
354
2016] Patent Generating Artificial Intelligence 1109
most effective way to structure a creative computer.208 On top of that, it would be difficult or impossible for the Patent Office and the courts to distinguish between different computers’ architectures.
2. The Turing Test and a Functionalist Approach
The problem of speaking precisely about thought with regards to comput- ers was identified by Alan Turing, one of the founders of computer science, who in 1950 considered the question, “Can machines think?”209 He found the question to be ambiguous, and the term “think” to be unscientific in its collo- quial usage.210 Turing decided the better question to address was whether an individual could tell the difference between responses from a computer and an individual; rather than asking whether machines “think,” he asked whether machines could perform in the same manner as thinking entities.211 Dr. Turing referred to his test as the “Imitation Game” though it has come to be known as the “Turing test.”212
Although the Turing test has been the subject of criticism by some com- puter scientists, Turing’s analysis from more than sixty years ago demonstrates that a mental act requirement would be ambiguous, challenging to administer, and of uncertain utility.213 Incidentally, it is noteworthy that the Patent Office administers a sort of Turing test, which creative computers have successfully passed. The Patent Office receives descriptions of inventions then judges whether they are nonobvious—which is a measure of creativity and ingenui- ty.214 In the case of the Invention Machine’s Patent, it was already noted that “January 25, 2005 looms large in the history of computer science as the day that genetic programming passed its first real Turing test: The examiner had no idea that he was looking at the intellectual property of a computer.”215 In an-
208 This is analogous to one of the criticisms of the Turing test. Namely, that mimicking human responses may not be the best test of intelligence given that not all human responses are intelligent. See Editorial, Artificial Stupidity, THE ECONOMIST, Aug. 1, 1992, at 14.
209 Turing, Computing Machinery and Intelligence, supra note 131, at 433. “Nobody so far has been able to give a precise, verifiable definition of what general intelligence or thinking is. The only definition I know that, though limited, can be practically used is Alan Turing’s. With his test, Turing provided an operational definition of a specific form of thinking—human intelligence.” Tomaso Poggio,“Turing+”Questions,inWHATTOTHINKABOUTMACHINESTHATTHINK48(JohnBrockman ed., 2015).
210 See Turing, Computing Machinery and Intelligence, supra note 131, at 433.
211 See id. at 433–34.
212 See id. at 433.
213 See, e.g., Jose Hernandez-Orallo, Beyond the Turing Test, 9 J. LOGIC LANGUAGE & INFO. 447,
447 (2000).
214 See Koza et al., Evolving Inventions, supra note 49, at 59. The Patent Office “receives written
descriptions of inventions and then judges whether they are nonobvious,” which is a measure of crea- tivity and ingenuity. See id.
215 Keats, John Koza Has Built an Invention Machine, supra note 57.
 
355
1110 Boston College Law Review [Vol. 57:1079
other sense, GP had already also passed the test by independently recreating previously patented inventions: because the original human invention received a patent, the AI’s invention should have received a patent as well, leaving aside that the original patent would be prior art not relied upon by the GP.216
3. The Invention Matters, Not the Inventor’s Mental Process
The primary reason a mental act requirement should not prevent computer inventorship is that the patent system should be indifferent to the means by which invention comes about.
Congress came to this conclusion in 1952 when it abolished the Flash of Genius doctrine.217 That doctrine had been used by the Federal Courts as a test for patentability for over a decade.218 It held that in order to be patentable, a new device, “however useful it may be, must reveal the flash of creative geni- us, not merely the skill of the calling.”219 The doctrine was interpreted to mean that an invention must come into the mind of an inventor in a “flash of genius” rather than as a “result of long toil and experimentation.”220 As a commentator at the time noted, “the standard of patentable invention represented by [the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.”221
The Flash of Genius test was an unhelpful doctrine because it was vague, difficult for lower courts to interpret, involved judges making subjective deci- sions about a patentee’s state of mind, and made it substantially more difficult
216 See id.
217 See 35 U.S.C. § 103 (2012).
218 See, e.g., Hamilton Standard Propeller Co. v. Fay-Egan Mfg. Co., 101 F.2d 614, 617 (6th Cir.
1939) (“The patentee did not display any flash of genius, inspiration or imagination . . . .”). The doc- trine was formalized by the Supreme Court in 1941 in Cuno Engineering Corp. v. Automatic Devices Corp. 314 U.S. 84, 91 (1941). It was reaffirmed by the Court in 1950 in Great Atlantic & Pacific Tea Co. v. Supermarket Equipment Corp., 340 U.S. 147, 154 (1950) (Douglas, J., concurring).
219 Cuno Eng’g Corp., 314 U.S. at 91.
220 The Supreme Court later claimed the “Flash of Creative Genius” language was just a rhetori- cal embellishment and that requirement concerned the device not the manner of invention. Graham v. John Deere Co. of Kan. City, 383 U.S. 1, 15 n.7, 16 n.8 (1966). That was not, however, how the test was interpreted. See P.J. Federico, Origins of Section 103, 5 APLA Q.J. 87, 97 n.5 (1977) (noting the test led to a higher standard of invention in the lower courts). When Congress abolished the test, Con- gress noted it should be immaterial whether invention was made “from long toil and experimentation or from a flash of genius.” 35 U.S.C. § 103. Further, the Court stated in 1966 in Graham that “[t]he second sentence states that patentability as to this requirement is not to be negatived by the manner in which the invention was made, that is, it is immaterial whether it resulted from long toil and experi- mentation or from a flash of genius.” Graham, 383 U.S. at 16 n.8.
221 The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 87.
 
356
2016] Patent Generating Artificial Intelligence 1111
to obtain a patent.222 The test was part of a general hostility toward patents ex- hibited by mid-twentieth century courts, a hostility that caused United States Supreme Court Justice Robert Jackson to note in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.”223
Criticism of this state of affairs led President Roosevelt to establish a Na- tional Patent Planning Commission to study the patent system and to make recommendations for its improvement.224 In 1943, the Commission reported with regard to the Flash of Genius doctrine that “patentability shall be deter- mined objectively by the nature of the contribution to the advancement of the art, and not subjectively by the nature of the process by which the invention may have been accomplished.”225 Adopting this recommendation, the Patent Act of 1952 legislatively disavowed the Flash of Genius test.226 In the same manner, patentability of computational inventions should be based on the in- ventiveness of a computer’s output rather than on a clumsy anthropomorphism because, like Turing, patent law should be interested in a functionalist solution.
4. A Biological Requirement Would Be a Poor Test
Incidentally, even a requirement for biological intelligence might be a bad way to distinguish between computer and human inventors. Although function- ing biological computers do not yet exist, all of the necessary building blocks have been created.227 In 2013, a team of Stanford University engineers created
222 See DePaul College of Law, Patent Law—“Flash of Genius” Test for Invention Rejected, 5 DEPAUL L. REV. 144, 146 (1955); Stephen G. Kalinchak, Obviousness and the Doctrine of Equiva- lents in Patent Law: Striving for Objective Criteria, 43 CATH. U. L. REV. 577, 586 (1994).
223 Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
224 See William Jarratt, U.S. National Patent Planning Commission, 153 NATURE 12, 14 (1944). 225 The “Flash of Genius” Standard of Patentable Invention, supra note 18, at 85 (internal quota-
tion marks omitted).
226 See 35 U.S.C. § 103 (2012). Further, in Graham, the Supreme Court noted that “[i]t . . . seems
apparent that Congress intended by the last sentence of § 103 to abolish the test it believed this Court announced in the controversial phrase ‘flash of creative genius,’ used in Cuno Engineering.” Graham, 383 U.S. at 15.
227 See Sebastian Anthony, Stanford Creates Biological Transistors, the Final Step Towards Com- puters Inside Living Cells, EXTREMETECH (Mar. 29, 2013), http://www.extremetech.com/extreme/ 152074-stanford-creates-biological-transistors-the-final-step-towards-computers-inside-living-cells [https://perma.cc/ENX4-WZKA] (noting that, in addition to biological transistors, a method for data storage and a means of connecting transcriptors with memory would be necessary to create a biological computer and stating that “[f]ortunately, as we’ve covered a few times before, numerous research groups have successfully stored data in DNA—and Stanford has already developed an ingenious method of using the M13 virus to transmit strands of DNA between cells”); see also Monica E. Ortiz & Drew Endy, Engineered Cell-Cell Communication via DNA Messaging, J. BIOLOGICAL ENGINEERING (Dec. 1, 2012), https://jbioleng.biomedcentral.com/articles/10.1186/1754-1611-6-16 [https://perma.cc/6BWZ-GUUP]; Katherine Deria, Biological Supercomputer Can Solve Complex Problems Using Less Energy, TECH TIMES (Feb. 27, 2016), http://www.techtimes.com/articles/137017/20160227/biological-supercomputer- can-solve-complex-problems-using- less-energy.htm [https://perma.cc/A75T-3TVV] (describing a new biological supercomputer model powered by a biochemical that facilitates energy transfer among cells).
 
357
1112 Boston College Law Review [Vol. 57:1079
a biological version of an electrical transistor. Mechanical computers use nu- merous silicon transistors to control the flow of electrons along a circuit to cre- ate binary code.228 The Stanford group created a biological version with the same functionality by using enzymes to control the flow of RNA proteins along a strand of DNA.229 Envisioning a not-too-distant future in which com- puters can be entirely biological, there seems to be no principled reason why a biological, but not a mechanical version, of Watson should qualify as an inven- tor. In the event that policymakers decide computers should not be inventors, a rule explicitly barring nonhuman inventorship would be a better way to achieve that result.
D. Computer Inventors Are Permitted Under a Dynamic Interpretation of Current Law
Whether a computer can be an inventor in a constitutional sense is a ques- tion of first impression. If creative computers should be inventors, as this Arti- cle has argued, then a dynamic interpretation of the law should allow computer inventorship.230 Such an approach would be consistent with the Founders’ in- tent in enacting the Patent and Copyright Clause, and it would interpret the Patent Act to further that purpose.231 Nor would such an interpretation run afoul of the chief objection to dynamic statutory interpretation, namely that it interferes with reliance and predictability and the ability of citizens “to be able to read the statute books and know their rights and duties.”232 That is because a dynamic interpretation would not upset an existing policy; permitting comput- er inventors would allow additional patent applications rather than retroactive- ly invalidate previously granted patents, and there is naturally less reliance and predictability in patent law than in many other fields given that it is a highly dynamic subject area that struggles to adapt to constantly changing technolo- gies.233
228 See Anthony, supra note 227.
229 See id.
230 See William N. Eskridge, Jr. & Philip P. Frickey, Statutory Interpretation as Practical Rea-
soning, 42 STAN. L. REV. 321, 324 (1990).
231 See HENRY M. HART, JR. & ALBERT M. SACKS, THE LEGAL PROCESS: BASIC PROBLEMS IN
THE MAKING AND APPLICATION OF LAW 1124 (1994); see also Abbe R. Gluck, The States as Labora- tories of Statutory Interpretation: Methodological Consensus and the New Modified Textualism, 119 YALE L.J. 1750, 1764(2010) (noting that purposivists subscribe to dynamic methods of statutory in- terpretation).
232 See Eskridge, Jr. & Frickey, supra note 230, at 340.
233 See William C. Rooklidge & W. Gerard von Hoffmann, III, Reduction to Practice, Experi- mental Use, and the “On Sale” and “Public Use” Bars to Patentability, 63 ST. JOHN’S L. REV. 1, 49– 50 (1988).
 
358
2016] Patent Generating Artificial Intelligence 1113
Other areas of patent law have been the subject of dynamic interpreta- tion.234 For example, in the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was charged with deciding whether genetically modified organisms could be patented.235 It held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress . . . would frustrate the purposes of the patent law.”236 The court noted that Congress chose expansive language to protect a broad range of patentable subject mat- ter.237
Under that reasoning, computer inventorship should not be prohibited based on statutory text designed to favor individuals over corporations. It would be particularly unwise to prohibit computer inventors on the basis of literal interpretations of texts written when computational inventions were un- foreseeable. If computer inventorship is to be prohibited, it should only be on the basis of sound public policy. Drawing another analogy from the copyright context, just as the terms “Writings” and “Authors” have been construed flexi- bly in interpreting the Patent and Copyright Clause, so too should the term “Inventors” be afforded the flexibility needed to effectuate constitutional pur- poses.238 Computational inventions may even be especially deserving of pro- tection because computational creativity may be the only means of achieving certain discoveries that require the use of tremendous amounts of data or that deviate from conventional design wisdom.239
III. IMPLICATIONS OF COMPUTERINVENTORSHIP
This Part finds that a computer’s owner should be the default assignee of any invention because this is most consistent with the rules governing owner-
234 The Supreme Court has called the section of the U.S. Code relating to patentable subject mat- ter a “dynamic provision designed to encompass new and unforeseen inventions.” J.E.M. AG Supply, Inc. v. Pioneer Hi-Bred Int’l, Inc., 534 U. S. 124, 135 (2001). The Court noted in Bilski v. Kappos that “it was once forcefully argued that until recent times, ‘well- established principles of patent law prob- ably would have prevented the issuance of a valid patent on almost any conceivable computer pro- gram.’” 561 U.S. 593, 605 (2010) (quoting Diamond v. Diehr, 450 U.S 175, 195 (1981) (Stevens, J., dissenting)). The Court, however, went on to state that “this fact does not mean that unforeseen inno- vations such as computer programs are always unpatentable.” Id. (citing Diehr, 450 U.S at 192–93 (Stevens, J., dissenting)).
235 See Diamond v. Chakrabarty, 447 U. S. 303, 317 (1980).
236 Id. at 315.
237 See id. at 316.
238 In 1973, the Supreme Court in Goldstein v. California noted that the terms “Writings” and
“Authors,” have “not been construed in their narrow literal sense but, rather, with the reach necessary to reflect the broad scope of constitutional principles.” 412 U.S. 546, 561 (1973).
239 See Jason D. Lohn, Evolvable Systems for Space Application, NASA (Nov. 24, 2003), http:// www.genetic-programming.com/c2003jasonlohn20031124talk.pdf [https://perma.cc/BWC7-UPJK]; Adam Frank, The Infinite Monkey Theorem Comes to Life, NPR (Dec. 10, 2013), http://www.npr. org/blogs/13.7/2013/12/10/249726951/the-infinite-monkey-theorem-comes-to-life [https://perma.cc/ PT5R-53GS].
 
359
1114 Boston College Law Review [Vol. 57:1079
ship of property and it would most incentivize innovation.240 Additionally, this Part suggests that where a computer’s owner, developer, and user are different entities, such parties could negotiate alternative arrangements by contract.241 Computer ownership here generally refers to software ownership, although there may be instances in which it is difficult to distinguish between hardware and software, or even to identify a software “owner.”242 This Part also exam- ines the phenomenon of automation and the displacement of human inventors by computers and finds that computational invention remains beneficial de- spite legitimate concerns.243
This Part concludes by finding that the arguments in support of com- puter inventorship apply with equal force to non-human authors.244 Allowing animals to create copyrightable material would result in more socially valuable art by creating new incentives for people to facilitate animal creativity.245 It would also provide incentives for environmental conservation.246 Lastly, this Part examines some of the implications of computer inventorship for other are- as of patent law.247
A. Computational Invention Ownership
1. Options for Default Assignment Rules
In the event that computers are recognized as patent inventors, there still remains the question of who would own these patents. Computers cannot own property, and it is safe to assume that “computer personhood” is not on the horizon.248 This presents a number of options for patent ownership (assign- ment) such as a computer’s owner (the person who owns the AI as a chattel), developer (the person who programmed the AI’s software), or user (the person giving the AI tasks).249 The developer, user, and owner may be the same per- son, or they may be different entities.
Ownership rights to computational inventions should vest in a computer’s owner because it would be most consistent with the way personal property (in-
240 See infra notes 240–312 and accompanying text.
241 See infra notes 248–255 and accompanying text.
242 See generally GOVERNMENT OFFICE FOR SCIENCE, supra note 20.
243 See infra notes 256–278 and accompanying text.
244 See infra notes 279–312 and accompanying text.
245 See infra notes 279–287 and accompanying text.
246 See infra notes 279–287 and accompanying text.
247 See infra notes 288–313 and accompanying text.
248 See generally Adam Winkler, Corporate Personhood and the Rights of Corporate Speech, 30
SEATTLE U. L. REV. 863, 863 (2007) (describing the phenomenon of “corporate personhood”).
249 There are other, less conventional, options. For instance, even if legally listed as an inventor, it might be the case that no one could own a computer’s invention and that computational inventions would automatically become part of the public domain. New legislation could also establish that own-
ership rights to computational inventions automatically vest in a government agency.
 
360
2016] Patent Generating Artificial Intelligence 1115
cluding both computers and patents) is treated in the United States and it would most incentivize computational invention.250 Assignment of computa- tional inventions to a computer’s owner could be taken as a starting point alt- hough parties would be able to contract around this default, and as computa- tional inventions become more common, negotiations over these inventions may become a standard part of contract negotiations.251
2. Owner vs. User Assignment
To see why it would be problematic to have patent ownership rights vest in a computer’s user, consider the fact that IBM has made Watson available to numerous developers without transferring Watson’s ownership.252 To the extent that Watson creates patentable results as a product of its interactions with us- ers, promoting user access should result in more innovation.
There is theoretically no limit to the number of users that Watson, as a cloneable software program, could interact with at once. If Watson invents while under the control of a non-IBM user, and the “default rule” assigns the invention to the user, IBM might be encouraged to restrict user access; in con- trast, assigning the invention to IBM would be expected to motivate IBM to further promote access. If IBM and a user were negotiating for a license to Watson, the default rule might result in a user paying IBM an additional fee for the ability to patent results or receiving a discount by sticking with the default. It may also be the case that Watson co-invents along with a user; in which case, a system of default assignment to a computer’s owner would result in both IBM and the user co-owning the resulting patent. Where creative comput- ers are not owned by large enterprises with sophisticated attorneys, it is more likely the default rule will govern the final outcome.253
250 See Annemarie Bridy, Coding Creativity: Copyright and the Artificially Intelligent Author, 2012 STAN. TECH. L. REV. 1, 1–28 (arguing that “AI authorship is readily assimilable to the current copyright framework through the work made for hire doctrine, which is a mechanism for vesting cop- yright directly in a legal person who is acknowledged not to be the author-in-fact of the work in ques- tion”).
251 See generally Ian Ayres & Robert Gertner, Filling Gaps in Incomplete Contracts: An Econom- ic Theory of Default Rules, 99 YALE L.J. 87 (1990) (discussing default rules and how they are formed in the context of contract law).
252 Bruce Upbin, IBM Opens Up Its Watson Cognitive Computer for Developers Everywhere, FORBES (Nov. 14, 2013), http://www.forbes.com/sites/bruceupbin/2013/11/14/ibm-opens-up-watson- as-a-web-service/ [https://web.archive.org/web/20160310124933/http://www.forbes.com/sites/bruce upbin/2013/11/14/ibm-opens-up-watson-as-a-web-service/#1ccc9051d0ee].
253 See, e.g., Daniel D. Barnhizer, Power, Inequality and the Bargain: The Role of Bargaining Power in the Law of Contract—Symposium Introduction, 2006 MICH. ST. L. REV. 841, 842 (describ- ing various approaches to dealing with bargaining power asymmetries).
 
361
1116 Boston College Law Review [Vol. 57:1079 3. Owner vs. Developer Assignment
Likewise, patent ownership rights should vest in a computer’s owner ra- ther than its developer. Owner assignment would provide a direct economic incentive for developers in the form of increased consumer demand for crea- tive computers. Having assignment default to developers would interfere with the transfer of personal property in the form of computers, and it would be lo- gistically challenging for developers to monitor computational inventions made by machines they no longer own.
In some instances, however, owner assignment of intellectual property (IP) rights might produce unfair results. In the movie Her, the protagonist (who is a writer) purchases an AI named Samantha that organizes his existing writings into a book, which it then submits to be published.254 It is possible that Samantha would own the copyright in the selection and arrangement of his writings and would thus have a copyright interest in the book.255 Here, owner assignment of intellectual property rights seems unappealing if there is a min- imal role played by the consumer/owner. The consumer’s role in the process might be limited to simply purchasing a creative computer and asking it to do something (where the owner is the user) or purchasing a computer and then licensing it to someone else to use creatively. Further, assigning computer in- ventions to owners might impede the development or sharing of creative ma- chines because the machine developers might want to retain the rights to the computational inventions their computers produce.
These problems are more easily resolved than problems associated with assigning intellectual property rights to developers by default. Developers could either require owners to pay them the value of a creative machine, taking into account the likelihood of those machines engaging in computational in- vention, or avoid the problem by licensing rather than selling creative comput- ers. In the case of licensing, the developer remains the owner, and the consum- er is simply a user. One might imagine a creative computer, such as the AI in Her, coming with a license agreement under which consumers prospectively assign any inventions made by the system to the licensor.
This analysis also reveals an important reason why computational inven- tion works best when the computer is the legal inventor. If computational in- ventions were treated as patentable but computers could not be inventors, then presumably the first person to recognize a computer’s invention would be the legal inventor and patent owner. That means that the computer’s user, rather than its developer or owner, would likely be the patentee as the person in a po- sition to first recognize a computational invention. To the extent this is an un-
254 HER (Annapurna Pictures 2013).
255 See RONALD B. STANDLER, COPYRIGHT FOR COMPILATIONS IN THE USA 22 (2013).
 
362
2016] Patent Generating Artificial Intelligence 1117
desirable outcome, as this Article has argued, then the best solution is to permit computer inventorship.
In sum, assigning a computer’s invention by default to the computer’s owner seems the preferred outcome, and computer owners would still be free to negotiate alternate arrangements with developers and users by contract.
B. Coexistence and Competition
1. Computers and People Will Compete in Creative Fields
“IBM has bragged to the media that Watson’s question-answering skills are good for more than annoying Alex Trebek. The company sees a future in which fields like medical diagnosis, business analytics, and tech support are automated byquestion-answering software like Watson. Just as factory jobs were eliminated in the 20th century by new assembly-line robots, [Watson’s Jeopardy competitors] were the first knowledge-industry workers put out of work by the new generation of ‘thinking’ machines. ‘Quiz show contestant’ may be the first job made redundant by Watson, but I’m sure it won’t be the last.”256
With the expansion of computers into creative domains previously occu- pied only by people, machines threaten to displace human inventors. To better understand this phenomenon, consider the following hypothetical example in- volving the field of antibody therapy.
Antibodies are small proteins made naturally by the immune system, pri- marily to identify and neutralize pathogens such as bacteria and viruses.257 They are Y-shaped proteins that are largely similar to one another in structure although antibodies contain an extremely variable region which binds to target structures.258 Differences in that region are the reason different antibodies bind to different targets—for example the reason why one antibody binds to a can- cer cell while another binds to the common cold virus.259 The body generates antibody diversity in part by harnessing the power of random gene recombina- tions and mutations (much as GP does), and then it selects for antibodies with a desired binding (much as GP does).260 Following the discovery of antibody structure and the development of technologies to manufacture antibodies in the 1970s, human researchers began to create antibodies for diagnostic and thera-
256 Ken Jennings, My Puny Human Brain, SLATE (Feb 16, 2011), http://primary.slate.com/ articles/arts/culturebox/2011/02/my_puny_human_brain.html [https://perma.cc/BE9X-NUNX].
257 See Janice M. Reichert, Marketed Therapeutic Antibodies Compendium, 4 MABS 413, 413 (2012).
258 See Neil S. Lipman et al., Monoclonal Versus Polyclonal Antibodies: Distinguishing Charac- teristics, Applications, and Information Resources, 46 ILAR J. 258, 258 (2005).
259 See id. at 258–59. 260 See id. at 259.
 
363
1118 Boston College Law Review [Vol. 57:1079
peutic purposes.261 Therapeutic antibodies can block cell functions, modulate signal pathways, and target cancer cells among other functions.262 There are now dozens of artificially manufactured antibodies approved to treat a variety of medical conditions.263
One of the interesting things about antibodies from a computational in- vention perspective is that a finite number of antibodies exist. There are, at least, billions of possible antibodies, which is enough natural diversity for the human immune system to function and to keep human researchers active for the foreseeable future.264 Even so, there are only so many possible combina- tions of amino acids (the building blocks of proteins) that the body can string together to generate an antibody.265 It is not hard to imagine that, with enough computing power, an AI could sequence every possible antibody that could ever be created. Even if that was trillions of antibodies, the task would be rela- tively simple for a powerful enough computer but impossible for even the larg- est team of human researchers without computer assistance.
Generating the entire universe of antibody sequences would not reveal all of the possible functions of those antibodies; so, a computer’s owner could not obtain patents for all of the sequences on this basis alone because usefulness (utility) of the invention must be disclosed in addition to the sequence itself.266 The computer could, however, prevent any future patents on the structure of new antibodies (assuming the sequence data is considered an anticipatory dis- closure).267 If this occurred, a computer would have preempted human inven- tion in an entire scientific field.268
2. Computers May Refocus Human Activity
In the hypothetical scenario above, society would gain access to all possi- ble future knowledge about antibody structure at once rather than waiting dec- ades or centuries for individuals to discover these sequences. Early access to antibody sequences could prove a tremendous boon to public health if it led to
261 See Reichert, Marketed Therapeutic Antibodies Compendium, supra note 257, at 413. 262 See id.
263 See id.
264 See Lipman, supra note 258, at 258.
265 See, e.g., U.S. Patent No. 8,008,449 (filed May 2, 2006) (disclosing antibodies to the protein Programmed Death 1 (“PD-1”) by virtue of publishing their amino acid sequences).
266 See In re Fisher, 421 F.3d 1365, 1370 (Fed. Cir. 2005).
267 See MPEP, supra note 43, § 2131.
268 Along similar lines, projects such as “All Prior Art” and “All the Claims” are attempting to
use machines to create and publish vast amounts of information to prevent other parties from obtain- ing patents. See ALL PRIOR ART, allpriorart.com/about/ [https://perma.cc/3XJR-2FF5] (last visited Jul. 10, 2016); ALL THE CLAIMS, alltheclaims.com/about/ [https://perma.cc/QDE2-5J49] (last visited Jul. 10, 2016); see also Hattenbach & Glucoft, supra note 5, at 35–36 (describing the efforts of a compa- ny, Cloem, which is mechanically publishing possible patent claims to prevent others from obtaining patents).
 
364
2016] Patent Generating Artificial Intelligence 1119
the discovery of new drugs. Some antibody sequences might never be identi- fied without creative computers.
Creative computers may simply refocus, rather than inhibit, human crea- tivity. In the short term, scientists who were working on developing new anti- body structures might shift to studying how the new antibodies work, or find- ing new medical applications for those antibodies, or perhaps move on to stud- ying more complex proteins beyond the capability of AI to comprehensively sequence. For the foreseeable future, there will be plenty of room for human inventors—all with net gains to innovation.
Antibody therapies are just one example of how AI could preempt inven- tion in a field. A sophisticated enough computer could do something similar in the field of genetic engineering by creating random sequences of DNA. Living organisms are a great deal more complex than antibodies, but the same funda- mental principles would apply. Given enough computing power, an AI could model quintillions of different DNA sequences, inventing new life forms in the process. In fact, on a smaller scale, this is something GP already does.269 Alt- hough results have been limited by the computationally intense nature of the process, that will change as computers continue to improve.270 By creating novel DNA sequences, GP would be performing the same function as non- digital GP—natural evolution!
3. Dealing with Industry Consolidation
It will probably be the case that creative computers result in greater con- solidation of intellectual property in the hands of large corporations such as IBM. Such businesses may be the most likely to own creative computers ow- ing to their generally resource intense development.271 As previously dis- cussed, the benefits, however, may outweigh the costs of such an outcome. Imagine that Watson was the hypothetical AI that sequenced every conceivable antibody and, further, that Watson could analyze a human cancer and match it with an antibody from its library to effectively treat the cancer. Essentially, this could allow IBM to patent the cure for cancer.
Though this would be profoundly disruptive to the medical industry and might lead to market abuses, it is not a reason to bar computational invention. Society would obtain the cure for cancer, and IBM would obtain a twenty-year monopoly (the term of a patent) in return for publically disclosing the infor-
269 See, e.g., Tim Stefanini, The Genetic Coding of Behavioral Attributes in Cellular Automata, in ARTIFICIAL LIFE AT STANFORD 172–80 (John R. Koza ed., 1994); W.B. Langdon & B.F. Buxton, Genetic Programming for Mining DNA Chip Data from Cancer Patients, 5 GENETIC PROGRAMMING AND EVOLVABLE MACHINES 251, 251 (2004).
270 See Stefanini, supra note 269, at 172–80.
271 See Carter, supra note 199 (noting that most advanced computer systems are owned by gov- ernments and large businesses).
 
365
1120 Boston College Law Review [Vol. 57:1079
mation a competitor would need to duplicate Watson’s invention.272 In the ab- sence of creative computers, such an invention might never come about.
To the extent that price gouging and supply shortages are a concern, pro- tections are built into patent law to protect consumers against such prob- lems.273 For example, the government could exercise its march in rights or is- sue compulsory licenses.274
4. The Creative Singularity and Beyond
As creative computers become more and more sophisticated, at some point in the future, it is possible that they could have a very disruptive effect on human creativity. In recent years, a number of prominent scientists and entre- preneurs such as Bill Gates, Stephen Hawking, and Elon Musk have expressed concern about the “singularity”—a point in the future when machines outper- form humans.275 Likewise, a “creative singularity” in which computers over- take people as the primary source of innovation may be inevitable. Taken to its logical extreme and given that there is really no limit to the number of com- puters that could be built or their capabilities, it is not especially improbable to imagine that computers could eventually preempt much or all human inven-
272 See MPEP, supra note 43, § 2164.
273 See the case of Martin Shkreli, who has been pilloried for price gouging by drastically increasing the price of an old drug, Daraprim. See Andrew Pollack & Julie Creswell, Martin Shkreli, the Mercurial Man Behind the Drug Price Increase That Went Viral, N.Y. TIMES (Sept. 22, 2015), http://www. nytimes.com/2015/09/23/business/big-price-increase-for-an-old-drug-will-be-rolled-back-turing-chief- says.html?r_=0 [https://perma.cc/Q3Q3-95CS]. In this particular case, the monopoly was due to lack of competition, but the same economic principles apply to patent monopolies. See Ryan Abbott, Balancing Access and Innovation in India’s Shifting IP Regime, Remarks, 35 WHITTIER L. REV. 341, 344 (2014) (discussing patent law protections against practices including “evergreening”).
274 See, e.g., Abbott, supra note 273, at 345 (explaining India’s issuance of a compulsory license).
275 Of perhaps greater concern than automation displacing human workers, a number of promi- nent scientists are concerned about the implications of machines outperforming people. Professor Stephen Hawking has warned that self-improving computers could threaten the very existence of humanity. Rory Cellan-Jones, Stephen Hawking Warns Artificial Intelligence Could End Mankind, BBC (Dec. 2, 2014), http://www.bbc.com/news/technology-30290540 [https://perma.cc/2NWE- D8MB]. Elon Musk recently donated ten million dollars to an institute focused on threats posed by advances in AI. See Chris Isidore, Elon Musk Gives $10M to Fight Killer Robots, CNN MONEY (Jan. 16, 2015), http://money.cnn.com/2015/01/15/technology/musk-artificial-intelligence/ [https://perma. cc/6N3U-WTWF]; see also Dylan Love, Scientists Are Afraid to Talk About the Robot Apocalypse, and That’s a Problem, BUSINESS INSIDER (July 18, 2014), http://www.businessinsider.com/robot- apocalypse-2014-7#ixzz3QQpLJ0Jj [https://perma.cc/AH5J-5G34] (noting that robots may pose a “real risk” to humanity); Kevin Rawlinson, Microsoft’s Bill Gates Insists AI Is a Threat, BBC (Jan. 29, 2015), http://www.bbc.com/news/31047780 [https://perma.cc/9MXD-TQ68] (expressing Bill Gates’s concerns about advanced AI). Concerns about the threat posed by advanced AI are nothing new. See, e.g., Irving J. Good, Speculations Concerning the First Ultraintelligent Machine, 6 AD- VANCES IN COMPUTERS 31, 31–88 (1966). As an aside, the date the patent for the Creativity Machine issued is the same date as “Judgment Day” from the original Terminator movie, August 29, 1997.
 
366
2016] Patent Generating Artificial Intelligence 1121
tion.276 The future may involve iPads in place of fast food cashiers,277 robots empathizing with hospital patients,278 and AI responsible for research. For now, this is a distant possibility.
Moreover, patents on computational inventions would not prevent this outcome. If creative computers ever come to substantially outperform human inventors, they still will replace people—just without the ability to receive pa- tents.
C. Lessons for Copyright Law
1. Promoting the Useful Arts and Environmental Conservation
The need for computer inventorship also explains why the Copyright Of- fice’s Human Authorship Requirement is misguided. Nonhumans should be allowed to qualify as authors because doing so would incentivize the creation of new and valuable creative output. In the case of the Monkey Selfies, Mr. Slater, a photographer familiar with macaques, reported that he carefully staged the environment in such a way that Naruto would be likely to take his own photograph.279 If accurate, he probably did so in part due to an expectation of selling the resulting photographs.280 Had Mr. Slater known in advance that the images would pass into the public domain, he might never have taken the photographs. Although an owner default assignment rule would give copyright ownership of the Monkey Selfies to Naruto’s owner281 rather than to Mr. Slater, he could have contracted with Naruto’s owner to purchase or license the pho- tographs. Certainly in the aggregate, fewer photographers will engage in such activities without the prospect of copyright protection, and although animal selfies are not the cure for cancer, they have societal value as does any other form of art.282
276 See Brian Christian, Mind vs. Machine, THE ATLANTIC (Mar. 2011), http://www.theatlantic. com/magazine/archive/2011/03/mind-vs-machine/308386/ [https://perma.cc/7CMT-DAEZ].
277 See Victoria Taft, Protesters Aren’t Going to Like How McDonald’s Is Reacting to Their Mini- mum Wage Concerns, INDEP. J. REV. (May 26, 2015), http://ijr.com/2015/05/330129-guy-wont-taking- mickey-ds-order-much-longer-might-surprised-reasons/ [https://perma.cc/AZH6-GWXC].
278 See Bertalan Mesko, Will Robots Take Over Our Jobs in Healthcare?, in MY HEALTH UP- GRADED: REVOLUTIONARY TECHNOLOGIES TO BRING A HEALTHIER FUTURE 80, 80–83 (Richard E. Cytowic ed. 2015).
279 See Photographer ‘Lost £10,000’ in Wikipedia Monkey ‘Selfie’ Row, BBC (Aug. 7, 2014), http://www.bbc.com/news/uk-england-gloucestershire-28674167 [https://perma.cc/WQF8-KNYL]. 280 See Complaint, Naruto v. Slater, 2016 WL 362231 (No. 3:15-cv-04324) at *1, *9 (noting his
sale of copies of the Monkey Selfies).
281 Here that might be the government of Indonesia or the Tangkoko Reserve (Naruto’s home)
depending on Indonesian law and the reserve’s structure. See Complaint, Naruto, 2016 WL 362231 (No. 3:15-cv-04324) at *1.
282 See Johnson, supra note 134, at 16 (describing the sale of works of art created by a chimpan- zee whose “fans may have included . . . Pablo Picasso” and works created by seven Asian elephants) (internal quotation marks omitted). Alternatively, in the words of Justice Holmes:
 
367
1122 Boston College Law Review [Vol. 57:1079
Animal authorship might also have some ancillary conservation benefits. Continuing with the case of the Monkey Selfies, Naruto is a member of a criti- cally endangered species with a total population of between four and six thou- sand macaques.283 The species’ “numbers have decreased by approximately ninety percent (90%) over the last twenty-five years due to human population encroachment, being killed by humans in retribution for foraging on crops, and being trapped and slaughtered for bush meat.”284 Permitting Naruto’s activities to have a new source of value would be another economic incentive for private and public landowners to conserve biodiversity.285 Naruto lives in a reserve in Indonesia, but the United States also continues to suffer significant biodiversi- ty loss.286 Some environmentalist groups argue this is because conservation efforts are chronically underfunded.287 Nonhuman authorship might be an ad- ditional policy lever to reverse this trend.
D. Rethinking the Ultimate Test of Patentability
Considering the case for creative computers provides insight into other areas of patent law. Take, for instance, the nonobviousness requirement for grant of a patent.288 When Congress did away with the Flash of Genius doc- trine, it replaced that test with the current requirement for nonobviousness.289 Part of the requirement’s evaluation involves employing the legal fiction of a “person having ordinary skill in the art” (“PHOSITA” or simply the “skilled person”) who serves as a reference for determining whether an invention is nonobvious.290 Essentially, an applicant cannot obtain a patent if the skilled person would have found the difference between a new invention and the prior
It would be a dangerous undertaking for persons trained only to the law to constitute themselves final judges of the worth of pictorial illustrations, outside the narrowest and most obvious limits. At the one extreme some works of genius would be sure to miss appreciation. . . . At the other end, copyright would be denied to pictures which ap- pealed to a public less educated than the judge.
Bleistein v. Donaldson Lithographing Co., 188 U.S. 239, 251–52 (1903).
283 See Complaint, Naruto, 2016 WL 362231 (No. 3:15-cv-04324), at *3.
284 Id.
285 For a discussion of existing economic incentives for landowners to conserve biodiversity, see
generally Frank Casey et al., Incentives for Biodiversity Conservation: An Ecological Is Economic Assessment, DEFENDERS OF WILDLIFE (2006), http://www.defenders.org/publications/incentives_for_ biodiversity_conservation.pdf [https://perma.cc/LEM8-SPYC].
286 See id. at 7.
287 Id. at 8.
288 See 35 U.S.C. § 103 (2012).
289 See id.
290 Actually, the skilled person is relevant to many areas of patent law including claim construc-
tion, best mode, definiteness, enablement, and the doctrine of equivalents. See Dan L. Burk & Mark A. Lemley, Is Patent Law Technology-Specific?, 17 BERKELEY TECH. L.J. 1155, 1186–87 (2002).
 
368
2016] Patent Generating Artificial Intelligence 1123
art (what came before the invention) obvious.291 The test presumes that the skilled person is selectively omniscient, having read, understood, and remem- bered every existing reference from the prior art in the relevant field of inven- tion (analogous art).292 A federal judge explained that the way to apply the ob- viousness test is to “first picture the inventor as working in his shop with the prior art references, which he is presumed to know, hanging on the walls around him.”293
Needless to say, no actual person could have such knowledge, but the standard helps avoid difficult issues of proof related to an inventor’s actual knowledge; also, it prevents obvious variations of publically disclosed inven- tions from being patented.294 Stopping obvious variations from being patented is important because that prevents the removal of knowledge from the public domain.295 Inventions which would have been obvious to skilled persons are already within reach of the public.296 This raises the bar to obtaining a patent— a result that is desirable because patents should not be granted lightly given their anticompetitive effects.297 At the same time, creating too high a bar to patentability is undesirable because then patents would fail to adequately in- centivize researchers. A balance is needed.298 Ideally, the system would only issue patents for inventions that would not have been created but for the expec- tation of obtaining a patent.299 The importance of the nonobvious requirement to patentability has led to its characterization as the “ultimate condition of pa-
291 See 35 U.S.C. § 103(a).
292 See Standard Oil Co. v. Am. Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985). See generally Lance Leonard Barry, Cézanne and Renoir: Analogous Art in Patent Law, 13 TEX. INTELL. PROP. L.J. 243 (2005) (discussing analogous art).
293 Application of Winslow, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
294 See Jonathan J. Darrow, The Neglected Dimension of Patent Law’s PHOSITA Standard, 23 HARV. J.L. & TECH. 227, 228 (2009); Daralyn J. Durie & Mark A. Lemley, A Realistic Approach to the Obviousness of Inventions, 50 WM. & MARY L. REV. 989, 991–92, 1017 (2008).
295 See Sakraida v. Ag Pro, Inc., 425 U.S. 273, 281 (1976) (“A patent . . . which only unites old elements with no change in their respective functions . . . obviously withdraws what already is known into the field of its monopoly and diminishes the resources available to skillful men . . . .”) (internal quotation marks omitted) (quoting Great Atl. & Pac. Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 152–53 (1950))
296 See id.
297 See Eldred v. Ashcroft, 537 U.S. 186, 246 (2003) (Breyer, J., dissenting) (discussing the views of Madison, Jefferson “and others in the founding generation, [who] warned against the dangers of monopolies”).
298 See Edmund W. Kitch, Graham v. John Deere Co.: New Standards for Patents, 1966 SUP. CT. REV. 293, 301 (“The non-obviousness test makes an effort, necessarily an awkward one, to sort out those innovations that would not be developed absent a patent system.”).
299 Graham v. John Deere Co., 383 U.S. 1, 11 (1965) (“The inherent problem was to develop some means of weeding out those inventions which would not be disclosed or devised but for the inducement of a patent.”).
 
369
1124 Boston College Law Review [Vol. 57:1079
tentability.”300 The idea of a PHOSITA understanding all of the prior art in her field was always fictional,301 but now it is possible for a skilled entity, in the form of a computer, to possess such knowledge. For example, Watson’s data- base could be populated with every published food recipe available to the Pa- tent Office. This makes the skilled computer a natural substitute for the hypo- thetical skilled person. The standard would require a skilled computer rather than a creative computer for the same reason that the skilled person is not an inventive person.302 PHOSITA has traditionally been characterized as skilled at repetitive processes that produce expected results.303 If the skilled person were capable of inventive activity, then inventive patent applications would appear obvious.304
Replacing the skilled person with the skilled computer suggests a change to the nonobviousness test. At present, the test takes into account the skilled person’s knowledge of the prior art. Decreasing the universe of prior art makes it easier to get a patent because, with less background knowledge, a new inven- tion is more likely to appear inventive.305 Likewise, expanding the universe of prior art would raise the patentability bar.306 Yet although it may be unrealistic
300 See NONOBVIOUSNESS—THE ULTIMATE CONDITION OF PATENTABILITY 23 (John With- erspoon ed., 1980) (commemorating the twenty-fifth anniversary of the passage of 35 U.S.C. § 103 (2012)).
301 See Dan L. Burk, The Role of Patent Law in Knowledge Codification, 23 BERKELEY TECH. L.J. 1009, 1025 (2008) (“[T]he PHOSITA is a fictional composite, a conceptual construct imagined for the purpose of assessing the claimed invention against its technological antecedents.”); Cyril A. Soans, Some Absurd Presumptions in Patent Cases, 10 PAT. TRADEMARK & COPY. J. RES. & ED. 433, 438–39 (1967) (railing against the judicially created “superhuman Frankenstein monster Mr. Phosi- ta”).
302 Factors to consider in determining the level of ordinary skill in the art include: (1) “type of problems encountered in the art”; (2) “prior art solutions to those problems”; (3) “rapidity with which innovations are made”; (4) “sophistication of the technology”; and (5) “educational level of active workers in the field.” In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995). The U.S. Court of Ap- peals for the Federal Circuit has acknowledges that “[i]n a given case, every factor may not be pre- sent, and one or more factors may predominate.” See id.; see also Custom Accessories, Inc. v. Jeffrey- Allan Indus., Inc., 807 F.2d 955, 962–63 (Fed. Cir. 1986) (discussing PHOSITA); Envtl. Designs, Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 696–97 (Fed. Cir. 1983) (providing precedent upon which Federal Circuit Court of Appeals in GPAC, Inc. relied).
303 Hotchkiss v. Greenwood, 52 U.S. (11 How.) 248, 253 (1850) (noting patentability requires more “ingenuity or skill” than would be possessed by “an ordinary mechanic acquainted with the business”). The Court noted in 2007 in KSR International Co. v. Teleflex Inc. that “[a] person of ordi- nary skill is also a person of ordinary creativity, not an automaton.” 550 U.S. 398, 421, 424 (2007) (referring to PHOSITA as a “pedal designer of ordinary skill” in a case involving pedal design).
304 See KSR International Co., 550 U.S. at 424.
305 See In re Clay, 966 F.2d 656, 658 (Fed. Cir. 1992) (noting that “the scope and content of the prior art” is relevant to a determination of obviousness).
306 See id.; see also Brenda M. Simon, The Implications of Technological Advancement for Obvi- ousness, 19 MICH. TELECOMM. & TECH. L. REV. 331, 333, 350–51 (2013) (arguing that “the availabil- ity of information in a searchable form and the use of increased processing capabilities” will result in “very few” inventions being held nonobvious and that at some point AI “might become sufficiently
 
3