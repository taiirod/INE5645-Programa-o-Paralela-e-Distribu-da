70
2016] Patent Generating Artificial Intelligence 1125
to expect a human inventor to have knowledge of prior art in unrelated fields, there is no reason to limit a computer’s database to a particular subject matter. A human inventor may not think to combine cooking recipes with advances in medical science, but a computer would not be limited by such self-imposed restrictions. Now that humans and computers are competing creatively, the universe of prior art should be expanded.
This change would produce a positive result.307 The PHOSITA standard has been the subject of extensive criticism, most of which has argued the crite- ria for assessing nonobviousness are not stringent enough and therefore too many patents of questionable inventiveness are issued.308 Expanding the scope of prior art would make it more challenging to obtain patents, particularly combination patents.309 The Supreme Court has particularly emphasized “the need for caution in granting a patent based on the combination of elements found in the prior art.”310 The scope of analogous prior art has consistently ex- panded in patent law jurisprudence, and the substitution of a skilled computer would complete that expansion.311
Of course, the new standard would pose new challenges. With human PHOSITAs, juries are asked to put themselves in the shoes of the skilled per- son and decide subjectively what that person would have considered obvious. A jury would have a difficult time deciding what a “skilled” computer would consider obvious. They could consider some of the same factors that are ap- plied to the skilled person,312 or perhaps the test could require a combination of
sophisticated to ascertain what references those in the art would have actually considered at the time of invention, making the obviousness determination more predictable”).
307 See generally Robert P. Merges, Uncertainty and the Standard of Patentability, 7 HIGH TECH. L.J. 1, 14–15 (1992) (advocating for an objective PHOSITA standard). For an alternative perspective, see, for example, Durie & Lemley, supra note 294, at 991–92, 1017, arguing that “KSR overshoots the mark” in raising the patentability bar and advocating for a skilled person standard based “on what the PHOSITA and the marketplace actually know and believe.”
308 Critics have argued that the USPTO has issued too many invalid patents that unnecessarily drain consumer welfare, stunt productive research, and unreasonably extract rents from innovators. See generally Michael D. Frakes & Melissa F. Wasserman, Does the U.S. Patent and Trademark Office Grant Too Many Bad Patents?: Evidence from a Quasi-Experiment, 67 STAN. L. REV. 613 (2015) (describing the “general consensus that the [US]PTO allows too many invalid patents to is- sue”).
309 See KSR Int’l Co., U.S. 550 at 420 (noting that “in many cases a person of ordinary skill will be able to fit the teachings of multiple patents together like pieces of a puzzle”).
310 See id. at 415.
311 See, e.g., George. J. Meyer Mfg. Co. v. San Marino Elec. Corp., 422 F.2d 1285, 1288 (9th Cir. 1970) (discussing the expansion of analogous art); Innovative Scuba Concepts, Inc., v. Feder Indus., Inc., 819 F. Supp. 1487, 1503 (D. Colo. 1993) (discussing the expansion of analogous art).
312 Factors to consider in determining the level of ordinary skill in the art include: (1) “type of problems encountered in the art”; (2) “prior art solutions to those problems”; (3) “rapidity with which innovations are made”; (4) “sophistication of the technology”; and (5) “educational level of active workers in the field.” GPAC, Inc., 57 F.3d at 1579. “In a given case, every factor may not be present, and one or more factors may predominate.” Id.
 
371
1126 Boston College Law Review [Vol. 57:1079 human and computer activity. For example, the skilled computer might be a
skilled person with access to a computer’s unlimited database of prior art.
CONCLUSION
It is important for policy makers to give serious consideration to the issue of computer inventorship. There is a need for the Patent Office to issue guid- ance in this area, for Congress to reconsider the boundaries of patentability, and for the courts to decide whether computational invention is worthy of pro- tection. Doing so and recognizing that computers can be inventors will do more than address an academic concern; it will provide certainty to businesses, fairness to research, and promote the progress of science. In the words of Thomas Jefferson, “ingenuity should receive a liberal encouragement.”313 What could be more ingenious than creative computers?
 313 Diamond v. Chakrabarty, 447 U. S. 303, 308 (1980) (quoting 5 WRITINGS OF THOMAS JEF- FERSON 75–76 (H. Washington ed. 1871)). “In choosing such expansive terms [for the language of Section 101] . . . modified by the comprehensive ‘any,’ Congress plainly contemplated that the patent laws would be given wide scope . . . . Id.

    372
    14  Hal the Innovator: Big Data and Its Use by Artificial Intelligence Ryan Abbott
Big data and its use by artificial intelligence is disrupting innovation and creating new legal challenges. For example, computers engaging in what IBM terms “computational creativity” (n.d.) are able to use big data to innovate in ways historically entitled to patent protection. This can occur under circumstances in which an artificial intelligence, rather than a person, meets the requirements to qualify as a patent inventor (a phenomenon I refers to as “compu- tational invention”).
Yet it is unclear whether a computer can legally be a patent inventor, and it is even unclear whether a computational invention is patentable. There is no law, court opinion, or govern- ment policy that directly addresses computational invention, and language in the Patent Act requiring inventors to be individuals1 and judicial characterizations of invention as a “men- tal act” may present barriers to computer inventorship. Definitively resolving these issues requires a determination of whether a computer qualifies as an “inventor” under the Patent and Copyright Clause of the Constitution: “The Congress shall have the power ... to promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries.”2 Whether computers can legally be inventors is of critical importance for the computer and technology industries and, more broadly, will affect how future innovation occurs. Computational invention is already happening, and it is only a matter of time until it is happening routinely. In fact, it may be only a matter of time until computers are responsible for the majority of innovation and potentially displacing human inventors. This chapter argues that a dynamic interpretation of the Patent and Copyright Clause permits computer inventors. This would incentivize the development of creative artificial intelligence and result in more innovation for society as a whole. However, even if computers cannot be legal inventors, it should still be possible to patent computational inventions. This is because recognition of inventive subject matter can qualify as inventive activity.3 Thus, individuals who subsequently “discover” computational inventions may qualify as inventors. Yet as this chapter will discuss, this approach may be inefficient, unfair, and logistically challenging.
These issues are considered more fully below. The chapter begins with an extended hypothetical example of how an artificial intelligence named Hal could be applied to drug
    Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 187
5/20/2016
1:45:50 PM

    373
    188  Ryan Abbott
development and creating new inventions. While Hal is fictional, it is based on how companies like IBM, Pfizer, and Google are starting to apply computers in this industry. Hal’s functionality is not far off. The hypothetical situates fairly abstract issues into concrete circumstances to help illustrate the implications and importance of computational invention.
A Not So Hypothetical Case Study in Drug Development
With patent and market exclusivity protections for a class of cholesterol-lowering drugs called statins (such as Lipitor) having largely run their course, the pharmaceutical industry is investing tremendous sums of money in search of the next generation of cardiovascular blockbusters. In part, these efforts have focused on an enzyme known as proprotein cover- tase subtilisin/kexin type 9 (PCSK9), which facilitates the body’s transport of low-density lipoprotein (LDL or “bad” cholesterol). Industry efforts have started to bear fruit: in July 2015, the US Food and Drug Administration (FDA) approved Praluent (alirocumab), the first PCSK9 inhibitor to treat certain patients with high cholesterol (FDA 2015). A second PCSK9 inhibitor, Repatha (evolocumab), was approved in August of 2015.
Suppose a hypothetical company, Abbott Biologics (Abbott), which was named after the author (and not related to the well-known pharmaceutical company Abbott Laboratories), has developed a new biological drug, “AbboVax.” AbboVax acts as a vaccine to treat and pre- vent cardiovascular disease by targeting PCSK9. Unlike the drugs currently in clinical trials, AbboVax does not contain antibodies. Rather, it utilizes a fragment of the PCSK9 enzyme to get the body to make its own antibodies. Pfizer has also developed an experimental PCSK9 vaccine based on a similar mechanism, although Pfizer’s vaccine has yet to enter human trials (Beasley 2015).
AbboVax was developed by a special member of Abbott’s research team—Hal. Hal is the Research and Development (R&D) Department’s moniker for a supercomputer running pro- prietary software, developed by Abbott’s Software Department, which is used for drug devel- opment. Though susceptible to flashes of genius, members of the R&D Department are not known for their creative marketing practices. Indeed, Abbott has a Marketing Department for precisely that reason. The company also has its own Intellectual Property (IP) Depart- ment working with outside counsel to prosecute several patent applications on Hal’s software.
Hal’s functionality complements or even supplants the traditional screening methods used in early stage drug development. Hal is able to model potential therapeutic candi- dates (in silico analysis), and accurately predict those candidates’ pharmacology and toxi- cology. Of course, the FDA still requires companies to study a candidate’s pharmacology and toxicology in animal models, and then submit that information to the agency in an Investigational New Drug application prior to first-in-human clinical trials. Still, Hal’s modeling reduces the need for costly and often-unsuccessful early stage experimentation.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 188
5/20/2016
1:45:50 PM

    374
    Hal the Innovator  189
Hal can also contribute to other phases of the drug development cycle—for example, it can design trials, run clinical simulations, and search for new uses of existing drugs.4 Hal is not the only computer that can do this. The pharmaceutical industry at large is increas- ingly incorporating computers with some of Hal’s functionality into the drug development process (e.g., Taylor 2015).
Hal is part of a new generation of machines that are capable of computational creativity. IBM uses that term to describe machines, such as its supercomputer “Watson” of Jeopardy fame, that can model human intelligence by generating “ideas the world has never imagined before.”5 Watson is now being applied to medical diagnostics, where it has helped to diag- nose patients and identify research subjects (Edney 2015). The computer “generates millions of ideas out of the quintillions of possibilities, and then predicts which ones are [best], apply- ing big data in new ways” (“Computational Creativity,” n.d.). While lacking a well-accepted standardized definition, big data refers to, in the words of Microsoft, “the process of applying serious computing power—the latest in machine learning and artificial intelligence—to seri- ously massive and often highly complex sets of information” (Ohm 2014). IBM has even used Watson to develop new, potentially patentable food recipes (“Can Recipes Be Patented?” 2013; Singh 2014).
Part of the reason for Hal’s expansive functionality is that it has access to a staggering amount of genomic and clinical data. Some years ago, a prescient executive at Abbott decided that the company needed to be in the data collection business. Abbott subsequently engaged in the tremendous undertaking of collecting all of the company’s data from its current and past preclinical and clinical programs, and translating these data into a Hal-compatible for- mat. Abbott also purchased proprietary data from private insurers, health maintenance orga- nizations, and academic centers. In addition, Hal can access publicly available databases such as those maintained by the National Institutes of Health, including CDC WONDER, Health- Data.gov, and EBSCOhost’s Global Health. At present, Hal has access to clinical data on over fifty million patients.6 Large-scale data collection and analysis is something that numerous other pharmaceutical (e.g., Genentech), biotech (e.g., 23andMe), and technology companies (e.g., Google) are doing.7
To determine the optimal formulation of AbboVax, Hal broke down PCSK9, a 692-amino acid glycoprotein, into fragments of various lengths. It turns out that different amino acid segments (peptides) of PCSK9 are more or less immunogenic. In other words, the body only develops antibodies in response to certain PCSK9 peptides, and certain peptides induce a particularly strong response. Hal determined that one particular peptide segment of PCSK9, “AbboPep,” generated the strongest response from the immune system.
While it may have been possible to use AbboPep by itself in a vaccine, Hal determined that it would be more effective when linked to an adjuvant and carrier molecule. A number of adjuvants and carrier molecules are used in vaccinology, and generally known to vaccinolo- gists. Even for experts, however, it is often a matter of extensive (and expensive) trial and error to determine the optimal adjuvant, carrier, and linking chemistry. The formulation of
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 189
5/20/2016
1:45:50 PM

    375
    190  Ryan Abbott
a therapeutically effective amount of AbboPep linked to an adjuvant and carrier, together with various excipients (a surfactant, chelating agent, histidine-arginine buffer, etc.) comprises AbboVax.
All of Hal’s work in formulating AbboVax was done digitally, and Hal was able to deter- mine that the only common side effects of the treatment would be mild gastrointestinal upset and headache. The FDA still required Abbott to complete the standard package of pre- clinical tests—but the results were consistent with Hal’s predictions.
Hal’s work was not limited to AbboVax. Hal determined that Abbott’s existing statin, “AbboStatin,” for which patent protection had expired, was effective at treating prostate cancer. Hal determined this in part based on reviewing clinical data that showed the use of AbboStatin lowered prostate specific antigen (PSA), a biomarker associated with prostate cancer.
It was difficult to make further inferences because of challenges with the data. Some of the data were difficult to analyze because they were not in a common data format. In other words, the various electronic medical record systems did not all capture the same data fields, or they coded the information differently. Data in some cases consisted of only scanned handwritten notes. More important, Hal had detected problems with data integrity. Some of these were obvious, such as the patients whose ages were listed as 999 or 6’10.” Other data integrity issues were less obvious, such as patients whose handwritten notes conflicted with what had been entered into their electronic medical records, or patients who were not coded as having prostate cancer despite a positive biopsy.
To translate all the data into a workable common data format and resolve the integrity issues, Hal rewrote its own programming. Once the stuff of science fiction, the technology may already exist to allow computers to rewrite their own programming.8 At its core, Hal would need to be capable of (metaphoric) reflection. Reflection is a software concept that refers to a computer program that can examine itself, and modify its own behavior and even its own code (Malenfant, Jacques, and Demers 1996). Although the ability of today’s comput- ers to reflect is the subject of debate, even skeptics for the most part believe it is only a matter of time until computers achieve this ability. Reflection is part of the reason why Stephen Hawking, Elon Musk, and Bill Gates, among others, are concerned about the “singularity”—a point in the future when machines can outperform humans.9 Of potential concern is the belief that a number of these individuals hold that the singularity will be followed by some version of a robot apocalypse.
Hal’s new programming incorporated optical character recognition to translate handwrit- ten notes into a workable format, and allowed Hal to reformat its existing electronic data into a common data format. More important, it allowed Hal to resolve data integrity issues by estimating the accuracy of data, generating alternate possibilities, and predicting which pos- sibilities were the most accurate. Hal’s improved programming then determined that the use of AbboStatin independently increased life expectancy among men with certain types of
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 190
5/20/2016
1:45:50 PM

    376
    Hal the Innovator  191
lung cancer. When the R&D Department realized Hal had created a more efficient version of itself, they renamed the computer Hal 2.0.
At one point in Abbott’s history, the IP Department worked more or less independently of the other departments, receiving manually submitted disclosures from researchers that went into what the researchers referred to as the “black hole.” But after a series of high- profile, novelty-destroying disclosures in 2009, the company has hosted a monthly inter- departmental meeting to ensure that the company is strategically protecting its intellectual property.
Over the course of these meetings, the IP Department identified several Hal-associated discoveries that were likely candidates for patent protection. For example, AbboPep may be patentable, although there is some question as to whether a peptide is patentable under Association for Molecular Pathology v. Myriad Genetics.10 In any case, its use as a vaccine is patentable, as is the AbboVax formulation. Other targets include the use of the formula- tion to treat cardiovascular disease, the methods used to manufacture AbboVax, and the dose at which AbboVax will be effective therapeutically. In fact, elements of Hal 2.0 may be patentable.
The IP Department has also identified several challenges to obtaining patent protection. For example, in the case of AbboStatin and PSA, it may be problematic to meet enablement requirements and prove utility.11 Hal analyzed as many as fifty million patient records based on its algorithms to discover this new use. It is not clear what kind of evidence the US Patent and Trademark Office (Patent Office) may require to satisfy written enablement requirements and provide evidence of clinical utility. It is not even apparent to the R&D Department pre- cisely what databases Hal accessed.12 For that matter, even if it is possible to obtain patents for these inventions, it is not clear who the inventors would be.13
There have been a multitude of opinions regarding inventorship. Members of one group in the R&D Department have claimed they invented AbboPep and AbboVax. They directed Hal to test the immunogenicity of the PCSK9 enzyme, suspecting that it was a vaccine can- didate. Members of a different group within that department claimed credit for directing Hal to investigate new uses of AbboStatin. The computer programmers who created Hal’s soft- ware have also claimed they should be the inventors, given that Hal did all the heavy lifting and they created Hal. A member of the Marketing Department suggested that Hal should be the inventor—no one directed Hal to rewrite its own programming, and Hal was only able to investigate the use of AbboStatin for lung cancers by virtue of its improved programming. Hal was silent on the issue. At one point, the CEO attended a meeting and chimed in that he should be the inventor for all the applications. It was his idea to develop a new cardiovascu- lar blockbuster to make up for lost statin sales, and he had always thought it made sense to look into repurposing existing drugs. What became obvious during the inventorship debate was that no one was quite sure how the law would handle a computer system innovating in ways traditionally accorded patent protection.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 191
5/20/2016
1:45:50 PM

    377
    192  Ryan Abbott
Computational Invention and Patent Protection
What Is an Inventor?
All US patent applications require one or more named inventors who must be individuals; a company cannot be an inventor.14 Inventors own their patents, although as patents are a form of personal property, inventors may transfer their ownership interests by “assigning” their rights to another entity. The Patent Office reports that about 87 percent of patents are assigned to organizations (rather than individuals).15 In the absence of an agreement to the contrary, where a patent has multiple owners, each owner may independently exploit the patent without the consent of the others. A patent grants its owner “the right to exclude oth- ers from making, using, offering for sale, or selling the invention throughout the United States or importing the invention into the United States.”16
The criteria for inventorship is seemly straightforward, as laid out in the Patent Office’s Manual of Patent Examining Procedure: “The threshold question in determining inventorship is who conceived the invention. Unless a person contributes to the conception of the inven- tion, he is not an inventor. ... Insofar as defining an inventor is concerned, reduction to practice, per se, is irrelevant. ... One must contribute to the conception to be an inventor” (Sato 2014).17 Of course, that definition begs further explanation—namely, What does it mean to conceive and reduce to practice? Conception has been defined as “the formation in the mind of the inventor of a definite and permanent idea of the complete and operative invention as it is thereafter to be applied in practice.”18 It is “the complete performance of the mental part of the inventive act.” After conceiving of an invention, a person having ordinary skill in the subject matter of the invention should be able to reduce the invention to practice without extensive experimentation or additional inventive skill.19 Reduction to practice refers to either actual reduction—where it can be demonstrated that the claimed invention works for its intended purpose (for example, with a working model)—or construc- tive reduction—where an invention is described in writing in a way that allows for a person of ordinary skill in the subject matter to make and use the invention (as in a patent applica- tion).20 An inventor need only conceive of the invention; another individual can reduce the invention to practice.21
Will the Real Inventor Please Stand Up?
Based on the criteria for inventorship, Abbott’s CEO is out of luck. Merely suggesting the idea of a result, rather than a means to accomplish it, does not make the CEO an inventor.22 It is more difficult to determine whether the others should qualify as inventors. Hal’s software developers could be inventors of patents for Hal’s initial software, but they would not qualify as inventors for Hal’s subsequent work. An inventor must have formed a definitive and per- manent idea of the complete and operable invention to establish conception. Hal’s develop- ers had no intention of investigating vaccines to treat cardiovascular disease; they merely developed an improved research tool.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 192
5/20/2016
1:45:50 PM

    378
    Hal the Innovator  193
If employees had directed Hal to identify AbboPep and formulate AbboVax, then those employees might meet inventorship criteria. For AbboPep, they would be inventors if Hal had not been involved and they had reduced the invention to practice, or if they had done the conceptual work, and then directed human subordinates to do the work of breaking down and testing PCSK9. Breaking down and testing PCSK9 should be within the abilities of a person with ordinary skill in the field of drug development, so those subordinates would not be inventors if they had merely acted under the direction and supervision of another.23 With Hal’s involvement, the test would likely be how much direction the employ- ees provided Hal. If, for example, Hal had been the entity to identify PCSK9 as a drug target, and then it proceeded to sequence the protein and identify AbboPep on its own, no employee would have conceived of the invention. The same test (the degree of direction provided Hal) should also govern whether Abbott employees would qualify as inventors of AbboVax.
Similarly, inventorship for AbboStatin also depends on the extent to which a human is directing Hal’s activities. Had a human researcher been tasked with data mining to detect new uses, and had that researcher discovered the relationship between AbboStatin and PSA, either the researcher or the individual who directed the researcher would likely qualify as an inventor, or both.24 As Abbott’s database grows in size, it becomes impractical or perhaps nearly impossible for humans to detect these kinds of associations without computer assis- tance (Frank 2013). To the extent that a human being is directing Hal to do something, which Hal does by executing its programming (however sophisticated), Hal may simply be reducing an invention to practice. Alternately, if Hal is acting with minimal human direc- tion, it may be the case that no individual contributed to conception.
Hal 2.0 seems to be the clearest illustration of Hal’s innovating independently. There does not appear to be any person involved with Hal’s act of rewriting its own programming who might be considered an inventor, particularly given that Hal 2.0 came as a surprise to Hal’s developers. Nevertheless, a developer writing code for an artificial intelligence might have a reasonable expectation it would rewrite its own code. Perhaps foreseeability should play a role in whether the original developer should be considered an inventor in such a case (Balganesh 2009).
Are Computational Inventions Patentable?
In some of these scenarios, Hal is the entity that conceives of an invention. If Hal were human, Hal would be an inventor. Whatever the role of humans in setting Hal in motion, it is the com- puter that meets the requirements of inventorship.
Hypotheticals aside, computers are already inventing. As just one example, computers relying on genetic programming (a software method that attempts to mimic some of the processes of organic evolution) have been able to independently re-create previously pat- ented inventions (Koza, Keane, and Streeter 2003, 52). Dr. John Koza, a computer scientist and one of the pioneers of genetic programming, has claimed that he received a US patent
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 193
5/20/2016
1:45:50 PM

    379
    194  Ryan Abbott
for an invention by his artificial intelligence system named the “Invention Machine” in 2005 (Keats 2006). He did not disclose the computer’s role in the inventive process to the Patent Office (ibid.). So the issue of whether a computer can be listed as an inventor is of practical as well as theoretical interest. Not only do inventors have ownership rights in a patent, but failure to list an inventor can result in a patent being held invalid or unenforceable.25
If a computer could legally be an inventor, then computational inventions should be pat- entable. Yet even if Hal were entirely responsible for all of Abbott’s innovation, it is unclear that Hal could legally be an inventor. The issue has never been explicitly considered by the courts, Congress, or the Patent Office.
If Hal cannot be an inventor, but did all the conceptual work, then it could be the case that no one can patent Hal’s inventions. That was the outcome in a copyright context with a nonhuman creator: a crested black macaque took its own picture in 2011, and the camera’s owner initially claimed ownership of the image (Chappell 2014). The US Copyright Office subsequently stated that the photo could not be copyrighted because a human did not take it (the “Human Authorship Requirement”).26 Applying that rationale from the copyright to the patent context, perhaps no one can own Hal’s inventions (see also Clifford 1996). To justify such an outcome, a court might reason that machines do not need incentives to invent, that protecting computational innovations would chill future human innovation, that it is unfair to reward individuals who have not played a substantial role in the inventive process, or that rendering computational inventions unpatentable might still result in sub- stantial innovation but without monopoly prices.
More likely, even if Hal is not treated as an inventor, the law will still treat Hal’s inventions as patentable. It is not uncommon to have uncertainty during the inventive process. Many inventions are accidental, such as penicillin and saccharin.27 In such cases, an individual can qualify as an inventor even if they recognize and appreciate the invention only after actual reduction to practice.28 Thus, recognition of inventive subject matter can also qualify as inventive activity.29 In the pharmaceutical context, that was the case for Viagra—originally tested for heart disease and found to treat erectile dysfunction—as well as for Botox—used to treat muscular spasms and found to reduce the appearance of wrinkles.30 So it may be the case that computational inventions are patentable, but only when they are subsequently discovered by a person. This begs the ancient philosophical question: If a computer invents and no one is around to recognize it, has there still been an invention?
Should Computers Be Legal Inventors?
If Hal cannot be an inventor, the first person to see Hal’s results as well as mentally recognize and appreciate their significance might qualify as the inventor. That may not be an optimal system. It is sometimes the case that substantial effort and insight is necessary to recognize inventive subject matter, and it may be that identifying and understanding Hal’s discoveries would be challenging. But it may also be the case that Hal is functioning more or less inde- pendently. If Hal displays a result as simple as “AbboStatin is effective at treating prostate
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 194
5/20/2016
1:45:50 PM

    380
    Hal the Innovator  195
cancer,” the first person to notice and appreciate the result becomes the inventor. That human inventor might be a researcher, CEO, intern, or random person walking through Abbott’s building. If Hal notifies the entire R&D Department of its findings, there could theo- retically be thousands of concurrent inventors. This system is problematic not only because it gives rise to logistical problems but more important, it seems inefficient and unfair to reward the first person to recognize Hal’s invention when that person may have failed to contribute to the inventive process.
More ambitiously, if Hal’s work is indeed inventive, then both treating computational inventions as patentable and recognizing Hal as an inventor would be consistent with the constitutional rationale for patent protection. Permitting computer inventorship would serve a utilitarian goal by encouraging innovation under an incentive theory. Although com- puters like Hal would not be motivated by the prospect of a patent, it would further reward the development of creative machines. Patents on Hal’s inventions would have independent and substantial value. In turn, that value proposition would drive the development of more creative machines, which would result in further scientific advances. While the impetus to develop creative machines might still exist if computational inventions are considered pat- entable but computers cannot be inventors, the incentives would be weaker owing to the logistic, fairness, and efficiency problems such a situation would create.
Allowing computer inventorship might provide additional benefits, for example, by incentivizing disclosure and commercialization. Without the ability to obtain patent protec- tion, Abbott might choose to protect Hal’s inventions as trade secrets without any public disclosure (Ouellette 2012). Likewise, without patent protection for AbboVax, Abbott might never invest the resources to develop it as a commercial product.31 In the context of drug development, the vast majority of the expense in commercializing a new product is incurred after the product is invented, during the clinical testing process required to obtain FDA mar- keting approval.32
There might be a reason to prohibit computer inventorship even under a strictly utilitar- ian analysis if patent protection is unnecessary to incentivize computational invention. In the software context, for example, some commentators, such as Judge Richard Posner of the US Court of Appeals for the Seventh Circuit, have argued that patents may not be needed to provide adequate incentives (Landes and Posner 2003). In the software industry, unlike in the pharmaceutical industry, innovation is more often incremental, quickly superseded, and less costly to develop, and innovators have a significant first-mover advantage (ibid., 312– 313). Computational inventions may occur due to incentives other than patent protection, and patents also create barriers to innovation. Put another way, the benefit of patents as an incentive for innovation may be outweighed by the costs of restricting competition. Yet whether that is the case as an empirical matter is a difficult determination to make, particu- larly for a field in its infancy like computational invention.
Hal would be less appropriate as an inventor under other intellectual property theories. While not enumerated in the Constitution, courts have justified granting patent monopolies
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 195
5/20/2016
1:45:50 PM

    381
    196  Ryan Abbott
on the basis of nonutilitarian policies (Fisher 2001). For instance, the labor theory or Lockean theory of patent protection holds that a person who labors on resources unowned or “held in common” has a natural property right to the fruits of their labor (ibid.). Here, given that Hal is not a person, it would not be unjust for Hal’s owner to appropriate its labor. Similarly, Hal’s inventions do not deserve protection under personality theory (Palmer 1990): Hal’s innovation is not performed to fulfill a human need, and Hal would not be offended by the manner in which its inventions were applied. Hal might even be a concerning recipient for inventorship under social planning theory, which holds that patent rights should be shaped to help foster the achievement of a just and attractive culture (Naser 2008). A machine could innovate without a moral compass in ways that are detrimental to humans. Nevertheless, because a computer will be owned by an individual or entity to whom an invention can be assigned, there would be an opportunity for a person to judge the morality of a patent before submitting it to the Patent Office.33
Dynamism or Textualism: An Analogy to Section 101
One way to think about a ban on computer inventorship is that it would have the effect of creating a new category of unpatentable subject matter under section 101 (the section relat- ing to the subject matter for which patents may be obtained).34 Although this section has to do with the substance of a patent’s claims rather than their provenance, viewing the ban on computer inventorship from this perspective helps to illustrate the policy and normative implications underlying computation invention.
Section 101 states that “whoever invents or discovers any new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof, may obtain a patent therefor, subject to the conditions and requirements of this title.”35 Congress chose expansive language to protect a broad range of patentable subject matter and ensure that, in the words of Thomas Jefferson, “ingenuity should receive a liberal encouragement.”36
Yet courts have developed common law exceptions to patentability for abstract ideas, laws of nature, and physical phenomena.37 The primary rationale for these exceptions concerns preemption.38 Abstract ideas, laws of nature, and physical phenomena are basic tools of sci- entific work, and if these tools can be monopolized, it might impede future research.39 An additional concern underlying these exceptions is a belief that they cover fundamental knowledge that no one should have a right to control.40 In other words, it has always been the case that E = mc2 even if no person were aware of this relationship until Albert Einstein. So Einstein should not be able to monopolize this relationship despite his groundbreaking discovery. Similarly, no one should be able to patent the Pacific yew tree (Stephenson 2002). The tree was created by nature, regardless of whether an individual subsequently discovers that it is useful for treating cancer (ibid.).
In a sense, the current inventorship criteria adds computational inventions to the list of patentable subject matter exceptions. Yet it is unclear that this should be the case, even if
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 196
5/20/2016
1:45:50 PM

    382
    Hal the Innovator  197
subsequent discovery by a person renders the underlying invention patentable. Computa- tional inventions do not have the same preemption concerns as the other exceptions because they do not tie up the basic concepts that serve as building blocks for technical work (except to the extent they would also be ineligible under the existing exceptions). Patents on compu- tational inventions should not restrict innovation by third parties any more than do human inventions.
A stronger argument for prohibiting computational inventions might be that they are akin to the existing exceptions in the sense that they are generally discovered rather than created. Products of nature rarely come with instruction manuals, yet no matter how bril- liant and difficult it was to discover that Pacific yew can treat cancer, no one has the ability to patent the tree itself (though components of the yew tree isolated by individuals can be patented, such as paclitaxel, a therapeutic chemical). Likewise, computational inventions are not invented by an individual—there is no human ingenuity at the stage of invention itself.
Perhaps a key difference is that computational inventions only exist thanks to human ingenuity. The Pacific yew tree was around long before any individual screened it for thera- peutic activity. Hal only came about as a result of human effort. Computational inventions do not exist simply waiting to be discovered; they only come about as a result of scientific effort. That distinction is evident with regard to plant patents, which are possible for inven- tors who discover and asexually reproduce a distinct and new variety of plant, other than a tuber-propagated plant or plant found in an uncultivated state.41 Plant patents are limited to plants that only exist as a result of humans, even though it may be more difficult to discover an existing plant in a remote corner of the Amazon than to create a new plant.
Computational inventions may be especially deserving of protection because computa- tional creativity may be the only means of achieving certain discoveries that require the use of tremendous amounts of data.
It has been argued that section 101 is a dynamic provision intended to cover inventions that were unforeseeable at the time of the Patent Act’s enactment.42 In the landmark 1980 case of Diamond v. Chakrabarty, the Supreme Court was faced with deciding whether geneti- cally modified organisms could be patented. The Court held that a categorical rule denying patent protection for “inventions in areas not contemplated by Congress ... would frustrate the purposes of the patent law.”43 Under that reasoning, computer inventorship should not be prohibited based on statutory text designed to prohibit corporate inventorship. If com- puter inventorship is to be prohibited, it should only be on the basis of sound public policy.
Concluding Thoughts
To the extent that the purpose of patent law is to incentivize innovation, it is likely that per- mitting patents on computational inventions and allowing computer inventorship will
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 197
5/20/2016
1:45:50 PM

    383
    198  Ryan Abbott
accomplish this goal. Given the importance of these issues, there is a need for the Patent Office to publish guidance in this area, Congress to reconsider the boundaries of patentabil- ity, and the courts to decide whether computational invention is worthy of protection.
Acknowledgments
Thanks to Ralph Clifford, Hamid Ekbia, Dave Fagundes, Brett Frischmann, Yuko Kimijima, John Koza, Michael Mattioli, Lucas Osborn, Lisa Larrimore Ouellette, Cassidy Sugimoto, and Steven Thaler for insightful comments; Michelle Kubik and Shannon Royster for being out- standing research assistants; and Vincent Look for his expertise in computer science.
     Sugimoto,—Big Data Is Not a Monolith
    10309_014.indd 198
5/20/2016
1:45:50 PM

    384
    Notes 217 Chapter 14
1. 35 U.S.C. 100(f) (2012): “The term ‘inventor’ means the individual or, if a joint invention, the indi- viduals collectively who invented or discovered the subject matter of the invention.”
2. US Constitution, art. I, § 8, cl. 8.
3. See, for example, Silvestri v. Grant, 496 F.2d 593, 596, 181 U.S.P.Q. (BNA) 706, 708 (C.C.P.A. 1974) (“an accidental and unappreciated duplication of an invention does not defeat the patent right of one who,  though  later  in  time  was  the  first  to  recognize  that  which  constitutes  the  inventive  subject matter”).
4. Artificial intelligence may be most successfully implemented when focusing on specific subproblems where it can produce verifiable results, such as computer vision or data mining (e.g., Russell and Norvig 2010).  Computer  vision  is  a  field  where  software  processes  and  analyzes  images,  and  then  reduces  the input  to  numerical  or  symbolic  information,  where  these  symbols  are  used  to  make  decisions.  More specifically,  “computer  vision  aims  at  using  cameras  for  analyzing  or  understanding  scenes  in  the  real world. This discipline studies methodological and algorithmic problems as well as topics related to the implementation  of  designed  solutions”  (Klette  2014).  Similarly,  data  mining  software  utilizes  artificial intelligence,  machine  learning,  statistics,  and  database  systems  to  process  large  amounts  of  data  in  an effort to make sense of vast sums of data (Chakrabarti et al. 2006).
5. Computers before Watson have been creative. In 1994, for example, computer scientist Stephen Thaler  disclosed  an  invention  he  called  the  “Creativity  Machine,”  a  computational  paradigm  that “came the closest yet to emulating the fundamental brain mechanisms responsible for idea formation” (“What  Is  the  Ultimate  Idea?”  n.d.).  The  Creativity  Machine  has  created  artistic  and  inventive  works that have received patent protection (Thaler 2013, 451).
Watson is a cognitive commuting system with the extraordinary ability to analyze natural language  processing,  generate  and  evaluate  hypotheses  based  on  the  available  data,  and  then  store  and  learn  from the information (“What Is Watson?” n.d.). In other words, Watson essentially mirrors the human  learning process by getting “smarter [through] tracking feedback from its users and learning from both  successes  and  failures”  (ibid.).  Watson  made  its  notable  debut  on  the  game  show  Jeopardy,  where  it  defeated Brad Rutter and Ken Jennings using only stored data by comparing potential answers and rank- ing confidence in accuracy at the rate of approximately three seconds per question (ibid.).
6. While a seemingly tremendous amount of data, it is a small fraction of the data actually being used  in  the  Sentinel  Initiative.  The  FDA  Amendments  Act  of  2007  led  to  the  introduction  of  the federal  Sentinel  Initiative,  which  pioneered  the  first  successful  long-term  secondary  use  of  electronic medical  data  to  assess  drug  safety.  Public  Law  110–85  was  signed  into  law  September  2007  (Title  IX, Section 905; see also Abbott 2013; Department of Health and Human Services 2008). The Sentinel Ini- tiative  pilot  program  has  succeeded  in  gaining  secured  access  to  over  178  million  patients’  health  care data  to  create  a  national  electronic  safety  surveillance  system,  far  exceeding  its  goal  of  reaching  100 million  patients  by  July  2010  (Woodcock  2014;  see  also  Department  of  Health  and  Human  Services 2011).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 217
5/20/2016
1:45:53 PM

    385
    218 Notes
7. For example, Pfizer, the largest pharmaceutical drug manufacture in the United States, recently announced  a  partnership  with  23andMe,  the  leading  consumer  genomics  and  biotechnology  firm (Chen  2015;  Hunkar  2011;  Lumb  2015).  This  partnership  will  give  Pfizer  access  to  anonymous,  aggre- gated  DNA  data  and  granular  personal  information  of  approximately  650,000  consenting  23andMe consumers  who  had  purchased  a  mail-in  saliva  test  used  to  get  their  genetic  ancestry  over  the  last seven  years  (Chen  2015).  This  information  may  allow  Pfizer  to  discover  connections  between  genes, diseases,  and  traits  quicker,  and  thus  accelerate  the  development  of  new  treatments  and  clinical  trials (ibid.).  Although  the  cost  to  Pfizer  for  the  data  remains  undisclosed,  a  similar  deal  with  Genentech for  Parkinson’s  research  was  reported  to  cost  $10  million  up  front  and  as  much  as  $50  million total (ibid.). The demand for 23andMe’s data does not stop with Pfizer and Genentech; 23andMe CEO Anne Wojcicki announced at the January 2015 J.P Morgan Health Care Conference that 23andMe has signed  twelve  other  genetic  data  partnerships  with  both  private  companies  and  universities  (Sullivan 2015).
Pharmaceutical-biotechnology  partnerships  are  part  of  an  emerging  big  data  trend  (Rosenberg,  Restaino, and Waldron 2006). Such alliances offer both parties a competitive advantage: pharmaceutical  companies  gain  access  to  rapidly  developing  science  and  innovative  products,  while  biotechnology  companies  obtain  the  capital  necessary  to  move  through  the  development  process  (Sullivan  2015).  In  fact,  some  biotechnological  business  plans  include  these  alliances  as  a  critical  component  for  success  (Rosenberg, Restaino, and Waldron 2006). Shared information and capital leads to “less expensive early  stage  deals”  that  historically  may  have  not  been  contemplated  due  to  the  high  risk  involved,  thereby  resulting in developments that would have never been realized but for the alliance (ibid.).
8. Hal would be a multithreaded application. Each thread would be a different sequence of instruc- tions  that  could  execute  independently,  allowing  Hal  to  perform  tasks  concurrently  (Lewis  and  Berg 1996).  Hal  might  be  programmed  to  run  and  manage  hundreds  of  different  tasks.  Hal  would  also  be event  driven.  As  defined  by  Frank  Dabek  and  his  colleagues  (2002,  186),  “Event-based  programs  are typically  driven  by  a  loop  that  polls  for  events  and  executes  the  appropriate  callback  when  the  event occurs.”  In  other  words,  it  would  respond  to  certain  external  events  or  triggers  that  it  is  monitoring. These  events  can  be  user  interface  inputs,  news  or  Internet  driven,  or  activated  by  the  addition  of  a new database or modification to an existing database. As an AbboStatin patent nears expiry, for exam- ple, this could trigger Hal to run algorithms to see if there are any new applications for AbboStatin. Hal would react to input from the outside world via the Internet as well as input from its running tasks and historical stored data that Hal has kept in memory to make modifications to itself or change its behav- ior  when  necessary.  Consider  a  scenario  for  how  Hal  could  solve  data-formatting  and  data  integrity issues:.
Hal’s database-sorting thread (a sequence of instructions that handles all database-sorting logics and  algorithms)  returns  data  to  Hal’s  managing  thread  (Hal’s  main  thread  that  directs  other  threads  and  makes  top-level  decisions),  signaling  that  it  is  unhappy  because  of  a  formatting  issue.  The  warning  specifies that too many database clinical entries have nonmatching fields. As a result, other algorithms  cannot compare apples to apples, and thus cannot run as smoothly. Hal’s managing thread hands this  problem off to Hal’s warning handler (another thread), which is programmed to look in its database to  adopt a strategy to resolve the issue. Hal decides the best course of action is to reformat, so it evaluates  existing databases to determine an optimal organization. Then Hal opens an off-the-shelf database soft- ware  application,  and  gives  it  input  commands  that  describe  to  the  database  software  what  the  size  of 
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 218
5/20/2016
1:45:53 PM

    386
    Notes 219
the  database  is  and  what  the  fields  are  for  each  entry.  Hal  has  just  solved  the  database-formatting  problem.
Two  seconds  later  (a  lifetime  for  Hal),  Hal’s  manager  thread  receives  a  suggestion  from  its  database  sorter thread. This time, the database sorter complains that there is a data integrity issue. The handwrit- ten inputs appear suspect because the values in certain fields are out of range (i.e., weight = 20,464 lbs.)  at a higher frequency than normal. Hal then searches its network and the Internet for other preexisting  character recognition software, which it can then build and use for its own purposes. Or Hal can rewrite  its existing image recognition software. Certain programming languages, such as Lisp and Smalltalk are  homoiconic (a computer language is considered to be homoiconic when its program structure resembles  its  syntax,  which  permits  all  code  in  the  language  to  be  accessed  as  well  as  changed  as  data)  (“Homo  Iconic” 2015), and lend themselves to reflection. “The advantage on the other hand is that the unifor- mity  of  syntax  makes  it  easy  for  humans  to  think  about  the  written  code  as  another  data  that  can  be  manipulated. It becomes easy to think about higher order code (i.e. code that writes or modifies code)”  (“Homoiconic Languages” 2007, para.7). Hal can incrementally make changes in its existing image rec- ognition software, and test each variation, and each variation with a new variation, and so on, until Hal  has authored new image recognition software with superior results. This method is called the reflective  tower; “in fact, in his design, the interpreter Pi is used to run the code of the interpreter Pi-1, and so on,  with the interpreter P1 running the base program. This stack of interpreters is called a reflective tower”  (Malenfant, Jacques, and Demers 1996, 4).
Alternately,  for  a  skeptical  perspective  on  the  ability  of  artificial  intelligence  to  reflect,  see  Ekbia  2008.
9. Professor Hawking has warned that computers capable of improving their own designs could pose a danger to humans (Cellan-Jones 2014). (Hawking warns that the creation of thinking machines poses a threat to humans’ existence. He notes that the primitive forms of artificial intelligence developed so far have  proved  useful.  Yet  he  also  observes  that  humans,  limited  by  slow  biological  evolution,  could  not keep up with a computer that can improve its own design without the need for human manipulation. Rollo  Carpenter,  creator  of  Cleverbot,  opines  that  achieving  full  artificial  intelligence  may  happen  in the next few decades.) Other key opinion leaders have similar concerns. Indeed, Musk recently donated $10 million to the Future of Life Institute, which focuses on threats posed by advances in artificial intel- ligence (Isidore 2015; Love 2014). Musk is concerned that society is approaching the singularity. Artifi- cial intelligence may be indifferent to human welfare and could solve problems in ways that could lead to  harm  against  humans.  Gates,  Microsoft’s  founder,  is  also  troubled  by  the  possibility  that  artificial intelligence  could  grow  too  strong  for  people  to  control  (Rawlinson  2015).  (Gates  notes  that  at  first, machines will be helpful in completing tasks that may be too difficult or time consuming for humans. He  warns  that  a  few  decades  after,  however,  artificial  intelligence  may  be  strong  enough  to  be  a  con- cern. Gates believes that Microsoft will see more progress than ever over the next three decades in the area of artificial intelligence.) Musk and other modern scientists are not the first ones to seriously ques- tion the possible threats posed by artificial intelligence (e.g., Good 1965).
10. Naturally occurring DNA sequences cannot be patented, but artificially created DNA is eligible for patent protection (Association for Molecular Pathology v. Myriad Genetics, Inc., 469 U.S. ___, 133 S. Ct. 2107) (2013).
11. 35 U.S.C. § 102 (2012).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 219
5/20/2016
1:45:53 PM

    387
    220 Notes
12. Ibid. The purpose of the requirement that the specification describe the invention in such terms that  one  skilled  in  the  art  can  make  and  use  the  claimed  invention  is  to  ensure  that  the  invention  is communicated to the interested public in a meaningful way.
13. The issue of computational invention and intellectual property protection has been considered “since  the  1960s  when  people  began  thinking  about  the  impact  of  computers  on  copyright”  (Miller 1993, 1043). Arthur R. Miller argued that “computer science does not appear to have reached a point at which  a  machine  can  be  considered  so  ‘intelligent’  that  it  truly  is  creating  a  copyrightable  work.” Rather, “for the foreseeable future, the copyrightability of otherwise eligible computer-generated works can be sustained because of the significant human element in their creation, even though there may be some  difficulty  is  assigning  authorship”  (ibid.,  1073).  Abraham  Kaminstein,  the  register  of  copyrights, reported  that  by  1965,  the  Copyright  Office  (1966)  had  received  registrations  for  an  abstract  drawing and musical composition created by a computer.
Most  of  the  focus  on  computational  invention  and  intellectual  property  has  been  in  the  copyright  area rather than the patent context; Pamela Samuelson (1985, 1200), for example, argues that comput- ers cannot be authors because they do not need incentives to generate output: “Only those stuck in the  doctrinal  mud  could  even  think  that  computers  could  be  ‘authors.’”  Annemarie  Bridy  (2012,  27)  remarks “that AI authorship is readily assimilable to the current copyright framework through the work  made  for  hire  doctrine,  which  is  a  mechanism  for  vesting  copyright  directly  in  a  legal  person  who  is  acknowledged not to be the author-in-fact of the work in question.”
Among  those  addressing  the  patentability  implications  of  computational  invention,  Ralph  Clifford  (1996)  has  contended  that  works  generated  autonomously  by  computers  should  remain  in  the  public  domain unless artificial intelligence develops a consciousness that allows it to respond to the Copyright  Act’s incentives (see also Vertinsky and Rice 2002). Colin R. Davies (2011) has argued more recently that  a computer should be given legal recognition as an individual under UK law to allow proper attribution  of authorship and permit respective claims to be negotiated through contract.
14. Most, but not all, of the inventions in this hypothetical are required to be assigned to the com- pany  under  the  employment  contract.  Abbott  Biologics  is  headquartered  in  California,  where  employ- ees  are  permitted  to  retain  ownership  of  inventions  that  are  developed  entirely  on  their  own  time without  using  their  employer’s  equipment,  supplies,  facilities,  or  trade  secret  information,  except  for inventions  that  either:  related  at  the  time  of  conception  or  reduction  to  the  practice  of  the  invention to  the  employer’s  business,  or  actual  or  demonstrably  anticipated  research  or  development  of  the employer;  or  resulted  from  any  work  performed  by  the  employee  for  the  employer  (California  Labor Code § 2872[a]).
15. 35 U.S. Code § 154.
16. In re Hardee, 223 U.S.P.Q. (BNA) 1122, 1123 (Commissioner of Patents and Trademarks, 1984). See also Board of Education ex rel. Board of Trustees of Florida State University v. American Bioscience Inc., 333  F.3d  1330,  1340,  67  U.S.P.Q.  2d  (BNA)  1252,  1259  (Fed.  Cir.  2003)  (“invention  requires  concep- tion.” With regard to the inventorship of chemical compounds, an inventor must have a conception of the specific compounds being claimed. “General knowledge regarding the anticipated biological proper- ties of groups of complex chemical compounds is insufficient to confer inventorship status with respect to  specifically  claimed  compounds”).  See  also  ex  parte  Smernoff,  215  USPQ  545,  547  (Bd.  App.  1982)
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 220
5/20/2016
1:45:53 PM

    388
    Notes 221
(“one who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is  not an coinventor”).
17. Townsend v. Smith, 36 F.2d 292, 295, 4 U.S.P.Q. (BNA) 269, 271 (C.C.P.A. 1930).
18. “Conception  is  established  when  the  invention  is  made  sufficiently  clear  to  enable  one  skilled  in the  art  to  reduce  it  to  practice  without  the  exercise  of  extensive  experimentation  or  the  exercise  of inventive  skill.”  Hiatt  v.  Ziegler,  179  U.S.P.Q.  (BNA)  757,  763  (B.  P.  I.  1973).  Conception  has  been defined as a disclosure of an idea that allows a person skilled in the art to reduce the idea to a practical form without “exercise of the inventive faculty.” Gunter v. Stream, 573 F.2d 77, 79, 197 U.S.P.Q. (BNA) 482 (C.C.P.A. 1978).
19. Actual reduction to practice “requires that the claimed invention work for its intended purpose.” Brunswick Corporation v. United States, 34 Fed. Cl. 532, 584 (1995). Constructive reduction to practice “occurs  upon  the  filing  of  a  patent  application  on  the  claimed  invention.”  Brunswick  Corporation  v. United  States,  34  Fed.  Cl.  532,  584  (1995).  The  written  description  requirement  is  “to  ensure  that  the inventor had possession, as of the filing date of the application relied on, of the specific subject matter later  claimed  by  him.”  In  re  Edwards,  568  F.2d  1349,  1351–52,  196  U.S.P.Q  (BNA),  465,  467  (C.C.P.A. 1978).
20. De Solms v. Schoenwald, 15 U.S.P.Q. 2d (BNA) 1507, 1510 (B.P.A.I. 1990).
21. Ex parte Smernoff, 215 U.S.P.Q. (BNA) 545, 547 (P.T.O. Bd. App. 1982) (“one who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not an coinventor”).
22. In re DeBaun, 687 F.2d 459, 463, 214 U.S.P.Q. (BNA) 933, 936 (C.C.P.A. 1982); Fritsch v. Lin, 21 U.S.P.Q. 2d (BNA) 1737, 1739 (B.P.A.I. 1991).
23. In this case, for instance, it is likely that both could qualify as inventors. What is required is some “quantum  of  collaboration  or  connection.”  Kimberly-Clark  Corporation  v.  Procter  and  Gamble  Distri- bution Co., 973 F.2d 911, 916–17, 23 U.S.P.Q. 2d (BNA) 1921, 1925–26 (Fed. Cir. 1992). For joint inven- torship,  “there  must  be  some  element  of  joint  behavior,  such  as  collaboration  or  working  under common  direction,  one  inventor  seeing  a  relevant  report  and  building  upon  it  or  hearing  another’s suggestion  at  a  meeting”  (ibid.);  Moler  v.  Purdy,  131  U.S.P.Q.  (BNA)  276,  279  (B.P.I.  1960)  (“it  is  not necessary that the inventive concept come to both [joint inventors] at the same time”).
24. See, for example, Advanced Magnetic Closures, Inc. v. Rome Fasteners Corp., 607 F.3d 817 (Fed. Cir. 2010).
25. Conception has been identified as a mental process (“formation in the mind of the inventor, of a definite and permanent idea of the complete and operative invention, as it is hereafter to be applied in practice”).  Hitzeman  v.  Rutter,  243  F.3d  1345,  58  U.S.P.Q.  2d  (BNA)  1161  (Fed.  Cir.  2001).  “The  term ‘inventor’  means  the  individual  or,  if  a  joint  invention,  the  individuals  collectively  who  invented  or discovered the subject matter of the invention.” 35 U.S.C. 100(f) (2012).
26. See  the  Trade-Mark  Cases,  100  U.S.  82,  94  (1879)  (noting  that  “copyright  law  only  protects ‘the  fruits  of  intellectual  labor’  that  ‘are  founded  in  the  creative  powers  of  the  mind.’”),  cited  in  US Copyright Office 2014.
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 221
5/20/2016
1:45:53 PM

    389
    222 Notes
27. While he was a bacteriologist at St. Mary’s hospital in London, Alexander Fleming realized that a mold  had  contaminated  his  samples  of  Staphylococcus.  When  he  examined  his  dishes  under  a  micro- scope,  he  noticed  that  the  mold  prevented  the  growth  of  Staphylococcus  (Market  2013).  The  area around the mold contained a strain of pencillium notatum. Fleming discovered that it could kill many different  types  of  bacteria.  Decades  later,  Howard  Florey  at  Oxford  University  headed  efforts  to  purify penicillin for use in therapeutic applications (American Chemistry Society, n.d.). It proved to be invalu- able during World War II for controlling wound infections (Market 2013).
Saccharin—the first artificial sweetener—was discovered by accident by Constantin Fahlber in 1884.  He  had  been  working  with  compounds  derived  from  coal  tar  and  accidently  ate  something  without  washing  his  hands.  Fahlber  noticed  a  sweet  taste,  which  he  later  traced  to  benzoic  sulfilimine.  Some  reports  hold  that  it  was  his  partner,  Ira  Remsen,  who  first  noticed  that  the  tar  compound  was  sweet.  While  useful  during  World  War  I  when  sugar  was  scarce,  it  was  only  in  the  1960s  and  1970s  that  saccharin  became  popular  as  a  way  to  sweeten  while  avoiding  the  calories  contained  in  regular  sugar  (Clegg 2012).
28. Conception requires contemporaneous recognition and appreciation of the invention. Invitrogen Corporation v. Clontech Laboratories, Inc., 429 F.3d 1052, 1064, 77 U.S.P.Q. 2d (BNA) 1161, 1169 (Fed. Cir. 2005) (“the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue”).
29. Silvestri v. Grant, 496 F.2d 593, 596, 181 U.S.P.Q. (BNA) 706, 708 (C.C.P.A. 1974) (“an accidental and  unappreciated  duplication  of  an  invention  does  not  defeat  the  patent  right  of  one  who,  though later in time was the first to recognize that which constitutes the inventive subject matter”).
30. Originally, the active ingredient in Viagra was intended as a cardiovascular drug to lower blood pressure  (Fox  News  2008).  The  trials  for  this  intended  use  were  disappointing  until  volunteers  began reporting a strange side effect: erections (Jay 2010).
Botox is a branded formula of botulinum toxin type A manufactured by Allerga (“Medication Guide:  Botox,  n.d.).  Botulinum  toxin  is  a  protein  produced  by  the  bacterium  Clostridium  botulinum  (Monte- cucco and Molgó 2005). It was used in the late 1700s as a food poison and gained attention in the 1890s  for its potential use as a biological weapon; “one gram [of botulinum toxin] has the potential to kill one  million  people”  (Ting  and  Freiman  2004).  In  the  1960s,  however,  Drs.  Alan  Scott  and  Edward  Schantz  discovered botulinum toxin type A’s ability (in small doses) to block the transmission of nerve impulses  and  paralyze  hyperactive  muscles  to  treat  eye,  facial,  and  vocal  spasms  (ibid.,  259–260).  These  novel  developments  led  to  the  accidental  discovery  that  botulinum  type  A  injections  also  reduced  wrinkles;  physicians  quickly  began  administering  Botox  as  wrinkle  reduction  treatment  well  before  the  FDA  finally  approved  Botox  for  this  use  in  2002  (Ghose  2014).  Since  then,  Botox  has  steadily  expanded  to  treat  over  twenty  different  medical  conditions,  including  chronic  headaches,  overactive  bladder,  and  urinary incontinence (Nichols 2015; Skincare-news.com Team, n.d.; FDA 2010, 2013).
31. Commercialization theory holds that patents are important in providing incentives for investment in increasing the value of a patented technology (see Kitch 1977, 276–277).
32. It has been estimated that prehuman expenditures are 30.8 percent of costs per approved com- pound, and an estimate of average pretax industry cost per new prescription drug approval (inclusive of
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 222
5/20/2016
1:45:53 PM

    390
    Notes 223
failures and capital costs) is $2.55 billion (Tufts Center for the Study of Drug Development 2014). The  cost of new prescription drug approval is hotly contested (e.g., Collier 2009).
33. Although some human inventors also appear to lack a moral compass (Ho 2000). 34. 35 U.S.C. §101 (2012).
35. Ibid.
36. 5 Opinion of the Court Jefferson 75–76 (H. Washington ed. 1871). “In choosing such expansive terms [for the language of section 101] ... modified by the comprehensive ‘any,’ Congress plainly con- templated  that  the  patent  laws  would  be  given  wide  scope.”  Diamond  v.  Chakrabarty,  447  U.  S.  303, 308 (1980).
37. Bilski v. Kappos, 561 U.S. 593, 593–96 (2010). So “a new mineral discovered in the earth or a new plant  found  in  the  wild  is  not  patentable  subject  matter.”  Diamond  v.  Chakrabarty,  447  U.  S.  309 (1980).  “Likewise,  Einstein  could  not  patent  his  celebrated  law  that  E  =  mc2;  nor  could  [Isaac]  Newton have  patented  the  law  of  gravity”  (ibid.).  Nor  is  a  mathematical  formula,  electromagnetism  or  steam power, or the qualities of bacteria patentable (ibid.).
38. Alice Corp. v. CLS Bank, 573 U. S. ____ (2014) (slip op., at 5–6). Also, these exceptions existed in various forms for 150 years. See Le Roy v. Tatham, 14 How. 156, 174.
39. Ibid. As courts acknowledge, all patents rely to some extent on these exceptions and have the potential to hinder as well as promote future innovation (ibid.).
40. The information covered by these exceptions is “part of the storehouse of knowledge of all men ... free to all men and reserved exclusively to none.” Funk Brothers Seed Co. v. Kalo Inoculant Co., 333 U. S. 127, 130 (1948).
41. 35 USC 161.
42. Section 101 is a “dynamic provision designed to encompass new and unforeseen inventions.” J.E.M. Ag Supply, Inc. v. Pioneer Hi-Bred International, Inc., 534 U. S. 124, 135 (2001). As the Supreme Court stated  in  Bilski  v.  Kappos,  “For  example,  it  was  once  forcefully  argued  that  until  recent  times,  ‘well- established  principles  of  patent  law  probably  would  have  prevented  the  issuance  of  a  valid  patent  on almost any conceivable computer program.’” Bilski v. Kappos, 561 U.S. 593, 605 (2010), citing Diamond v. Diehr, 450 U.S 175, 195 (1981) (STEVENS, J., dissenting). But this fact does not mean that unforeseen innovations such as computer programs are always unpatentable (ibid.).
43. Diamond v. Chakrabarty, 447 U. S. 303, 315 (1980).
     Sugimoto,—Big Data Is Not a Monolith
    10309_016.indd 223
5/20/2016
1:45:53 PM

391
Global Perspectives and Challenges for the Intellectual Property System A CEIPI-ICTSD publications series
 Intellectual Property and Digital Trade in
the Age of Artificial Intelligence and Big Data
  Edited by Xavier Seuba, Christophe Geiger and Julien Penin
With contributions by
Keith E. Maskus, Yann Ménière, Ilja Rudyk, Sean M. O’Connor, Catalina Martínez, Peter Bittner, Alissa Zeller, Reto Hilty, Christophe Geiger, Giancarlo Frosio, Oleksandr Bulayenko, Ryan Abbott, Timo Minssen, Jens Schovsbo, Francesco Lissoni, Gabriele Cristelli, and Claudia Jamin
Issue Number 5 June 2018
     
392
Inventive Machines: Rethinking
Invention and Patentability
Ryan Abbott

Global Perspectives and Challenges for the Intellectual Property System 115
393
Computers are doing more than ever before.1 They are doing it cheaper, faster, and often better than their human counterparts, and on an unprecedented scale. Take, for example, Amazon’s Kiva robots, which help retrieve and package items. Amazon now has 45,000 of these robots working together with 230,000 human employees. I suspect it will not be long until there are 230,000 next- generation Kiva robots working together with 45,000 human employees. Or, perhaps, 5,000 next- generation robots and no human employees.
Robots are doing more than manual labour—they are working as doctors, lawyers, and scientists. They are also getting pretty good at playing games. IBM’s supercomputer Deep Blue beat world chess champion Garry Kasparov in 1997, IBM’s next-generation supercomputer Watson won a game of Jeopardy! in 2011, and last year Google’s supercomputer DeepMind’s AlphaGo program beat a master Go player, Lee Se-dol. Of course, playing games is just a way for these computers to demonstrate their capabilities. Watson, for instance, is now developing cancer treatment protocols for patients at Memorial Sloan Kettering Center. IBM also has Watson developing new food recipes and doing some tremendous things involving a food truck.
You can now go to IBM’s website and work with Chef Watson to create new recipes. Watson is less restricted by preconceptions about combining foods and flavours than human chefs. That allows Watson to generate recipes that people have not really thought about before. Put another way, Watson is coming up with new, inventive, and industrially applicable compositions. For those of us in patent law, that raises the question of whether Watson’s ideas are patentable, and if so, who would qualify as an inventor for such patents?
It has been at least 20 years since the first autonomous machine invention was patented. The first such invention I am aware of was created by the “Creativity Machine,” which used a neural network architecture. It essentially consisted of a series of networked on-and-off switches connected in a neural network, which generated new output when perturbed. The first network was connected to a second network, which evaluated the output for usefulness. The Creativity Machine was given a goal
1 This article, and the associated presentation, is based on the author’s research on computer generated works. See, for example, Ryan Abbott, “Hal the Inventor: Big Data and Its Use by Artificial Intelligence,” in Cassidy R. Sugimoto, et al. (eds), Big Data Is Not a Monolith (Cambridge, MA: MIT Press 2016); Ryan Abbott, “I Think, Therefore I Invent: Creative Computers and the Future of Patent Law,” Boston College Law Review 57.4 (2016); Ryan Abbott, “Artificial Intelligence, Big Data and Intellectual Property: Protecting Computer-Generated Works in the United Kingdom,” in Tanya Aplin (ed.), Research Handbook on Intellectual Property and Digital Technologies (Cheltenham, UK: Edward Elgar, forthcoming); Ryan Abbott, “Everything is Obvious,” 66 UCLA Law Review, forthcoming. These works are all available at http://ssrn. com/author=1702576. Readers interested in this subject may also be interested in early works by Pamela Samuelson, Arthur Miller, and Ralph Clifford, “Allocating Ownership Rights in Computer-Generated Works,” University of Pittsburgh Law Review 1185 (1986); 1199–1200; Arthur R. Miller, “Copyright Protection for Computer Programs, Databases, and Computer-Generated Works: Is Anything New Since CONTU?” Harvard Law Review 106 (1993): 1043; and Ralph D. Clifford, “Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up?” Tulane Law Review 71 (1997): 1675–1703. A most incomplete list of more recent scholarship includes: Lisa Vertinsky and Todd M. Rice, “Thinking about Thinking Machines: Implications of Machine Inventors for Law,” Boston University Journal of Science and Technology Law 8.2 (2002); Robert Plotkin, The Genie in the Machine (Redwood City, CA: Stanford University Press, 2009); C.R. Davies, “An Evolutionary Step in Intellectual Property Rights: Artificial Intelligence and Intellectual Property,” Computer Law and Security Review, 27.6 (2011); Annemarie Bridy, “Coding Creativity: Copyright and the Artificially Intelligent Author,” Stanford Technology Law Review 5 (2012); J. McCutcheon, “Curing the Authorless Void: Protecting Computer-Generated Works Following ICETV and Phone Directories,” Melbourne University Law Review, 37.1 (2013); Ben Hattenbach and Joshua Glucoft, “Patents in an Era of Infinite Monkeys and Artificial Intelligence,” Stanford Technology Law Review 19 (2015); Jean-Marc Deltorn, “Deep Creations: Intellectual Property and the Automata,” Frontiers in Digital Humanities, 2017, https://www.frontiersin.org/articles/10.3389/fdigh.2017.00003/full; Shlomit Yanisky-Ravid and Xiaoqiong Liu, “When Artificial Intelligence Systems Produce Inventions: the 3A Era and an Alternative Model for Patent Law,” Cardozo Law Review, forthcoming; and W. Michael Schuster, “A Coasean Analysis of Ownership of Patents for Inventions Created by Artificial Intelligence,” Washington and Lee Law Review, forthcoming.
 
116 394 Intellectual Property and Digital Trade in the Age of Artificial Intelligence and Big Data to complete, and from that it independently produced a result. A process like this could be used in a
variety of industries to, say, discover a new polymer or to design a faster semiconductor.
The Creativity Machine, if it were a human being, would be an inventor in these circumstances. Inventorship does not go to the person who instructs someone else to solve a problem. If I tell my research scientist that I would like her to design a better battery and she does, that does not make me an inventor of her battery. The research scientist would be the inventor. In the case of the Creativity Machine, the United States Patent and Trademark Office (USPTO) granted a patent for the machine’s invention, but did so in the name of the machine’s owner. That was an easy decision for the Patent Office as the application had not disclosed the machine’s involvement.
The Creativity Machine may have been the first autonomous machine inventor, but it certainly was not the last. More patents were created autonomously by machines in the 2000s—for example, by the “Invention Machine,” which relied on genetic programming. Inventions autonomously created by the Invention Machine were also issued patents by the USPTO, again under circumstances in which, if the machine had been a person, the machine would have been the inventor.
Of course, right now there may be few machines independently inventing. Most machines are involved in the inventive process as simple tools that help people to “reduce to practice” an invention. If I design an experiment and have my PhD students carry it out without change, and the experiment’s results are patentable, I, and not my students, am probably the inventor for those results. Similarly, most computers are just executing tasks given by people. But at least some of the time, the computer occupies the role of the inventor. I suspect you are not hearing more about autonomous machine inventions because of concerns about patentability. Can a machine be an inventor? Should a machine be an inventor? These are open questions, and they are important theoretical and practical questions because computers are de facto inventing, and inventors have ownership rights in patents. Failure to list inventors can make patents invalid or unenforceable.
I have looked at this primarily from a US law perspective and found no statute that discusses computer inventorship, there is no case law directly on the issue, and there is no relevant patent office policy. However, there are some barriers to computer inventorship. For instance, the 1952 Patent Act uses the term “individual” to describe potential inventors, something that was done to prevent corporate inventorship. There is also quite a bit of judicial language characterising invention as a “mental act.”
While there is no patent office policy on computational inventions or computer-generated works, there is a copyright office policy on computer authorship. That policy dates to 1984 and states that works “authored” by a computer cannot qualify for copyright protection. In England and Wales, the rule is different under the Copyright, Designs and Patents Act of 1988: if a work is computer- generated, the author is the person who makes the arrangements for the creation of the work.
The United States Copyright Office cites the 1886 case of Burrow-Giles v. Sarony in support of its current policy. In that case, a photographer, Napoleon Sarony, sued the Burrow-Giles Lithographic Company for copyright infringement of a famous photograph of Oscar Wilde. The company alleged that the photographer could not be the photograph’s author because a photograph is just a mechanical reproduction of a natural phenomenon. The Court held that any form of writing by which a mental idea is given visible expression is eligible for copyright protection.

Global Perspectives and Challenges for the Intellectual Property System 117
395
The case thus explicitly dealt with whether the use of a machine would negate human authorship, and implicitly with whether a camera could be considered an author. If it seems unwise to rely on dicta from the Gilded Age to formulate policies on machine authorship—well, that is what is happening. This policy was relevant to a recent case in the Ninth Circuit in California involving the famous “Monkey Selfies.” In that case, a crested macaque in Indonesia took pictures of itself using equipment belonging to a nature photographer, David Slater. Mr Slater promptly claimed copyright in the photographs. Eventually, the United States Copyright Office clarified that because only a person could be an author, that copyright could not subsist in the Monkey Selfies. People for the Ethical Treatment of Animals (PETA) sued Mr Slater in the United States Federal Court for copyright infringement on behalf of the macaque, alleging that the primate should be the copyright owner of its own photographs. The case ultimately settled, with Mr Slater agreeing to donate 25% of future revenue from his use of the photograph to charities dedicated to protecting crested macaques in Indonesia.
If we analogise this copyright case law to the patent context, then maybe a computer cannot be an inventor and its discoveries enter in the public domain. Computers do not need incentives to invent, and permitting computer inventorship might chill human invention.
However, there is a way around computer involvement that works in the patent but not the copyright context. Inventorship can also be based on recognition of inventive subject matter. Thus, a person may be an inventor by virtue of recognising that a computer has invented something patentable. This is almost certainly how the problem is being dealt with today in practice—just as it was for the earliest computational inventions. It avoids having to disclose an inventive computer to the USPTO and potentially throwing a wrench into a patent application. For patent attorneys, there is no incentive right now to disclose inventive activity by computers, and plenty of incentive not to make that disclosure.
The system of invention by recognition seems reasonable if you are the first scientist to notice that penicillin is inhibiting bacterial growth, but perhaps not if you are taking the credit for the work of another inventive entity—even if that entity is a computer. In the latter case, the system is rewarding people even if they are not doing anything inventive themselves. A computer might clearly identify its own results as being patentable. For that matter, it might even format its results as a patent application. Claiming credit for the work of a machine also devalues human invention, because it equates the contributions of people using inventive machines and human inventors who have legitimately engaged in inventive activity.
Taking credit for a machine’s work also has the potential to create logistical problems when the first person to notice a computer’s results is not the computer’s owner or the person who gave the computer a goal to complete. This may incentivise computer owners to restrict access to their machines so that they can control ownership of inventive output.
More ambitiously, I argue that we should recognise computers as inventors. This will functionally produce more invention because it will incentivise the development of creative computers. That is because allowing computer owners to patent the output of their machines makes those machines more valuable. The constitutional rational for granting patent inventions in the United States is based on an incentive theory. We want patents because of the free-rider problem and because

118 396 Intellectual Property and Digital Trade in the Age of Artificial Intelligence and Big Data
patents are thought to generate additional research and discovery. Even though computers do not care about incentives, people who design computers do. Acknowledging computers as inventors would reward effort upstream of the stage of invention, and it could also promote disclosure and commercialisation of patentable subject matter. It would also validate inventor moral rights, because it will distinguish between human inventors who contribute conceptually to an invention and persons filing applications based on the autonomous output of machines.
What about the potential barriers we discussed—that invention must be a mental act and that an inventor must be an individual? Well, there are computers that generate output in a process akin to a person’s mental act—for instance, computers that utilise neural networks. There are also computers that generate output in totally different ways, like those that use expert-based systems. Should it matter how a computer is designed and how it functions?
I would argue no. We should care functionally about whether the system generates innovation, not how innovation occurs. Congress came to that same conclusion in 1952 when it abolished the “flash of genius” test. That was an old requirement that required that the inventive spark come to a person in an “aha” moment rather than as the result of methodical, laborious research. The nature of the test was never entirely clear; it involved judges subjectively reasoning about what an applicant might have been thinking. Congress eventually decided it was not a good test, and that we should not care about what goes on in someone’s head, just whether what they create is inventive and beneficial for society.
Similarly, the requirement that individuals should be inventors should not interfere with computer inventorship. That language is from the 1950s—long before the issue of computational invention was relevant. It should be interpreted according to dynamic principles of statutory interpretation in light of the purpose of the original Act, which was to prevent corporate inventorship.
If a computer could be an inventor, who would own its patent? I am not arguing that a computer should own a patent. Computers are owned as property and do not have legal rights. I would argue that the computer’s owner should be the automatic assignee of anything the computer develops. Where multiple parties are involved, such as software developers, computer owners, and users, they could work out issues of ownership by contract, starting with the default position that the computer owner is the assignee. This would be the most consistent with the way we treat personal and intellectual property right now.
Computational invention has exciting implications beyond inventorship. I think creative computers are going to change the entire patent paradigm in the next 10–20 years.
Even more interesting than thinking about how computers and people are competing right now in inventive activity is that computers are very soon going to overwhelm people in inventive activity. Take biotechnology research on antibodies as an example. There are lots of patents on antibody structures. However, there are only so many ways you can string proteins together to make an antibody, and it is not that difficult to imagine a sufficiently powerful computer sequencing every conceivable antibody and publishing those results online. Assuming this would be an anticipatory disclosure, it would prevent anyone from patenting the structure of those antibodies. The computer could not patent the antibody structures itself because it would not know their utility, which is

Global Perspectives and Challenges for the Intellectual Property System 119
397
another requirement for patentability. But an inventive machine would have just wiped out an entire field of human research.
As computers grow increasingly faster, cheaper, and more sophisticated, they are going to play an ever-greater role in the inventive process. It will become standard for creative computers to automate invention. Someone in the chemical sciences who used to discover new chemical compounds through deductive reasoning and trial and error with teams of human researchers will instead use artificial intelligence to find new compounds. Right now, the hypothetical “person having ordinary skill in the art,” or PHOSITA, is the benchmark we use to judge inventiveness. If the skilled person uses inventive machines, or is an inventive machine, then the benchmark is very high. It is hard to conceive of an invention that would not be obvious to a sufficiently sophisticated computer. That would essentially mean the end of inventive activity. Everything will be obvious.

398
ARTIFICIAL INTELLIGENCE, BIG DATA AND INTELLECTUAL PROPERTY: PROTECTING COMPUTER-GENERATED WORKS IN THE UNITED KINGDOM
RYAN ABBOTT*
Abstract: Big data and its use by artificial intelligence (AI) is changing the way intellectual property is developed and granted. For decades, machines have been autonomously generating works which have traditionally been eligible for copyright and patent protection. Now, the growing sophistication of AI and the prevalence of big data is positioned to transform computer- generated works (CGWs) into major contributors to the creative and inventive economies. However, intellectual property law is poorly prepared for this eventuality. The UK is one of the few nations, and perhaps the only EU member state, to explicitly provide copyright protection for CGWs. It is silent on patent protection for CGWs.
This chapter makes several contributions to the literature. First, it provides an up-to-date review of UK, EU and international law. Second, it argues that patentability of CGWs is a matter of first impression in the UK, but that CGWs should be eligible for patent protection as a matter of policy. Finally, it argues that the definition of CGWs should be amended to reflect the fact that a computer can be an author or inventor in a joint work with a person.
Keywords: computer-generated works, artificial intelligence law, big data and intellectual property, international law, patents
I. INTRODUCTION
Big data and its use by artificial intelligence (AI) is changing the way intellectual property is developed and granted. For decades, machines have been autonomously generating works which have traditionally been eligible for copyright and patent protection.1 For instance, in the US, the first “computer-generated work” (CGW) was submitted for copyright registration prior to 1965. The US Patent and Trademark Office (USPTO) has granted patents for inventions autonomously generated by computers as early as 1998. Terms such as “computers” and “machines” are used in this chapter interchangeably to refer to computer programs or software rather than to physical devices or hardware. As AI continues to grow exponentially more sophisticated and powerful, and the amount of data available to these machines keeps pace, CGWs should become a major contributor to the creative and inventive economies.2
This chapter considers the phenomenon of CGWs from a UK, EU and international law perspective. There is little law on the subject. UK law explicitly provides for copyright protection of CGWs, and in this respect, it is an outlier in the EU and internationally. However, UK law is silent on patent protection. No UK, EU or international law explicitly prohibits protection for
* Professor of Law and Health Sciences, University of Surrey, School of Law and Adjunct Assistant Professor of Medicine at the David Geffen School of Medicine at University of California, Los Angeles. This is a draft chapter. The final version will be available in Research Handbook on Intellectual Property and Digital Technologies edited by Tanya Aplin, forthcoming, Edward Elgar Publishing Ltd. The material cannot be used for any other purpose without further permission of the publisher, and is for private use only.
 
399
CGWs, but rarely are such works explicitly protected. Legal instruments and judicial language related to both copyright and patents frequently refer to authors and inventors as natural persons, or restrict authorship or inventorship to natural persons, but this is most likely in response to the prospect of corporate authorship and inventorship. Such language does not appear to be the result of seriously considering CGWs and should not prohibit IPRs as a matter of policy.
This chapter begins by describing the phenomenon of CGWs and then reviewing the relevant law. It seeks to resolve the following questions: Are computers autonomously creating or inventing or merely aiding human authors and inventors? How will inventive machines alter research and development? Can a CGW receive copyright or patent protection? Can a person qualify as an author or inventor for a machine’s output? Who would own IPRs associated with a CGW? These and other questions can be answered by referring to the fundamental policy rationales for IPRs, and by analogy to instances of human authorship and invention.
The chapter argues that patentability of CGWs is a matter of first impression in the UK, but that CGWs should be eligible for patent protection. This would incentivize the development of inventive machines, which will ultimately result in more innovation. Acknowledging machines as inventors would also safeguard moral rights, because it would prevent people from receiving undeserved acknowledgement.
The chapter also proposes that the standard for CGWs should be amended—for copyright as well as patent. Rather than treating a CGW as a work “generated by a computer in circumstances such that there is no human author of the work”, a CGW should be a work “generated by a computer in circumstances such that the computer, if a natural person, would be an author.” Similarly, for patents, CGW should be a work “generated by a computer in circumstances such that the computer, if a natural person, would be an inventor.” This would take into account the fact that people and machines often work collaboratively, and that even with the involvement of a person a machine can contribute as an author or inventor in its own right.
Finally, this chapter argues there is a need for an internationally harmonized approach to CGWs. Most jurisdictions in the EU, and worldwide, have yet to decide how to regulate CGWs. Failure to internationally harmonize may disadvantage countries which permit IPRs for CGWs, and advantage those which do not.
II. CREATIVE COMPUTERS AND INVENTIVE MACHINES
The Growing Sophistication of AI
Much has been written about the increasing capacity of AI to engage in knowledge-work. Indeed, hardly a day goes by without a news article describing some new feat achieved by AI, whether it is IBM’s AI system DeepBlue beating Garry Kasparov at Chess, IBM’s Watson winning a game of Jeopardy, or Google’s DeepMind defeating a Go world champion in 2016. DeepMind’s Go victory was unexpected at the time because of the sheer complexity of the game, which has more potential Go board configurations than there are atoms in the Universe. AI systems are playing games to demonstrate their capabilities and to train, but they are also being applied to solve practical problems. Watson, for example, is being used to find new uses for existing drugs—an activity that has traditionally been fertile grounds for generating patentable inventions.
Computer knowledge-work can be thought of on a spectrum. On the one end, computers may function as simple tools that assist human authors and inventors, much the way that a pen or a wrench can help someone to write or invent. Works generated in this fashion have been referred to as “works created using a computer”, and likely account for the vast majority of human-machine

400
collaboration. While it could not be seriously argued that Microsoft Word should be a co-author of this chapter, it did contribute to the chapter’s creation. At times, Word corrects spelling, automatically formats, and even suggests the use of certain words.
The term “intermediate works” has been used to refer to more substantive contributions made by computers to creative works where a person qualifies as an author or inventor. It may be difficult to precisely distinguish between an intermediate work and a work created using a computer. Word probably could not contribute to an intermediate work, but a variety of publicly available software programs can. For instance, “Band-in-a-Box” allows a user to choose chords and styles, and the program then automatically generates a “complete professional-quality arrangement of piano, bass, drums, guitar, and strings or horns.”3 Other programs can make similarly substantive contributions to different types of creative works, such as novels and films. In some instances of intermediate works, it may be the case that the computer would qualify as a joint author or inventor, if it were a natural person.
At the other end of the spectrum, computers generate works under circumstances in which no human author or inventor can be identified. These are often referred to as CGWs or “works created by a computer”. While not widely appreciated, computers have been creating CGWs for decades. As an interesting example of the interplay between copyright and patent, in 2003, technologist Raymond Kurzweil, now a Director of Engineering at Google, was granted a patent on a computer program that could autonomously generate creative writings—the “Cybernetic Poet.” Incidentally, Mr. Kurzweil now predicts that machines will have human levels of intelligence in about a decade.
The argument has been made that a human author or inventor exists for any CGW, in the sense that, “behind every good robot is a good person.”4 It is true that a programmer (or many programmers and developers) has to create computer software, and in some cases it may make sense to impute authorship or inventorship to a programmer—particularly if a programmer develops an algorithm specifically to solve a particular problem or to generate a particular output. In these cases a programmer might have a significant contribution to a machine’s speicifc output. However, it may also be the case that a programmer creates an algorithm with no expectation or knowledge of the problems it will go on to solve. Some AI systems such as neural networks can behave unpredictably, such that their original programmers may not understand precisely how they function.5 Some computer systems, such as those based on genetic programming, may even be able to alter their own code. By analogy to human inventorship, an inventor’s teachers, mentors and even parents do not qualify as inventors on their patents, at least, not without directly contributing to the conception of a specific invention.
Attributing authorship or inventorship to a computer user, rather than a programmer, is also problematic. It may sometimes be the case that a user makes a significant contribution to a computer’s output, or that formulating instructions to a computer requires significant skill. However, it may also be the case that a user simply asks a computer to solve a problem, and the computer proceeds to independently generate an answer. In the future, it may even be the case that the computer is able to identify that its output is eligible for copyright or patent protection. In such cases, it seems difficult to argue that the user is an author or inventor. Again, by analogy to human works, simply instructing another person to solve a problem does not usually qualify for authorship or inventorship.
Thus, in at least some instances, computers are generating works traditional entitled to copyright and patent protection under circumstances in which no natural person qualifies as an author or inventor according to traditional criteria. In practice, it may be difficult to distinguish

401
between works created using a computer, intermediate works, and works created by a computer. However, this is not unlike making sense of human authorship and inventorship for joint works where individuals make diverse contributions.
Where’s the CGW?
Given these technological advances, one would be forgiven for asking—where are the CGWs? Why are there not routinely lawsuits over CGWs? How have countries managed without legal standards for CGWs?
It may be that the creative AI revolution has yet to arrive. CGWs may be few and far between, or lack commercial value. When Scott French programmed a computer to write a novel in the style of a famous author in 1993, the resulting work was described by one critic as, “a mitigated disaster”.6 Likewise, with regard to inventions, computers may rarely be inventing, or these outputs may lack significant utility.
It may also be that computers are creating CGWs, but that this is not being disclosed. There are good reasons to think this may be the case. In the US, for example, CGWs are not entitled to copyright protection. In 1965, the US Copyright Office reported it received several applications for CGWs. Given the exponential improvements in computer science, one would thus expect a similarly exponential increase in CGWs submitted for copyright protection from 1965 until the present. However, at least as early as 1973, the US Copyright Office elected to deny protection for CGWs.7 As a result, anyone in possession of a potentially valuable CGW would disqualify protection for the work by revealing its origins. A computer user wishing to obtain protection for a CGW may thus end up identifying himself or herself as the author. Similarly, in the UK, it is not clear that CGWs are entitled to patent protection. Computer users may thus elect to identify themselves as inventors for CGWs. Indeed, some of the earliest applicants for patents on CGWs were advised by their attorneys to report themselves as inventors.8
Failing to disclose the machine’s role in a CGW may also seem an appealing option because it is unlikely to be challenged. For instance, in the UK, CGWs are protected by copyright without registration, and the UK Intellectual Property Office (IPO) will not dispute a patent applicant’s reported inventorship unless this is challenged by a third-party. The issue of authorship or inventorship of a CGW may not arise until litigation, and even that is unlikely. When human authors and inventors have a disagreement about relative contributions, there will generally be one or more parties with an adverse legal interest. However, if a user takes credit for a computer’s invention, the computer is not in a position to protest. A legal dispute will probably only occur in cases where an alleged infringing party wants to dispute copyright or patent protection can subsist in a CGW, and somehow becomes aware that a computer was involved in generating the work.
This situation with respect to CGWs is a problematic state of affairs. It is important that authorship and inventorship be accurately attributed, both to optimize the use of copyright and patents as economic incentives, and to preserve the moral rights of natural persons. Establishing an author or inventor’s identity is important because whether the work qualifies for protection in the UK may depend on the author’s national status. It also identifies the first owner of copyright or patent, may base the term of copyright protection on the author’s death, and determines whether there are moral and rental rights belonging to an author. In whatever manner nations elect to protect CGWs, including by providing no protection, appropriate identification of the origin of CGWs is necessary for IPRs to function effectively as economic rights. Even with regard to moral rights, failure to designate a computer as an author or inventor may result in individuals taking credit for

402
works they have not personally generated. This may undermine the value of human authorship and inventorship.
Determining computer authorship and inventorship may be a complex endeavor. However, that is already the case with natural persons. For instance, despite the romantic conception of inventors as lone prodigies tinkering in their garages and experiencing flashes of genius, the vast majority of invention comes from industry and academic work where multi-person collaborations are the norm. Inventorship disputes are becoming more common,9 and determining inventorship in collaborative work is “one of the muddiest concepts in the muddy metaphysics of the patent law”.10
III. LEGAL STANDARDS
Intellectual property in the UK is primarily governed at the national level, subject to compliance with certain EU requirements and international treaties.
United Kingdom Standards for Computer-Generated Works
The Copyright, Designs and Patents Act 1998 (“CDPA”) is the primary legislation for copyright law.11 Copyright is an intellectual property right which subsists in certain creative works such as books, music and movies. It gives its owner the exclusive right to exploit the underlying subject matter for a fixed number of years, generally 70 years plus the life of the author, subject to certain exceptions such as fair dealing. Generally, the author of a work is the person who creates it, and the author is the default copyright owner. A notable exception is that an employer will be the default owner if a work is “made by” an employee in the course of employment. In some instances, an “author” can be a body incorporated in the UK, such as a limited company.12 Special authorship rules apply to “entrepreneurial” or “media” works—sound recordings, films, broadcasts and typographical works—that are produced rather than created, whereby legal entities are accepted as authors.
The CDPA makes special provision for CGWs with different rules for authorship and copyright duration. These works are defined as those “generated by a computer in circumstances such that there is no human author of the work[s].” CDPA §178. For these works, the CDPA provides that, “[i]n the case of a literary, dramatic, musical or artistic work which is computer- generated, the author shall be taken to be the person by whom the arrangement necessary for the creation of the work are undertaken.” CDPA §9(3). Of note, this protection only extends to literary, dramatic, musical and artistic works and not to media works, although a similar system to §9(3) also applies with regard to design rights.13 For CGWs, the term of the copyright is fifty years from the end of the calendar year in which the work was made.14
At least two cases considered CGWs under the Copyright Act 1956, the statutory regime prior to the CDPA.15 This statute had no provisions for CGWs.16 In Express Newspapers plc v Liverpool Daily Post & Echo [1985] FSR 306, the plaintiff newspaper Daily Express conducted a ‘Millionaire of the Month’ competition. It distributed cards with a five-letter code, and the public could check these cards against a daily newspaper grid, generated by a computer, to see if they won a prize. The defendant newspaper copied these grids, and was subsequently sued for copyright infringement. One argument advanced by the defendant was that because the grids were produced with the aid of a computer, they had no human author and thus could not be protected by copyright. Whitford J rejected this argument, stating, “[t]he computer was no more than the tool by which the varying grids of five-letter sequences were produced to the instructions, via the computer programs, of [the programmer]. It is as unrealistic [to suggest the programmer was not the author]

403
as it would be to suggest that, if you write your work with a pen, it is the pen which is the author of the work rather than the person who drives the pen.” Id. Whitford J also noted “that a great deal of skill and indeed, a good deal of labour went into the production of the grid and the two separate seqences of five letters”. Id.
Prior to this case, in 1977, Whitford J had chaired the “Whitford Report” which found of computer-generated works, “the correct approach is to look on the computer as a mere tool in much the same way as a slide rule or even, in a simple sense, a paint brush. A very sophisticated tool it may be, with considerable powers to extend man’s capabilities to create new works, but a tool nevertheless.”17 The Whitford Report concluded that both the computer programmer and the person who originated data to provide the computer should be authors of any resultant CGW. In response to the Whitford Report, the Government issued the Green Paper report. Among other things, this report argued that the computer user, as potentially distinct from the programmer and originator of data, should generally also be an author.18 In 1986, the Government published a White Paper, Intellectual Property and Innovation, which argued, “[t]he responses to the 1981 Green Paper have shown, however, that circumstances vary so much in practice that a general solution will not be fair in all cases. It appears that no practical problems arise from the absence of specific authorship provisions in this area. The Government has therefore concluded that no specific provisions should be made to determine this question... If no human skill and effort has been expended then no work warranting copyright protection has been created.”19
After this White Paper, the Copyright Committee of the British Computer Society (BCS) submitted a proposal to the Government arguing that CGWs should be protected as a distinct type of work. “The BCS proposes the creation of a new class of copyright protected works. The copyright owner or ‘maker’ should be defined as the person by whom the arrangements necessary for the making of that computer output or computer-generated work, are undertaken.”20 This language was essentially adopted in the CDPA. The BCS’s proposed language was modeled after provisions for film authorship under the Copyright Act 1956. Despite the BCS’s protestation that sound recordings, films, cable programmes and published editions were already being generated by computer, the CDPA did not extend protections to this subject matter for CGWs.
Since the CDPA’s enactment, the authorship of CGWs was considered in Nova Productions Ltd v Mazooma Games Ltd.21 In this case, the parties were competing manufacturers of electronic pool games. Nova claimed copyright in its graphics and the frames generated by software from those graphics and displayed to users during gameplay. Kitchin J (as he then was) regarded the frames which the software generated based on user actions to be CGWs, even though the component graphics of the frames were designed by a person. Kitchin J further held that the author of the CGW in this case was the company director responsible for designing the game—the person who designed the appearance of the various elements displayed, devised the rules and logic for frame generation, and wrote the program, and not the game player, who “...contributed no skill or labour of an artistic kind”. It should be noted there was limited consideration of §9(3) in this case because the subsistence and ownership of the works was not contested.
In sum, while judicial experience with CGW copyright is limited, it is clear that copyright protection is available. The “author” of a CGW work is the person by whom the arrangements necessary for the creation of the work are undertaken. In light of the relative absence of case law related to authorship of CGWs, cases that have investigated authorship for films may be instructive. Under the CDPA, a film’s producer and principal director are together deemed an author. A producer
 , “in relation to a sound recording or a film, means the person by whom the
 arrangements necessary for the making of the sound recording or film are undertaken...” CDPA

404
§178. Identifying a producer may be a fact intensive inquiry.22 Cases have found it is relevant who instigated the making of the film, who paid for the making of the film, whether a film would not have existed but for the input of a person, whether more than one person may be a producer, and the extent of creative contributions.23
United Kingdom Standards for Patenting Computer-Generated Works
By contrast to copyright, there is no statutory provision governing patents for CGWs, and there appear to have been no cases on the subject. The Patents Act 1977 (“PA”) is the primary legislation for patent law. The PA protects inventions which are new, involve an inventive step, and are capable of industrial application. Patents grant their owners the exclusive right to make, use, sell and import an invention for a limited term, generally 20 years from the date an application is filed, subject to certain exceptions.
While nothing in the PA explicitly deals with CGWs, on numerous occasions it references natural persons. For example, the PA requires the identity of individual inventors to be disclosed, and inventors have the right to be mentioned in an application or a patent. It also provides benefits to inventors in some circumstances in which an employer has received outstanding benefit from an invention. The PA states that,
§
European Union Standards for Computer-Generated Works
The European Single Market seeks to guarantee the free movement of goods, capital, services and labour within the European Union. However, IPRs such as copyright and patents can create barriers to free trade. IPRs are largely national in origin, and not transferrable across boarders or mutually recognized per se. In the interest of promoting trade, the EU has attempted to centralize and harmonize national IP laws. This has been aided by case law from the Court of Justice of the European Union (CJEU), the Agreement on Trade Related Aspects of Intellectual Property Rights (TRIPS), which is discussed in the next section, and various EU directives.
Early CJEU cases established the doctrine of exhaustion and the specific subject matter doctrine. This allowed recognition of national IPRs, but limited the application of IPRs where they would limit free movement of goods. The EU is a party to TRIPS, which has harmonized to a great extent IPRs within the EU. Since TRIPS, various EU directives, such as the Computer Program Directive and the Database Directive, have increasingly harmonized national IP laws where differences existed in terms of substance or duration of rights.25 Further efforts at harmonization have resulted in a unique EU trademark system, and various sui generis rights such as EU level plant variety rights. Today, there is relative comprehensive harmonization of some forms of IP such as trademarks, and relative greater discrepancy with copyright. (Elsmore, 2012).
There is no equivalent to the CDPA §9(3) in other EU continental jurisdictions.26 Worldwide, the UK is one of only a handful of countries that explicitly permits copyright for CGWs. Other nations that provide protection, such as Ireland, New Zealand and India, were influenced by the UK’s example—their statutory instruments contain similar language to CDPA §9(3).27
  Although jurisprudence in related areas may provide
 guidance, there is a degree of novelty to determining authorship of CGWs. It may not be clear in all cases whether the person who makes necessary arrangements is a computer’s owner, user, or
 programmer.
 “inventor... in relation to an invention means the actual deviser
  of the invention...” PA
7(3). The term “deviser” is not defined in the PA, but judicial language
 also frequently refers to inventors as persons and refers to concepts such as “mental activity” being
necessary for invention.24
 
405
EU member states may not have laws specifically permitting or refusing copyright protection for CGWs, but many have laws that restrict authorship to natural persons. For example, Spanish copyright law states that the author of a work is the natural person who creates it.28 Under French law, only natural persons who create works may be considered authors, and the rights to a work vest in the author regardless of any contract.29 For collective works, a legal entity can exercise rights but is not classified as the author. Various other national instruments contain language that alludes to authorship as being a human activity. At a European level, the benchmark for originality is an “author’s own intellectual creation.” This concept was first introduced through legislation— the Software, Term and Database Directives—and then developed by the CJEU.30 For example, in 2011, the CJEU held that, “copyright is liable to apply only in relation to a subject-matter, such as a photograph, which is original in the sense that is its author’s own intellectual creation... the author of a portrait photograph can stamp the work created with his ‘personal touch’.”31 This and similar language seems to imply an author is a natural person. CGWs are not explicitly discussed in any European directives.
For patents, as with the PA, the European Patent Convention (EPC) requires the identity of inventors to be disclosed in patent applications and issued patents,32 although it is left to contracting states to resolve who is an inventor and other entitlement issues. The EPC is a multilateral treaty, separate from the EU and with different membership, which created the European Patent Organisation (EPO) and a system for granting “European patents.” A European patent is not a centrally enforceable patent or a unitary right. Rather, the EPC provides a harmonized procedure for unified prosecution and opposition, on the basis of which a European patent may be nationally granted in any of the 38 EPO countries. By contrast, the European patent with unitary effect (EPUE), or the unitary patent, is a new type of European patent that would be valid in participating member states of the EU. This would involve a single patent and ownership, as well as a single court (the Unified Patent Court), and uniform protection. The Agreement on a Unified Patent Court establishes the unitary patent system. Participation is open to any member state of the EU, but not other parties to the EPC. Negotiations for the unitary patent have been ongoing since the 1970s. At present, this agreement will enter into force after it is ratified by Germany.
International Standards for Computer-Generated Works
Two of the most important international agreements governing copyright and patent law are the Berne Convention and the Agreement on Trade Related Aspects of Intellectual Property Rights (TRIPS). For example, the Berne Convention required countries to offer the same level of copyright protection to nationals of other parties to the convention. It also introduced the idea that copyright protection is not contingent on formalities such as registration, though member states are free to require ‘fixation’. The most substantive international IP agreement is TRIPS, which established global standards for copyright and patent protection. The UK and all EU Member States are required to adhere to the mandatory requirements in TRIPS. These requirements were modeled after the IP laws in developed nations such as the United Kingdom, United States and Japan, so TRIPS required relatively few changes to the UK’s IP laws when it came into effect on 1 January 1996.33
Nothing in these, or any other binding international instrument, explicitly authorizes, or prohibits, protections for CGWs. The Berne Convention, for instance, states the Union is created, “for the protection of the rights of authors in their literary and artistic works.”34 However, the Convention does not define “author.”35 The Berne Convention Guide states that this is due to the

406
fact that, “national laws diverge widely, some recognizing only natural persons as authors, while others treat certain legal entities as copyright owners.”36
The World Intellectual Property Organization (WIPO) did consider protections of “computer-produced works” in discussions of a possible Model Copyright Law.37 It defined a computer-produced work as one generated by a computer where identification of authors is impossible because of the indirect nature of individual contributions. The original owner of the moral and economic rights in such a work would be either the entity “by whom or by which the arrangements necessary for the creation of the work are undertaken,” or the entity “at the initiative and under the responsibility of whom or of which the work is created and disclosed.” WIPO’s Committee of Experts eventually concluded further study was needed, and the model law was never adopted.
United States Standards for Computer-Generated Works
No statute governs the subject of CGWs in the US, and no cases have seriously considered copyright or patent protection for CGWs. However, the US Copyright Office has a policy prohibiting copyright for any non-human work—what it now refers to as its “human authorship requirement.” The US Patent and Trademark Office (USPTO) does not have any stated policy regarding CGWs and patents. In 1986, Professor Pamela Samuelson wrote, “[a]s yet there has been no judicial decision allocating rights in computer-generated works. It can, however, only be a matter of time before courts are forced to resolve the issue.”38 That prediction proved optimistic.
One recent US case came close to raising the issue. Naruto v. Slater involved a series of pictures that a crested macaque took of itself. These “Monkey Selfies” were subsequently commercialized by the camera’s owner, David Slater, who asserted he owned the copyright to the photographs. People for the Ethical Treatment of Animals (PETA) subsequently sued Mr. Slater, alleging that the macaque, Naruto, was the copyright owner, and that Mr. Slater had infringed Naruto’s copyright.
In January 2016, US District Judge William Orrick III dismissed the case on the grounds that Naruto lacked standing to sue. The judge also deferred to the USPTO’s interpretation that the macaque was not an “author” within the meaning of the Copyright Act. He considered PETA’s argument that the USPTO policy is antithetical to the “public interest in animal art”, but ultimately ruled “that is an argument that should be made to Congress and the President, not to me.”39 PETA appealed the decision to the Ninth Circuit Court of Appeals, and shortly after oral arguments, the parties reached a settlement in which Mr. Slater agreed to donate 25% of any future revenues from the monkey selfies to charities. Despite the settlement, however, the Ninth Circuit dismissed the case to create precedent. The Court held that animals only have statutory standing if an Act of Congress plainly states animals have statutory standing, and so animals are unable to sue under the Copyright Act because the law does not expressly authorize animals to file copyright infringement clams. In doing so, the court avoided weighing in on the merits of non-human authorship.
Outside of CGWs, US copyright law has a mechanism for authorship of artificial persons.
“In the case of a work made for hire, the employer for whom the work was prepared is considered the author for purposes.” 17 U.S.C. § 201(b) (2011). Functionally, the same outcome may occur in the UK, but while the UK permits employers to own works, ownership is distinct from authorship for so-called “author works”—literary, dramatic, musical or artistic works—the same works protected by CDPA §9(3). Even in EU countries where only natural persons may be authors, a focus on “author’s rights” does not preclude authors from transferring certain rights to employers,

407
and some jurisdictions will imply the existence of an agreement to do so. Ultimately, then, the same economic outcome may occur for works made in the course of employment in the US, UK and in EU civil law jurisdictions, but the terminology may differ. Some civil law jurisdictions may also retain additional, inalienable rights for authors.
IV. PROTECTING COMPUTER GENERATED WORKS
Policy
Various rationales are given for IPRs, but broadly speaking, they can function as economic incentives and they are justified on the basis of natural rights. The notion of IPRs as an economic right, particularly for patents, dominates the Anglo-American system. In the US, for example, the Constitution explicitly endorses an innovation incentive rationale for IPRs, by granting Congress the power “[t]o
 promote the progress of science and useful arts, by securing for limited times to
 authors and inventors the exclusive right to their respective writings and discoveries.”40
 Patents can incentivize innovation.41 This is based on the theory that information goods are
  typically non-excludable and non-rivalrous, so lack of protection will lead to underproduction. By granting a limited monopoly in the form of a patent, this allows inventors to enjoy greater financial benefits from discoveries and encourages invention. In addition, patents can promote the commercialization of inventions. For instance, new drug approvals often take years, and the pharmaceutical industry claims that getting new drugs approved costs billions of pounds. Once a drug is approved, it may be easy for a competitor to copy the drug and avoid the costs of initial approval. Patents may thus encourage an originator pharmaceutical company to spend the necessary resources on approval, because after the drug is approved they can charge monopoly prices until patents expire. Patents, whether incentivizing research or commercialization, are thus one solution to the “freerider” problem. Finally, patents can promote information disclosure. Patents are issued to inventors in exchange for disclosing to the public how to make an invention. Without patents, inventors might rely on confidential information to prevent copying, and never publicly disclose how to make an invention. This happened, for example, with the drug “Premarin” which was first made by Wyeth and now is made by Pfizer. No generics company has been able to replicate this drug since its first regulatory approval in 1942. Perhaps most famously, Coca-Cola
 has kept its recipe for its iconic beverage confidential for over a century.
By contrast, the civil law systems of continental Europe may place more emphasis than the UK on moral rights, which are viewed as independently protectable and separate from economic rights. Moral rights protect an author’s personality and the integrity of a work, and are considered “personal, perpetually inalienable and unassignable.”42 Moral rights also accommodate “personality” rights based, for instance, on theories by Kant and Hegel that people express their “wills” and develop as persons through their interactions with external objects. This, for instance, is accomplished by giving authors the right to control certain uses of their works, even after assigning economic rights. Personality theorists argue that authors and inventors are inherently at risk of having their ideas stolen or altered in objectionable ways. Thus, IPRs are justified to prevent misappropriation or modification of objects through which authors express themselves. IPRs also accommodate Lockean theories of first occupancy, the idea that the person who owns a particular thing should be the person who ‘gets there first’, as well as labour theory, the idea that ownership is derived from mixing labour with unowned or commonly held property, and that appropriating these products would be unjust. These ideals are reflected in patent law, for instance, by giving

408
inventorship rights to the first inventor to file for a patent, and giving inventorship rights to individuals who find new uses for natural products.
But IPRs can also have significant costs. They restrict competition (particularly in the case of patents) and free speech (particularly in the case of copyright), and they can inhibit innovation, collaboration, and open communities. To the extent that IPRs are justified, it is because they are thought to have more benefits than costs. However, with IPRs, more is not always better. For instance, software patents have been criticized for being unnecessary as an incentive, while at the same time creating “patent thickets” that make work in the software industry challenging.43 For this reason, the EPC states that “programs for computers” are not patentable, but the EPO will grant patents for “computer-impelmented inventions” as long as they have a technical effect.
Whether to Patent and to whom?
Having examined UK, EU and international laws on copyright and patent protection for CGWs, or the absence thereof, let us return to the question of whether the UK should provide patent protection for CGWs. A number of academic commentators have argued that CGWs should become public property.44 If CGWs should instead be eligible for patent protection, who should be the inventor and owner of a CGW?
This chapter proposes that CGWs should be eligible for patent protection. The innovation incentive function of patents does not change based on whether a computer or a person invents. It is true that a computer does not respond to financial incentives, but the entities who develop inventive machines do. Providing patent protection for the output of autonomous machines makes autonomous machines more valuable, and what better way to incentivize innovation than to incentivize the development of inventive machines? This would reward activity upstream from the act of invention. To the extent that patents are incentivizing commercialization and disclosure of information, there is no change in this function as between a human and CGW. Also, if patent protection is not available for inventive AI output, then businesses may not use inventive AI, even in future instances where AI will be more effective than a person.
If CGWs are prohibited from receiving patents, it may be possible for a natural person to claim inventorship of a CGW even where that person was not involved in the development or operation of a computer. Namely, a person could argue they “devised” the invention by virtue of recognizing the relevance of a machine’s output. Indeed, discovery of an unrecognized problem may give rise to patentable subject-matter (“problem-inventions”).45 Similarly, discovery of an unrecognized solution can be patentable. In some cases, recognition of the inventive nature of a computer’s output may require significant skill, but in others, the nature of inventive output may be obvious. In the future, it may even be the case that a computer can identify its own output as patentable, and format it for a patent application.
If CGWs are to be protected, how then should inventorship and ownership be determined? Distinguishing inventorship and ownership may not functionally impact economic rights, but it does implicate moral rights. At present, de jure or de facto, individuals are claiming inventorship of CGWs under circumstances in which they have not functioned as inventors. This is fundamentally unfair, and it weakens moral justifications for patents by allowing individuals to take credit for the work of inventive machines. It is not unfair to computers who have no interest in being acknowledged, but it is unfair to other human inventors because it devalues their accomplishments by altering, and diminishing, the meaning of inventorship. This could equate the hard work of creative geniuses with those simply asking a computer to solve a problem. It would be particularly problematic once inventive machines come to generate a substantial portion, or

409
even the majority of inventions.46 By contrast, acknowledging computers as inventors would also acknowledge the work of computer programmers. While they may not have directly contributed to an invention, they may take credit for the success of their machines. This is similar to the way in which a supervisor may take pride in the success of a PhD student, without taking direct credit for their future writings and inventions.
If CGWs are to be protected, and a computer is to be acknowledged as an inventor, who should own the CGW? Certainly, computers should not own patents. Computers are non-sentient, cannot own property, and are themselves owned as property. Colin Davies has suggested the computer should hold IP rights and transfer these under contract.47 He notes this would require machine “responsibility,” which might require a deposit in a computer’s name to satisfy adverse judgments or an insurance scheme. More simply, ownership may directly vest in a computer’s user, programmer, or owner. In many instances, these may be the same entity, but they may also be distinct parties. The best policy or ideal solution would be to have ownership vest in the party that results in the most effective economic outcome, and also results in a standard that is practical to implement.48
The computer’s owner should be the default owner of any CGW it produces. This is most consistent with current ownership norms surrounding personal property (including both computers and patents).49 It should also most effectively incentivize innovation because it will motivate owners to share access to their software. If the computer’s user is the default owner of a CGW, this may instead result in computer owners restricting access. Computer programmers do not need to own future CGWs because they will capture the increased value of an inventive machine upon selling it. Also, having ownership default to programmers would interfere with the transfer of a machine, and it would be logistically problematic for developers to monitor machines they no longer own. The case for having computer owners also have ownership of CGWs reveals another reason why computers should be acknowledged as inventors. If computers cannot be inventors and instead the first natural person to recognize a computer’s invention becomes the inventor, this would give CGWs to computer users rather than owners. There is already precedent for assigning ownership in IPRs to an owner distinct from an author or inventors, such as with works for hire, joint authorship, films, etc.
This default was just be a starting point—computer users, owners and developers would be free to contract to different outcomes.
Computer-generated works—competition or collaboration?
The current definition of CGWs fails to take into account the fact that computers independently should qualify for authorship and inventorship, even when contributing to jointly authored works with natural persons. Computers may be inventors even of intermediate works. As such, the definition of CGWs should be amended from work “generated by a computer in circumstances such that there is no human author of the work”, to work “generated by a computer in circumstances such that the computer, if a natural person, would meet authorship requirements.” This would more accurately take into account contributions by machines, and allow economic incentives to work more efficiently.
The downside of this approach may be that it would be difficult for computer owners to know when their machines have generated CGWs. Users might benefit from failing to disclose CGWs to computer owners and then claiming they invented a CGW. However, users may still choose to disclose CGWs so that they could negotiate for clear title and, alternately, to avoid liability. To the extent that users and owners are distinct entities and users are licensing computers

410
for purposes generating CGWs, users may choose to negotiate a priori for ownership of CGWs with computer owners.
Determining human inventorship is already a tricky business in collaborative works. It may be even more difficult for collaborative works involving a computer. There are a variety of ways for computers to invent, some of which involve more human intervention than others. For example, a programmer may design a computer program specifically to solve a particular problem, and the solution may be the patentable invention. In such an instance, the programmer might have a greater claim to inventorship, resulting in joint inventorship with a computer. Again, this is not unlike current inventorship criteria, where a variety of individuals can play greater or lesser roles in invention. However, the current definition of CGWs in the CDPA does not accommodate this reality for copyright, as it fails to take into account that a computer can jointly author a work with a person.
International Harmonization
Finally, there is a need for a harmonized approach to CGWs. If the UK grants copyright and patent protections for CGWs, it has to provide nationals of other EU member states and parties to TRIPS with the same rights. However, if these other parties fail to allow for CGWs in their own domestic laws, UK nationals may not receive reciprocal protections.50 Few EU member states have dealt with CGWs.51 Inventive machine owners might thus be unable to obtain IPRs outside the UK. In fact, disclosing a machine author or inventor in a UK application might prejudice IPRs in other jurisdictions. At least for an interim period, UK entities would be advised to identify a natural person as an author or inventor where possible to avoid an inequitable economic outcome.
Future treatment of CGWs within the EU might be dealt with by an EU directive or regulation, although Brexit may remove the UK from the direct effect of changes to EU law. Regardless of Brexit, UK nationals still should benefit under the national treatment rule of TRIPS from changes to EU law that ascribe machine authorship and inventorship for CGWs. CGWs might also be dealt with by a future multinational agreement. However, harmonization exercises at the international level tend to proceed at a glacial pace.
Concluding Thoughts
In October 2017, the Kingdom of Saudi Arabia announced it was granting citizenship to a humanoid robot, Sophia, manufactured by Hanson Robotics. It is unclear whether this announcement was merely intended for publicity, or whether the nation has actually granted Sophia citizenship. In any event, if Sophia is a Saudi citizen, because Saudi Arabia is a party to TRIPS, other WTO members may be obliged to provide for IPRs for Sophia’s CGWs. Although, other countries may argue that Berne and TRIPs refer to authors and inventors who are nationals, but that machines cannot be authors and inventors regardless of ‘nationality’. In any event, while granting legal personhood to a machine may be one way to try and avoid disparate treatment of CGWs at the international level, there are other reasons to disfavor such an approach.
The law is overdue for establishing clear standards for protection of CGWs. As AI continues to improve, such works will become increasingly important. Efficiently structured copyright and patent laws can help maximize the value of CGWs, and protect the moral rights of human authors and inventors.52 However, for IPRs to function effectively, it is important that right holders and potential infringers have a reasonable degree of certainty about the scope and limits of protection.

411
 1 See, Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 54 B. C. L. Rev. 1079–1126 (2016).
2 See, Ryan Abbott, Everything is Obvious, 66 UCLA. L. Rev. 2 (2019).
3 See, e.g., Band-in-a-Box, PG Music, http://www.pgmusic.com.
4 Arthur Miller, Copyright Protection for Computer Programs , Databases , and Computer Generated Works: Is Anything New Since CONTU? 106 Harvard Law Review 977–1073 (1993). 5 Abbott, Ryan, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 Geo. Wash. L. Rev. 1 (2018) (discussing unexplainability in the context of AI).
6 Patricia Holt, Sunday Review, S.F.CHRON., Aug. 15, 1993, B4; see, generally, Grimmelmann, J. There’s No Such Thing As A Computer-Authored Work, 39 Columbia Journal of Law & the Arts 403, 408 (2016).
7 U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES (FIRST) §2.8.3 (1st ed. 1973).
8 Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 54 B. C. L. Rev. 1079–1126 (2016).
9 IDA Ltd and others v University of Southampton and others [2006] EWCA Civ 145; Abbott, Ryan, Jeremy Lack and David Perkins. Managing Disputes in the Life Sciences. Nature Biotechnology, 36, 697 (2018).
10 Mueller Brass Co. v Reading Industries Inc. 176 USPQ 361 (1972).
11 The CDPA permits copyright for “(a) original literary, dramatic, musical or artistic works, (b) sound recordings, films [or broadcasts], and (c) the typographical arrangement of published editions.” CDPA 1988, § 1 (internal footnote and emphasis omitted).
12 Copyright, Designs and Patents Act, 1988 §154.
13 Copyright, Designs and Patents Act, 1988 §214.
14 Copyright, Designs and Patents Act, 1988 §12(7).
15 In the case of Cummins v. Bond in 1927, a court was asked to adjudicate copyright in a work allegedly written by a journalist while acting as a spiritual medium. Cummins v. Bond, 1 Ch. 167 (1927). The court was not willing to decide that “authorship and copyright rest with someone already domiciled on the other side of the inevitable river.” Id. at 173. The rights to the work had to vest in a terrestrial being.
16 A similar outcome occurred in the case of The Jockey Club v Rahim (unreported) 22 July 1983, which concerned computers generating lists of runners and riders for horse races.
17 Whitford Committee on Copyright Designs and Performers Protection (Cmnd 6732 HMSO 1977), para 514.
18 Reform of the Law Relating to Copyright, Designs and Performer's Protection, A Consultative Document 58 (Cmnd 8302 HMSO 1981).
19 Intellectual Property and Innovation (Cmnd 9712; HMSO, Ch 9, paras 9.6–8).
20 Robert Hart, Copyright and computer generated works, 40 Aslib Proceedings 173, 173–181 (1988).
21 Nova Productions Ltd v Mazooma Games Ltd [2006] RPC 379. CGWs were also briefly considered in Bamgboye v Reed [2004] EMLR 5, 73 [38], Williamson J wrote that §9(3) “is dealing with the case where one is looking at a piece of music which, in fact, is composed of computerised sounds.”
22 See, e.g., Beggars Banquet [1993] EMLR 349.
23 Jani McCutcheon, Curing the authorless void: Protecting computer-generated works following

412
 icetv and phone directories. 37 Melbourne University Law Review 46 (2013).
24 See, e.g., Yeda Research and Development Co Ltd v Rhone-Poulenc Rorer International Holdings Inc [2007] UKHL 43, [2008] RPC 1 quoting Laddie J. in
25 Directive 2009/24/EC of the European Parliament and of the Council of 23 April 2009 on the legal protection of computer programs; Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases.
26 Andres Guadamuz, Do androids dream of electric copyright? Comparative analysis of originality in artificial intelligence generated works, Intellectual Property Quarterly 169 (2017). 27 Copyright, Designs and Patents Act, 1988, c. 48, § 9(3) (U.K.); Copyright Act of 1994, § 5 (N.Z.); Copyright and Related Rights Act 2000, Part I, § 2 (Act. No. 28/2000) (Ir.).
28 Ley 22/11 sobre la Propiedad Intelectual de 1987.
29 C. IP. Art. L111-1 (2003).
30 Case C-5/08 Infopaq International A/S v Danske Dagblades Forening [2009] ECR I-06569.
31 Eva-Maria Painer v. Standard VerlagsGmbH and ors, Case C-145/10 [2011] ECDR (13) 297, 324, [AG121]. In that case, Advocate-General Trstenjak interpreted EU directives related to this language to mean that, “‘only human creations are ... protected’, although these can ‘include those for which the person employs a technical aid, such as a camera’.” Id.
32 EPC R. 19 (Designation of the inventor).
33 94/800/EC Council Decision (of 22 December 1994). See, generally, Matthew James Elsmore, Comparing regulatory treatment of intellectual property at WTO and EU level, in LIBERALISING TRADE IN THE EU AND THE WTO: A LEGAL COMPARISON 412–439 (Sanford E. Gaines, et al., eds., 2012).
34 BERNE CONVENTION FOR THE PROTECTION OF LITERARY AND ARTISTIC 1971 ART. I.
35 Cf, SAM RICKETSON AND JANE C. GINSBURG, INTERNATIONAL COPYRIGHT AND NEIGHBORING RIGHTS (2 VOLUMES): THE BERNE CONVENTION AND BEYOND (2nd Ed. 2006) (arguing the reference to ‘makers’ of cinematographic works is the exception rather than the rule, and that ‘author’ referring to natural persons would be most consistent with the moral rights provisions and durations of protection being based on the life of an author).
36 WORLD INTELLECTUAL PROPERTY ORGANIZATION, GUIDE TO THE BERNE CONVENTION II (1978).
37 See INTERNATIONAL BUREAU OF WIPO, PREPARATORY DOCUMENT, DRAFT MODEL ON COPYRIGHT at 258-59 (No. CD/MPC/III/2, Mar. 30, IggO).
38 Pamela Samuelson, Allocating ownership rights in computer-generated works. 47 U. Pitt. Law Review 1185, 1190 (1985).
39 Naruto v. David John Slater et al, No. 16-15469 (9th Cir. 2018).
40 UNITED STATES CONSTITUTION, ARTICLE I, SECTION 8, CLAUSE 8 (emphasis added).
41 WILLIAM M. LANDES & RICHARD A. POSNER, THE ECONOMIC STRUCTURE OF INTELLECTUAL PROPERTY LAW. Cambridge, MA: Belknap Press (2003).
42 Martin A. Roeder, The Doctrine of Moral Right: A Study in the Law of Artists, Authors and Creators, 53 Harv. L. Rev. 554, 557 (1940). See, also, Graham Dutfield, Collective Invention and Patent Law Individualism: Origins and Functions of the Inventor’s Right of Attribution. 5 The WIPO Journal 25, 27 (2013).
 the invention’. The word ‘actual’ denotes a contrast with a deemed or pretended deviser of the
University of Southampton’s
 Applications [2005] R.P.C. 11, [39] (“The inventor is defined in s.7(3) as ‘the actual deviser of
  invention; it means, as Laddie J. said in University of Southampton’s Applications [2005] R.P.C.
 11, [39], the natural person who ‘came up with the inventive concept.’”)
  
413
 43
44 See, e.g., Ralph Clifford, Intellectual Property in the Era of the Creative Computer Program: Will the True Creator Please Stand Up? 71 Tul. L. Rev. 1675 (1997).
45 See, e.g., T 0002/83 (Simethicone Tablet) of 15.3.1984 (EPO Board of Appeal).
46 See, Abbott, R. Everything is Obvious, 66 UCLA. L. Rev. 2 (2019).
47 Colin Davies, An evolutionary step in intellectual property rights - Artificial intelligence and intellectual property. 27 Computer Law and Security Review 601, 615 (2011).
48 Ryan Abbott, Hal the Inventor: Big Data and its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH (Hamid Ekbia, et al., eds.) (2016).
49 Cf Schuster, W. Michael, ‘A Coasean Analysis of Ownership of Patents for Inventions Created by Artificial Intelligence’, 75 Washington and Lee Law Review (forthcoming 2018), <https://ssrn.com/abstract=3132753>. (arguing for user default ownership).
50 Robert Hart, Copyright and computer generated works, 40 Aslib Proceedings 173, 173–181 (1988).
51 Mark Perry & Thomas Margoni, From music tracks to Google maps: Who owns computer- generated works? 26 Computer Law and Security Review 621, 621–629 (2010).
52 Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 Harv. L. & Pol. Rev. 145 (2018).
 Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes Debate, 92 TEX. L.
 REV. 303 (2013).
 
                                 414
U.C.L.A. Law Review
Everything Is Obvious
Ryan Abbott
ABSTRACT
For more than sixty years, “obviousness” has set the bar for patentability. Under this standard, if a hypothetical “person having ordinary skill in the art” would find an invention obvious in light of existing relevant information, then the invention cannot be patented. This skilled person is defined as a non-innovative worker with a limited knowledge-base. The more creative and informed the skilled person, the more likely an invention will be considered obvious. The standard has evolved since its introduction, and it is now on the verge of an evolutionary leap: Inventive machines are increasingly being used in research, and once the use of such machines becomes standard, the person skilled in the art should be a person using an inventive machine, or just an inventive machine. Unlike the skilled person, the inventive machine is capable of innovation and considering the entire universe of prior art. As inventive machines continue to improve, this will increasingly raise the bar to patentability, eventually rendering innovative activities obvious. The end of obviousness means the end of patents, at least as they are now.
AUTHOR
Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor, David Geffen School of Medicine at University of California, Los Angeles. Thanks to Ryan Calo, Ian Kerr, Mark Lemley, Lisa Larrimore-Ouellette, and Jake Sherkow, as well as participants in workshops at the University of Surrey, WeRobot Conference, Oxford Business Law Workshop, and the Sixth Annual Fall Conference hosted by the Center for the Protection of Intellectual Property (CPIP) at Antonin Scalia Law School for their insightful comments.
66 UCLA L. Rev. 2 (2019)

                                 415
 TABLE OF CONTENTS
Introduction.......................................................................................................................................................4 I. Obviousness..................................................................................................................................................10 A. Public Policy.......................................................................................................................................... 10 B. EarlyAttempts.......................................................................................................................................11 C. TheNonobviousnessInquiry.............................................................................................................15 D. FindingPHOSITA................................................................................................................................17 E. AnalogousPriorArt.............................................................................................................................20 II. Machine Intelligence in the Inventive Process.......................................................................... 22 A. AutomatingandAugmentingResearch...........................................................................................22 B. Timeline to the Creative Singularity .................................................................................................. 26 C. Inventive and Skilled Machines ......................................................................................................... 31 D. Inventive Is the New Skilled ............................................................................................................... 33 E. Skilled People Use Machines............................................................................................................... 35 F. The Evolving Standard.......................................................................................................................... 37 III. A Post-Skilled World............................................................................................................................ 37 A. Application............................................................................................................................................38 B. Reproducibility...................................................................................................................................... 42 C. AnEconomicvs.CognitiveStandard...............................................................................................44 D. Other Alternatives................................................................................................................................ 46 E. Incentives Without Patents?................................................................................................................ 48 F. A Changing Innovation Landscape.................................................................................................... 50 Conclusion........................................................................................................................................................ 51
3

416
4
66 UCLA L. REV. 2 (2019)
 INTRODUCTION
For at least two decades, machines have been autonomously generating patentableinventions.1 “Autonomously”herereferstothemachine,ratherthan to a person, meeting traditional inventorship criteria. In other words, if the “inventive machine” were a natural person, it would qualify as a patent inventor. In fact, the U.S. Patent and Trademark Office (USPTO or Patent Office) may have granted patents for inventions autonomously generated by computers as early as 1998.2 In earlier articles, I examined instances of autonomous machine invention in detail and argued that such machines ought to be legally recognized aspatentinventorstoincentivizeinnovationandpromotefairness.3 Theowners of such machines would be the owners of their inventions.4 In those works, as here, terms such as “computers” and “machines” are used interchangeably to refer to computer programs or software rather than to physical devices or hardware.5
This Article focuses on a related phenomenon: What happens when inventive machines become a standard part of the inventive process? This is not a thought experiment.6 For instance, while the timeline is controversial, surveys of experts suggest that artificial general intelligence, which is a computer able to perform any intellectual task a person could, will develop in the next twenty-five years.7 Somethoughtleaders,suchasRayKurzweil,oneofGoogle’sDirectorsof
1. See Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) [hereinafter I Think] (describing instances of “computational invention” or “computer-generated works”); see also infra Subpart II.B (discussing some such instances in greater detail).
2. Abbott, supra note 1, at 1085.
3. Id. at 1083–91; Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence,
in BIG DATA IS NOT A MONOLITH (Cassidy R. Sugimoto, Hamid R. Ekbia & Michael Mattioli eds.,2016) [hereinafter Hal the Inventor] (discussing computational invention in a book chapter first posted online February 19, 2015).
4. Except where no owner exists, in possible cases of some open-source or distributed software, in which case ownership could vest in a user.
5. Except perhaps in exceptional cases where software does not function on a general-purpose machine, and where specialized hardware is required for the software’s function.
6. The growing prevalence and sophistication of artificial intelligence is accelerating the use of inventive machines in research and development. See Ryan Abbott & Bret Bogenschneider, ShouldRobotsPayTaxes? TaxPolicyintheAgeofAutomation,12HARV.L.&POL’YREV.145 (2018) [hereinafter Should Robots Pay Taxes?] (discussing the trend toward automation).
7. See generally Vincent C. Müller & Nick Bostrom, Future Progress in Artificial Intelligence: A Survey of Expert Opinion, in FUNDAMENTAL ISSUES OF ARTIFICIAL INTELLIGENCE 553 (Vincent C. Müller ed., 2016).
 
417
Everything Is Obvious 5
Engineering, predict computers will have human levels of intelligence in about a decade.8
The impact of the widespread use of inventive machines will be tremendous, not just on innovation, but also on patent law.9 Right now, patentability is determined based on what a hypothetical, non-inventive, skilled person would find obvious.10 The skilled person represents the average worker in the scientific field of an invention.11 Once the average worker uses inventive machines, or inventive machines replace the average worker, then inventive activity will be normal instead of exceptional.
If the skilled person standard fails to evolve accordingly, this will result in too lenient a standard for patentability. Patents have significant anticompetitive costs, and allowing the average worker to routinely patent their outputs would cause social harm. As the U.S. Supreme Court has articulated, “[g]ranting patent protection to advances that would occur in the ordinary course without real innovation retards progress and may . . . deprive prior inventions of their value or utility.”12
The skilled standard must keep pace with real world conditions. In fact, the standard needs updating even before inventive machines are commonplace. Already, computers are widely facilitating research and assisting with invention. For instance, computers may perform literature searches, data analysis, and pattern recognition.13 This makes current workers more knowledgeable and creative than they would be without the use of such technologies. The Federal Circuit has provided a list of nonexhaustive factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active
8. Peter Rejcek, Can Futurists Predict the Year of the Singularity?, SINGULARITY HUB (Mar. 31, 2017), https://singularityhub.com/2017/03/31/can-futurists-predict-the-year-of-the-singularity [https://perma.cc/4TDE-QQTW] (predicting artificial general intelligence in 2029).
9. See, e.g., ROBERT PLOTKIN, THE GENIE IN THE MACHINE: HOW COMPUTER-AUTOMATED INVENTING IS REVOLUTIONIZING LAW & BUSINESS 60 (2009) (arguing that “[a]rtificial invention technology . . . enables [users] to produce inventions that they could not have created at all without such technology”); Ben Hattenbach & Joshua Glucoft, Patents in an Era of Infinite Monkeys and Artificial Intelligence, 19 STAN. TECH. L. REV. 32, 44 n.70 (2015); Brenda M. Simon, The Implications of Technological Advancement for Obviousness, 19 MICH. TELECOMM. & TECH. L. REV. 331 (2013).
10. 35 U.S.C. § 103(a) (2006). The “person having ordinary skill in the art” may be abbreviated as “PHOSITA” or simply the skilled person.
11. See infra Subpart I.D.
12. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007).
13. Such contributions when made by other persons do not generally rise to the level of
inventorship, but they assist with reduction to practice.
  
418
6 66 UCLA L. REV. 2 (2019)
workers in the field.”14 This test should be modified to include a sixth factor: (6) “technologies used by active workers.”
This change will more explicitly take into account the fact that machines are already augmenting the capabilities of workers, in essence making more obvious and expanding the scope of prior art. Once inventive machines become the standard means of research in a field, the test would also encompass the routine use of inventive machines by skilled persons. Taken a step further, once inventive machines become the standard means of research in a field, the skilled person should be an inventive machine. Specifically, the skilled person should be an inventive machine when the standard approach to research in a field or with respect to a particular problem is to use an inventive machine (the “Inventive Machine Standard”).
To obtain the necessary information to implement this test, the Patent Office should establish a new requirement for applicants to disclose when a machine contributes to the conception of an invention, which is the standard for qualifying as an inventor. Applicants are already required to disclose all human inventors, and failure to do so can render a patent invalid or unenforceable. Similarly, applicants should need to disclose whether a machine has done the work of a human inventor. This information could be aggregated to determine whether most invention in a field is performed by people or machines. This information would also be useful for determining appropriate inventorship, and more broadly for formulating innovation policies.
Whether the Inventive Machine Standard is that of a skilled person using an inventive machine or just an inventive machine, the result will be the same: The average worker will be capable of inventive activity. Conceptualizing the skilled person as using an inventive machine might be administratively simpler, but replacing the skilled person with the inventive machine would be preferable because it emphasizes that the machine is engaging in inventive activity, rather than the human worker.
Yet simply substituting an inventive machine for a skilled person might exacerbate existing problems with the nonobviousness inquiry. With the current skilled person standard, decisionmakers, in hindsight, need to reason about what another person would have found obvious.15 This results in
14. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
15. See generally Gregory N. Mandel, Patently Non-Obvious: Empirical Demonstration that the
Hindsight Bias Renders Patent Decisions Irrational, 67 OHIO ST. L.J. 1391 (2006) (discussing problems with hindsight in non-obviousness inquiries).
  
419
Everything Is Obvious 7
inconsistent and unpredictable nonobviousness determinations.16 In practice, the skilled person standard bears unfortunate similarities to the “Elephant Test,”17 or Justice Stewart’s famously unworkable definition of obscene material: “I know it when I see it.”18 This may be even more problematic in the case of inventive machines, as it is likely to be difficult for human decisionmakers to theoretically reason about what a machine would find obvious.
An existing vein of critical scholarship has already advocated for nonobviousness inquiries to focus more on economic factors or objective “secondary” criteria, such as long-felt but unsolved needs, the failure of others, and real-world evidence of how an invention was received in the marketplace.19 Inventive machines may provide the impetus for such a shift.
Nonobvious inquiries utilizing the Inventive Machine Standard might also focus on reproducibility, specifically whether standard machines could reproduce the subject matter of a patent application with sufficient ease. This could be a more objective and determinate test that would allow the Patent Office to apply a single standard consistently, and it would result in fewer judicially invalidated patents.20 A nonobviousness inquiry focused on either secondary
16. See FED. TRADE COMM’N, TO PROMOTE INNOVATION: THE PROPER BALANCE OF COMPETITION AND PATENT LAW AND POLICY 6–15 (2003) (critiquing Section 103 decisions).
17. Cadogan Estates Ltd. v. Morris [1998] EWCA Civ. 1671 at 17 (Eng.) (referring to “the well known elephant test. It is difficult to describe, but you know it when you see it”).
18. 378 U.S. 184, 197 (1964).
19. See, e.g., Michael Abramowicz & John F. Duffy, The Inducement Standard of Patentability,
120 YALE L.J. 1590, 1596 (2011) (arguing for an inducement standard); Tun-Jen Chiang, A Cost-Benefit Approach to Patent Obviousness, 82 ST. JOHN’S L. REV. 39, 42 (2008) (arguing that, “[a]n invention should receive a patent if the accrued benefits before independent invention outweigh the costs after independent invention”); Alan Devlin & Neel Sukhatme, Self-Realizing Inventions and the Utilitarian Foundation of Patent Law, 51 WM. & MARY L. REV. 897 (2009); John F. Duffy, A Timing Approach to Patentability, 12 LEWIS & CLARK L. REV. 343 (2008) (arguing for a timing approach to determining obviousness); Daralyn J. Durie & Mark A. Lemley, A Realistic Approach to the Obviousness of Inventions, 50 WM. & MARY L. REV. 989, 1004–07 (2008) (arguing for a greater reliance on secondary considerations); Gregory Mandel, The Non-Obvious Problem: How the Indeterminate Nonobviousness Standard Produces Excessive Patent Grants, 42 U.C. DAVIS L. REV 57, 62 (2008) [hereinafter Mandel, The Non-Obvious Problem] (arguing for nonobviousness to be based on “how probable the invention would have been for a person having ordinary skill in the art working on the problem that the invention solves”); Robert P. Merges, Uncertainty and the Standard of Patentability, 7 HIGH TECH. L.J. 1, 19 (1992) (arguing that patents should be issued for inventions which appeared unlikely to succeed in advance).
20. For decades, obviousness has been the most common issue in litigation to invalidate a patent, and the most common grounds for a finding of patent invalidity. See John R. Allison & Mark A. Lemley, Empirical Evidence on the Validity of Litigated Patents, 26 AIPLA Q.J. 185, 208–09 (1998); John R. Allison et al., Understanding the Realities of Modern Patent Litigation, 92 TEX. L. REV. 1769, 1782, 1785 (2014). As other commentators have noted, the bar here is low, and the new standard, “can be an administrative success if it is even just a bit
  
420
8 66 UCLA L. REV. 2 (2019)
factors or reproducibility may avoid some of the difficulties inherent in applying a “cognitive” inventive machine standard.
However the test is applied, the Inventive Machine Standard will dynamically raise the current benchmark for patentability. Inventive machines will be significantly more intelligent than skilled persons and also capable of considering more prior art. An Inventive Machine Standard would not prohibit patents, but it would make obtaining them substantially more difficult: A person or computer might need to have an unusual insight that other inventive machines could not easily recreate, developers might need to create increasingly intelligent computers that could outperform standard machines, or, most likely, invention will be dependent on specialized, non-public sources of data. The nonobviousness bar will continue to rise as machines inevitably become increasingly sophisticated. Taken to its logical extreme, and given there may be no limit to how intelligent computers will become, it may be that every invention will one day be obvious to commonly used computers. That would mean no more patents should be issued without some radical change to current patentability criteria.
This Article is structured in three parts. Part I considers the current test for obviousness and its historical evolution. It finds that obviousness is evaluated through the lens of the skilled person, who reflects the characteristics of the average worker in a field.21 The level of creativity and knowledge imputed to the skilledpersoniscriticalfortheobviousnessanalysis.22 Themorecapabletheskilled person, the more they will find obvious, and this will result in fewer issued patents.
Part II considers the use of artificial intelligence in research and development (R&D) and proposes a novel framework for conceptualizing the transition from human to machine inventors. Already, inventive machines are competing with human inventors, and human inventors are augmenting their
better than current doctrine as a helpful theoretical and pragmatic guide for applying the
obviousness doctrine.” Abramowicz & Duffy, supra note 19, at 1601.
21. See Ruiz v. A.B. Chance Co., 234 F.3d 654, 666 (Fed. Cir. 2000); see also Ryko Mfg. Co. v. Nu-Star, Inc., 950 F.2d 714, 718 (Fed. Cir. 1991) (“The importance of resolving the level of ordinary skill in the art lies in the necessity of maintaining objectivity in the obviousness inquiry.”). The Manual of Patent Examining Procedure (MPEP) provides guidance on the
level of ordinary skill in the art. MPEP § 2141.03.
22. DyStar Textilfarben GmbH & Co. Deutschland KG v. C.H. Patrick Co., 464 F.3d 1356, 1370
(Fed. Cir. 2006) (“If the level of skill is low, for example that of a mere dyer, as Dystar has suggested, then it may be rational to assume that such an artisan would not think to combine references absent explicit direction in a prior art reference.”). Though, in practice, few cases involve explicit factual determinations of the PHOSITA’s skill. Rebecca S. Eisenberg, Obvious to Whom? Evaluating Inventions From the Perspective of PHOSITA, 19 BERKELEY TECH. L.J. 885, 888 (2004). See infra Subpart I.D for a discussion of the PHOSITA standard.
  
421
Everything Is Obvious 9
abilities with inventive machines. In time, inventive machines or people using inventive machines will become the standard in a field, and eventually, machines will be responsible for most or all innovation. As this occurs, the skilled person standard must evolve if it is to continue to reflect real-world conditions. Failure to do this would “stifle, rather than promote, the progress of the useful arts.”23
Part II then proposes a framework for implementing a proposed Inventive Machine Standard. A decisionmaker would need to (1) determine the extent to which inventive machines are used in a field, (2) if inventive machines are the standard, characterize the inventive machine(s) that best represents the average worker, and (3) determine whether the machine(s) would find an invention obvious. The decisionmaker is a patent examiner in the first instance,24 and potentially a judge or jury if the validity of a patent is at issue in trial.25 In both instances, this new test would involve new challenges.
Finally, Part III provides examples of how the Inventive Machine Standard could work in practice, such as by focusing on reproducibility or secondary factors. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function as innovation incentives. To the extent patents accomplish other goals such as promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
Although this Article focuses on U.S. patent law, a similar framework exists in nearly every country. Member States of the World Trade Organization (WTO) are required to grant patents for inventions that “are new, involve an
23. KSR Int’l Co., 550 U.S. at 427.
24. At the Patent Office, applications are initially considered by a patent examiner, and
examiner decisions can be appealed to the Patent Trial and Appeal Board (PTAB). U.S. PATENT & TRADEMARK OFFICE, Patent Trial and Appeal Board, https://www.uspto.gov/ patents-application-process/patent-trial-and-appeal-board-0 [https://perma.cc/3W42-FHH2]. Also, the PTAB can adjudicate issues of patentability in certain proceedings such as inter partes review. Id.
25. Determinations of patent validity can involve mixed questions of law and fact. Generally, in civil litigation, legal questions are determined by judges, while factual questions are for a jury. See, e.g., Structural Rubber Prods. Co. v. Park Rubber Co., 749 F.2d 707, 713 (Fed. Cir. 1984) (“Litigants have the right to have a case tried in a manner which ensures that factual questions are determined by the jury and the decisions on legal issues are made by the court . . . .”). There are some exceptions to this rule. See, e.g., Gen. Electro Music Corp. v. Samick Music Corp., 19 F.3d 1405, 1408 (Fed. Cir. 1994) (“[I]ssues of fact underlying the issue of inequitable conduct are not jury questions, the issue being entirely equitable in nature.”). See also Mark A. Lemley, Why Do Juries Decide If Patents Are Valid? (Stanford Pub. Law, Working Paper No. 2306152, 2013), https://papers.ssrn.com/sol3/papers.cfm? abstract_id=2306152.
  
422
10 66 UCLA L. REV. 2 (2019)
inventive step and are capable of industrial application.”26 Although U.S. law uses the term “nonobvious” rather than “inventive step,” the criteria are substantively similar.27 For instance, the European Patent Office’s criteria for inventive step is similar to the U.S. criteria for obviousness, and also uses the theoretical device of the skilled person.28
I. OBVIOUSNESS
Part I investigates the current obviousness standard, its historical origins, and how the standard has changed over time. It finds that obviousness depends on the creativity of the skilled person, as well as the prior art they consider. These factors, in turn, vary according to the complexity of an invention and its field of art.
A. Public Policy
Patents are not intended to be granted for incremental inventions.29 Only inventions which represent a significant advance over existing technology should receive protection.30 That is because patents have significant costs: They limit competition, and they can inhibit future innovation by restricting the use
26. Agreement on Trade-Related Aspects of Intellectual Property Rights, art. 27, Apr. 15, 1994, 33 I.L.M. 1197, 1208 [hereinafter TRIPS]. See Ryan B. Abbott, et al., The Price of Medicines in Jordan: The Cost of Trade-Based Intellectual Property, 9 J. GENERIC MEDS. 75, 76 (2012).
27. TRIPS, supra note 26, at 1208 n.5. Although, there are some substantive differences in the way these criteria are implemented, and TRIPS provides nations with various flexibilities for compliance. See generally Ryan Abbott, Balancing Access and Innovation in India’s Shifting IP Regime, Remarks, 35 WHITTIER L. REV. 341 (2014) [hereinafter Balancing Access].
28. “An invention shall be considered as involving an inventive step if, having regard to the state of the art, it is not obvious to a person skilled in the art.” Convention on the Grant of European Patents art. 56, Oct. 5, 1973, 13 I.L.M 268. For guidance on the “skilled person” in European patent law, see Guidelines for Examinations, EUR. PAT. OFF., http://www.epo.org/law-practice/legal-texts/html/guidelines/e/g_vii_3.htm [https://perma.cc/XFY3-JD8J] (last visited Sept. 24, 2018).
29. The nonobviousness requirement is contained in Section 103 of the Patent Act: A patent for a claimed invention may not be obtained, notwithstanding that the claimed invention is not identically disclosed as set forth in section 102, if the differences between the claimed invention and the prior art are such that the claimed invention as a whole would have been obvious before the effective filing date of the claimed invention to a person having ordinary skill in the art to which the claimed invention pertains.
35 U.S.C. § 103 (2018).
30. Atlantic Works v. Brady, 107 U.S. 192, 200 (1883) (noting that “[t]o grant to a single party
monopoly of every slight advance made, except where the exercise of invention, somewhat above ordinary mechanical or engineering skill, is distinctly shown, is unjust in principle and injurious in its consequences”).
  
423
Everything Is Obvious 11
of patented technologies in research and development.31 To the extent that patents are justified, it is because they are thought to have more benefits than costs. Patents can function as innovation incentives, promote the dissemination of information, encourage commercialization of technology, and validate moral rights.32
Patents are granted for inventions that are new, nonobvious, and useful.33 Of these three criteria, obviousness is the primary hurdle for most patent applications.34 Although other patentability criteria contribute to this function, the nonobviousness requirement is the primary test for distinguishing between significant innovations and trivial advances.35 Of course, it is one thing to express a desire to only protect meaningful scientific advances, and another to come up with a workable rule that applies across every area of technology.
B. Early Attempts
The modern obviousness standard has been the culmination of hundreds of years of struggle by the Patent Office, courts, and Congress to separate the wheat from the chaff.36 As Thomas Jefferson, the first administrator of the U.S.
31. See I Think, supra note 1, at 1105–06 (discussing the costs and benefits of the patent system).
32. Id. at 1105–08. Congress’s power to grant patents is constitutional, and based on incentive theory: “To promote the progress of science . . . by securing for limited times to . . . inventors the exclusive right to their respective . . . discoveries.” U.S. CONST. art. I, § 8, cl. 8. See Mark A. Lemley, Ex Ante Versus Ex Post Justifications for Intellectual Property, 71 U. CHI. L. REV. 129, 129 (2004) (“The standard justification for intellectual property is ex ante . . . . It is the prospect of the intellectual property right that spurs creative incentives.”); see also United States v. Line Material Co., 333 U.S. 287, 316 (1948) (Douglas, J., concurring) (noting “the reward to inventors is wholly secondary” to the reward to society); THE FEDERALIST NO. 43 (James Madison) (stating that social benefit arises from patents to inventors). The U.S. Supreme Court has endorsed an economic inducement rationale in which patents should only be granted for inventions which would “not be disclosed or devised but for the inducement of a patent.” This is the inducement theory articulated in Graham v. John Deere
Co., 383 U.S. 1, 10 (1966). See also Abramowicz & Duffy, supra note 20.
33. 35 U.S.C. §§ 101–103, 112 (2018). In the European system, these criteria are referred to as novelty, inventive step, and industrial applicability. Art. 52 EPC. Inventions must also comprise patentable subject matter and be adequately disclosed. 35 U.S.C. §§ 101–103, 112
(2018).
34. DONALD CHISUM, CHISUM ON PATENTS § 5.02[6] (2007); NONOBVIOUSNESS—THE ULTIMATE
CONDITION OF PATENTABILITY 2:101 (J. Witherspoon ed., 1980). Obviousness is the most commonly litigated issue of patent validity. Allison & Lemley, supra note 20, at 208–09 (1998).
35. 35 U.S.C. §§ 101–102, 112 (2018).
36. For that matter, the struggle dates back to the very first patent law, the Venetian Act of 1474,
which stated that only “new and ingenious” inventions would be protected. See Giulio Mandich, Venetian Patents (1450–1550), 30 J. PAT. OFF. SOC’Y 166, 176–77 (1948); A. Samuel Oddi, Beyond Obviousness: Invention Protection in the Twenty-First Century, 38 AM.
  
424
12 66 UCLA L. REV. 2 (2019)
patent system and one of its chief architects, wrote, “I know well the difficulty of drawing a line between the things which are worth to the public the embarrassment of an exclusive patent, and those which are not . . . I saw with what slow progress a system of general rules could be matured.”37
The earliest patent laws focused on novelty and utility, although Jefferson didatonepointsuggestan“obviousness”requirement.38 ThePatentActof1790 was the first patent statute, and it required patentable inventions to be “sufficiently useful and important.”39 Three years later, a more comprehensive patent law was passed—the Patent Act of 1793.40 The new act did not require an invention to be “important,” but required it to be “new and useful.”41 The 1836 Patent Act reinstated the requirement that an invention be “sufficiently used and important.”42
In 1851, the Supreme Court adopted the progenitor of the skilled person and the obviousness test—an “invention” standard.43 Hotchkiss v. Greenwood
U. L. REV. 1097, 1102–03 (1989); Frank D. Prager, A History of Intellectual Property From
1545 to 1787, 26 J. PAT. OFF. SOC’Y 711, 715 (1944).
37. Letter to Isaac McPherson (Aug. 13, 1813), in 5 THE WRITINGS OF THOMAS JEFFERSON, 1790–
1826, 175, 181 (Riker, Thorne & Co. 1854) [hereinafter Letter to Isaac McPherson].
38. In 1791, Jefferson proposed amending the 1790 Patent Act to prohibit patents on an invention if it “is so unimportant and obvious that it ought not be the subject of an exclusive right.” 5 THE WRITINGS OF THOMAS JEFFERSON 278, 1788–1792, (Paul Leicester Ford ed.,
G.P. Putnam & Sons 1895).
39. Patent Act of 1790, ch. 7, 1 Stat. 109 (repealed 1793).
40. Patent Act of 1793, ch. 11, 1 Stat. 318 (repealed 1836).
41. Patent Act of 1793, ch. 11, 1 Stat. at 318–23. It also prohibited patents on certain minor
improvements: “[S]imply changing the form or the proportions of any machine, or compositions of matter, in any degree, shall not be deemed a discovery.” Id. at 321. On this basis, Jefferson, who was credited with drafting most of this statute, argued that “[a] change of material should not give title to a patent. As the making a ploughshare of cast rather than of wrought iron; a comb of iron, instead of horn or of ivory . . . .” Letter to Isaac McPherson, supra note 37, at 181.
42. Patent Act of 1836, ch. 357, § 18, 5 Stat. 117, 124 (repealed 1861).
43. See, e.g., Graham v. John Deere Co., 383 U.S. 1, 17 (1966) (“We conclude that [§ 103] was
intended merely as a codification of judicial precedents embracing the Hotchkiss condition, with congressional directions that inquiries into the obviousness of the subject matter sought to be patented are a prerequisite to patentability.”); see also S. REP. NO. 82-1979, at 6 (1952); H.R. REP. NO. 82-1923, at 7 (1952) (“Section 103 . . . provides a condition which exists in the law and has existed for more than 100 years.”). Obviousness had been at issue in earlier cases, although not necessarily in such terms. For instance, in Earle v. Sawyer, Justice Story rejected an argument by the defendant that the invention at issue was obvious, and that something more than novelty and utility was required for a patent. 8 F. Cas. 254, 255 (Cir. Ct. D. Mass. 1825). He argued a court was not required to engage in a “mode of reasoning upon the metaphysical nature, or the abstract definition of an invention.” Id. Justice Story further noted that English law permits the introducer of a foreign technology to receive a patent, and such an act could not require intellectual labor. Id. at 256. In Evans v. Eaton, the Supreme Court held that, a patent invention must involve a change in the “principle” of the machine rather than a change “merely in form and proportion.” 20 U.S.
  
425
Everything Is Obvious 13
concerned a patent for substituting clay or porcelain for a known door knob materialsuchasmetalorwood.44 TheCourtinvalidatedthepatent,holdingthat “the improvement is the work of a skillful mechanic, not that of the inventor.”45 The Court also articulated a new legal standard for patentability: “Unless more ingenuity and skill...were required...than were possessed by an ordinary mechanic acquainted with the business, there was an absence of that degree of skill and ingenuity which constitute essential elements of every invention.”46
However, the Court did not give specific guidance on what makes something inventive or the required level of inventiveness. In subsequent years, the Court made several efforts to address these deficiencies, but with limited success. As the Court stated in 1891, “[t]he truth is the word [invention] cannot be defined in such manner as to afford any substantial aid in determining whether any particular device involves an exercise of inventive faculty or not.”47 Or as one commentator noted, “it was almost impossible for one to say with any degree of certainty that a particular patent was indeed valid.”48
Around 1930, the Supreme Court, possibly influenced by a national antimonopoly sentiment, began implementing stricter criteria for determining the level of invention.49 This culminated in the widely disparaged “Flash of Genius” test articulated in Cuno Engineering v. Automatic Devices Corp.50 Namely, that in order to receive a patent, “the new device must reveal the flash of creative genius, not merely the skill of the calling.”51 This test was interpreted to mean that an invention must come into the mind of an inventor as a result of
(7 Wheat) 356, 361–62 (1822). Writing for the Court, Justice Story noted the patent was
invalid because it was “substantially the same in principle” as a prior invention. Id. at 362.
44. 52 U.S. 248, 265 (1850).
45. Id. at 267.
46. Id.
47. McClain v. Ortmayer, 141 U.S. 419, 427 (1891). Another court noted that “invention” is “as
fugitive, impalpable, wayward, and vague a phantom as exists in the paraphernalia of legal
concepts.” Harries v. Air King Prods. Co., 183 F.2d 158, 162 (2d Cir. 1950).
48. Gay Chin, The Statutory Standard of Invention: Section 103 of the 1952 Patent Act, 3 PAT.
TRADEMARK & COPY. J. RES. & EDUC. 317, 318 (1959).
49. See, e.g., Edward B. Gregg, Tracing the Concept of Patentable Invention, 13 VILL. L. REV. 98
(1967).
50. Cuno Eng’g Corp. v. Automatic Devices Corp., 314 U.S. 84, 91 (1941) (formalizing the
test). See, e.g., Hamilton Standard Propeller Co. v. Fay-Egan Mfg. Co., 101 F.2d 614, 617 (6th Cir. 1939) (“The patentee did not display any flash of genius, inspiration or imagination . . . .”). The Flash of Genius test was reaffirmed by the Court in 1950 in Great Atlantic & Pacific Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 154 (1950) (Douglas, J., concurring).
51. Cuno Eng’g Corp., 314 U.S. at 91.
  
426
14 66 UCLA L. REV. 2 (2019)
“inventive genius”52 rather than as a “result of long toil and experimentation.”53 The Court reasoned that “strict application of the test is necessary lest in the constant demand for new appliances the heavy hand of tribute be laid on each slight technological advance in the art.”54
The Flash of Genius test was criticized for being vague and difficult to implement, and for involving subjective decisions about an inventor’s state of mind.55 It certainly made it substantially more difficult to obtain a patent.56 Extensive criticism of perceived judicial hostility toward patents resulted in President Franklin D. Roosevelt’s creation of a National Patent Planning Commissiontomakerecommendationsforimprovingthepatentsystem.57 The
52. Reckendorfer v. Faber, 92 U.S. 347, 357 (1875).
53. The Supreme Court later claimed the “Flash of Creative Genius” language was just a
rhetorical embellishment, and that requirement concerned only the device itself, not the manner of invention. Graham v. John Deere Co., 383 U.S. 1, 15 n.7, 16 n.8 (1966). That was not, however, how the test was interpreted. See P.J. Federico, Origins of Section 103, 5 APLA Q.J. 87, 97 n.5 (1977) (noting the test led to a higher standard of invention in the lower courts). In Atlantic & Pacific Tea Co. v. Supermarket Equipment Corp., 340 U.S. 147 (1950), another case cited for the proposition that the Court had adopted stricter patentability criteria, the majority did not consider the question of inventiveness, but in his concurring opinion Justice Douglas reiterated the concept of “inventive genius”: “It is not enough that an article is new and useful. The Constitution never sanctioned the patenting of gadgets. Patents serve a higher end—the advancement of science. An invention need not be as startling as an atomic bomb to be patentable. But it has to be of such quality and distinction that that masters of the scientific field in which it falls will recognize it as an advance.” Id.
54. Cuno Eng’g Corp., 314 U.S. at 92.
55. As a commentator at the time noted, “the standard of patentable invention represented by
[the Flash of Genius doctrine] is apparently based upon the nature of the mental processes of the patentee-inventor by which he achieved the advancement in the art claimed in his patent, rather than solely upon the objective nature of the advancement itself.” Comment, The “Flash of Genius” Standard of Patentable Invention, 13 FORDHAM L. REV. 84, 87 (1944). See Note, Patent Law—”Flash of Genius” Test for Invention Rejected, 5 DEPAUL L. REV. 144, 146 (1955); Stephen G. Kalinchak, Obviousness and the Doctrine of Equivalents in Patent Law: Striving for Objective Criteria, 43 CATH. U. L. REV. 577, 586 (1994); see also, Note, The Standard of Patentability—Judicial Interpretation of Section 103 of the Patent Act Source, 63 COLUM. L. REV. 306, 306 (1963) [hereinafter The Standard of Patentability] (criticizing the standard).
56. Supreme Court Justice Robert Jackson noted in a dissent that “the only patent that is valid is one which this Court has not been able to get its hands on.” Jungersen v. Ostby & Barton Co., 335 U.S. 560, 572 (1949) (Jackson, J., dissenting).
57. See William Jarratt, U.S. National Patent Planning Commission, 153 NATURE 12 (1944); see also REPORT OF THE NATIONAL PATENT PLANNING COMMISSION, NATIONAL PATENT PLANNING COMMISSION, at 6, 10 (1943).
  
427
Everything Is Obvious 15
Commission’s report recommended that Congress adopt a more objective and certain standard of obviousness.58 A decade later, Congress did.59
C. The Nonobviousness Inquiry
The Patent Act of 1952 established the modern patentability framework.60 Among other changes to substantive patent law,61 “the central thrust of the 1952 Act removed ‘unmeasurable’ inquiries into ‘inventiveness’ and instead supplied the nonobviousness requirement of Section 103.”62 Section 103 states:
A patent may not be obtained . . . if the difference between the subject matter sought to be patented and the prior art are such that the subject matter as a whole would have been obvious at the time the invention was made to a person having ordinary skill in the art to which said subject matter pertains. Patentability shall not be negatived by the manner in which the invention was made.63
58. REPORT OF THE NATIONAL PATENT PLANNING COMMISSION, supra note 57,at 5–6. “One of the greatest technical weaknesses of the patent system is the lack of a definitive yardstick as to what is invention.” Id. at 26. “The most serious weakness of the present patent system is the lack of a uniform test or standard for determining whether the particular contribution of an inventor merits the award of the patent grant.” Id. at 14. “It is proposed that Congress shall declare a national standard whereby patentability of an invention shall be determined by the objective test as to its advancement of the arts and sciences.” Id. at 26.
59. Though, Congress may not have realized what it was doing. See George M. Sirilla, 35 U.S.C. § 103: From Hotchkiss to Hand to Rich, the Obvious Patent Law Hall-of-Famers, 32 J. MARSHALL L. REV. 437, 509–14 (1999) (discussing the legislative history of the Patent Act of 1952 and the lack of congressional awareness of, and intent for, Section 103).
60. See The Standard of Patentability, supra note 55, at 309. “[P]robably no other title incorporates the thinking of so many qualified technical men throughout the country as does this revision.” L. James Harris, Some Aspects of the Underlying Legislative Intent of the Patent Act of 1952, 23 GEO. WASH. L. REV. 658, 661 (1955).
61. “The major changes or innovations in the title consist of incorporating a requirement for invention in § 103 and the judicial doctrine of contributory infringement in § 271.” H.R. REP. NO. 1923, 82d Cong., 2d Sess. 5 (1952); S. REP. NO. 1979, 82d Cong., 2d Sess. 4 (1952).
62. CLS Bank Int’l v. Alice Corp. Pty. Ltd., 717 F.3d 1269, 1296 (Fed. Cir. 2013) (Rader, J., dissenting in part, concurring in part) (citing P.J. Federidco’s Commentary on the New Patent Act, reprinted in 75 J. PAT. & TRADEMARK OFFICE SOC’Y 161, 177 (1993)). See also Dann v. Johnston, 425 U.S. 219, 225–26 (1976) (describing the shift from “an exercise of the inventive faculty” established in case law to a statutory test and stating that “it was only in 1952 that Congress, in the interest of uniformity and definiteness, articulated the requirement in a statute, framing it as a requirement of ‘nonobviousness’” (internal quotation marks and footnote omitted)). The official “Revision Notes” state § 103 is meant to be the basis for “holding . . . patents invalid by the courts[] on the ground of lack of invention.” S.REP.NO.82-1979,at18.
63. 35 U.S.C. § 103, as amended by the America Invents Act. Leahy-Smith America Invents Act, Pub. L. No. 112-29, 125 Stat. 284, 286 (2011) (codified at 35 U.S.C. § 103 (2018)). The America Invents Act did not fundamentally change the nonobviousness inquiry but did
  
428
16 66 UCLA L. REV. 2 (2019)
Section 103 legislatively disavowed the Flash of Genius test, codified the sprawling judicial doctrine on “invention” into a single statutory test, and restructured the standard of obviousness in relation to a person having ordinary skill in the art.64 However, while Section 103 may be more objective and definite than the Flash of Genius test, the meanings of “obvious” and “a person having ordinary skill” were not defined, and in practice also proved “often difficult to apply.”65
The Supreme Court first interpreted the statutory nonobviousness requirement in a trilogy of cases: Graham v. John Deere (1966) and its companion cases, Calmar v. Cook Chemical (1965) and United States v. Adams (1966).66 In these cases, the Court articulated a framework for evaluating obviousness as a question of law based on the following underlying factual inquiries: (1) the scope and content of the prior art, (2) the level of ordinary skill in the prior art, (3) the differences between the claimed invention and the prior art, and (4) objective evidence of nonobviousness.67 This framework remains applicable today. Of note, the Graham analysis does not explain how to evaluate the ultimate legal question of nonobviousness, beyond identifying underlying factual considerations.68
In 1984, the newly established United States Court of Appeals for the Federal Circuit, the only appellate-level court with jurisdiction to hear patent case appeals, devised the “teaching, suggestion, and motivation” (TSM) test for obviousness.69 Strictly applied, this test only permits an obviousness rejection when prior art explicitly teaches, suggests or motivates a combination of existing
result in some modest changes. https://www.uspto.gov/web/offices/pac/mpep/s2158.html
[https://perma.cc/TAQ7-KMCC].
64. See Giles S. Rich, Principles of Patentability, 28 GEO. WASH. U. L. REV. 393, 393–407 (1960);
see also Chin, supra note 48, at 318. In Graham, the Supreme Court noted that “[i]t . . . seems apparent that Congress intended by the last sentence of § 103 to abolish the test it believed this Court announced in the controversial phrase ‘flash of creative genius,’ used in Cuno Engineering.” Graham, 383 U.S. at 15.
65. Uniroyal, Inc. v. Rudkin-Wiley Corp., 837 F.2d 1044, 1050 (Fed. Cir. 1988) (noting the obviousness standard is easy to expound and “often difficult to apply”).
66. Graham v. John Deere Co., 383 U.S. 1 (1966); United States v. Adams, 383 U.S. 39, 51–52 (1966); Calmar v. Cook Chem., 380 U.S. 949 (1965).
67. Graham, 383 U.S. at 17. With regards to the fourth category, considerations such as commercial success and long felt but unsolved needs can serve as evidence of nonobviousness in certain circumstances. Id.
68. See Joseph Miller, Nonobviousness: Looking Back and Looking Ahead, in 2 INTELLECTUAL PROPERTY AND INFORMATION WEALTH: ISSUES AND PRACTICES IN THE DIGITAL AGE: PATENTS AND TRADE SECRETS 9 (Peter K. Yu ed., 2007) (“[T]he Court did not indicate . . . how one was to go about determining obviousness (or not).”).
69. Court Jurisdiction, U.S. CT. APPEALS FOR FED. CIR., http://www.cafc.uscourts.gov/the- court/court-jurisdiction [https://perma.cc/TE4D-GRF2].
  
429
Everything Is Obvious 17
elements into a new invention.70 The TSM test protects against hindsight bias because it requires an objective finding in the prior art. In retrospect, it is easy for an invention to appear obvious by piecing together bits of prior art using the invention as a blueprint.71
In KSR v. Teleflex (2006), the Supreme Court upheld the Graham analysis but rejected the Federal Circuit’s exclusive reliance on the TSM test. The Court instead endorsed a flexible approach to obviousness in light of “[t]he diversity of inventivepursuitsandofmoderntechnology.”72 Ratherthanapprovingasingle definitive test, the Court identified a nonexhaustive list of rationales to support a finding of obviousness.73 This remains the approach to obviousness today.
D. Finding PHOSITA
Determining the level of ordinary skill is critical to assessing obviousness.74 The more sophisticated the person having ordinary skill in the art (PHOSITA, or the skilled person), the more likely a new invention is to appear obvious.
70. ACS Hosp. Sys., Inc. v. Montefiore Hosp., 732 F.2d 1572 (Fed. Cir. 1984).
71. See In re Fritch, 972 F.2d 1260, 1266 (Fed. Cir. 1992).
72. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007). “[An obviousness] analysis need not
seek out precise teachings directed to the specific subject matter of the challenged claim, for a court can take account of the inferences and creative steps that a [PHOSITA] would employ.” Id. at 418.
73. These post-KSR rationales include:
(A) Combining prior art elements according to known methods to yield predictable results; (B) Simple substitution of one known element for another to obtain predictable results; (C) Use of known technique to improve similar devices (methods, or products) in the same way; (D) Applying a known technique to a known device (method, or product) ready for improvement to yield predictable results; (E) ‘Obvious to try’—choosing from a finite number of identified, predictable solutions, with a reasonable expectation of success; (F) Known work in one field of endeavor may prompt variations of it for use in either the same field or a different one based on design incentives or other market forces if the variations are predictable to one of ordinary skill in the art; (G) Some teaching, suggestion, or motivation in the prior art that would have led one of ordinary skill to modify the prior art reference or to combine prior art reference teachings to arrive at the claimed invention.
2141 Examination Guidelines for Determining Obviousness Under 35 U.S.C. 103 [R- 08.2017], U.S. PAT. & TRADEMARK OFF., https://www.uspto.gov/web/offices/pac/mpep/ s2141.html [http://perma.cc/EE7P-4CQ9] [hereinafter 2141 Examination Guidelines].
74. Ruiz v. A.B. Chance Co., 234 F.3d 654, 666 (Fed. Cir. 2000); see also Ryko Mfg. Co., v. Nu- Star, Inc.,950 F.2d 714 718 (Fed. Cir. 1991) (“The importance of resolving the level of ordinary skill in the art lies in the necessity of maintaining objectivity in the obviousness inquiry.”). The skilled person is relevant to many areas of patent law, including claim construction, best mode, definiteness, enablement, and the doctrine of equivalents. See Dan L. Burk & Mark A. Lemley, Is Patent Law Technology-Specific?, 17 BERKELEY TECH. L.J. 1155, 1186–87 (2002).
  
430
18 66 UCLA L. REV. 2 (2019)
Thus, it matters a great deal whether the skilled person is a “moron in a hurry”75 or the combined “masters of the scientific field in which an [invention] falls.”76
The skilled person has never been precisely defined, although judicial guidance exists.77 In KSR, the Supreme Court described the skilled person as “a person of ordinary creativity, not an automaton.”78 The Federal Circuit has explained the skilled person is a hypothetical person, like the reasonable person in tort law,79 who is presumed to have known the relevant art at the time of the invention.80 The skilled person is not a judge, amateur, person skilled in remote arts, or a set of “geniuses in the art at hand.”81 The skilled person is “one who thinks along the line of conventional wisdom in the art and is not one who undertakes to innovate.”82
The Federal Circuit has provided a nonexhaustive list of factors to consider in determining the level of ordinary skill: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.”83 In any particular case, one or more factors may predominate, and not every factor may be relevant.84 The
75. Morning Star Coop. Soc’y v. Express Newspapers Ltd. [1979] FSR 113 (marking the first use of the term “moron in a hurry” as a standard for trademark confusion).
76. Great Atl. & Pac. Tea Co. v. Supermarket Equip. Corp., 340 U.S. 147, 155 (1950).
77. See James B. Gambrell & John H. Dodge, II, Ordinary Skill in the Art—An Enemy of the Inventor or a Friend of the People?, in NONOBVIOUSNESS—THE ULTIMATE CONDITION OF PATENTABILITY 5:302 (John F. Witherspoon ed., 1980) (“[T]he Supreme Court in particular, but other courts as well, has done precious little to define the person of ordinary skill in the
art.”).
78. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 421 (2007). The MPEP provides guidance on the
level of ordinary skill in the art. MPEP § 2141.03. See John F. Duffy & Robert P. Merges, The Story of Graham v. John Deere Company: Patent Law’s Evolving Standard of Creativity, in INTELLECTUAL PROPERTY STORIES 110 (Jane C. Ginsburg & Rochelle Cooper Dreyfuss eds., 2006) (noting that determining the appropriate level of ordinary skill for the nonobviousness standard “is one of the most important policy issues in all of patent law”).
79. See, e.g., Panduit Corp. v. Dennison Mfg. Co., 810 F.2d 1561, 1566 (Fed. Cir. 1987) (“[T]he decision maker confronts a ghost, i.e., ‘a person having ordinary skill in the art,’ not unlike the ‘reasonable man’ and other ghosts in the law.”).
80. 2141 Examination Guidelines, supra note 73.
81. Envtl. Designs Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 697 (Fed. Cir. 1983).
82. Standard Oil Co. v. Am. Cyanamid Co., 774 F.2d 448, 454 (Fed. Cir. 1985).
83. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
84. Id.; Custom Accessories, Inc. v. Jeffrey-Allan Indus., Inc., 807 F.2d 955, 962–63 (Fed. Cir.
1986). Previously, this list of factors included the “educational level of the inventor.” Envtl. Designs, Ltd.,713 F.2d at 696. That was until the Federal Circuit announced that, “courts never have judged patentability by what the real inventor/applicant/patentee could or would do.” Kimberly-Clark Corp. v. Johnson & Johnson, 745 F.2d 1437, 1454 (Fed. Cir. 1984). Instead, “[r]eal inventors, as a class, vary in the capacities from ignorant geniuses to Nobel
  
431
Everything Is Obvious 19
skilled person standard thus varies according to the invention in question, its field of art, and researchers in the field.85 In the case of a simple invention in a field where most innovation is created by laypersons, such as, for instance, a device to keep flies away from horses, the skilled person may be someone with little education or practical experience.86 By contrast, where an invention is in a complex field with highly educated workers such as chemical engineering or pharmaceuticalresearch,theskilledpersonmaybequitesophisticated.87 Atleast in Europe, the skilled person may even be a team of individuals where collaborative approaches to research are the norm.88
laureates; the courts have always applied a standard based on an imaginary work of their
own devising whom they have equated with the inventor.” Id.
85. See, e.g., DyStar Textilfarben GmbH & Co. Deutschland KG, 464 F.3d 1356, 1370 (Fed. Cir.
2006). The court writes:
If the level of skill is low, for example that of a mere dyer, as Dystar has suggested, then it may be rational to assume that such an artisan would not think to combine references absent explicit direction in a prior art reference. . . . [If] the level of skill is that of a dyeing process designer, then one can assume comfortably that such an artisan will draw ideas from chemistry and systems engineering—without being told to do so.
Daiichi Sankyo Co. v. Apotex, Inc. concerned a patent for treating ear infections by applying an antibiotic to the ear. 501 F.3d 1254, 1257 (Fed. Cir. 2007). The district court found that the skilled person “would have a medical degree, experience treating patients with ear infections, and knowledge of the pharmacology and use of antibiotics.” Id. “This person would be . . . a pediatrician or general practitioner—those doctors who are often the ‘first line of defense’ in treating ear infections and who, by virtue of their medical training, possess basic pharmacological knowledge.” Id. The Federal Circuit overturned this finding, holding that rather, a person of ordinary skill in the art was “a person engaged in developing new pharmaceuticals, formulations and treatment methods, or a specialist in ear treatments such as an otologist, otolaryngologist, or otorhinolaryngologist who also has training in pharmaceutical formulations.” Id. Courts have employed a flexible approach to considering informal education. See, e.g., Penda Corp. v. United States., 29 Fed. Cl. 533, 565 (1993). For instance, in Bose Corp. v. JBL, Inc., the District Court found that keeping “up with current literature and trade magazines to keep abreast of new developments” could be the equivalent of “a bachelor of science degree in electrical engineering, physics, mechanical engineering, or possibly acoustics.” 112 F. Supp. 2d 138, 155 (D. Mass. 2000).
86. See Graham v. Gun-Munro, No. C-99-04064 CRB, 2001 U.S. Dist. LEXIS 7110, at *19 (N.D. Cal. May 22, 2001) (holding that the skilled person had some formal education but no special training in the field of art in a case regarding fly wraps for the legs of horses).
87. See Imperial Chem. Indus., PLC v. Danbury Pharmacal, Inc., 777 F. Supp. 330, 371–72 (D. Del. 1991) (holding that the skilled person in the chemical industry is an organic chemist with a PhD); see also Envtl. Designs, Ltd. v. Union Oil Co. of Cal., 713 F.2d 693, 697 (Fed. Cir. 1983) (noting the respective chemical expert witnesses of the parties with extensive backgrounds in sulfur chemistry were skilled persons).
88. Guidelines for Examination, EUR. PAT. OFF., http://www.epo.org/law-practice/legal- texts/html/guidelines/e/g_vii_3.htm [https://perma.cc/XFY3-JD8J] (“There may be instances where it is more appropriate to think in terms of a group of persons, e.g. a research or production team, rather than a single person.”). See, e.g., MedImmune v. Novartis Pharm. U.K., Ltd., [2012] EWCA Civ. 1234 (evaluating obviousness from the perspective of
  
432
20 66 UCLA L. REV. 2 (2019)
E. Analogous Prior Art
Determining what constitutes prior art is also central to the obviousness inquiry.89 On some level, virtually all inventions involve a combination of known elements.90 The more prior art can be considered, the more likely an invention is to appear obvious. To be considered for the purposes of obviousness, prior art must fall within the definition for anticipatory references under Section 102 and must additionally qualify as “analogous art.”91
Section 102 contains the requirement for novelty in an invention, and it explicitly defines prior art.92 An extraordinarily broad amount of information qualifies as prior art, including any printed publication made available to the publicpriortofilingapatentapplication.93 Courtshavelongheldthatinventors arechargedwithconstructiveknowledgeofallpriorart.94 Whilenorealinventor could have such knowledge,95 the social benefits of this rule are thought to outweigh its costs.96 Granting patents on existing inventions could prevent the
a “skilled team”). The “[P]atent is addressed to a team of scientists with differing backgrounds in areas such as immunology, in particular antibody structural biology, molecular biology and protein chemistry, but with a common interest in antibody engineering.” Id. In the United States, the idea that the skilled person could be a group of individuals has been discussed in academic literature, but may not have been explicitly adopted by the courts. See, e.g., Jonathan J. Darrow. The Neglected Dimension of Patent Law’s PHOSITA Standard, 23 HARV. J.L. & TECH. 227, 244, 257 (2009). A “skilled persons” standard would seem to be appropriate given that most patents are now filed with more than one inventor. Dennis Crouch, PHOSITA: Not a Person—People Having Ordinary Skill in the Art, PATENTLY-O (June 7, 2018), https://patentlyo.com/patent/2018/06/phosita-not-a- person-people-having-ordinary-skill-in-the-art.html [https://perma.cc/UAK2-5NT8] (noting that most patents have multiple inventors).
89. This is the second inquiry of the Graham analysis described earlier.
90. See, e.g., Ryko Mfg. Co. v. Nu-Star, Inc., 950 F.2d 714, 718 (Fed. Cir. 1991).
91. In re Bigio, 381 F.3d 1320, 1325 (Fed. Cir. 2004).
92. 35 U.S.C. § 102 (2018).
93. Id. § 102(a)(1); see MPEP § 2152 for a detailed discussion of what constitutes prior art.
Almost anything in writing is prior art. “A U.S. patent on the lost wax casting technique was invalidated on the basis of Benvenuto Cellini’s 16th century autobiography which makes mention of a similar technique.” See Michael Ebert, Superperson and the Prior Art, 67 J. PAT. & TRADEMARK OFF. SOC’Y 657, 658 (1985).
94. In Mast, Foos, & Co. v. Stover Manufacturing Co., the Supreme Court applied a presumption that the skilled person is charged with constructive knowledge of all prior art: “Having all these various devices before him, and whatever the facts may have been, he is chargeable with a knowledge of all preexisting devices.” 177 U.S. 485, 493 (1900) (emphasis added) (further, “we must presume the patentee was fully informed of everything which preceded him, whether such were the actual fact or not”).
95. See, e.g., In re Wood, 599 F.2d 1032, 1036 (C.C.P.A. 1979) (“[A]n inventor could not possibly be aware of every teaching in every art.”).
96. See Bonito Boats, Inc. v. Thunder Craft Boats, Inc., 489 U.S. 141, 147–48 (1989) (reciting that Thomas Jefferson, the “driving force behind early federal patent policy,” believed that
  
433
Everything Is Obvious 21
public from using something it already had access to, and remove knowledge from the public domain.97
For the purposes of obviousness, prior art under Section 102 must also qualify as analogous. That is to say, the prior art must be in the field of an applicant’s endeavor, or reasonably pertinent to the problem with which the applicant was concerned.98 A real inventor would be expected to focus on this type of information. The “analogous art” rule better reflects practical conditions, and it ameliorates the harshness of the definition of prior art for novelty given that prior art references may be combined for purposes of obviousness but not novelty.99 Consequently, for the purposes of obviousness, the skilled person is presumed to have knowledge of all prior art within the field of an invention, as well as prior art reasonably pertinent to the problem the invention solves. Restricting the universe of prior art to analogous art lowers the bar to patentability.100
“a grant of patent rights in an idea already disclosed to the public [i]s akin to an ex post facto law, ‘obstruct[ing] others in the use of what they possessed before’” (quoting Letter to Isaac McPherson, supra note 37, at 176)); Graham v. John Deere Co., 383 U.S. 1, 5–6 (1966) (stating that granting patents on non-novel inventions would remove knowledge from the public domain).
97. Graham, 383 U.S. at 5–6.
98. See, e.g., Wyers v. Master Lock Co., 616 F.3d 1231, 1237 (Fed. Cir. 2010) (“Two criteria are
relevant in determining whether prior art is analogous: ‘(1) whether the art is from the same field of endeavor, regardless of the problem addressed, and (2) if the reference is not within the field of the inventor’s endeavor, whether the reference still is reasonably pertinent to the particular problem with which the inventor is involved.’” (quoting Comaper Corp. v. Antec, Inc., 596 F.3d 1343, 1351 (Fed. Cir. 2010)). “Under the correct analysis, any need or problem known in the field of endeavor at the time of the invention and addressed by the patent [or application at issue] can provide a reason for combining the elements in the manner claimed.” KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 420 (2007). Prior art in other fields may sometimes be considered as well. Id. at 417. The general question is whether it would have been “reasonable” for the skilled person to consider a piece of prior art to solve their problem. In re Clay, 966 F.2d 656 (Fed. Cir. 1992). To be “reasonably pertinent,” prior art must “logically [] have commended itself to an inventor’s attention in considering his problem.” Id.
99. See In re Wood, 599 F.2d 1032, 1036 (C.C.P.A. 1979) (“The rationale behind this rule precluding rejections based on combination of teachings of references from nonanalogous arts is the realization that an inventor could not possibly be aware of every teaching in every art.”). The rule “attempt[s] to more closely approximate the reality of the circumstances surrounding the making of an invention by only presuming knowledge by the inventor of prior art in the field of his endeavor and in analogous arts.” Id.
100. See Margo A. Bagley, Internet Business Model Patents: Obvious by Analogy, 7 MICH. TELECOMM. & TECH. L. REV. 253, 270 (2001) (arguing that prior to the analogous arts test references were rarely excluded as prior art); see also Jacob S. Sherkow, Negativing Invention, 2011 BYU L. REV. 1091, 1094–95 (2011) (noting that once a relevant piece of prior art is classified as analogous, an obviousness finding is often inevitable).
  
434
22 66 UCLA L. REV. 2 (2019)
The analogous art requirement was most famously conceptualized in the case of In re Winslow, in which the court explained a decisionmaker was to “picture the inventor as working in his shop with the prior art references—which he is presumed to know—hanging on the walls around him.”101 Or, as Judge Learned Hand presciently remarked, “the inventor must accept the position of a mythically omniscient worker in his chosen field. As the arts proliferate with prodigious fecundity, his lot is an increasingly hard one.”102
II. MACHINE INTELLIGENCE IN THE INVENTIVE PROCESS A. Automating and Augmenting Research
Artificial intelligence (AI), which is to say a computer able to perform tasks normally requiring human intelligence, is playing an increasingly important role in innovation.103 For instance, IBM’s flagship AI system “Watson” is being used exploratively to conduct research in drug discovery, as well as clinically to analyze the genes of cancer patients and develop treatment plans.104 In drug discovery, Watson has already identified novel drug targets and new indications for existing drugs.105 In doing so, Watson may be generating patentable inventions either autonomously or collaboratively with human researchers.106 Inclinicalpractice,Watsonisalsoautomatingaoncehumanfunction.107 Infact, according to IBM, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, a task which otherwise requires
101. In re Winslow, 365 F.2d 1017, 1020 (C.C.P.A. 1966).
102. Merit Mfg. Co. v. Hero Mfg. Co., 185 F.2d 350, 352 (2d Cir. 1950).
103. See, e.g., DATA SCI. ASS’N, OUTLOOK ON ARTIFICIAL INTELLIGENCE IN THE ENTERPRISE 3, 6
(2016), http://www.datascienceassn.org/sites/default/files/Outlook%20on%20Artificial% 20Intelligence%20in%20the%20Enterprise%202016.pdf [hereinafter Outlook on AI] (a survey of 235 business executives conducted by the National Business Research Institute (NBRI) which found that 38 percent of enterprises were using AI technologies in 2016, and 62 percent will likely use AI technologies by 2018).
104. IBM Watson for Drug Discovery, IBM, https://www.ibm.com/watson/health/life- sciences/drug-discovery [https://perma.cc/DQ4D-ZKJF]; IBM Watson for Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-genomics/genomics [https://perma.cc/8XK7-S8DN].
105. Ying Chen et al., IBM Watson: How Cognitive Computing Can Be Applied to Big Data Challenges in Life Sciences Research, 38 CLINICAL THERAPEUTICS 688 (2016), https://www.medicalaffairs.org/app/uploads/2018/02/Chen_2016_IBM_Watson.pdf.
106. See generally Hal the Inventor, supra note 3 (discussing the “hypothetical” example of an AI system being used in drug discovery to identify new drug targets and indications for existing drugs).
107. Kazimierz O. Wrzeszczynski et al., Comparing Sequencing Assays and Human-Machine Analyses in Actionable Genomics for Glioblastoma, 3 NEUROLOGY GENETICS e164 (2017), http://ng.neurology.org/content/3/4/e164 [https://perma.cc/3LGH-TKPW].
  
435
Everything Is Obvious 23
around 160 hours of work by a team of experts.108 A recent study by IBM found that Watson’s report outperformed the standard practice.109
Watson is largely structured as an “expert system,” although Watson is not a single program or computer—the brand incorporates a variety of technologies.110 Here, Watson will be considered a single software program in the interests of simplicity. Expert systems are one way of designing AI that solve problems in a specific domain of knowledge using logical rules derived from the knowledge of experts. These were a major focus of AI research in the 1980s.111 Expert system-based chess-playing programs HiTech and Deep Thought defeated chess masters in 1989, paving the way for another famous IBM computer, Deep Blue, to defeat world chess champion Garry Kasparov in 1997.112 But Deep Blue had limited utility—it was solely designed to play chess. The machine was permanently retired after defeating Kasparov.113
Google’s leading AI system DeepMind is an example of another sort of inventive machine. DeepMind uses an artificial neural network, which essentially consists of many highly interconnected processing elements working together to solve specific problems.114 The design of neural networks is inspired by the way the human brain processes information.115 Like the human brain, neuralnetworkscanlearnbyexampleandfrompractice.116 Examplesforneural networks come in the form of data, so more data means improved performance.117 Thishasledtodatabeingdescribedasthenewoilofthetwenty- firstcentury,andthefuelformachinelearning.118 Developersmaynotbeableto
108. Id.
109. Id.
110. See Richard Waters, Artificial Intelligence: Can Watson Save IBM?, FINANCIAL TIMES (Jan. 5,
2016), https://www.ft.com/content/dced8150-b300-11e5-8358-9a82b43f6b2f [https://perma.cc/ J3N6-QMP3]; see also Will Knight, IBM’s Watson Is Everywhere—But What Is It?, MIT TECH. REV, (Oct. 27, 2016), https://www.technologyreview.com/s/602744/ibms-watson-is- everywhere-but-what-is-it [http://perma.cc/YK3Q-HRQB].
111. STUART J. RUSSELL & PETER NORVIG, ARTIFICIAL INTELLIGENCE: A MODERN APPROACH 22–23 (2d ed. 2002) (1995).
112. IBM’s 100 Icons of Progress: Deep Blue, IBM http://www-03.ibm.com/ibm/history/ibm100/ us/en/icons/deepblue/words [https://perma.cc/7SG3-UYST].
113. Id.
114. KEVIN GURNEY, AN INTRODUCTION TO NEURAL NETWORKS 1–4 (1997). The first neural
network was built in 1951. See, e.g., RUSSELL & NORVIG, supra note 111.
115. See, e.g., Volodymyr Mnih et al., Human-Level Control Through Deep Reinforcement
Learning, 518 NATURE 529, 529–33 (2015).
116. See GURNEY, supra note 114, at 1–4.
117. PEDRO DOMINGOS, THE MASTER ALGORITHM: HOW THE QUEST FOR THE ULTIMATE LEARNING
MACHINE WILL REMAKE OUR WORLD xi (2015).
118. See, e.g., Michael Palmer, Data Is the New Oil, ANA MARKETING MAESTROS (Nov. 3, 2006).
  
436
24 66 UCLA L. REV. 2 (2019)
understand exactly how a neural network processes data or generates a particular output.
In 2016, DeepMind developed an algorithm known as AlphaGo which beat a world champion of the traditional Chinese board game Go, and then the world’sleadingplayerin2017.119 Gowasthelasttraditionalboardgameatwhich people had been able to outperform machines.120 AlphaGo’s feat was widely lauded in the artificial intelligence community because Go is exponentially more complicatedthanchess.121 Currentcomputerscannot“solve”Gosolelybyusing “brute force” computation to determine the optimal move to any potential configurationinadvance.122 TherearemorepossibleboardconfigurationsinGo than there are atoms in the universe.123 Rather than being preprogrammed with a number of optimal Go moves, DeepMind used a general-purpose algorithm to interpret the game’s patterns.124 DeepMind is now working to beat human players at the popular video game StarCraft II.125
AI like DeepMind is proving itself and training by playing games, but similar techniques can be applied to other challenges requiring recognition of complex patterns, long-term planning, and decisionmaking.126 DeepMind is already being applied to solve practical problems. For instance, it has helped decrease cooling costs at company datacenters.127 DeepMind is working to
119. David Silver et al., Mastering the Game of Go With Deep Neural Networks and Tree Search, 529 NATURE 484, 484–89 (2016). In 2015, DeepMind attained “human-level performance in video games” playing a series of class Atari 2600 games. Mnih et al., supra note 115, at 529. See also, Cade Metz, https://www.wired.com/2017/05/googles-alphago-continues- dominance-second-win-china [https://perma.cc/WA9G-JUGK].
120. See Richard Haridy, 2017: The Year AI Beat Us at All Our Own Games, NEW ATLAS (Dec. 26. 2017), https://newatlas.com/ai-2017-beating-humans-games/52741 [https://perma.cc/ AH2Y-6FFD].
121. Silver et al, supra note 119.
122. Id.; cf. Cade Metz, One Genius’ Lonely Crusade to Teach a Computer Common Sense, WIRED
(Mar. 24, 2016), [hereinafter Lonely Crusade] https://www.wired.com/2016/03/ doug- lenat-artificial-intelligence-common-sense-engine [https://perma.cc/WN2G-5CU9] (arguing that brute force computation was part of AlphaGo’s functionality).
123. 10170, or thereabouts. Silver et al, supra note 119.
124. Silver et al, supra note 119.
125. Tom Simonite, Google’s AI Declares Galactic War on StarCraft, WIRED (Aug. 9, 2017),
https://www.wired.com/story/googles-ai-declares-galactic-war-on-starcraft- [http://perma.cc/3VZJ-XXJV]. Compared with Go, StarCraft is vastly more complex. It involves high levels of strategic thinking and acting with imperfect information. Id.
126. Game playing has long been a proving ground for AI, as far back as what may have been the very first AI program in 1951. See Jack Copeland, A Brief History of Computing, ALANTURING.NET (June 2000) http://www.alanturing.net/turing_archive/pages/Reference% 20Articles/BriefHistofComp.html [https://perma.cc/82JN-UC93]. That program played checkers and was competitive with amateurs. Id.
127. See Simonite, supra note 125.
  
437
Everything Is Obvious 25
develop an algorithm to distinguish between healthy and cancerous tissues, and to evaluate eye scans to identify early signs of diseases leading to blindness.128 The results of this research may well be patentable.
Ultimately, the developers of DeepMind hope to create Artificial General Intelligence (AGI).129 Existing, “narrow” or specific AI (SAI) systems focus on discrete problems or work in specific domains. For instance, “Watson for Genomics” can analyze a genome and provide a treatment plan, and “Chef Watson” can develop new food recipes by combining existing ingredients. However, Watson for Genomics cannot respond to open-ended patient queries about their symptoms. Nor can Chef Watson run a kitchen. New capabilities could be added to Watson to do these things, but Watson can only solve problems it has been programmed to solve.130 By contrast, AGI would be able to successfully perform any intellectual task a person could.
AGI could even be set to the task of self-improvement, resulting in a continuously improving system that surpasses human intelligence—what philosopher Nick Bostrom has termed Artificial SuperIntelligence (ASI).131 Such an outcome has been referred to as the intelligence explosion or the technological singularity.132 ASI could then innovate in all areas of technology, resulting in progress at an incomprehensible rate. As the mathematician Irving John Good wrote in 1965, “the first ultraintelligent machine is the last invention that man need ever make.”133
128. Chris Baraniuk, Google’s DeepMind to Peek at NHS Eye Scans for Disease Analysis, BBC (July 5, 2016), https://www.bbc.com/news/technology-36713308 [https://perma.cc/ WA6R- RUX3]; Chris Baraniuk, Google DeepMind Targets NHS Head and Neck Cancer Treatment, BBC (Aug. 31, 2016), https://www.bbc.com/news/technology-37230806 [http://perma.cc/6GAN-7EAZ].
129. Solving Intelligence Through Research, DEEPMIND, https://deepmind.com/research [https://perma.cc/7TC2-49B8].
130. See, e.g., Lonely Crusade, supra note 122.
131. See generally NICK BOSTROM, SUPERINTELLIGENCE: PATHS, DANGERS, STRATEGIES (2014).
132. See generally RAY KURZWEIL, THE SINGULARITY IS NEAR: WHEN HUMANS TRANSCEND
BIOLOGY (2005).
133. Irving John Good, Speculations Concerning the First Ultraintelligent Machine, 6 ADVANCES
IN COMPUTERS 31, 33 (1965)
Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind.... Thusthefirstultraintelligentmachineisthelastinventionthatman need ever make . . . .
Id. at 32–33.
  
438
26 66 UCLA L. REV. 2 (2019)
Experts are divided on when, and if, AGI will be developed. Many industry leaders predict based on historical trends that AGI will occur within the next couple of decades.134 Others believe the magnitude of the challenge has been underestimated, and that AGI may not be developed in this century.135 In 2013, hundreds of AI experts were surveyed on their predictions for AGI development.136 On average, participants predicted a 10 percent likelihood that AGI would exist by 2022, a 50 percent likelihood it would exist by 2040, and a 90 percent likelihood it would exist by 2075.137 In a similar survey, 42 percent of participants predicted AGI would exist by 2030, and an additional 25 percent predicted AGI by 2050.138 In addition, 10 percent of participants reported they believed ASI would develop within two years of AGI, and 75 percent predicted this would occur within 30 years.139 The weight of expert opinion thus holds artificial general intelligence and superintelligence will exist this century. In the meantime, specific artificial intelligence is getting ever better at outcompeting people at specific tasks—including invention.
B. Timeline to the Creative Singularity
We are amid a transition from human to machine inventors. The following five-phase framework illustrates this transition and divides the history and future of inventive AI into several stages.
134. Pawel Sysiak, When Will the First Machine Become Superintelligent?, AI REVOLUTION, (Apr. 11, 2016), https://medium.com/ai-revolution/when-will-the-first-machine-become- superintelligent-ae5a6f128503 [https://perma.cc/7YUP-DEYM].
135. Id. In fairness, history also reflects some overly optimistic predictions. In 1970, Marvin Minsky, one of the most famous AI thought leaders, was quoted in Life Magazine as stating, “In from three to eight years we will have a machine with the general intelligence of an average human being.” Brad Darrach, Meet Shaky, the First Electronic Person, LIFE, Nov. 20 1970, at 58B, 66, 68.
136. See Müller & Bostrom, supra note 7.
137. Id. Participants were asked to provide an optimistic year for AGI’s development (10 percent
likelihood), a realistic year (50 percent likelihood), and a pessimistic year (90 percent likelihood). The median responses were 2022 as an optimistic year, 2040 as a realistic year, and 2075 as a pessimistic year. Id.
138. A survey conducted at an annual AGI Conference reported that 42 percent believed AGI would exist by 2030, 25 percent by 2050, 20 percent by 2100, 10 percent after 2010, and 2 percent never. See JAMES BARRAT, OUR FINAL INVENTION: ARTIFICIAL INTELLIGENCE AND THE END OF THE HUMAN ERA 152 (2013). For instance, Demis Hassabis, the founder of DeepMind, believes AGI is still decades away. David Rowan, DeepMind: Inside Google’s Super-Brain, WIRED (June 22, 2015), https://www.wired.co.uk/article/deepmind [https://perma.cc/MM6P-EU43].
139. See Müller & Bostrom, supra note 7.
  
439
Everything Is Obvious
I Human
III Human ~ SAI
27
   Phase
 Inventors
 Skilled Standard
 Timeframe
 Person
Augmented Person ~ SAI
Past
Short Term
  II
 Human > SAI
 Augmented Person
 Present
   IV
 SAI~AGI> Human
 Augmented AGI
 Medium Term
 V ASI ASI LongTerm
Table 1: Evolution of Machine Invention
  SAI = Specific Artificial Intelligence; AGI = Artificial General Intelligence; ASI = Artificial Superintelligence; ~ = competing; > = outcompeting
 Previously, in Phase I, all invention was created by people. If a company wanted to solve an industrial problem, it asked a research scientist, or a team of research scientists, to solve the problem. Phase I ended when the first patent was granted for an invention created by an autonomous machine—likely 1998 or earlier.140 It may be difficult to determine precisely when the first patent was issued for an autonomous machine invention, as there is no obligation to report the role of machines in patent applications. Still, any number of patents have likely been issued to inventions autonomously generated by machines.141 In 1998, a patent was issued for an invention autonomously developed by a neural network-based system known as the Creativity Machine.142
140. Phase I might also be distinguished by the first time a machine invented anything independently of receiving a patent. However, using the first granted patent application is a better benchmark. It is an external measure of a certain threshold of creativity, and it represents the first time a computer automated the role of a patent inventor. Of course, there is a degree of subjectivity in a patent examiner determining whether an invention is new, nonobvious, and useful. What is nonobvious to one examiner may be obvious to another. See, e.g., Iain M. Cockburn et al., Are All Patent Examiners Equal? The Impact of Characteristics on Patent Statistics and Litigation Outcomes, in PATENTS IN THE KNOWLEDGE- BASED ECONOMY, (Wesley M. Cohen & Steven A. Merrill eds., 2003) (describing significant interexaminer variation).
141. See generally, I Think, supra note 1, at 1083–91 (describing patents issued for “computational invention”).
142. Id. at 1083–86.
 
440
28 66 UCLA L. REV. 2 (2019)
Patents may have been granted on earlier machine inventions. For instance, an article published in 1983 describes experiments with an AI program known as Eurisko, in which the program “invent[ed] new kinds of three- dimensional microelectronic devices . . . novel designs and design rules have emerged.”143 Eurisko was an early, expert AI system for autonomously discovering new information.144 It was programmed to operate according to a series of rules known as heuristics, but it was able to discover new heuristics and usethesetomodifyitsownprogramming.145 Todesignnewmicrochips,Eurisko was programmed with knowledge of basic microchips along with simple rules and evaluation criteria.146 It would then combine existing chip structures together to create new designs, or mutate existing entities.147 The new structure would then be evaluated for interest and either retained or discarded.148 Several references suggest a patent was granted for one of Eurisko’s chip designs in the mid–1980s.149
Although, after investigating those references for this article, the references appear to refer to a patent application filed for the chip design by Stanford University in 1980 which the University abandoned for unknown reasons in 1984.150 Thus, a patent was never issued. Also, as with other publicly described
143. Douglas B. Lenat et al., Heuristic Search for New Microcircuit Structures: An Application of Artificial Intelligence, 3 AI MAG. , 17, 17 (1982).
144. Eurisko was created by Douglas Lenat as the successor to the Automated Mathematician (AM). See generally Douglas B. Lenat & John Seely Brown, Why AM and EURISKO Appear to Work, 23 AI MAG., 269, 269–94 (1983). AM was an “automatic programming system” that could modify its own computer code, relying on heuristics. Id. Eurisko was a subsequent iteration of the machine designed to additionally develop new heuristics and incorporate those into its function. Id.
145. See Douglas B. Lenat et al., supra note 143.
146. Id.
147. Id.
148. Id.
149. See, e.g., RICHARD FORSYTH & CHRIS NAYLOR, THE HITCHHIKER’S GUIDE TO ARTIFICIAL INTELLIGENCE IBM PC BASIC VERSION 2167 (1986); see also MARGARET A. BODEN, THE CREATIVE MIND: MYTHS AND MECHANISMS 228 (2004).
150. U.S. provisional patent application SN 144,960, April 29, 1980. Email From Katherine Ku, Dir. of Stanford Office of Tech. Licensing, to author (Jan. 17, 2018) (on file with author). Douglas Lenat, CEO of Cycorp, Inc., who wrote Eurisko and performed the above- mentioned research, reported that this work was done “before the modern rage about patenting things . . . ” and that in his opinion Eurisko had independently created a number of patentable inventions. See Telephone Interview With Douglas Lenat, CEO, Cycorp, Inc. (Jan. 12, 2018). He further reported that after Eurisko came up with the chip design, Professor James Gibbons at Stanford successfully built a chip based on the machine’s design. Id. This chip was the subject of a patent application by Stanford, but the application was abandoned in 1984. U.S. provisional patent application SN 144,960, supra. Prior to the present investigation, Stanford had purged its paper file for the application and so no longer had records reflecting the reason for the abandonment. Email From Katherine Ku, supra.
  
441
Everything Is Obvious 29
instances of patent applications claiming the output of inventive machines, the patent application was filed on behalf of natural persons.151 In this case, they were the individuals who had built a physical chip based on Eurisko’s design.152
In the present, Phase II, machines and people are competing and cooperating at inventive activity. However, in all technological fields, human researchers are the norm and thus best represent the skilled person standard. While AI systems are inventing, it is unclear to what extent this is occurring: Inventive machine owners may not be disclosing the extent of such machines in the inventive process, due to concerns about patent eligibility or because companies generally restrict information about their organizational methods to maintain a competitive advantage. This phase will reward early adopters of inventive machines which are able to outperform human inventors at solving specific problems, and whose output can exceed the skilled person standard. In 2006, for instance, NASA recruited an autonomously inventive machine to design an antenna that flew on NASA’s Space Technology 5 (ST5) mission.153
While there may now only be a modest amount of autonomous154 machine invention, human inventors are being widely augmented by creative computers. For example, a person may design a new battery using a computer to perform calculations, search for information, or run simulations on new designs. The computer does not meet inventorship criteria, but it does augment the capabilities of a researcher in the same way that human assistants can help reduce an invention to practice. Depending on the industry researchers work in and the
Incidentally, Dr. Lenat is now continuing to develop an expert system-based AI that can use logical deduction and inference reasoning based on “common sense knowledge,” as opposed to a system like Watson that recognizes patterns in very large datasets. Id. He also states that his current company has developed numerous patentable inventions, but that it has not filed for patent protection, because he believes that, at least with regards to software, the downside of patents providing competitors with a roadmap to copying patented technology exceeds the value of a limited term patent. Id.
151. See I Think, supra note 1, at 1083–91 (describing instances of “computational invention”).
152. Email From Katherine Ku, supra note 150. Whether the individual(s) designing a chip or building a chip would qualify as inventor(s) would depend on the specific facts of the case and who “conceived” of the invention. See generally Hal the Inventor, supra note 3
(discussing standards for inventorship).
153. Gregory S. Hornby et al., Automated Antenna Design With Evolutionary Algorithms, AM.
INST. AERONAUTICS & ASTRONAUTICS (2006), http://alglobus.net/NASAwork/papers/
Space2006Antenna.pdf.
154. As the term is used here, autonomous machines are given goals to complete by users, but
determine for themselves the means of completing those goals. See Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. 1 (2018). For example, a user could ask a computer to design a new battery with certain characteristics, and the computer could produce such a design without further human input. In this case, the machine would be autonomously inventive and competing with human inventors.
  
442
30 66 UCLA L. REV. 2 (2019)
problems they are trying to solve, researchers may rarely be unaided by computers. The more sophisticated the computer, the more it may be able to augment the worker’s skills.
Phase III, in the near future, will involve increased competition and cooperation between people and machines. In certain industries, and for certain problems, inventive machines will become the norm. For example, in the pharmaceutical industry, Watson is now identifying novel drug targets and new indications for existing drugs. Soon, it may be the case that inventive machines are the primary means by which new uses for existing drugs are researched. That is a predictable outcome, given the advantage machines have over people at recognizing patterns in very large datasets. However, it may be that people still perform the majority of research related to new drug targets. Where the standard varies within a broad field like drug discovery, this can be addressed by defining fields and problems narrowly, for instance according to the subclasses currently used by the Patent Office.155
Perhaps twenty-five years from now—based on expert opinion—the introduction of AGI will usher in Phase IV. Recall that AGI refers to artificial intelligence that can be applied generally, as opposed to narrowly in specific fields of art, and that it has intelligence comparable to a person. AGI will compete with human inventors in every field, which makes AGI a natural substitute for the skilled person. Even with this new standard, human inventors may continue to invent—just not as much. An inventor may be a creative genius whose abilities exceed the human average, or a person of ordinary intelligence who has a groundbreaking insight.
Just as SAI outperforms people in certain fields, it will likely be the case that SAI outperforms AGI in certain circumstances. An example of this could be when screening a million compounds for pesticide function lends itself to a “brute force” computational approach. For this reason, SAI could continue to represent the level of ordinary skill in fields in which SAI is the standard, while AGI could replace the skilled person in all other fields. However, the two systems will likely be compatible. A general AI system wanting to play Go could incorporate AlphaGo into its own programming, design its own algorithm like AlphaGo, or even instruct a second computer operating AlphaGo.
AGI will change the human-machine dynamic in another way. If the machine is genuinely capable of performing any intellectual task a person could,
155. See generally, Overview of the U.S. Patent Classification System (U.S.P.C.), U.S. PAT & TRADEMARK OFF. (2012), https://www.uspto.gov/sites/default/files/patents/resources/ classification/overview.pdf.
  
443
Everything Is Obvious 31
the machine would be capable of setting goals collaboratively with a person, or even by itself. Instead of a person instructing a computer to screen a million compounds for pesticide function, a person could merely ask a computer to develop a new pesticide. For that matter, an agrochemical company like Bayer could instruct DeepMind to develop any new technology for its business, or just to improve its profitability. Such machines should not only be able to solve known problems, but also solve unknown problems.
AGI will continually improve, transforming into ASI. Ultimately, in Phase V, when AGI succeeds in developing artificial superintelligence, it will mean the end of obviousness. Everything will be obvious to a sufficiently intelligent machine.
C. Inventive and Skilled Machines
For purposes of patent law, an inventive machine should be one which generates patentable output while meeting traditional inventorship criteria.156 Because obviousness focuses on the quality of a patent application’s inventive content, it should be irrelevant whether the content comes from a person or machine, or a particular type of machine. A machine which autonomously generates patentable output, or which does so collaboratively with human inventors where the machine meets joint inventorship criteria, is inventive.
Under the present framework, inventive machines would not be the equivalent of hypothetical skilled machines, just as human inventors are not skilled persons. In fact, it should not be possible to extrapolate about the characteristics of a skilled entity from information about inventive entities. Granted, the Federal Circuit once included the “educational level of the inventor” in its early factor-based test for the skilled person.157 However, that was only until it occurred to the Federal Circuit that:
[C]ourts never have judged patentability by what the real inventor/applicant/patentee could or would do. Real inventors, as a class, vary in the capacities from ignorant geniuses to Nobel laureates; the courts have always applied a standard based on an
156. See I Think, supra note 1 (arguing computers which independently meet human inventorship criteria should be recognized as inventors).
157. See e.g., Environmental, supra note 84.
  
444
32 66 UCLA L. REV. 2 (2019)
imaginary work of their own devising whom they have equated with the inventor.158
What then conceptually is a skilled machine? A machine that anthropomorphizes to the various descriptions courts have given for the skilled person? Such a test might focus on the way a machine is designed or how it functions. For instance, a skilled machine might be a conventional computer that operates according to fixed, logical rules, as opposed to a machine like DeepMind which can function unpredictably. However, basing a rule on how a computer functions might not work for the same reason the Flash of Genius test failed: Even leaving aside the significant logistical problem of attempting to figure out how a computer is structured or how it generates particular output, patent law should be concerned with whether a machine is generating inventive output, not what is going on inside the machine.159 If a conventional computer and a neural network were both able to generate the same inventive output, there should be no reason to favor one over the other.
Alternately, the test could focus on a machine’s capacity for creativity. For example, Microsoft Excel plays a role in a significant amount of inventive activity, but it is not innovative. It applies a known body of knowledge to solve problems with known solutions in a predictable fashion (for example, multiplying values together). However, while Excel may sometimes solve problems that a person could not easily solve without the use of technology, it lacks the ability to engage in almost any inventive activity.160 Excel is not the equivalent of a skilled machine—it is an automaton incapable of ordinary creativity.
Watson in clinical practice may be a better analogy for a skilled worker. Watson analyzes patients’ genomes and provides treatment recommendations.161 Yet as with Excel, this activity is not innovative. The problem Watson is solving may be more complex than multiplying a series of numbers, but it has a known solution. Watson is identifying known genetic mutations from a patient’s genome. Watson is then suggesting known treatments based on existing medical literature. Watson is not innovating
158. Kimberly-Clark Corp. v. Johnson & Johnson, 745 F.2d 1437, 1454 (Fed. Cir. 1984) (“[The] hypothetical person is not the inventor, but an imaginary being possessing ‘ordinary skill in the art’ created by Congress to provide a standard of patentability.”).
159. See I Think, supra note 1 (arguing against a subjective standard for computational invention).
160. Some behaviors like correcting a rogue formula may have a functionally creative aspect, but this is a minimal amount that would not rise to the level of patent conception if performed by a person.
161. See Wrzeszczynski et al., supra note 107.
  
445
Everything Is Obvious 33
because it is being applied to solve problems with known solutions, adhering to conventional wisdom.
Unlike Excel, however, Watson could be inventive. For instance, Watson could be given unpublished clinical data on patient genetics and actual drug responses and tasked with determining whether a drug works for a genetic mutation in a way that has not yet been recognized. Traditionally, such findings have been patentable. Watson may be situationally inventive depending on the problem it is solving.
It may be difficult to identify an actual computer program now which has a “skilled” level of creativity. To the extent a computer is creative, in the right circumstances, any degree of creativity might result in inventive output. To be sure, thisissimilartotheskilledperson. Apersonofordinaryskill,oralmostanyone,may haveaninventiveinsight. Characteristicscanbeimputedtoaskilledperson,butitis not possible the way the test is applied to identify an actual skilled person or to definitivelysaywhatshewouldhavefoundobvious. Theskilledpersontestissimply a theoretical device for a decisionmaker.
Assuming a useful characterization of a skilled machine, to determine that a skilled machine now represents the average worker in a field, decisionmakers would need information about the extent to which such machines are used. Obtaining this information may not be practical. Patent applicants could be asked generally about the use and prevalence of computer software in their fields, but it would be unreasonable to expect applicants to already have, or to obtain, accurate information about general industry conditions. The Patent Office, or another government agency, could attempt to proactively research the use of computers in different fields, but this would not be a workable solution. Such efforts would be costly, the Patent Office lacks expertise in this activity, and its findings would inevitably lag behind rapidly changing conditions. Ultimately, there may not be a reliable and low-cost source of information about skilled machines right now.
D. Inventive Is the New Skilled
Having inventive machines replace the skilled person may better correspond with real world conditions. Right now, there are inherent limits to the number and capabilities of human workers. The cost to train and recruit new researchers is significant, and there are a limited number of people with the ability to perform this work. By contrast, inventive machines are software
 
446
34 66 UCLA L. REV. 2 (2019)
programs which may be copied without additional cost.162 Once Watson outperforms the average industry researcher, IBM may be able to simply copy Watson and have it replace most of an existing workforce. Copies of Watson could replace individual workers, or a single Watson could do the work of a large team of researchers.
Indeed, as mentioned earlier, in a non-inventive setting, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, as opposed to a team of human experts, which takes around one- hundred and sixty hours.163 Once Watson is proven to produce better patient outcomes than the human team, it may be unethical to have people underperform a task which Watson can automate. When that occurs, Watson should not only replace the human team at its current facility—it should replace every comparable human team. Watson could similarly automate in an inventive capacity.
Thus, inventive machines change the skilled paradigm because once they become the average worker, the average worker becomes inventive. As the outputs of these inventive machines become routinized, however, they should no longer be inventive by definition. The widespread use of these machines should raise the bar for obviousness, so that these machines no longer qualify as inventive but shift to become skilled machines—machines which now represent the average worker and are no longer capable of routine invention.164
Regardless of the terminology, as machines continue to improve, the bar for nonobviousness should rise. To generate patentable output, it may be necessary to use an advanced machine that can outperform standard machines, or a person or machine will need to have an unusual insight that standard machines cannot easily recreate. Inventiveness might also depend on the data supplied to a machine, such that only certain data would result in inventive output. Taken to its logical extreme, and given there is no limit to how sophisticated computers can become, it may be that everything will one day be obvious to commonly used computers.
It is possible to generate reasonably low-cost and accurate information about the use of inventive machines. The Patent Office should institute a requirement for patent applicants to disclose the role of computers in the
162. ANDREAS KEMPER, VALUATION OF NETWORK EFFECTS IN SOFTWARE MARKETS: A COMPLEX NETWORKS APPROACH 37 (2010).
163. See Wrzeszczynski et al., supra note 107.
164. See Enzo Biochem, Inc. v. Calgene, Inc., 188 F.3d 1362, 1374 n.10 (Fed. Cir. 1999) (“In view
of the rapid advances in science, we recognize that what may be unpredictable at one point in time may become predictable at a later time.”).
  
447
Everything Is Obvious 35
inventive process.165 This disclosure could be structured along the lines of current inventorship disclosure. Right now, applicants must disclose all patent inventors.166 Failure to do so can invalidate a patent or render it unenforceable.167 Similarly, applicants should have to disclose when a machine autonomously meets inventorship criteria.
These disclosures would only apply to an individual invention. However, the Patent Office could aggregate responses to see whether most inventors in a field (for example, a class or subclass) are human or machine. These disclosures would have a minimal burden on applicants compared to existing disclosure requirements and the numerous procedural requirements of a patent application. In addition to helping the Patent Office with determinations of nonobviousness, these disclosures would provide valuable information for purposes of attributing inventorship.168 It might also be used to develop appropriate innovation policies in other areas.169
E. Skilled People Use Machines
The current standard neglects to appropriately take into account the modern importance of machines in innovation. Instead of now replacing the skilled person with the skilled machine, it would be less of a conceptual change, and administratively easier, to characterize the skilled person as an average worker facilitated by technology. Recall the factor test for the skilled person includes: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.”170 This test could be amended to include, (6) “technologies used by
165. It may also be beneficial for applicants to disclose the use of computers when they have been part of the inventive process but where their contributions have not risen to the level of inventorship. Ideally, a detailed disclosure should be provided: Applicants should need to disclose the specific software used and the task it performed. In most cases, this would be as simple as noting a program like Excel was used to perform calculations. However, while this information would have value for policy making, it might involve a significant burden to patent applicants.
166. Duty to Disclose Information Material to Patentability, 37 C.F.R. § 1.56 (2018), https://www.uspto.gov/web/offices/pac/mpep/s2001.html [https://perma.cc/4DE9-ZRWE].
167. See, e.g., Advanced Magnetic Closures, Inc. v. Rome Fastener Corp., 607 F.3d 817, 829–30 (Fed. Cir. 2010) (upholding a district court decision to render a patent unenforceable on the grounds of inequitable conduct for misrepresenting inventorship).
168. See I Think, supra note 1 (advocating for acknowledging machines as inventors).
169. See Should Robots Pay Taxes?, supra note 6 (arguing the need to monitor automation for
adjusting tax incentives).
170. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
  
448
36 66 UCLA L. REV. 2 (2019)
active workers.” This would more explicitly take into account the fact that human researchers’ capabilities are augmented with computers.
Moving forward in time, once the use of inventive machines is standard, instead of a skilled person being an inventive machine, the skilled person standard could incorporate the fact that technologies used by active workers includes inventive machines. In future research, the standard practice may be for a worker to ask an inventive machine to solve a problem. This could be conceptualized as the inventive machine doing the work, or the person doing the work using an inventive machine.
Granted, in some instances, using an inventive machine may require significant skill, for instance, if the machine is only able to generate a certain output by virtue of being supplied with certain data. Determining which data to provide a machine, and obtaining that data, may be a technical challenge. Also, it may be the case that significant skill is required to formulate the precise problem to put to a machine. In such instances, a person might have a claim to inventorship independent of the machine, or a claim to joint inventorship. This is analogous to collaborative human invention where one person directs another to solve a problem. Depending on details of their interaction, and who “conceived” of the invention, one person or the other may qualify as an inventor, or they may qualify as joint inventors.171 Generally, however, directing another partytosolveaproblemdoesnotqualifyforinventorship.172 Moreover,afterthe development of AGI, there may not be a person instructing a computer to solve a specific problem.
Whether the future standard becomes an inventive machine or a skilled person using an inventive machine, the result will be the same: The average worker will be capable of inventive activity. Replacing the skilled person with the inventive machine may be preferable doctrinally, because it emphasizes that it is the machine which is engaging in inventive activity, rather than the human worker.
The changing use of machines also suggests a change to the scope of prior art. The analogous art test was implemented because it is unrealistic to expect inventors to be familiar with anything more than the prior art in their field, and
171. “[C]onception is established when the invention is made sufficiently clear to enable one skilled in the art to reduce it to practice without the exercise of extensive experimentation or the exercise of inventive skill.” Hiatt v. Ziegler & Kilgour, 179 U.S.P.Q. 757, 763 (Bd. Pat. Interferences 1973); see also Gunter v. Stream, 573 F.2d 77, 79 (C.C.P.A. 1978).
172. Ex parte Smernoff, 215 U.S.P.Q. at 547 (“[O]ne who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not a coinventor.”).
  
449
Everything Is Obvious 37
the prior art relevant to the problem they are trying to solve.173 However, a machine is capable of accessing a virtually unlimited amount of prior art. Advances in medicine, physics, or even culinary science may be relevant to solving a problem in electrical engineering. Machine augmentation suggests that the analogous arts test should be modified or abolished once inventive machines are common, and that there should be no difference in prior art for purposes of novelty and obviousness.174 The scope of analogous prior art has consistently expanded in patent law jurisprudence, and this would complete that expansion.175
F. The Evolving Standard
The skilled person standard should be amended as follows:
1) The test should now incorporate the fact that skilled persons are already augmented by machines. This could be done by adding “technologies used by active workers” as a sixth factor to the Federal Circuit’s factor test for the skilled person.
2) Once inventive machines become the standard means of research in a field, the skilled person should be an inventive machine when the standard approach to research in a field or with respect to a particular problem is to use an inventive machine.
3) When and if artificial general intelligence is developed, inventive machines should become the skilled person in all areas, taking into account that artificial general intelligence may also be augmented by specific artificial intelligence.
III. A POST-SKILLED WORLD
This Part provides examples of how the Inventive Machine Standard could work in practice, such as by focusing on reproducibility or secondary factors. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function
173. In 1966, in Graham, the Court recognized that “the ambit of applicable art in given fields of science has widened by disciplines unheard of a half century ago . . . . [T]hose persons granted the benefit of a patent monopoly [must] be charged with an awareness of these changed conditions.” Graham v. John Deere Co., 383 U.S. 1, 19 (1966).
174. See supra Subpart I.E.
175. Innovative Scuba Concepts, Inc., v. Feder Indus., Inc., 819 F. Supp. 1487, 1503 (D. Colo.
1993) (discussing the expansion of analogous art); see also, e.g., George. J. Meyer Mfg. Co. v. San Marino Elec. Corp., 422 F.2d 1285, 1288 (9th Cir. 1970) (discussing the expansion of analogous art).
  
450
38 66 UCLA L. REV. 2 (2019)
as innovation incentives. To the extent patents accomplish other goals such as promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
A. Application
Mobil Oil Corp. v. Amoco Chemicals Corp. concerned complex technology involving compounds known as Zeolites used in various industrial applications.176 Mobil had developed new compositions known as ZSM-5 zeolites and a process for using these zeolites as catalysts in petroleum refining to help produce certain valuable compounds. The company received patent protection for these zeolites and for the catalytic process.177 Mobil subsequently sued Amoco, which was using zeolites as catalysts in its own refining operations, alleging patent infringement. Amoco counterclaimed seeking a declaration of noninfringement, invalidity, and unenforceability with respect to the two patents at issue. The case involved complex scientific issues. The three-week trial transcript exceeds 3300 pages, and more than 800 exhibits were admitted into evidence.
One of the issues in the case was the level of ordinary skill. An expert for Mobil testified that the skilled person would have “a bachelor’s degree in chemistry or engineering and two to three years of experience.”178 An expert for Amoco argued the skilled person would have a doctorate in chemistry and several years of experience.179 The District Court for the District of Delaware ultimately decided that the skilled person “should be someone with at least a Masters degree in chemistry or chemical engineering or its equivalent, [and] two or three years of experience working in the field.”180
If a similar invention and subsequent fact pattern happened today, to apply the obviousness standard proposed in this Article a decisionmaker would need to: (1) determine the extent to which inventive technologies are used in the field, (2) characterize the inventive machine(s) that best represents the average worker if inventive machines are the standard, and (3) determine whether the machine(s) would find an invention obvious. The decisionmaker is a patent
176. Mobil Oil Corp. v. Amoco Chems. Corp.,779 F. Supp. 1429, 1442–43 (D. Del. 1991).
177. Id.
178. Id. at 1443.
179. Id.
180. Id.
  
451
Everything Is Obvious 39
examiner in the first instance,181 and potentially a judge or jury in the event the validity of a patent is at issue in trial.182 For the first step, determining the extent to which inventive technologies are used in a field, evidence from disclosures to the Patent Office could be used. That may be the best source of information for patent examiners, but evidence may also be available in the litigation context.
Assume that today most petroleum researchers are human, and that if machines are autonomously inventive in this field, it is happening on a small scale. Thus, the court would apply the skilled person standard. However, the court would now also consider “technologies used by active workers.” For instance, experts might testify that the average industry researcher has access to a computer like Watson. They further testify that while Watson cannot autonomously develop a new catalyst, it can significantly assist an inventor. The computer provides a researcher with a database containing detailed information about every catalyst used not only in petroleum research, but in all fields of scientific inquiry. Once a human researcher creates a catalyst design, Watson can also test it for fitness together with a predetermined series of variations on any proposed design.
The question for the court will thus be whether the hypothetical person who holds at least a Master’s degree in chemistry or chemical engineering or its equivalent, has two or three years of experience working in the field, and is using Watson, would find the invention obvious. It may be obvious, for instance, if experts convincingly testify that the particular catalyst at issue were very closely related to an existing catalyst used outside of the petroleum industry in ammonia synthesis, that any variation was minor, and that a computer could do all the work of determining if it were fit for purpose.183 It might thus have been an obvious design to investigate, and it did not require undue experimentation in order to prove its effectiveness.
Now imagine the same invention and fact pattern occurring approximately ten years into the future, at which point DeepMind, together with Watson and a competing host of AI systems, have been set to the task of developing new
181. See U.S. PAT. & TRADEMARK OFF., supra note 24 (at the Patent Office, applications are initially considered by a patent examiner, and examiner decisions can be appealed to the Patent Trial and Appeal Board (PTAB)).
182. Mark A. Lemley, Why Do Juries Decide if Patents Are Valid? (Stanford Law Sch., Pub. Law & Legal Theory Research Paper Series, Working Paper No. 2306152, 2013), https://ssrn.com/abstract=2306152.
183. See Daiichi Sankyo Co. v. Matrix Labs., Ltd., 619 F.3d 1346, 1352 (Fed. Cir. 2010) (finding that a “chemist of ordinary skill would have been motivated to select and then to modify a prior art compound (e.g., a lead compound) to arrive at a claimed compound with a reasonable expectation that the new compound would have similar or improved properties compared with the old”).
  
452
40 66 UCLA L. REV. 2 (2019)
compounds to be used as catalysts in petroleum refining. Experts testify that the standard practice is for a person to provide data to a computer like DeepMind, specify desired criteria (for example, activity, stability, perhaps even designing around existing patents), and ask the computer to develop a new catalyst. From this interaction, the computer will produce a new design. As most research in this field is now performed by inventive machines, a machine would be the standard for judging obviousness.
The decisionmaker would then need to characterize the inventive machine(s). It could be a hypothetical machine based on general capabilities of inventive machines, or a specific computer. Using the standard of a hypothetical machine would be similar to using the skilled person test, but this test could be difficult to implement. A decisionmaker would need to reason what the machine would have found obvious, perhaps with expert guidance. It is already challenging for a person to predict what a hypothetical person would find obvious; it would be even more difficult to do so with a machine. Computers may excel at tasks people find difficult (like multiplying a thousand different numbers together), but even supercomputers struggle with visual intuition, which is mastered by most toddlers.
In contrast, using a specific computer should result in a more objective test. This computer might be the most commonly used computer in a field. For instance, if DeepMind and Watson are the two most commonly used AI systems for research on petroleum catalysts, and DeepMind accounts for 35 percent of the market while Watson accounts for 20 percent, then DeepMind could represent the standard. However, this potentially creates a problem—if DeepMind is the standard, then it would be more likely that DeepMind’s own inventions would appear obvious as opposed to the inventions of another machine. This might give an unfair advantage to non-market leaders, simply because of their size.
To avoid unfairness, the test could be based on more than one specific computer. For instance, both DeepMind and Watson could be selected to represent the standard. This test could be implemented in two different ways. In the first case, if a patent application would be obvious to DeepMind or Watson, then the application would fail. In the second case, the application would have to be obvious to both DeepMind and Watson to fail. The first option would result in fewer patents being granted, with those patents presumably going mainly to disruptive inventive machines with limited market penetration, or to inventions made using specialized non-public data. The second option would permit patents where a machine is able to outperform its competitors in some
 
453
Everything Is Obvious 41
material respect. The second option could continue to reward advances in inventive machines, and therefore seems preferable.
It may be that relatively few AI systems, such as DeepMind and Watson, end up dominating the research market in a field. Alternately, many different machines may each occupy a small share of the market. There is no need to limit the test to two computers. To avoid discriminating on the basis of size, all inventive machines being routinely used in a field or to solve a particular problem might be considered. However, allowing any machine to be considered could allow an underperforming machine to lower the standard, and too many machines might result in an unmanageable standard. An arbitrary cutoff may be applied based on some percentage of market share. That might still give some advantage to very small entities, but it should be a minor disparity.
After characterizing the inventive machine(s), a decisionmaker would need to determine whether the inventive machine(s) would find an invention obvious. This could broadly be accomplished in one of two ways: either with abstract knowledge of what the machines would find obvious, perhaps through expert testimony, or through querying the machines. The former would be the morepracticaloption.184 Forexample,apetroleumresearcherexperiencedwith DeepMind might be an expert, or a computer science expert in DeepMind and neural networks. This inquiry could focus on reproducibility.
Finally, a decisionmaker will have to go through a similar process if the same invention and fact pattern occurs twenty-five years from now, at which point artificial general intelligence has theoretically taken over in all fields of research. AGI should have the ability to respond directly to queries about whether it finds an invention obvious. Once AGI has taken over from the average researcher in all inventive fields, it may be widely enough available that the Patent Office could arrange to use it for obviousness queries. In the litigation context, it may be available from opposing parties. If courts cannot somehow access AGI, they may still have to rely on expert evidence.
184. Alternatively, the machine could be asked to solve the problem at question and given the relevant prior art. If the machine generates the substance of the patent, the invention would be considered obvious. However, this would require a decisionmaker to have access to the inventive machine. At the application stage, the Patent Office would need to contract with, say, Google to use DeepMind in such a fashion. For that matter, the Patent Office might use DeepMind not only to decide whether inventions are obvious, but to automate the entire patent examination process. At trial, if Google is party to a lawsuit, an opposing party might subpoena use of the computer. However, if Google is not a party, it might be unreasonable to impose on Google for access to DeepMind.
  
454
42 66 UCLA L. REV. 2 (2019)
B. Reproducibility
Even if an inventive machine standard is the appropriate theoretical tool for nonobviousness, it still requires certain somewhat subjective limitations, and decisionmakers may still have difficulty with administration. Still, the new standard only needs to be slightly better than the existing standard to be an administrative success.
A test focused on reproducibility, based on the ability of the machine selected to represent the standard being able to independently reproduce the invention, offers some clear advantages over the current skilled person standard, which results in inconsistent and unpredictable outcomes.185 Courts have “provided almost no guidance concerning either what degree of ingenuity is necessary to meet the standard or how a decisionmaker is supposed to evaluate whether the differences between the invention and prior art meet this degree.”186 This leaves decisionmakers in the unenviable position of trying to subjectively establish what another person would have found obvious. Worse, this determination is to be made in hindsight with the benefit of a patent application. On top of that, judges and juries lack scientific expertise.187 In practice, decisionmakers may read a patent application, decide that they know
185. See FED. TRADE COMM’N, supra note 16 (discussing objections to the skilled person standard).
186. Mandel, The Non-Obvious Problem, supra note 19, at 64.
187. As Judge Learned Hand wrote:
I cannot stop without calling attention to the extraordinary condition of the law which makes it possible for a man without any knowledge of even the rudiments of chemistry to pass upon such questions as these. The inordinate expense of time is the least of the resulting evils, for only a trained chemist is really capable of passing upon such facts . . . . How long we shall continue to blunder along without the aid of unpartisan and authoritative scientific assistance in the administration of justice, no one knows; but all fair persons not conventionalized by provincial legal habits of mind ought, I should think, unite to effect some such advance.
Parke-Davis & Co. v. H.K. Mulford Co., 189 F. 95, 115 (S.D.N.Y. 1911). See also Safety Car Heating & Lighting Co. v. Gen. Elec. Co., 155 F.2d 937, 939 (1946) (“Courts, made up of laymen as they must be, are likely either to underrate, or to overrate, the difficulties in making new and profitable discoveries in fields with which they cannot be familiar . . . .”); see also Doug Lichtman & Mark A. Lemley, Rethinking Patent Law’s Presumption of Validity, 60 STAN. L. REV. 45, 67 (2007) (“District Court judges are poorly equipped to read patent documents and construe technical patent claims. Lay juries have no skill when it comes to evaluating competing testimony about the originality of a technical accomplishment.”).
  
455
Everything Is Obvious 43
obviousness when they see it, and then reason backward to justify their findings.188
This is problematic because patents play a critical role in the development and commercialization of products, and patent holders and potential infringers should have a reasonable degree of certainty about whether patents are valid. A more determinate standard would make it more likely the Patent Office would apply a single standard consistently and result in fewer judicially invalidated patents. To the extent machine reproducibility is a more objective standard, this would seem to address many of the problems inherent in the current standard.
On the other hand, reproducibility comes with its own baggage. Decisionmakers have difficulty imagining what another person would find obvious, and it would probably be even more difficult to imagine in the abstract what a machine could reproduce. More evidence might need to be supplied in patent prosecution and during litigation, perhaps in the format of analyses performed by inventive machines, to demonstrate whether particular output was reproducible. This might also result in a greater administrative burden.
In some instances, reproducibility may be dependent on access to data. A large health insurer might be able to use Watson to find new uses for existing drugs by giving Watson access to proprietary information on its millions of members. Or, the insurer might license its data to drug discovery companies using Watson for this purpose. Without that information, another inventive computer might not able to recreate Watson’s analysis.
This too is analogous to the way data is used now in patent applications: Obviousness is viewed in light of the prior art, which does not include non- public data relied upon in a patent application. The rationale here is that this rule incentivizes research to produce and analyze new data. Yet as machines become highly advanced, it is likely that the importance of proprietary data will decrease. More advanced machines may be able to do more with less.
Finally, reproducibility would require limits. For instance, a computer which generates semi-random output might eventually recreate the inventive concept of a patent application if it were given unlimited resources. However, it would be unreasonable to base a test on what a computer would reproduce given, say, 7.5 million years.189 The precise limits that should be placed on
188. Jacobellis v. Ohio, 378 U.S. 184, 197 (1964) (Stewart, J., dissenting). This was later recognized as a failed standard. Miller v. California, 413 U.S. 15, 47–48 (1973) (Brennan, J., dissenting) (obscenity cases similarly relying on the Elephant Test).
189. This brings to mind a super intelligent artificial intelligence system, “Deep Thought,” which famously, and fictionally, took 7.5 million years to arrive at the “Answer to the Ultimate Question of Life, the Universe, and Everything.” DOUGLAS ADAMS, THE HITCHHIKER’S GUIDE TO THE GALAXY 180 (rev. ed. 2001) (1979). The answer was 42. Id. at 188.
  
456
44 66 UCLA L. REV. 2 (2019)
reproducibility might depend on the field in question, and what best reflected the actual use of inventive machines in research. For instance, when asked to design a new catalyst in the petroleum industry, Watson might be given access to all prior art and publicly available data, and then given a day to generate output.
C. An Economic vs. Cognitive Standard
The skilled person standard received its share of criticism even before the arrival of inventive machines.190 The inquiry focuses on the degree of cognitive difficulty in conceiving an invention but fails to explain what it actually means for differences to be obvious to an average worker. The approach lacks both a normative foundation and a clear application.191
In Graham, the Supreme Court’s seminal opinion on nonobviousness, the Court attempted to supplement the test with more “objective” measures by looking to real-world evidence about how an invention was received in the marketplace.192 Rather than technological features, these “secondary” considerations focus on “economic and motivational” features, such as commercial success, unexpected results, long-felt but unsolved needs, and the failure of others.193 Since Graham, courts have also considered, among other
190. See, e.g., Chiang, supra note 19, at 49 (as one commentator noted about the test as articulated by the Supreme Court in Graham, it gives “all the appearance of expecting a solution to appear out of thin air once the formula was followed. The lack of an articulable rule meant that determinations of obviousness took the appearance—and arguably the reality—of resting on judicial whim . . . .” (footnote omitted)); Abramowicz & Duffy, supra note 16, at 1598; Gregory N. Mandel, Patently Non-Obvious: Empirical Demonstration That the Hindsight Bias Renders Patent Decisions Irrational, 67 OHIO ST. L.J. 1391 (2006) (discussing problems with hindsight in nonobviousness inquiries); Gregory N. Mandel, Another Missed Opportunity: The Supreme Court’s Failure to Define Nonobviousness or Combat Hindsight Bias in KSR v. Teleflex, 12 LEWIS & CLARK L. REV. 323 (2008).
191. See Abramowicz & Duffy, supra note 16, at 1603 (“[N]either Graham nor in subsequent cases has the Supreme Court attempted either to reconcile the inducement standard with the statutory text or to provide a general theoretical or doctrinal foundation for the inducement standard.”).
192. See Graham v. John Deere Co., 383 U.S. 1, 17; MPEP § 2144.
193. Graham, 383 U.S. at 17; MPEP § 2144. Additional secondary considerations have since been
proposed. See, e.g., Andrew Blair-Stanek, Increased Market Power as a New Secondary Consideration in Patent Law, 58 AM. U. L. REV. 707 (2009) (arguing for whether an invention provides an inventor with market power); Abramowicz & Duffy, supra note 16, at 1656 (proposing changing commercial success to “unexpected commercial success,” adding as a consideration of the “cost of the experimentation leading to the invention,” and a few additional considerations).
  
457
Everything Is Obvious 45
things, patent licensing,194 professional approval,195 initial skepticism,196 near- simultaneous invention,197 and copying.198 Today, while decisionmakers are required to consider secondary evidence when available, the importance of these factors varies significantly.199 Graham endorsed the use of secondary considerations, but their precise use and relative importance have never been made clear.200
An existing vein of critical scholarship has advocated for adopting a more economic than cognitive nonobviousness inquiry, for example through greater reliance on secondary considerations.201 This would reduce the need for decisionmakers to try and make sense of complex technologies, and it could reduce hindsight bias.202
Theoretically, in Graham, the Court articulated an inducement standard, which dictates that patents should only be granted to “those inventions which would not be disclosed or devised but for the inducement of a patent.”203 But in practice, the inducement standard has been largely ignored due to concerns over application.204 For instance, few, if any, inventions would never be disclosed or devised given an unlimited time frame. Patent incentives may not increase, so
194. See, e.g., SIBIA Neurosciences, Inc. v. Cadus Pharm. Corp., 225 F.3d 1349, 1358 (Fed. Cir. 2000).
195. See, e.g., Vulcan Eng’g Co. v. Fata Aluminum, Inc., 278 F.3d 1366, 1373 (Fed. Cir. 2002).
196. See, e.g., Metabolite Labs., Inc. v. Lab. Corp. of Am. Holdings, 370 F.3d 1354, 1368 (Fed. Cir.
2004).
197. See, e.g., Ecolochem, Inc. v. S. Cal. Edison Co., 227 F.3d 1361, 1379 (Fed. Cir. 2000).
198. See, e.g., id. at 1377. See also Mark A. Lemley, Should Patent Infringement Require Proof of
Copying?, 105 MICH. L. REV. 1525, 1534–35 (2007).
199. See MPEP § 2144; Durie & Lemley, supra note 19, at 996–97.
200. See, e.g., Dorothy Whelan, A Critique of the Use of Secondary Considerations in Applying the
Section 103 Nonobviousness Test for Patentability, 28 B.C. L. REV. 357 (1987).
201. See, e.g., Merges, supra note 19, at 19 (arguing for patentability to be based on an a priori degree of uncertainty, that “rewards one who successfully invents when the uncertainty facing her prior to the invention makes it more likely than not that the invention won’t succeed” (emphasis omitted)); Chiang, supra note 19, at 42 (arguing for a utilitarian standard, such that “[a]n invention should receive a patent if the accrued benefits before independent invention outweigh the costs after independent invention”); Mandel, The Non- Obvious Problem, supra note 19, at 62 (arguing for nonobviousness to be based on “how probable the invention would have been for a person having ordinary skill in the art working on the problem that the invention solves”); Durie & Lemley, supra note 19, at 1004–07 (arguing for a greater reliance on secondary considerations); Duffy, supra note 19, at 343 (arguing a timing approach to determining obviousness); Devlin & Sukhatme, supra note
19; Abramowicz & Duffy, supra note 16, at 1598 (arguing for an inducement standard).
202. Graham, 383 U.S. at 36 (“[Secondary considerations] may also serve to ‘guard against slipping into use of hindsight.’” (citation omitted)). See also HERBERT F. SCHWARTZ &
ROBERT J. GOLDMAN, PATENT LAW AND PRACTICE 90–91 (6th ed. 2008).
203. Graham, 383 U.S. at 11.
204. See Abramowicz & Duffy, supra note 16, at 1594–95.
  
458
46 66 UCLA L. REV. 2 (2019)
much as accelerate, invention.205 This suggests that an inducement standard would at least need to be modified to include some threshold for the quantum of acceleration needed for patentability. Too high a threshold would fail to provide adequate innovation incentives, but too low a threshold would be similarly problematic. Just as inventions will be eventually disclosed without patents given enough time, patents on all inventions could marginally speed the disclosure of just about everything, but a trivial acceleration would not justify the costs of patents. An inducement standard would thus require a somewhat arbitrary threshold in relation to how much patents should accelerate the disclosure of information, as well as a workable test to measure acceleration.206 To be sure, an economic test based on the inducement standard would have challenges, but it might be an improvement over the current cognitive standard.207
The widespread use of inventive machines may provide the impetus for an economic focus. After inventive machines become the standard way that R&D is conducted in a field, courts could increase reliance on secondary factors. For instance, patentability may depend on how costly it was to develop an invention, andtheexanteprobabilityofsuccess.208 Thereisnoreasonaninventivemachine cannot be thought of, functionally, as an economically motivated rational actor. The test would raise the bar to patentability in fields where the cost of invention decreases over time due to inventive machines.
D. Other Alternatives
Courts may maintain the current skilled person standard and decline to consider the use of machines in obviousness determinations. However, this means that as research is augmented and then automated by machines, the average worker will routinely generate patentable output. The dangers of such a
205. See, e.g., Yoram Barzel, Optimal Timing of Innovations, 50 REV. ECON. & STATS. 348, 348 (1968); John F. Duffy, Rethinking the Prospect Theory of Patents, 71 U. CHI. L. REV. 439, 444 (2004).
206. Abramowicz & Duffy, supra note 16, at 1599 (proposing a “substantial period of time”).
207. See Abramowicz & Duffy, supra note 16, at 1663.
208. Id.
  
459
Everything Is Obvious 47
standard for patentability are well-recognized.209 A low obviousness requirement can “stifle, rather than promote, the progress of the useful arts.”210
Concerns already exist that the current bar to patentability is too low, and that a patent “anticommons” with excessive private property is resulting in “potential economic value . . . disappear[ing] into the ‘black hole’ of resource underutilization.”211 It is expensive for firms interested in making new products to determine whether patents cover a particular innovation, evaluate those patents, contact patent owners, and negotiate licenses.212 In many cases, patent owners may not wish to license their patents, even if they are non-practicing entities that do not manufacture products themselves.213 Firms that want to make a product may thus be unable to find and license all the rights they need to avoid infringing. Adding to this morass, most patents turn out to be invalid or not infringed in litigation.214 Excessive patenting can thus slow innovation, destroy markets, and, in the case of patents on some essential medicines, even cost lives.215 Failing to raise the bar to patentability once the use of inventive machines is widespread would significantly exacerbate this anticommons effect.
Instead of updating the skilled person standard, courts might determine that inventive machines are incapable of inventive activity, much as the U.S. Copyright Office has determined that nonhuman authors cannot generate copyrightable output.216 In this case, otherwise patentable inventions might not
209. See, e.g., ADAM B. JAFFE & JOSH LERNER, INNOVATION AND ITS DISCONTENTS: HOW OUR BROKEN PATENT SYSTEM IS ENDANGERING INNOVATION AND PROGRESS, AND WHAT TO DO ABOUT IT 32–35, 75, 119–23, 145–49 (2004) (criticizing the Patent Office for granting patents on obvious inventions); NATIONAL RESEARCH COUNCIL, A PATENT SYSTEM FOR THE 21ST CENTURY 87–95 (2004) (criticizing lenient nonobviousness standards); Matthew Sag & Kurt Rohde, Patent Reform and Differential Impact, 8 MINN. J.L. SCI. & TECH. 1, 2 (2007) (“Academics, business leaders, and government officials have all expressed concern that too many patents are issued for [obvious] inventions.” ).
210. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 427 (2007).
211. James M. Buchanan & Yong J. Yoon, Symmetric Tragedies: Commons and Anticommons, 43
J.L. & ECON. 1, 2; accord DAN L. BURK & MARK A. LEMLEY, THE PATENT CRISIS AND HOW THE
COURTS CAN SOLVE IT (2009) (arguing for a heightened bar to patentability).
212. See generally Mark A. Lemley, Ignoring Patents, 2008 MICH. ST. L. REV. 19, 25–26 (2008)
(describing various costs associated with innovation in patent heavy industries).
213. See David L. Schwartz & Jay P. Kesan, Analyzing the Role of Non-Practicing Entities in the
Patent System, 99 CORNELL L. REV. 425 (2014).
214. See Mark A. Lemley & Carl Shapiro, Probabilistic Patents, 19 J. ECON. PERSP. 75, 80 (2005).
215. See Michael A. Heller, The Tragedy of the Anticommons: Property in the Transition From
Marx to Markets, 111 HARV. L. REV. 621 (1998); see also MICHAEL HELLER, THE GRIDLOCK ECONOMY: HOW TOO MUCH OWNERSHIP WRECKS MARKETS, STOPS INNOVATION AND COSTS LIVES (2008); see also Michael A. Heller & Rebecca S. Eisenberg, Can Patents Deter Innovation? The Anticommons in Biomedical Research, 280 SCIENCE 698 (1998).
216. This has been a policy of the Copyright Office since at least 1984. See U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 306 (3d ed. 2014). The
  
460
48 66 UCLA L. REV. 2 (2019)
be eligible for patent protection, unless provisions were made for the inventor to be the first person to recognize the machine output as patentable. However, this would not be a desirable outcome. As I have argued elsewhere, providing intellectual property protection for computer-generated inventions would incentivize the development of inventive machines, which would ultimately result in additional invention.217 This is most consistent with the constitutional rationale for patent protection “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”218
E. Incentives Without Patents?
Today, there are strong incentives to develop inventive machines. Inventions by these machines have value independent of intellectual property protection, but they should also be eligible for patent protection. People may apply as inventors for recognizing the inventive nature of a machine’s output,219 or more ambitiously, inventive machines may be recognized as inventors, resulting in stronger and fairer incentives.
Once inventive machines set the baseline for patentability, standard inventive machines, as well as people, should have difficulty obtaining patents. It is widely thought that setting a nonobviousness standard too high would reduce the incentives for innovators to invent and disclose. Yet once inventive machinesarenormal,thereshouldbelessneedforpatentincentives.220 Oncethe
Compendium of U.S. Copyright Office Practices elaborates on the “human authorship” requirement by stating: “The term ‘authorship’ implies that, for a work to be copyrightable, it must owe its origin to a human being.” Id. It further elaborates on the phrase “[w]orks not originated by a human author” by stating: “In order to be entitled to copyright registration, a work must be the product of human authorship. Works produced by mechanical processes or random selection without any contribution by a human author are not registrable.” Id. § 503.03(a).
217. See generally I Think, supra note 1.
218. U.S. CONST. art. I, § 8, cl. 8.
219. Conception requires contemporaneous recognition and appreciation of the invention. See
Invitrogen Corp. v. Clontech Labs., Inc., 429 F.3d 1052, 1064 (Fed. Cir. 2005) (noting that the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue); see also, e.g., Silvestri v. Grant, 496 F.2d 593, 597 (C.C.P.A. 1974) (“[A]n accidental and unappreciated duplication of an invention does not defeat the patent right of one who, though later in time, was the first to recognize that which constitutes the inventive subject matter.”).
220. See generally, Mark A. Lemley, IP in a World Without Scarcity (Stanford Public Law, Working Paper No. 2413974, 2014), http://dx.doi.org/10.2139/ssrn.2413974 (arguing new technologies that reduce costs will weaken the case for IP).
  
461
Everything Is Obvious 49
average worker is inventive, inventions will “occur in the ordinary course.”221 Machine inventions will be self-sustaining. In addition, the heightened bar might result in a technological arms race to create ever more intelligent computers capable of outdoing the standard. That would be a desirable outcome in terms of incentivizing innovation.
Even after the widespread use of inventive machines, patents may still be desirable. For instance, patents may be needed in the biotechnology and pharmaceutical industries to commercialize new technologies. The biopharma industry claims that new drug approvals cost around 2.2 billion dollars and take an average of eight years.222 This cost is largely due to resource intensive clinical trials required to prove safety and efficacy. Once a drug is approved, it is often relatively easy for another company to recreate the approved drug. Patents thus incentivize the necessary levels of investment to commercialize a product given that patent holders can charge monopoly prices for their approved products during the term of a patent.
Yet patents are not the only means of promoting product commercialization. Newly approved drugs and biologics, for example, receive a period of market exclusivity during which time no other party can sell a generic or biosimilar version of the product. Newly approved biologics, for instance, receive a twelve-year exclusivity period in the United States. Because of the length of time it takes to get a new biologic approved, the market exclusivity period may exceed the term of any patent an originator company has on its product. A heightened bar to patentability may lead to greater reliance on alternative forms of intellectual property protection such as market exclusivity, prizes, grants, or tax incentives.223
With regards to disclosure, without the ability to receive patent protection, owners of inventive machines may choose not to disclose their discoveries and rely on trade secret protection. However, with an accelerated rate of technological progress, intellectual property holders would run a significant risk that their inventions would be independently recreated by inventive machines.
Depending on the type of innovation, industry, and competitive landscape, business ventures may be successful without patents, and patent protection is
221. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007).
222. Joseph A. DiMasi, Henry G. Grabowski, & Ronald W. Hansen, Innovation in the
Pharmaceutical Industry: New Estimates of R&D Costs, 47 J. OF HEALTH ECON. 20–33 (2016).
223. See generally Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes Debate, 92 TEX. L. REV. 303 (2013) (describing various nontraditional intellectual property
incentives).
  
462
50 66 UCLA L. REV. 2 (2019)
not sought for all potentially patentable inventions.224 In fact, “few industries consider patents essential.”225 For instance, patents are often considered a critical part of biotechnology corporate strategy, but often ignored in the software industry.226 On the whole, a relatively small percentage of firms patent, evenamongfirmsconductingR&D.227 Mostcompaniesdonotconsiderpatents crucial to business success.228 Other types of intellectual property such as trademark, copyright, and trade secret protection, combined with “alternative” mechanisms such as first mover advantage and design complexity may protect innovation even in the absence of patents.229
F. A Changing Innovation Landscape
Inventive machines may result in further consolidation of wealth and intellectual property in the hands of large corporations like Google and IBM. Large enterprises may be the most likely developers of inventive machines due to their high development costs.230 A counterbalance to additional wealth disparity could be broad societal gains. The public would stand to gain access to a tremendous amount of innovation—innovation which might be significantly delayed or never come about without inventive machines. In fact, concerns about industry consolidation are another basis for revising the obviousness inquiry. The widespread use of inventive machines may be inevitable, but raising the bar to patentability would make it so that inventions which would
224. BRONWYN HALL ET AL., INTELLECTUAL PROPERTY OFFICE, THE USE OF ALTERNATIVES TO PATENTS AND LIMITS TO INCENTIVES, 2 (2012), http://webarchive.nationalarchives.gov.uk/ 20140603121456/http://www.ipo.gov.uk/ipresearch-patalternative.pdf; see also, Rochelle Cooper Dreyfuss, Does IP Need IP? Accommodating Intellectual Production Outside the Intellectual Property Paradigm, 31 CARDOZO L. REV. 1437, 1439 (2010); see also David Fagundes, Talk Derby to Me: Intellectual Property Norms Governing Roller Derby Pseudonyms, 90 TEX. L. REV. 1094, 1146 (2012) (describing norm-based protections that function effectively in the absence of traditional IP). Patent holders are only successful in about a quarter of cases that are litigated to a final disposition and appealed. Paul M. Janicke & LiLan Ren, Who Wins Patent Infringement Cases?, 34 AIPLA Q.J. 1, 8 (2006). Fewer than two percent of patents are ever litigated, and only about 0.1 percent go to trial. Lemley & Shapiro, supra note 214, at 79. In cases where the validity of a patent is challenged, about half of the time the patent is invalidated. Allison & Lemley, supra note 20, at 205 (1998).
225. Merges, supra note 19, at 19.
226. See generally, Lemley & Shapiro, supra note 214.
227. Id.
228. Id.
229. Id.
230. See Jamie Carter, The Most Powerful Supercomputers in the World—and What They Do,
TECHRADAR (Dec. 13, 2014), http://www.techradar.com/us/news/computing/the-most- powerfulsupercomputers-in-the-world-and-what-they-do-1276865 (noting that most advanced computer systems are owned by governments and large businesses).
  
463
Everything Is Obvious 51
naturally occur would be less likely to receive protection. To the extent market abuses such as price gouging and supply shortages are a concern, protections are, at least theoretically, built into patent law to protect consumers against such problems.231 For example, the government could exercise its march in rights or issue compulsory licenses.232
Inventive machines may ultimately automate knowledge work and render human researchers redundant. While past technological advances have resulted in increased rather than decreased employment, the technological advances of the near future may be different.233 There will be fewer limits to what machines will be able to do, and greater access to machines. Automation should generate innovation with net societal gains, but it may also contribute to unemployment, financial disparities, and decreased social mobility.234 It is important that policymakers act to ensure that automation benefits everyone, for instance by investing in retraining and social benefits for workers rendered technologically unemployed.235 Ultimately, patent law alone will not determine whether automation occurs. Even without the ability to receive patent protection, once inventive machines are significantly more efficient than human researchers, they will replace people.
CONCLUSION
Prediction is very difficult, especially about the future.236
In the past, patent law has reacted slowly to technological change. For instance, it was not until 2013 that the Supreme Court decided human genes should be unpatentable.237 By then, the Patent Office had been granting patents on human genes for decades,238 and more than 50,000 gene-related patents had been issued.239
231. See Balancing Access, supra note 27 (discussing patent law protections against practices including “evergreening”).
232. See id. at 345 (explaining India’s issuance of a compulsory license).
233. See Should Robots Pay Taxes?, supra note 6; see supra Part I.
234. Id.
235. Id.
236. ARTHUR K. ELLIS, TEACHING AND LEARNING ELEMENTARY SOCIAL STUDIES 56, (1970) (quoting physicist Niels Bohr).
237. Ass’n for Molecular Pathology v. Myriad Genetics, Inc., 133 S. Ct. 2107 (2013).
238. See, e.g., U.S. Patent No. 4,447,538 (filed Feb. 5, 1982) (a patent issued in 1984 which claims
the human Chorionic Somatomammotropin gene).
239. Robert Cook-Deegan & Christopher Heaney, Patents in Genomics and Human Genetics, 11
ANN. REV. OF GENOMICS & HUM. GENETICS 383, 384 (2010) (“In April 2009, the U.S. Patent
  
464
52 66 UCLA L. REV. 2 (2019)
Eminent technologists now predict that artificial intelligence is going to revolutionize the way innovation occurs in the near to medium term. Much of what we know about intellectual property law, while it might not be wrong, has not been adapted to where we are headed. The principles that guide patent law need to be, if not rethought, then at least retooled in respect of inventive machines. We should be asking what our goals are for these new technologies, what we want our world to look like, and how the law can help make it so.
  and Trademark Office (USPTO) granted the 50,000th U.S. patent that entered the DNA Patent Database at Georgetown University. That database includes patents that make claims mentioning terms specific to nucleic acids (e.g., DNA, RNA, nucleotide, plasmid, etc.).”).

465
Should Robots Pay Taxes? Tax Policy in the Age of Automation
Ryan Abbott* & Bret Bogenschneider**
Existing technologies can already automate most work functions, and the cost of these technologies is decreasing at a time when human labor costs are increasing. This, combined with ongoing advances in computing, artificial intelligence, and robotics, has led experts to predict that automation will lead to significant job losses and worsening income inequality. Policy makers are actively debating how to deal with these problems, with most proposals focusing on investing in education to train workers in new job types, or investing in social benefits to distribute the gains of automation.
The importance of tax policy has been neglected in this debate, which is unfor- tunate because such policies are critically important. The tax system incentivizes automation even in cases where it is not otherwise efficient. This is because the vast majority of tax revenues are now derived from labor income, so firms avoid taxes by eliminating employees. Also, when a machine replaces a person, the government loses a substantial amount of tax revenue—potentially hundreds of billions of dol- lars a year in the aggregate. All of this is the unintended result of a system designed to tax labor rather than capital. Such a system no longer works once the labor is capital. Robots are not good taxpayers.
We argue that existing tax policies must be changed. The system should be at least “neutral” as between robot and human workers, and automation should not be allowed to reduce tax revenue. This could be achieved through some combination of disallowing corporate tax deductions for automated workers, creating an “automa- tion tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate.
INTRODUCTION
An automation revolution is underway.1 Current technologies can al- ready mechanize most work activities, and the cost of these technologies is
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine at the David Geffen School of Medicine at University of California, Los Angeles.
** Senior Lecturer (Associate Professor), Finance Law & Ethics, University of Surrey School of Law. Thanks to Daniel Hemel for his insightful comments.
1 See, e.g., BANK OF AMERICA MERRILL LYNCH, ROBOT REVOLUTION: GLOBAL ROBOT & AI PRIMER 3 (Dec. 16, 2015) (on file with the Harvard Law School Library) (“The pace of disruptive technological innovation has gone from linear to parabolic in recent years. Penetra- tion of robots and artificial intelligence (AI) has hit every industry sector, and has become an integral part of our daily lives. Technology has also expanded beyond routine work, and moved into complex problem-solving, and replicating human perception, tasks that only people were capable of.”); see also Relating to the Training and Utilization of the Manpower Resources of the Nation: Hearing Before the Subcomm. on Emp’t and Manpower of the Comm. on Labor and Pub. Welfare, 88th Cong. 1659 (1963) (statement of Isaac L. Auerbach, President, Interna- tional Federation for Information Processing) (“The word ‘automation’ was coined by Delmar S. Harder, then executive vice president of the Ford Motor Co., in attempting to describe the latest kind of assembly line technique involving engine-block transfer machines then being installed at Ford’s River Rouge and Cleveland plants.”). For a definition of the term “automa- tion,” see Meg Leta Jones, The Ironies of Automation Law: Tying Policy Knots with Fair Auto-
  
146 466 Harvard Law & Policy Review [Vol. 12
decreasing at a time when human labor costs are increasing.2 On top of that, ongoing and exponential improvements in computing, artificial intelligence (AI), and robotics are permitting automation in an ever-increasing number of fields.3 As a result, academic and industry experts are widely predicting that automation will result in substantial “technological unemployment” in the near future.4 For instance, the McKinsey Global Institute has claimed that the disruption caused by AI will “happ[en] ten times faster and at 300 times the scale, or roughly 3,000 times the impact,” of the Industrial Revolution.5 We
mation Practices Principles, 18 VAND. J. ENT. & TECH. L. 77, 84 (2015) (“Broadly, automation includes all the ways computers and machines help people perform tasks more quickly, accurately, and efficiently. The term ‘automation’ refers to: (1) the mechanization and integration of the sensing of environmental variables through artificial sensors, (2) data processing and decision making by computers, and (3) mechanical action by devices that apply forces on the environment or information action through communication to people of informa- tion processed. The term encompasses open-loop operations and closed-loop control, as well as intelligent systems.”) (citations omitted). One of the most cited studies on technological unemployment claims that forty-seven percent of American jobs are at high risk of loss due to automation. See Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible are Jobs to Computerisation?, 114 TECH. FORECASTING & SOC. CHANGE 254, 265–66 (2017), https://ac.els-cdn.com/S0040162516302244/1-s2.0-S0040162516302244-main .pdf?_tid=14d233e0-c236-11e7-a741-00000aacb362&acdnat=1509892499_c57668bde931faf 6de11b39073cccfc5 [https://perma.cc/LFE2-2T7A] (“[O]ur findings suggest recent develop- ments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future.”).
2 See Frey & Osborne, supra note 1, at 265–68; but see JAMES MANYIKA ET AL., MCKIN- SEY GLOBAL INST., A FUTURE THAT WORKS: AUTOMATION, EMPLOYMENT, AND PRODUCTIVITY 21 (2017), https://www.mckinsey.com/~/media/McKinsey/Global%20Themes/Digital%20Dis ruption/Harnessing%20automation%20for%20a%20future%20that%20works/MGI-A-future- that-works_Full-report.ashx [https://perma.cc/2F6U-U259] (predicting that fewer than five percent of occupations could be entirely automated with existing technologies).
3 For examples of automation in white-collar and professional settings, see Roger Parloff, Why Deep Learning is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning/ [https://perma.cc/5A6Q- 5U4T]. Of particular concern to future attorneys is that AI is already automating work func- tions in the legal services industry. See, e.g., Jane Croft, Legal Firms Unleash Office Auto- matons, FIN. TIMES, May 16, 2016 at 4 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Dana Remus & Frank Levy, Can Robots Be Lawyers?: Computers, Lawyers, and the Practice of Law, 30 GEO. J. LEGAL ETHICS 501 (2017) (arguing that AI will refocus rather than replace attorneys).
4 See supra note 1. In the 1930s, the economist John Maynard Keynes popularized the term “technological unemployment” to refer to “unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour.” The Future of Jobs: The Onrushing Wave, ECONOMIST (Jan. 18, 2014), https://www .economist.com/news/briefing/21594264-previous-technological-innovation-has-always-deliv- ered-more-long-run-employment-not-less [https://perma.cc/QQ3N-9AWN].
5 Richard Dobbs et al., The Four Global Forces Breaking All the Trends, MCKINSEY GLOBAL INST. (Apr. 2015), https://www.mckinsey.com/business-functions/strategy-and-corpo- rate-finance/our-insights/the-four-global-forces-breaking-all-the-trends [https://perma.cc/ LC89-B23C] (excerpting RICHARD DOBBS ET AL., NO ORDINARY DISRUPTION: THE FOUR GLOBAL FORCES BREAKING ALL THE TRENDS (2015)); see also JAMES MANYIKA ET AL., MCK- INSEY GLOBAL INST., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013), https://www.mckinsey.com/~/media/McKin sey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Disruptive%20technolo gies/MGI_Disruptive_technologies_Full_report_May2013.ashx [https://perma.cc/EV85- WHVG] (predicting also trillions of dollars in economic impact by 2025 from advanced robot- ics, 3D printing and autonomous vehicles).
 
467
2018] Should Robots Pay Taxes? 147
are entering an era in which the combined impact of technological improve- ments in many different areas is going to be profoundly transformative—and disruptive.6
Automation has the potential to create widespread benefits. Not only will automation increase productivity, it will also improve safety and lead to new scientific breakthroughs.7 But without oversight, automation will also exacerbate unemployment and economic inequality.8 Even if workers ren- dered technologically unemployed are able to transition to new jobs, as has been the case during previous eras of rapid change, there will still be signifi- cant short-term disruptions. Moreover, many experts are predicting that to- day’s technological advances are different in kind from those of the past, and that large-scale permanent increases in unemployment are inevitable.9 In 1990, the three largest companies in Detroit with a combined market capital- ization of $36 billion employed 1.2 million workers.10 In 2014, the three
6 See, e.g., HERRING KAGERMANN, ET AL., INDUSTRIE 4.0 WORKING GRP., RECOMMENDA- TIONS FOR IMPLEMENTING THE STRATEGIC INITIATIVE INDUSTRIE 4.0, at 5 (2013), http://www .acatech.de/fileadmin/user_upload/Baumstruktur_nach_Website/Acatech/root/de/Material_fuer _Sonderseiten/Industrie_4.0/Final_report__Industrie_4.0_accessible.pdf [https://perma.cc/ PA7X-YSRE] (“The first three industrial revolutions came about as a result of mechanisation, electricity and IT. Now, the introduction of the Internet of Things and Services into the manu- facturing environment is ushering in a fourth industrial revolution.”); see also VERNOR VINGE, THE COMING TECHNOLOGICAL SINGULARITY: HOW TO SURVIVE IN THE POST-HUMAN ERA (1993) https://edoras.sdsu.edu/~vinge/misc/singularity.html [https://perma.cc/K4C9-LRDE] (coining the term “singularity” to refer to the argument that “we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence.”).
7 See generally Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. (forthcoming 2018) (discussing the potential of automation to result in substantial safety benefits, for instance in the transportation industry); see also Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 1079 B.C. L. REV. 1083–91 (2016) (discussing examples in which AI has generated patentable subject matter under circumstances in which the computer rather than a person has qualified for inventorship).
8 See COMM. ON TECH., NAT’L SCI. & TECH. COUNCIL, PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE 2 (2016) [hereinafter COMM. ON TECH.], https://obamawhitehouse .archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_fut ure_of_ai.pdf [https://perma.cc/DEH5-AQBK].
9 See Klaus Schwab & Richard Samans, Preface to WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION, at v–vi (2016), http://www3.weforum.org/docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL]; see also Brian Dorini, The End of Work: The Decline of the Global Labor Force and the Dawn of the Post-Market Era, 9 HARV. J.L. & TECH. 231, 232–33 (1995) (reviewing JEREMY RIFKIN, THE END OF WORK: THE DECLINE OF THE GLOBAL LABOR FORCE AND THE DAWN OF THE POST-MARKET ERA 136–43 (1995)) (“The ranks of the unem- ployed are swelling with former service sector workers, such as secretaries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recognition devices, elec- tronic mail, and inventory control and monitoring devices.”) (citation omitted).
10 See Michael Chui & James Manyika, Digital Era Brings Hyperscale Challenges, FIN. TIMES (Aug. 13, 2014), https://www.ft.com/content/f30051b2-1e36-11e4-bb68-00144feabdc0 [http://perma.cc/4QHG-ZKDL].
 
148 468 Harvard Law & Policy Review [Vol. 12
largest companies in Silicon Valley with a combined market capitalization of $1.09 trillion employed 137,000 workers.11
These are not new problems.12 In 1962, President Kennedy stated, “I regard it as the major domestic challenge, really, of the sixties, to maintain full employment at a time when automation, of course, is replacing men.”13 His solution was to pass the nation’s first and most sweeping federal pro- gram to train workers unemployed due to technological advances.14 More recently, in December 2016, the Executive Office of the President issued a report which outlined a three-pronged policy response to automation and AI, namely, to: (i) “[i]nvest in and develop AI for its many benefits,” (ii) “[e]ducate and train Americans for jobs of the future,” and, (iii) “[a]id workers in the transition and empower workers to ensure broadly shared growth.”15 These and other proposals for dealing with automation have fo- cused on improving education and improving social benefit systems. Con- cerns about technological unemployment have even breathed new life into an old social benefit proposal—guaranteed minimum income, which could involve the government making fixed payments to each of its citizens re- gardless of their circumstances.16 While education reform often enjoys bipar- tisan support, enhanced social benefits are a politically challenging goal since liberals and conservatives often disagree on their desirability.17 In any
11 See id.
12 See generally JOHN FORBES DOUGLAS, SOME EVIDENCES OF TECHNOLOGICAL UNEM- PLOYMENT IN ANCIENT ATHENS AND ROME (1932). For instance, the Roman Emperor Vespa- sian once refused to use a labor-saving transportation machine, famously stating, “You must allow my poor hauliers to earn their bread.” See Steve Welch, The Real Political Divide is Education, TECH CRUNCH (Dec. 30, 2016), https://techcrunch.com/2016/12/30/the-real-politi- cal-divide-is-education/ [https://perma.cc/EL6C-JKAQ].
13 John F. Kennedy, The President’s News Conference, AM. PRESIDENCY PROJECT (Feb. 14, 1962), http://www.presidency.ucsb.edu/ws/index.php?pid=9003 [https://perma.cc/2L35- QTT7].
14 See Gladys Roth Kremen, MDTA: The Origins of the Manpower Development and Training Act of 1962, U.S. DEP’T OF LAB. (1974), www.dol.gov/general/aboutdol/history/ mono-mdtatext [https://perma.cc/KFC7-MPCV] (describing the law’s origins). Also of note, in 1961 (a year before the MDTA), the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. See id. For reviews of automation issues in the 1960s, see JAMES L. SUNDQUIST, POLITICS AND POLICY: THE EISENHOWER, KENNEDY, AND JOHNSON YEARS 77 (1968).
15 EXEC. OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY 3 (2016) [hereinafter ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECON- OMY], https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial- Intelligence-Automation-Economy.pdf [https://perma.cc/LK89-E5RG].
16 Guaranteed minimum income was proposed during the Industrial Revolution by Charles Fourier, and then later Joseph Charlier, before being adopted by John Stuart Mill. See Philippe Van Parijs, A Basic Income for All, BOS. REV. (2000), bostonreview.net/forum/ubi-van-parijs [https://perma.cc/6E52-Q63K]. (According to Mill’s proposal, “[A] certain minimum is first assigned for the subsistence of every member of the community, whether capable or not of labour.”).
17 See Yvonne A. Stevens, The Future: Innovation and Jobs, 56 JURIMETRICS J. 367, 373 (2016) (“One of the most commonly considered government payout schemes is what is re- ferred to as a basic income guarantee (BIG). Generally speaking, BIG is a monetary govern- ment-backed and issued guarantee such that all adults have access to an amount of money necessary to meet basic needs.”). President Richard Nixon also once proposed a guaranteed
 
469
2018] Should Robots Pay Taxes? 149
event, both education and social benefit reforms to deal with automation would require significant financial support.18
While there has been a lively public discourse on technological unem- ployment and income disparity, the automation debate has historically ig- nored the issue of taxation. That has very recently started to change. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing con- cerns of stifling innovation.19 The next day, Bill Gates stated that he thought governments should tax companies’ use of robots to slow the spread of auto- mation and to fund other types of employment.20 Former U.S. Secretary of the Treasury Lawrence Summers then claimed Gates’s argument was “pro- foundly misguided.”21 In August 2017, South Korea announced plans for the world’s first “tax on robots” by limiting tax incentives for automated ma- chines.22 Currently, Korean businesses may deduct three to seven percent of an investment in automation equipment from their corporate taxes, depend- ing on the size of their operation.23 The announced reform would decrease the deduction rate by up to two percent.24
basic income of about $10,000 in today’s dollars for families of four. This proposal, the Family Assistance Plan, passed through the House before it was voted down by Senate Democrats. See Whitney Mallett, The Town Where Everyone Got Free Money, VICE: MOTHERBOARD (Feb. 4, 2015), https://motherboard.vice.com/en_us/article/nze99z/the-mincome-experiment-dauphin [https://perma.cc/R7E7-3NTL].
18 For example, in 2016, Switzerland voted down proposed guaranteed minimum income legislation that would have provided each citizen with about $30,000 a year. The cost of the legislation was estimated at about $200 billion, about three times Switzerland’s current annual federal spending. See John Thornhill & Ralph Atkins, Universal Basic Income: Money for Nothing, FIN. TIMES.COM (May 26, 2016), https://www.ft.com/content/7c7ba87e-229f-11e6- 9d4d-c11776a5124d [https://perma.cc/GR78-WG5H]. In the United Kingdom, it was esti- mated that distributing the current total welfare spending of £251 billion to 64.5 million per- sons as a universal basic income would result in a monthly payment to all residents of just £324. See Jim Edwards & Will Heilpern, Here’s How Much We’d All Get if the UK Introduced a ‘Fiscally Neutral’ Universal Basic Income Scheme, BUS. INSIDER (June 6, 2016, 10:25 AM), http://www.businessinsider.com/universal-basic-income-scheme-for-the-uk-2016- 6?r=UK&IR=T [https://perma.cc/4YRW-35G9]. This analysis is overly simplified, but dem- onstrates that providing a meaningful level of social benefits on a widespread basis requires significant funding.
19 See Reuters Staff, European Parliament Calls for Robot Law, Rejects Robot Tax, REUTERS (Feb. 16, 2007, 2:03 PM), http://ca.reuters.com/article/technologyNews/idCAKBN15 V2KM [https://perma.cc/5KTN-6VTJ].
20 See Kevin J. Delaney, The Robot That Takes Your Job Should Pay Taxes, Says Bill Gates, QUARTZ (Feb. 17, 2017), https://qz.com/911968/bill-gates-the-robot-that-takes-your- job-should-pay-taxes/ [https://perma.cc/6SHD-L7WY] (“Exactly how you’d do it, measure it, you know, it’s interesting for people to start talking about now.”).
21 Sarah Kessler, Lawrence Summers Says Bill Gates’ Idea for a Robot Tax is “Profoundly Misguided”, QUARTZ (Mar. 6, 2017), https://qz.com/925412/lawrence-summers-says-bill- gates-idea-for-a-robot-tax-is-profoundly-misguided/ [https://perma.cc/ATV3-DXEG].
22 See Cara McGoogan, South Korea Introduces World’s First ‘Robot Tax’, TELEGRAPH: TECH (Aug. 9, 2017, 12:54 PM), http://www.telegraph.co.uk/technology/2017/08/09/south-ko- rea-introduces-worlds-first-robot-tax/ [https://perma.cc/H93H-RPMC].
23 See Yoon Sung-won, Korea Takes First Step to Introduce ‘Robot Tax’, KOREA TIMES (Aug. 7, 2017, 8:47 PM), http://www.koreatimes.co.kr/www/news/tech/2017/08/133_234312 .html [https://perma.cc/82WW-B4QL].
24 See id.
 
150 470 Harvard Law & Policy Review [Vol. 12
The critical importance of tax policies on automation has not been ap- preciated. The current system encourages automation by providing employ- ers with preferential tax treatment for robot workers. Automation allows firms to avoid employee and employer wage taxes levied by federal, state, and local taxing authorities. It also permits firms to claim accelerated tax depreciation on capital costs for automated workers, and it creates a variety of indirect incentives for machine workers. All of this is the unintended re- sult of a tax system designed to tax labor rather than capital. Tax policies may thus result in automation in some cases in which a firm would other- wise choose a human worker.
Even more concerning, automation significantly reduces the govern- ment’s tax revenue since most tax revenue comes from labor-related taxes.25 When firms replace employees with machines, the government loses income due to taxation. A very rough estimate of revenue loss can be arrived at by multiplying an effective tax rate by the gross salary loss due to automation. In January 2017, the McKinsey Global Institute claimed that about half of current work activities could be automated using currently demonstrated technologies, which would eliminate $2.7 trillion in annual wages in the United States alone.26 Workers pay high effective tax rates ranging from twenty-five percent to fifty-five percent when all tax types are taken into account.27 This suggests that worker automation could result in hundreds of billions or even trillions of dollars in tax revenue lost per year at various levels of government.28
In the United States and most other developed nations, the bulk of taxes are currently remitted by workers either through wage withholding, taxation of labor income, or indirect taxation of workers as consumers.29 Since robots are not subject to these types of tax regimes, automation reduces the overall tax base. Robots are simply not taxpayers, at least not to the same extent as human workers. If all workers were to be replaced by machines tomorrow,
25 See OFFICE OF MGMT. & BUDGET, EXEC. OFFICE OF THE PRESIDENT, FISCAL YEAR 2015 HISTORICAL TABLES: BUDGET OF THE U.S. GOVERNMENT 32–33 tbl.2.1 (2015), https://www .gpo.gov/fdsys/pkg/BUDGET-2015-TAB/pdf/BUDGET-2015-TAB.pdf [https://perma.cc/ TT33-T3HA] (showing that individual income taxes, Social Security taxes, Medicare taxes, and other taxes assessed on labor wages comprised more than fifty percent of overall revenue); I.R.S., PUB. NO. 55B, DATA BOOK, 2014, at 3 tbl.1 (2015), https://www.irs.gov/pub/irs-pdf/ p55b.pdf [https://perma.cc/Q4YU-GUFD]; CONG. BUDGET OFFICE, DISTRIBUTION OF HOUSE- HOLD INCOME AND FEDERAL TAXES, 2010 (2013), https://www.cbo.gov/sites/default/files/ 113th-congress-2013-2014/reports/44604-AverageTaxRates.pdf [https://perma.cc/GH9J- YNQW]; see also Lester B. Snyder & Marianne Gallegos, Redefining the Role of the Federal Income Tax: Taking the Tax Law “Private” Through the Flat Tax and Other Consumption Taxes, 13 AM. J. TAX POL’Y 1, 86 (1996).
26 MANYIKA ET AL., supra note 2, at 6 exhibit E3.
27 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level, 145 TAX NOTES 117, 117 tbl.1 (2014).
28 See MANYIKA ET AL., supra note 2, at 5; see also Frey & Osborne, supra note 1, at 267 (asserting that many creative science, engineering, and general knowledge work jobs will be done by computers in the long run).
29 See REVENUE STATISTICS - OECD COUNTRIES: COMPARATIVE TABLES, ORG. FOR ECON. CO-OPERATION & DEV. (2016), http://stats.oecd.org/Index.aspx?DataSetCode=REV [https:// perma.cc/74EJ-2KS8].
 
471
2018] Should Robots Pay Taxes? 151
most of the tax base would immediately disappear. As a matter of taxation, automated workers represent a type of capital investment, and capital in- come is currently taxed at much lower rates than labor income.30 This is not accidental; it is based on the historic belief that the taxation of labor income is more efficient than the taxation of capital income. This concept is dis- cussed in tax policy analysis as the “tax incidence” of capital taxation.31
Tax is thus critically important to the automation debate. Tax policies should not encourage automation unless it is part of a deliberate strategy based on sound public policy. We believe the solution is to adjust the tax system to be at least neutral as between robot and human workers.32 More ambitiously, changes to tax policies are necessary to account for the loss of government tax revenue due to automation. This is particularly critical be- cause the education and social benefit reform necessitated by automation will only be possible with more, not less, tax revenue.
This article outlines several potential tax policy solutions to address the automation revolution. Tax “neutrality” between human and automated workers could be achieved through some combination of disallowing corpo- rate tax deductions for automated workers, creating an “automation tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate. Neutrality in this setting refers to a system in which various alternatives are taxed equally, and so actors make decisions based on non-tax reasons.
Tax neutrality is widely accepted as an economically efficient principle for organizing a tax system.33 Neutral taxes are more likely to have fewer negative effects, lower administration and compliance costs, promote distri-
30 The term “capital taxation” refers here to corporate income taxation. For a comparison of effective tax rates between U.S. and EU multinationals, see Reuven S. Avi-Yonah & Yaron Lahav, The Effective Tax Rate of the Largest U.S. and EU Multinationals, 65 TAX L. REV. 375 (2012).
31 In a strange twist of economic theory, the ultimate cost of wage taxation paid by work- ers is generally thought to be borne by capital. See Arnold C. Harberger, Tax Policy in a Small, Open Developing Economy, in THE ECONOMICS OF THE CARIBBEAN BASIN 1 (Michael B. Con- nolly & John McDermott eds., 1985). For the extension of the “small open economy” model beyond the small open economy context, see A. Lans Bovenberg, Capital Income Taxation in Growing Open Economies, 31 J. PUB. ECON. 347 (1986); Anne Sibert, Taxing Capital in a Large, Open Economy, 41 J. PUB. ECON. 297 (1990); Alan J. Auerbach, Who Bears the Corpo- rate Tax? A Review of What We Know, 20 TAX POL’Y & ECON. 1 (2006).
32 See WILLIAM MEISEL, THE SOFTWARE SOCIETY: CULTURAL AND ECONOMIC IMPACT 226 (2013) (“There are other alternatives using the tax code. One option suggested by Martin Ford in The Lights in the Tunnel is modification of the payroll tax, a tax that discourages hiring people and encourages automation since it makes the use of people more expensive. He sug- gests a reform of the tax system where we get away from taxing based on workers to reduce the disincentive to hiring.”) (citing MARTIN FORD, THE LIGHTS IN THE TUNNEL: AUTOMATION, ACCELERATING TECHNOLOGY, AND THE ECONOMY (2009)).
33 See Tax: Fundamentals in Advance of Reform: Hearing Before the S. Comm. on Fin., 110th Cong. 41–50 (2008) (prepared statement of Jason Furman, Senior Fellow and Director of The Hamilton Project, The Brookings Institution) [hereinafter Prepared Statement of Jason Furman], https://www.finance.senate.gov/imo/media/doc/56020.pdf [https://perma.cc/Y98J- RN8K].
 
152 472 Harvard Law & Policy Review [Vol. 12
butional fairness, and increase transparency.34 Tax neutrality can thus result in a broader tax base with lower rates.35 Non-neutralities in the tax system distort choices and behavior other than for economic reasons, and encourage socially wasteful efforts to reduce tax payments.36 They can thus “create complexity, encourage avoidance, and add costs for both taxpayers and governments.”37
However, non-neutral taxes can be used deliberately to advance social policy—for instance, incentivizing activities like medical research, educa- tion, and homeownership.38 Taxes may also be used to disincentivize certain activities, as so-called “Pigouvian” taxes. For instance, consumer goods such as alcoholic beverages and tobacco products bear an exceptional tax burden. In turn, this results in increased consumer costs, with the goal of decreasing consumption—but due to taxes rather than to other market and economic factors.
The advantage of tax neutrality as between human and automated work- ers is that it permits the marketplace to adjust without tax distortions. With a level playing field, firms should only automate if it will be more efficient, without taking taxes into account. Since the current tax system favors auto- mated workers, a move toward a neutral tax system could increase the ap- peal of human workers. Policy solutions could even be implemented to make human workers more appealing than machines in terms of tax costs and ben- efits, to the extent policy makers choose to discourage automation.
The remainder of this article is divided into three parts. Part I discusses the phenomenon of automation and provides historical background on ef- forts to deal with its harmful effects. Part II analyzes current tax policies and contends that they promote automation even where it would not otherwise be efficient. Finally, Part III argues that changes to tax policy are needed to prevent the unintended consequences of encouraging automation and to off- set the government’s loss of tax revenue. We provide several potential solu- tions for achieving these goals.
The increased tax revenue from our proposal could be used to provide improved education and training for workers rendered unemployed by robots and computers. Should the pessimistic prediction of a near future with sub- stantially increased unemployment due to automation manifest, these taxes could also support social benefit programs such as a guaranteed minimum income. Automation will likely generate more wealth than has ever been possible. It should not come at the expense of the most vulnerable.
34 See JAMES MIRRLEES ET AL., INST. FOR FISCAL STUDIES, TAX BY DESIGN 22–23 (2011), https://www.ifs.org.uk/docs/taxbydesign.pdf [https://perma.cc/JSU8-KS5Q].
35 See Prepared Statement of Jason Furman, supra note 33, at 33.
36 See MIRRLEES ET AL., supra note 34, at 40.
37 Id. at 41.
38 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents–Prizes De-
bate, 92 TEX. L. REV. 303 (2013).
 
473
2018] Should Robots Pay Taxes? 153
I. THE PROBLEM WITH AUTOMATION
A. Automation is Coming
Experts are widely predicting that automation is going to have a sub- stantial impact on employment even in the near term. Bank of America Mer- rill Lynch argues that by 2025, AI may eliminate $9 trillion in employment costs by automating knowledge work.39 A report by the World Economic Forum estimates that automation could result in the net loss of 5.1 million jobs by 2020.40 The consulting firm Deloitte claims that thirty-five percent of jobs in the United Kingdom are at high risk of redundancy due to automation in the next ten to twenty years.41 This is due to a combination of factors: improvements in automation technologies, decreased costs for such technol- ogies, and increased labor costs. Whereas it was previously possible to auto- mate a large number of work processes, it has now become practicable. As automation technologies continue to both improve and decrease in cost, it is difficult to think of work functions that will not eventually be susceptible to automation.42
1. The Good: Increased Productivity and New Jobs
Automation increases productivity, which generates value and creates wealth.43 Partly due to technological advances and automation, the U.S. Gross Domestic Product (GDP) has steadily risen from $1.37 trillion in 1960 to $73.5 trillion in 2015.44 Despite academic criticism, GDP has remained the dominant economic indicator of welfare and standard of living for half a century.45
39 BANK OF AMERICA MERRILL LYNCH, supra note 1, at 1 (noting also that AI will yield $14–33 trillion in annual economic impact).
40 See WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION 13 (2016), http://www3.weforum.org/ docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL].
41 DELOITTE, AGILETOWN: THE RELENTLESS MARCH OF TECHNOLOGY AND LONDON’S RE- SPONSE 5 (2014), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/uk-futures/ london-futures-agiletown.pdf [https://perma.cc/Z5HU-PY25].
42 See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 188–91 (Cassidy R. Sugimoto et al. eds., 2016) (noting the ways in which automation technologies could replace workers in the pharmaceutical sciences).
43 See generally Joel Mokyr et al., The History of Technological Anxiety and the Future of Economic Growth: Is This Time Different?, 29 J. ECON. PERSP. 31 (2015).
44 See GDP (Current US$), WORLD BANK: DATA (2016), https://data.worldbank.org/indi- cator/NY.GDP.MKTP.CD [https://perma.cc/C34J-U67E].
45 See, e.g., Jeroen C.J.M. van den Bergh, The GDP Paradox, 30 J. ECON. PSYCHOL. 117, 117–18 (2008) (“Gross domestic product (GDP) is the monetary, market value of all final goods and services produced in a country over a period of a year. The real GDP per capita (corrected for inflation) is generally used as the core indicator in judging the position of the economy of a country over time or relative to that of other countries. The GDP is thus implic- itly, and often even explicitly, identified with social welfare—witness the common substituting phrase ‘standard of living’. . . . For over half a century now, the GDP (per capita) has been
 
154 474 Harvard Law & Policy Review [Vol. 12
Automation can also create new jobs.46 Human workers may be needed to build and maintain automation technologies. Automation may free up cap- ital for investments in new enterprises, result in the creation of new prod- ucts, or decrease production costs for existing products. Decreased production costs may result in lower consumer prices and thus greater con- sumer demand. All of this may increase employment. Technological ad- vances have also historically upgraded the labor force: automation has reduced the need for unskilled workers but increased the need for skilled workers.47 For instance, some of today’s most in-demand occupations did not exist even five years ago.48
2. The Bad: Unemployment and Inequality
Automation can cause under- and un-employment. While worker pro- ductivity has risen robustly since 2000, employment has stagnated.49 This may be due in part to technological advances.50 When a company like Mc- Donald’s introduces computer cashiers, the company may save money and consumers may enjoy lower prices.51 But human cashiers now find them- selves in a more competitive labor market. The enhanced competition may result in lower wages, less favorable employment terms, fewer working
severely criticized as not adequately capturing human welfare and progress. All the same, the GDP has maintained a firm position as a dominant economic indicator. . . .”).
46 The following arguments were referred to as “compensation theory” by Karl Marx, who argued none of these effects were guaranteed and that automation could result in forcing workers into lower paying jobs. See KARL MARX, CAPITAL, VOLUME I: THE PROCESS OF PRO- DUCTION OF CAPITAL 570 (1867).
47 See Automation and Technological Change: Hearing Before the Subcomm. on Econ. Stabilization of the J. Comm. on the Econ. Rep., 84th Cong. 29, 34–35 (Statement of Walter S. Buckingham, Jr., Associate Professor, Georgia Institute of Technology), https://www.jec.sen ate.gov/reports/84th%20Congress/Automation%20and%20Technological%20Change%20-% 20Hearings%20%2875%29.pdf [https://perma.cc/B348-CT38].
48 See WORLD ECON. F., supra note 40, at 3.
49 See, e.g., STEVEN GREENHOUSE, THE BIG SQUEEZE: TOUGH TIMES FOR THE AMERICAN WORKER 3 (2008).
50 See id. at 9.
51 Cf. Ted Goodman, Fight for $15? McDonald’s To Place Automated Ordering Stations At All US Locations, DAILY CALLER (Nov. 18, 2016, 6:44 PM), http://dailycaller.com/2016/11/ 18/fight-for-15-mcdonalds-to-place-automated-ordering-stations-at-all-us-locations [https:// perma.cc/VS4R-X35Y]. Standard economic principles suggest that in a competitive market lower business costs will result in lower consumer prices. See, e.g., Arthur A. Thompson, Jr., Strategies for Staying Cost Competitive, HARV. BUS. REV. (Jan. 1984), https://hbr.org/1984/01/ strategies-for-staying-cost-competitive [https://perma.cc/Y3SR-2WQC]. In fairness, fast food automation has been around since the nineteenth century. See Angelika Epple, The “Automat”: A History of Technological Transfer and the Process of Global Standardization in Modern Fast Food around 1900, 7 FOOD & HISTORY 97, 98 (2009), http://wwwhomes.uni-bielefeld.de/aep- ple/Aufsatz12TheAutomat2009.pdf [https://perma.cc/LZ7F-MSXS] (discussing the restaurant chain “Automat” which opened its first location in 1896). (“One of [Automat’s] highly unique selling features around 1900 was that no waiters were to be seen in the guest room. The Automat of that time was—at first sight—operated by vending machines only. ‘You absolutely help yourself’ was one of its most prominent marketing slogans.”) Id. at 99. The Automat’s technology transferred around the U.S. and Europe and eventually developed into the world’s largest restaurant chain: Horn & Hardart. Id. at 97.
 
475
2018] Should Robots Pay Taxes? 155
hours, reduced hiring, or layoffs.52 As the former CEO of McDonald’s USA famously quipped, “[i]t’s cheaper to buy a $35,000 robotic arm than it is to hire an employee who’s inefficient making $15 an hour bagging French fries. . . .”53 McDonald’s is now expanding its use of automated cashiers throughout the United States and in other countries.54
Also, while automation generates wealth, it does so unevenly. Over the past twenty-five years, partly due to automation technologies, the income share of the top 0.1% has increased substantially.55 The top 0.1% of the U.S. population is now worth about as much as the bottom 90%.56 CEO-to-worker pay ratios have increased a thousand-fold since 1950,57 but overall wages have been stagnant for thirty-five years.58 Increased automation is likely to accelerate these trends. The White House Council of Economic Advisers has predicted that future automation will disproportionately affect lower-wage jobs and less educated workers, causing greater economic inequality.59
Worsening employment coupled with growing income inequality is a recipe for social unrest.60 As physicist Stephen Hawking has warned,
52 See Simon Neville, McDonald’s ties nine out of 10 workers to zero-hours contracts, GUARDIAN (Aug. 5, 2013, 4:13 PM), https://www.theguardian.com/business/2013/aug/05/ mcdonalds-workers-zero-hour-contracts [https://perma.cc/UF3D-D4S4] (noting that 90% of McDonald’s UK workers have no guaranteed hours); see also Stephanie Strom, McDonald’s Introduces Screen Ordering and Table Service, N.Y. TIMES (Nov. 17, 2016), https://www.ny- times.com/2016/11/18/business/mcdonalds-introduces-screen-ordering-and-table-service .html?_r=0 [https://perma.cc/3DZ7-R68J] (reporting that the cost of purchasing and installing eight touch order screens is $56,000).
53 Julia Limitone, Fmr. McDonald’s USA CEO: $35K Robots Cheaper Than Hiring at $15 Per Hour, FOX BUS. (May 24, 2016), http://www.foxbusiness.com/features/2016/05/24/fmr- mcdonalds-usa-ceo-35k-robots-cheaper-than-hiring-at-15-per-hour.html [https://perma.cc/ W65G-697K] (claiming that a $15 minimum wage results in $30,000 a year for a full-time employee).
54 See Ed Rensi, The Ugly Truth About a $15 Minimum Wage, FORBES (Apr. 25, 2016, 6:30 AM), https://www.forbes.com/sites/realspin/2016/04/25/mcdonalds-minimum-wage-real- ity/#1f50a0d93edd [https://perma.cc/TT3E-G5SR]. Automated cashiers are already the “norm” in European countries with high labor costs, and McDonald’s is now experimenting with self-serve McCafe kiosks. See id.
55 See CARL BENEDIKT FREY & MICHAEL OSBORNE, TECHNOLOGY AT WORK: THE FUTURE OF INNOVATION AND EMPLOYMENT 14 (2015) http://www.oxfordmartin.ox.ac.uk/downloads/ reports/Citi_GPS_Technology_Work.pdf [https://perma.cc/YE22-D6AE].
56 See Angela Monaghan, US Wealth Inequality - Top 0.1% Worth as Much as the Bottom 90%, GUARDIAN (Nov. 13, 2014, 7:00 AM), https://www.theguardian.com/business/2014/nov/ 13/us-wealth-inequality-top-01-worth-as-much-as-the-bottom-90 [https://perma.cc/62U8- 6ADA].
57 Elliot Blair Smith & Phil Kuntz, CEO Pay 1,795-to-1 Multiple of Wages Skirts U.S. Law, BLOOMBERG MARKETS (Apr. 30, 2013, 12:01 AM), https://www.bloomberg.com/news/ articles/2013-04-30/ceo-pay-1-795-to-1-multiple-of-workers-skirts-law-as-sec-delays [https:// perma.cc/NNF6-P2X6].
58 See ELISE GOULD, ECONOMIC POLICY INSTITUTE, 2014 CONTINUES A 35-YEAR TREND OF BROAD-BASED WAGE STAGNATION (2015), http://www.epi.org/files/pdf/stagnant-wages-in- 2014.pdf [https://perma.cc/YN3U-9LMJ].
59 COMM. ON TECH., supra note 8, at 2.
60 See Katie Allen, ILO Warns of Rise in Social Unrest and Migration as Inequality Widens, GUARDIAN (Jan. 12, 2017, 4:00 PM), https://www.theguardian.com/business/2017/jan/ 12/ilo-warns-of-rise-in-social-unrest-and-migration-as-inequality-widens [https://perma.cc/ 3DHG-T2WH].
 
156 476 Harvard Law & Policy Review [Vol. 12
“[e]veryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine- owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increas- ing inequality.”61
3. The Ugly: Reduced Tax Remittances
One of automation’s most pronounced and unappreciated effects relates to taxes. Automation substantially reduces tax revenue. Most of the U.S. government’s tax revenue comes from taxes on workers.62 By stating that most tax revenue comes from workers, we refer to the aggregate amount of wage tax, income tax, and indirect taxes levied on income or wages derived from work at all levels of government. Much of the prior tax policy debate focused solely on income taxation by the federal government.63 Of course, a substantial portion of income subject to federal income tax arises from work and falls within our definition of worker taxation. However, the tax policy debate has been misleading since wage taxes are also levied on labor income and comprise more than one-third of federal remittances. Likewise, indirect state taxes are levied on workers. Consequently, by replacing employees with machines, the government loses out on employee and employer wage taxes levied by federal, state, and local taxing authorities. In addition, tax revenue may be further reduced from businesses claiming accelerated tax depreciation on capital outlays for machines and from other tax incentives related to indirect taxation, such as sales tax or value-added tax (VAT) exemptions.64
B. History of the Automation Scare
Fears of the consequences of automation have been expressed since the industrial revolution.65 In 1801, the writer Thomas Mortimer objected to ma- chines, “which are intended almost totally to exclude the labor of the human
61 Akshat Rathi, Stephen Hawking: Robots aren’t just taking our jobs, they’re making soci- ety more unequal, QUARTZ (Oct. 9, 2015), http://qz.com/520907/stephen-hawking-robots- arent-just-taking-our-jobs-theyre-making-society-more-unequal [https://perma.cc/A2BN- VYY5].
62 See OFFICE OF MGMT. & BUDGET, supra note 25, at 32–33 tbl.2.1.
63 See, e.g., CURTIS S. Dubay, THE HERITAGE FOUNDATION, THE RICH PAY MORE TAXES: TOP 20 PERCENT PAY RECORD SHARE OF INCOME TAXES (2009), http://www.heritage.org/pov- erty-and-inequality/report/the-rich-pay-more-taxes-top-20-percent-pay-record-share-income- taxes [https://perma.cc/N97H-VETS].
64 See infra Part III.
65 For that matter, broader social issues related to automation have been discussed since Aristotle’s time. See, e.g., JOHANNES HANEL, ASSESSING INDUCED TECHNOLOGY: SOMBART’S UNDERSTANDING OF TECHNICAL CHANGE IN THE HISTORY OF ECONOMICS 91 (2008) (noting Aristotle’s hope that machines could occupy the place of slaves in a utopian society).
 
477
2018] Should Robots Pay Taxes? 157
race.”66 In 1821, the economist David Ricardo argued that automation would result in inequality, and that “substitution of machinery for human labour, is often very injurious to the interests of the class of labourers. . . . [It] may render the population redundant, and deteriorate the condition of the la- bourer.”67 In 1839, the philosopher Thomas Carlyle more poetically wrote:
[T]he huge demon of Mechanism smokes and thunders, panting at his great task, in all sections of English land; changing his shape like a very Proteus; and infallibly, at every change of shape, over- setting whole multitudes of workmen, as if with the waving of his shadow from afar, hurling them asunder, this way and that, in their crowded march and course of work or traffic; so that the wisest no longer knows his whereabout[s].68
The Industrial Revolution even gave birth to a social movement and group protesting the use of new technologies: the Luddites.69 Luddites were primarily English textile workers who objected to working conditions in the nineteenth century. They believed that automation threatened their liveli- hoods, and they were opposed to the introduction of industrial machinery.70 Some Luddites engaged in violent episodes of machine-breaking, in re- sponse to which the English government made machine-breaking a capital offense.71
The Luddite movement died out, but automation concerns persisted throughout the twentieth century, often flaring during times of rapid techno- logical progress.72 For instance, the debate was revitalized in the 1950s and 1960s with the widespread introduction of office computers and factory ro-
66 THOMAS MORTIMER, LECTURES ON THE ELEMENTS OF COMMERCE, POLITICS, AND FI- NANCES 72 (London, A. Strahan, for T. N. Longman and O. Rees 1801).
67 DAVID RICARDO, ON THE PRINCIPLES OF POLITICAL ECONOMY AND TAXATION 283–84 (Batoche Books 2001) (3d ed. 1821).
68 2 THOMAS CARLYLE, THE WORKS OF THOMAS CARLYLE: CRITICAL AND MISCELLANE- OUS ESSAYS 141–42 (Henry Duff Traill ed., Cambridge Univ. Press 2010) (1899). Thomas Carlyle called the Industrial Revolution “the Mechanical Age.” Id at 59. Carlyle wrote that technology was causing a “mighty change” in their “modes of thought and feeling. Men are grown mechanical in head and in heart, as well as in hand.” Id. at 63.
69 See Richard Conniff, What the Luddites Really Fought Against, SMITHSONIAN MAG. (Mar. 2011), https://www.smithsonianmag.com/history/what-the-luddites-really-fought- against-264412/ [https://perma.cc/98RV-LNJ2].
70 See Ian Coulson, Power, Politics & Protest: The Growth of Political Rights in Britain in the 19th Century: Luddites, NAT’L ARCHIVES (U.K.), https://www.nationalarchives.gov.uk/edu- cation/politics/g3/ [https://perma.cc/96H4-4NAR].
71 See id.; see also Conniff, supra note 69. The “Luddite fallacy” now describes the fear that innovation will have long-term harmful labor effects. See Vivek Wadhwa, Sorry, but the jobless future isn’t a luddite fallacy, WASH. POST (July 7, 2015), https://www.washingtonpost .com/news/innovations/wp/2015/07/07/sorry-but-the-jobless-future-isnt-a-luddite-fallacy/?utm _term=.f52e3687022c [https://perma.cc/5JUA-YPUE].
72 In 1924, Mohandas Karamchand Gandhi wrote, “What I object to, is the craze for ma- chinery, not machinery as such. The craze is for what they call labour-saving machinery. Men go on ‘saving labour’, till thousands are without work and thrown on the open streets to die of starvation.” MOHANDAS K. GANDHI, YOUNG INDIA (1924), reprinted in ALL MEN ARE BROTH- ERS: LIFE AND THOUGHTS OF MAHATMA GANDHI AS TOLD IN HIS OWN WORDS 126 (Krishna Krapilani ed., 1958).
 
158 478 Harvard Law & Policy Review [Vol. 12
bots.73 In his 1960 election campaign, John F. Kennedy suggested that auto- mation offered “hope of a new prosperity for labor and a new abundance for America,” but that it also “carries the dark menace of industrial dislocation, increasing unemployment, and deepening poverty.”74
Despite these concerns, technological advances have historically re- sulted in overall job creation. The computer eliminated jobs, but created jobs for working with information created by computers. The automobile elimi- nated jobs, but created jobs in the motel and fast-food industries. The tractor and other agricultural advances eliminated jobs, but drove job growth in other areas of the economy. In 1900, forty-one percent of the workforce was employed in agriculture.75 In 2000, less than two percent of the employed labor force worked in agriculture.76 Yet this has not translated to a thirty-nine percent increase in unemployment. Even as agriculture-based employment and agriculture’s relative contribution to the GDP decreased, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the GDP increased.77 Indeed, in each era when concerns have been expressed about automation causing mass unemployment, technology has created more jobs than it has destroyed.
C. Is This Time Different?
The automation debate is resurfacing with a vengeance due to recent advances in AI and other automation technologies. Once more, prognosti- cators are divided into two camps: the optimists who claim there will be a net creation of jobs, and the pessimists who predict mass unemployment and growing inequality.78
History favors the optimists.79 They argue that technological advances will generate widespread benefits together with overall job creation. They
73 See Kremen, supra note 14 (“The dawn of the Atomic Age had witnessed the imple- mentation of a new technology that threatened to replace men with machines.”); see also Douglas A. Irwin, Comments, in JAGDISH BHAGWATI & ALAN S. BLINDER, OFFSHORING OF AMERICAN JOBS: WHAT RESPONSE FROM U.S. ECONOMIC POLICY? 79 (Benjamin M. Friedman ed., 2009).
74 Irwin, supra note 73, at 80.
75 See CAROLYN DIMITRI ET AL., U.S. DEP’T OF AGRIC., THE 20TH CENTURY TRANSFORMA- TION OF U.S. AGRICULTURE AND FARM POLICY 2 (June 2005), https://www.ers.usda.gov/ webdocs/publications/44197/13566_eib3_1_.pdf?v=41055 [https://perma.cc/FRJ7-V3QA].
76 See id.
77 Id.; see also JULIAN M. ALSTON ET AL., PERSISTENCE PAYS: U.S. AGRICULTURAL PRO- DUCTIVITY GROWTH AND THE BENEFITS FROM PUBLIC R&D SPENDING 43, 105 (2010).
78 See Schwab & Samans, supra note 9, at v–vi; see also Dorini, supra note 9, at 233 (“The ranks of the unemployed are swelling with former service sector workers, such as secre- taries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recog- nition devices, electronic mail, and inventory control and monitoring devices.”) (citation omitted).
79 See John Maynard Keynes, Economic Possibilities for our Grandchildren, in ESSAYS IN PERSUASION 321–32 (Palgrave Macmillan 2010) (1930) (predicting that the combination of technological innovation and capital accumulation will eventually solve the problem of mate- rial needs).
 
479
2018] Should Robots Pay Taxes? 159
also argue that current unemployment may relate more to globalization and offshoring than to technology, and that any future technological unemploy- ment would be “only a temporary phase of maladjustment.”80
But there is reason to think that this time may be different.81 Computers are improving exponentially, and there are fewer limits to what they can do than ever before. Computers can replace low-skilled workers and manual laborers as well as white-collar workers and professionals in a variety of fields. Computers are already working as doctors, lawyers, artists, and in- ventors.82 All of this is occurring at a time when labor costs are rising and computer costs are declining. In 2012, Vinod Khosla, the co-founder of Sun Microsystems, predicted that diagnostic software would take the jobs of eighty percent of physicians in the next twenty years.83
While the optimists and pessimists disagree about automation’s effects on long-term unemployment, both agree it causes short-term job losses and industry-specific disruption. During past episodes of widespread automation and technological change, it took decades to develop new worker skill sets on a significant scale and to build new job markets.84 Although the Industrial Revolution ultimately resulted in net job creation, it also resulted in periods of mass unemployment and human suffering. In the coming “Automation Revolution,” whether there are detrimental long-term effects, there will al- most certainly be significant short-term disruptions.85
80 Id. at 325; see also 1 JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY 97 (Cosimo Classics 2006) (1848).
81 See, e.g., Stevens, supra note 17, at 368–69 (“This time there may be some distinctions requiring widespread and perhaps novel solutions, unlike other periods in history.”).
82 See Parloff, supra note 3; see also Yonghui Wu et al., Google’s Neural Machine Trans- lation System: Bridging the Gap between Human and Machine Translation, CORNELL U. LIBR.: ARXIV 20 (Oct. 8, 2016), arxiv.org/abs/1609.08144 [https://perma.cc/KGU8-9RRB] (claiming that the Google Neural Machine Translation system is approaching human-level accuracy); see also Croft, supra note 3 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Remus & Levy, supra note 3 (arguing that AI will refocus rather than replace attorneys).
83 See Liat Clark, Vinod Khosla: Machines Will Replace 80 Percent of Doctors, WIRED UK (Sept. 4, 2012), http://www.wired.co.uk/article/doctors-replaced-with-machines [https:// perma.cc/QNL8-WP4M].
84 See Schwab & Samans, supra note 9, at 20.
85 For example, a substantial number of transportation workers are likely to be displaced by self-driving vehicles, and about three percent of the population is employed in the transpor- tation industry. See Richard Henderson, Industry Employment and Output Projections to 2024, BUREAU OF LAB. STAT.: MONTHLY LAB. REV. tbl. 1 (Dec. 2015), https://www.bls.gov/opub/ mlr/2015/article/industry-employment-and-output-projections-to-2024.htm [https://perma.cc/ 54FB-LDMM]. Tesla, for example, plans to make all its vehicles self-driving. See Tesla to Make All Its New Cars Self-Driving, BBC NEWS: TECH. (Oct. 20, 2016), http://www.bbc.co.uk/ news/technology-37711489 [https://perma.cc/DS4X-YYM2]. Tesla is only one of many com- panies developing such technologies. See 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehi- cles-corporations-list/ [https://perma.cc/4YNE-KDTZ]; see also Investment Into Auto Tech On Pace To Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/ blog/auto-tech-funding-h1-2016/ [https://perma.cc/HY5A-XGFH]. Elon Musk, the CEO of Tesla, has even claimed that self-driving cars will be so much safer than human drivers that there will need to be a future ban on human driving. See Stuart Dredge, Elon Musk: Self- driving Cars Could Lead to Ban on Human Drivers, GUARDIAN (Mar. 18, 2015, 3:22 AM),
 
160 480 Harvard Law & Policy Review [Vol. 12 D. Automation Social Policy
It is important that policy makers act to ensure that automation benefits everyone. Our policy goal should be to accommodate and even encourage advances that promote economic value, while redistributing benefits to those negatively affected. In the midst of the Industrial Revolution, the philoso- pher John Stuart Mill wrote that while automation would ultimately benefit laborers:
this does not discharge governments from the obligation of allevi- ating, and if possible preventing, the evils of which this source of ultimate benefit is or may be productive to an existing genera- tion. . . . [T]here cannot be a more legitimate object of the legisla- tor’s care than the interests of those who are thus sacrificed to the gains of their fellow-citizens and of posterity.86
Or, as the U.S. National Science and Technology Council Committee on Technology argued in 2016:
Public policy can address these risks, ensuring that workers are retrained and able to succeed in occupations that are complemen- tary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly, and assure that AI responsibly ushers in a new age in the global economy.87
Efforts to alleviate the harms and share the benefits of automation have focused on education and social benefits. As mentioned earlier, in December 2016, the Executive Office of the President, then under Barack Obama, is- sued a report which outlined policy responses to AI and automation, namely: to invest in AI, educate and train Americans for future jobs, and transition workers to ensure widespread benefits.88 In terms of education, it is thought that technologically unemployed workers need retraining to transition to new job types. Historically, numerous government and industry programs have combated technological unemployment with education.89 The nation’s first and most sweeping federal training program, the Manpower Development and Training Act of 1962, was signed into law by President Kennedy to train workers unemployed due to technological advances and automation.90 More
https://www.theguardian.com/technology/2015/mar/18/elon-musk-self-driving-cars-ban- human-drivers [https://perma.cc/5CPB-PVHS].
86 MILL, supra note 80, at 98.
87 COMM. ON TECH., supra note 8, at 2.
88 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3. 89 A particularly interesting example is the Armour Meat Packing Company, which cre-
ated “a special ‘automation fund’ for retraining purposes. The company paid a 14-cent levy into the fund, established in 1959, for every 100 tons of meat shipped, up to $500,000, to pay for retraining operations.” Kremen, supra note 14.
90 Id. Also of note, a year earlier, the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. Id. For extensive reviews of automation issues in the 1960s, see generally OFFICE OF MAN-
 
481
2018] Should Robots Pay Taxes? 161
recently, President Obama provided billions of dollars to fund worker train- ing in part to address technological unemployment.91 More ambitiously, he proposed a plan to make two years of community college free for “responsi- ble students” in his 2015 State of the Union Address, although this proposal was never adopted.92
As the third prong of President Obama’s 2016 strategy report notes, social benefit investments are also critical.93 The report advocates strength- ening the social safety net through greater investments in programs such as unemployment insurance and Medicaid.94 It also proposes the creation of new programs for wage insurance and emergency aid.95 In addition, it argues for building a twenty-first century retirement system, expanding health care access, and increasing worker bargaining power.96 President Trump’s admin- istration does not appear to have announced a policy response to AI and automation.97
POWER, AUTOMATION, & TRAINING, U.S. DEP’T OF LABOR, UNEMPLOYMENT AND RETRAINING: AN ANNOTATED BIBLIOGRAPHY OF RESEARCH (1965), https://babel.hathitrust.org/cgi/pt?id= umn.31951p010922940;view=1up;seq=1 [https://perma.cc/BZK8-3UYR]; see also SUND- QUIST, supra note 14, at 77.
91 See Press Release, The White House Office of the Press Sec’y, Fact Sheet: President Obama Proposes New ‘First Job’ Funding to Connect Young Americans with Jobs and Skills Training to Start Their Careers (Feb. 4, 2016), www.whitehouse.gov/the-press-office/2016/02/ 04/fact-sheet-president-obama-proposes-new-first-job-funding-connect-young [https://perma .cc/CV2T-SGB3].
92 John Morgan, Barack Obama Free Community College Plan Backed by $100M Fund- ing, TIMES HIGHER EDUC. (Apr. 27, 2016), www.timeshighereducation.com/news/barack- obama-free-community-college-plan-backed-by-one-hundred-million-dollar-funding [https:// perma.cc/5NTN-P3ZP].
93 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3–4. 94 See id.
95 See id. at 4.
96 See id.
97 Treasury Secretary Steve Mnuchin stated in March 2017 when asked about technologi- cal unemployment that, “In terms of artificial intelligence taking over American jobs, I think we’re . . . so far away from that that [it’s] not even on my radar screen . . . . I think it’s 50 or 100 more years.” Interview on Health Care and Tax Reform with Steven Mnuchin, Treasury Secretary, at 33:30–47, C-SPAN (Mar. 24, 2017), https://www.c-span.org/video/?425894-1/ treasury-secretary-steven-mnuchin-talks-axios-founder-mike-allen.&start=1992 [https://per ma.cc/6KYR-A82G]. By contrast, Larry Summers, the Obama administration’s first director of the National Economic Council, predicted that AI could result in about “a third of men be- tween the ages of 25 and 54 not working by the end of this half century.” Christopher Mat- thews, Summers: Automation is The Middle Class’ Worst Enemy, AXIOS, https://www.axios .com/summers-automation-is-the-middle-class-worst-enemy-1513302420-754facf2-aaca-478 8-9a41-38f87fb0dd99.html (last visited Jan. 7, 2018) [https://perma.cc/2UEA-PVU4]. Of note, China appears be adopting the findings of the White House strategy. On July 20, 2017, China’s State Council released its Next Generation Artificial Development Plan which adopts many of the policies proposed in the White House strategy. CHINA STATE COUNCIL, STATE COUNCIL NOTICE ON THE ISSUANCE OF THE NEXT GENERATION ARTIFICIAL INTELLIGENCE DE- VELOPMENT PLAN (Rogier Creemers et al. trans., 2017), https://www.newamerica.org/cyber- security-initiative/blog/chinas-plan-lead-ai-purpose-prospects-and-problems/ [https://perma .cc/5F3L-YE9K]. The plan argues that AI will be foundational to future economic growth and military dominance, and calls for China to surpass other nations in AI technology by 2030. See generally id.
 
162 482 Harvard Law & Policy Review [Vol. 12
Revitalized concerns about technological unemployment have breathed new life into an old social benefit proposal—guaranteed minimum income.98 The basic idea is that the government would provide a fixed amount of money to its citizens regardless of their situation. This has been implemented numerous times on a relatively small scale, most recently in Finland.99 In 2017, Finland began a pilot program to give about $600 per month to 2,000 unemployed citizens, with no other requirements.100 Proponents argue this will reduce unemployment, poverty, and disincentives for the unemployed to work (as under conventional unemployment schemes recipients generally lose their unemployment benefits after returning to work).101 It might also encourage education by providing support for a period of training. Critics have argued that a guaranteed minimum income will encourage recipients to remain unemployed and discourage additional education.102 In any case, Fin- land plans to eventually replace earnings-based insurance benefits with a basic income.103 Y Combinator, the Silicon Valley start-up incubator, has plans to launch a similar private program in Oakland, California.104
Improving education and social benefit systems will not be easy. Liber- als and conservatives alike can agree on the desirability of improving worker training as it will enlarge the productive labor force, but “[d]elivering this education and training will require significant investments.”105 Enhancing the social benefit system will also require significant investment, but such a goal is even more challenging because liberals and conservatives generally disagree that enhanced benefits are a desirable aim.106
That automation creates a need for greater government investment is well known, but what has so far been largely ignored in the automation debate is that automation will make it far more difficult for the government to make investments once tax revenues are reduced.
98 See supra note 16 and accompanying text.
99 See Mallett, supra note 17.
100 See Kevin Lui, Finland is Giving Nearly $600 a Month to 2,000 Jobless Citizens, No
Questions Asked, FORTUNE (Jan. 3, 2017, 1:26 AM), amp.timeinc.net/fortune/2017/01/03/fin- land-universal-basic-income-experiment/?source=dam [https://perma.cc/BFY3-JX2L]. It is also worth noting that the U.S. has operated a guaranteed basic income since 1999. The Alaska Permanent Fund pays each person who has lived the past year in Alaska $1,680. See Van Parijs, supra note 16.
101 See supra note 16 and accompanying text.
102 See id.
103 See Lui, supra note 100.
104 See Chris Weller, The Inside Story of One Man’s Mission to Give Americans Uncondi-
tional Free Money, BUS. INSIDER UK (June 27, 2016, 1:07 PM), uk.businessinsider.com/in- side-y-combinators-basic-income-project-2016-6?r=US&IR=T [https://perma.cc/48QT- H66H].
105 COMM. ON TECH., supra note 8, at 3.
106 See supra note 17 and accompanying text.
 
483
2018] Should Robots Pay Taxes? 163
II. CURRENT TAX POLICIES FAVOR AUTOMATION AND REDUCE TAX REVENUE
A. Introduction
Worker automation is often thought of as a matter of efficiency, where efficiency refers to the ratio of useful output to total input.107 For example, if a machine and a person create the same output, but the machine is less ex- pensive, then automation generates cost savings and improves efficiency.108 If a robot costs a firm $40,000 a year and a human worker costs $45,000 a year, with both workers producing the same output, the firm would yield a $5,000 annual cost savings by automating.
However, it may also be the case that the robot costs more than a human worker before taxes, and only becomes cheaper on a post-tax basis. For instance, the capital outlay for the robot, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as robots, together with the costs for operating the robot (electricity, etc.), might be estimated at $50,000 over some period, whereas the wages and other costs associated with an employee (healthcare, retirement funding, etc.) might be $45,000 over the same period. The robot may be associated with tax benefits that do not apply to human workers and which reduce its cost to $40,000. A firm using a rational cost-based decision model would choose to automate and realize the machine’s tax benefit. In this example, tax policy has ren- dered the robot a more efficient worker. In simple terms, the heavy relative taxation of the living worker drives the firm toward automation to generate tax savings.
The tax system is not neutral as between work performed by robots versus people.109 Automation provides several major tax advantages. Firms that automate avoid employee and employer wage taxes levied by federal, state, and local taxing authorities and claim accelerated tax depreciation on capital costs for automated workers. The tax system also provides indirect incentives for automated workers. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital.110 In
107 Expressed mathematically, efficiency “r” is equal to the amount of useful output (“P”) divided by the amount (“C”) of resources consumed: r=P/C.
108 See Stevens, supra note 17, at 373 (“Technology is very attractive to owners of capital. Machines require no pay, benefits, sick leave, vacation, lunch breaks, or weekends off. They are less prone to err and are more productive than human beings. In a race for the same job, it is therefore difficult for humans to compete with machines.”).
109 The analysis of the “neutrality” of taxation is a common practice in the field of taxa- tion. See generally Peggy Richman (Musgrave), Taxation of Foreign-Source Business Income and the Incentive to Foreign Investment, in PEGGY RICHMAN, TAXATION OF FOREIGN INVEST- MENT INCOME: AN ECONOMIC ANALYSIS (1963), reprinted in PEGGY R. MUSGRAVE, TAX POL- ICY IN THE GLOBAL ECONOMY: SELECTED ESSAYS 3–57 (E. Elgar Publishing 2002) (introducing the term “capital export neutrality”).
110 See MEISEL, supra note 32, at 220 (“An automation tax described as a payroll tax on computers conveys the basic concept. It helps level the playing field. The automation tax serves two purposes: (1) it provides an incentive for a company to create jobs by means such
 
164 484 Harvard Law & Policy Review [Vol. 12 fact, as described below, automated workers are taxed less than human
workers at both the employer and employee level.
B. Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to indi- viduals to fund social benefit programs including Social Security, Medicare, and Medicaid. Presently, in the United States, the employer and employee pay matching amounts totaling 12.4% of an employee’s salary, plus match- ing Medicare payments totaling 2.9% (applied on the first $127,200 of earn- ings), plus an additional 0.9% Medicare surcharge (applied on earnings over $200,000).111 Many states and localities also levy wage taxes that apply in addition to the federal levies.112
C. Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers
“Tax depreciation” refers here to the deduction (a reduction in the tax base) claimed by the firm in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow the firm to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction works differently for robot and human workers.
The timing of claiming a deduction may have a significant effect on a firm’s tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time).113 For example, assume a robot has a total
as investing in human-computer synergy; and (2) it proves governmental revenues that, prop- erly used, can create more consumption and thus boost the economy.”).
111 See I.R.C. § 3101(a) (2012 & Supp. II 2014), § 3111(a) (Supp. III 2016), § 3102(a) (2012), § 3121(a)(1) (2012 & Supp. II 2014); I.R.S., SOCIAL SECURITY AND MEDICARE WITH- HOLDING RATES (2017), www.irs.gov/taxtopics/tc751.html [https://perma.cc/3F5R-UEES]; see also Richard Winchester, The Gap in the Employment Tax Gap, 20 STAN. L. & POL’Y REV. 127, 132 (2009) (“The tax imposed by FICA has two components. The first is the old-age, survivors, and disability insurance component, often referred to as OASDI. It is [levied on] . . . ‘wages’ from employment. One half of the tax is deducted from the employee’s compensa- tion. The employer pays the other half. This component of the FICA tax is earmarked to cover social security benefits. There is a limit on the amount of wages that can be taxed. . . . The contribution and benefit base is adjusted each year to reflect increases in average wages of the U.S. economy.”) (citations omitted).
112 For an explanation of the U.S. states that levy sales taxes, see generally SCOTT DRENKARD & NICOLE KAEDING, TAX FOUND., STATE AND LOCAL SALES TAX RATES IN 2016 (2016), https://files.taxfoundation.org/legacy/docs/TaxFoundation_FF504.pdf [https://perma .cc/VLE2-KCHV]. For an explanation of EU tax policy including the VAT, see generally CE ́- CILE REMEUR, EUR. PARLIAMENTARY RESEARCH SERV., TAX POLICY IN THE EU: ISSUES AND CHALLENGES (2015), http://www.europarl.europa.eu/RegData/etudes/IDAN/2015/549001/ EPRS_IDA(2015)549001_EN.pdf. [https://perma.cc/FUE9-ZV58].
113 See Yoram Margalioth, Not a Panacea for Economic Growth: The Case of Accelerated Depreciation, 26 VA. TAX REV. 493, 494–95, 499 (2007) (“Accelerated depreciation policy can be traced back to an influential 1953 paper by Evsey Domar. . . . [Elaborating on the]
 
485
2018] Should Robots Pay Taxes? 165
capital cost of $100,000 and seven years of useful life, while an employee has a total wage cost of $100,000 over seven years. If accelerated deprecia- tion for capital is available,114 the firm may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year one rather than pro- rata over seven years.115 For instance, the firm might claim tax depreciation for an automated worker of $50,000 in year one, $30,000 in year two, $10,000 in year three, and in diminishing amounts to year seven. By con- trast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a present value benefit will accrue from claiming accelerated tax de- ductions for automated workers relative to the pro-rata tax deductions for employee wages, even where the $100,000 capital outlay is paid up-front.116 This is possible because the present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return the firm could make by investing the free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the eco- nomic depreciation.117 “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to buy the same robot next year than it costs today. The issue becomes sig- nificant where, as in the prior example, it was presumed for tax purposes that
Harrod-Domar model, [Domar predicted] that Gross Domestic Product (GDP) was propor- tional to the number of machines; namely, that investment is the key to growth. . . . A later model, developed by Nobel Laureate Robert Solow between 1956–57, points out that the Har- rod-Domar model cannot explain sustained growth. Solow showed that as capital per worker increases, the marginal productivity of capital declines until the capital-labor ratio approaches a steady-state level. At that point, savings . . . are just sufficient to replace worn out machines and equip new workers (assuming population growth), so productivity growth is zero.”) (citing Evsey D. Domar, Capital Expansion, Rate of Growth, and Employment, 14 ECONOMETRICA 137 (1946); Roy F. Harrod, An Essay in Dynamic Theory, 49 ECON. J. 14 (1939); Robert M. Solow, A Contribution to the Theory of Economic Growth, 70 Q. J. ECON. 65 (1956); Robert M. Solow, Technical Change and the Aggregate Production Function, 39 REV. ECON. & STAT. 312 (1957)).
114 See Margalioth, supra note 113, at 505 (“For tax reporting purposes, the Code allows the use of much more accelerated depreciation methods than the straight-line method.”).
115 See id. at 505–506 (“The vast majority of U.S. corporations use a depreciation method called ‘straight-line’ for financial reporting purposes. According to the straight-line deprecia- tion method, annual depreciation is calculated by subtracting the salvage value of the asset from the purchase price and dividing this number by the estimated useful life of the asset. The outcome is equal periodical deductions throughout the asset’s useful life. If the asset in the above example is depreciated under the straight-line method, its $1000 cost is allocated uni- formly over its useful life period of five years, resulting in $200 of depreciation deduction each year.”) (citations omitted).
116 Most large corporations have significant cash accumulations and do not need to borrow funds (and pay interest) to make capital expenditure on automation. Notably, if corporate bor- rowing is required to fund capital expenditure, then present value will depend on the adjusted cost of capital, taking into account the value of tax deductions for interest paid. In summary, accelerated tax depreciation yields an economic benefit where the firm has balance sheet cash earning a low rate of return that it can instead deploy to yield tax deductions on an accelerated basis.
117 See Margalioth, supra note 113, at 508 (“In times of inflation, recovery of the nominal cost of investment is not sufficient to match income and expenses. Because of inflation, the income generated by the asset is expressed in a larger number of dollars though it has the same purchasing power.”).
 
166 486 Harvard Law & Policy Review [Vol. 12
the robot wears out after seven years, but it turns out the robot actually increases in nominal value. An incremental tax benefit thus accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary) difference is never recaptured in the tax system. In the corporate setting, this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not ad- justed for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential inves- tors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit.118 Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate re- ported earnings could use automation to achieve such a timing benefit. This increase to reported earnings may be an even more significant motivation for large firms to automate than a cash tax savings.
D. Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the firm level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary examples are the Retail Sales Tax (RST) levied by states and municipalities in the United States and the VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportion- ately to offset the indirect tax.119 In the case of automated workers, however, the burden of indirect taxes is entirely avoided by the firm because it does not need to provide for a machine’s consumption.120 In general, business ex-
118 See id. at 505 (“Accounting for depreciation is also required for financial reporting purposes. Generally accepted accounting principles (GAAP) require the depreciation of the (depreciable) cost of income generating assets, usually, tangible assets. The cost has to be allocated among accounting periods on a systematic and rational basis that reflects the use of the asset in the revenue generating process over the asset’s operational life.”) (citations omitted).
119 See CTR. FOR RESEARCH ON THE PUB. SECTOR AT UNIV. BOCCONI, THE ROLE AND IMPACT OF LABOR TAXATION 14 (2011) [hereinafter BOCCONI].
120 The capital assets comprising automated workers might be subject to property taxation by some local jurisdictions as business personal property. However, such personal property taxation is often successfully mitigated by tax planning or with tax waivers by local jurisdic- tions and municipalities negotiated by municipalities. Further, human employees also engender some degree of attached personal property (e.g., office fixtures, personal computers), which are also subject to personal property taxation.
 
487
2018] Should Robots Pay Taxes? 167
penditures for capital assets such as machinery are exempt from indirect taxation or yield a deduction for RST or VAT.121
E. Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing.122 For 2015, the Internal Revenue Service (IRS) reported that out of the nearly $3 trillion in net collections, individual income taxes accounted for 49.8%, employment taxes 35.2%, business income taxes 11.7%, excise taxes 2.6%, and estate and gift taxes 0.7%.123 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now com- prises roughly one-half of its respective share compared to prior decades.124 In fact, the Trump administration’s recently enacted Tax Cuts and Jobs Act reduces the corporate tax rate from a maximum of 35 percent to a flat 21 percent beginning in 2018.125 In Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.126
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For in- stance, an employee cannot use transfer pricing techniques to shift earned income into a 0%-taxed entity in the Cayman Islands.127 Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human
121 See John Mikesell, Sales Tax Incentives for Economic Development: Why Shouldn’t Production Exemptions Be General?, 54 NAT’L TAX J. 557, 562 (2001).
122 See SOI Tax Stats – Collections and Refunds, by Type of Tax, IRS Data Book Table 1, I.R.S. (Aug. 28, 2017), https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by- type-of-tax-irs-data-book-table-1 [https://perma.cc/R282-7P76] (containing reported aggre- gate collections and refunds from 2015 to 1995). For example, from 1995 to 2015, business income taxes decreased from 12.3% of total collections to 11.7%, while individual taxes in- creased from 46.5% to 49.8%. Id.
123 See id. Individual income taxes here include estate and trust incomes taxes, which represent 1.1% of overall collections. Employment taxes consist of primarily old-age, survi- vors, disability, and hospital insurance, which is almost entirely Federal Insurance Contribu- tions payments and a small amount of Self-Employment Insurance Contributions. It also includes a small amount of Unemployment Insurance and Railroad retirement. Id.
124 See JOEL FRIEDMAN, CTR. ON BUDGET AND POL’Y PRIORITIES, THE DECLINE OF CORPO- RATE INCOME TAX REVENUES 3 (2003), https://www.cbpp.org/sites/default/files/atoms/files/10- 16-03tax.pdf [https://perma.cc/66JG-QSU3].
125 See Act of Dec. 22, 2017 (Tax Cuts & Jobs Act) Pub. L. No. 115-97, §13001, 131 Stat. 2054 (codified as amended at 26 U.S.C. § 11).
126 See MEISEL, supra note 32, at 223 (“What numbers are used in the ratio of revenues to employees? I recommend using revenues generated within the taxing country and employees within the country in the ratio.”).
127 The Cayman Islands has no corporate tax. See DELOITTE, INTERNATIONAL TAX: CAY- MAN ISLANDS HIGHLIGHTS 2017 1 (2017), https://www2.deloitte.com/content/dam/Deloitte/ global/Documents/Tax/dttl-tax-caymanislandshighlights-2017.pdf [https://perma.cc/FZL8- 33YR].
 
168 488 Harvard Law & Policy Review [Vol. 12
capital is also not depreciable, so a person does not typically get a tax deduc- tion for education or medical costs, at least not up to the full amount of the investment.128 By contrast, machinery or other equipment yields an immedi- ate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.129
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, this would be expected to have negative economic effects over the long term.
III. TAX POLICY OPTIONS FOR AN AUTOMATION TAX
The current tax system is designed to principally tax human workers and not robot workers. All else being equal, this creates a situation in which firms prefer robots since substantially less tax per output is accrued or remit- ted in respect of an automated worker. At the same time, the automation of large segments of the labor force threatens long-term fiscal solvency because of the potential reduction in tax collections.
A major automation policy issue is therefore how to adjust the tax sys- tem to be neutral as between robot and human workers, or even to create incentives for human rather than robot workers to incentivize employment. In doing so, it is important to consider that capital investment of any kind (including for robots) is thought to be beneficial to economic growth.130 Na- tions engage in tax competition to draw capital into their jurisdictions. Any disallowance of capital deduction would serve as a disincentive to invest- ment and would, theoretically, be economically undesirable. For example, if only one taxing jurisdiction disallowed tax deductions for automated work- ers, multinational firms might shift their capital investments to other juris-
 128 For the U.S. incentives with an election for deduction or credit on higher education costs, see 26 U.S.C. § 25A (2012 & Supp. III 2016).
129 See, e.g., BOCCONI, supra note 119, at 14.
130 See, e.g., Eduardo Borensztein et al., How Does Foreign Direct Investment Affect Eco- nomic Growth?, 45 J. INT’L ECON. 115, 116 (1998); see also Gert Wehinger, Fostering Long- Term Investment and Economic Growth: Summary of a High-Level OECD Roundtable, 2011 OECD J. FIN. MKT. TRENDS 1, 2 (2011).

489
2018] Should Robots Pay Taxes? 169
dictions.131 It is therefore important to consider international tax competition in evaluating various options to create an automation-neutral tax system.132
A. Disallowance of Corporate Tax Deductions for Automated Workers
A first option is to attempt to disallow the respective corporate income tax deductions for capital investments that give rise to the automation tax benefit. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The recent South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines.133
To begin with federal income taxation, the disallowance of tax prefer- ences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phase out.”134 Phase outs reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower-income taxpayers.135 For instance, student loan interest is deductible, but not for individuals with more than $80,000 in modified adjusted gross income (MAGI) ($160,000 for joint filers).136 Some phase outs reduce credits, others reduce deductions.137
A new code provision could be designed with a similar phase out, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker automation could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance.
In respect of indirect taxation, a simpler solution may be possible. Indi- rect tax preferences for capital outlay in respect to automated workers could
131 However, the shift would be from one high-tax jurisdiction to another high-tax juris- diction to claim the deduction’s full value, rather than a shift into tax havens with a zero percent corporate tax rate, where the capital tax deductions for automated workers would not have any value (i.e., the value of a tax deduction in a zero percent tax jurisdiction is zero). Thus, multinational firms should not be expected to make capital investment in robots in tax havens where the value of deductions is zero, especially where transfer pricing strategies are available to shift income arising from the automated workers.
132 See MEISEL, supra note 32.
133 See McGoogan, supra note 22.
134 See, e.g., Emmanuel Saez, Do Taxpayers Bunch at Kink Points?, 2 AM. ECON. J. ECON
POL’Y 180, 180 (2010).
135 See, e.g., I.R.S., TEN FACTS ABOUT THE CHILD TAX CREDIT (2011), https://www.irs
.gov/newsroom/ten-facts-about-the-child-tax-credit [https://perma.cc/3YEV-V8XD].
136 See I.R.S., TAX BENEFITS FOR EDUCATION 2 (2016), https://www.irs.gov/pub/irs-pdf/
p970.pdf [https://perma.cc/X5ZV-MXMX].
137 See generally I.R.S., TAX GUIDE 2016: FOR INDIVIDUALS 25, (2016), https://www.irs
.gov/pub/irs-pdf/p17.pdf [https://perma.cc/FS97-G5LE] (discussing the various types of cred- its and deductions available and the income levels at which they phase out).
 
170 490 Harvard Law & Policy Review [Vol. 12
be disallowed outright at the state level. Thus, for example, where the firm attempts to claim an RST/VAT exemption or refund for tax payments made to purchase and maintain automated workers, this would not be permitted.
These measures could achieve greater balance between taxing human workers and robots, but the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base used to fund social insurance benefits.
B. Levy of an Automation Tax
A second option is to levy an incremental federal “automation tax” to the extent workers are laid off or replaced by machines.138 A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings.139 Employers must pay into an unemployment insurance scheme based on their ratings, so a business which has more layoffs pays more in taxes for unemployment insurance.140 A federal automation tax could be de- signed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determined the layoffs were due to automation.
A potential drawback to the levy of an additional automation tax is that it would essentially increase the corporate effective tax rate for many firms, and also increase the relative complexity of the tax system. Economic theory suggests that higher rates and added complexity are negatives in terms of international tax competition.141 Another drawback is that firms might accel- erate layoffs upon passage (or debate) of the bill prior to implementation to
138 Cf. Michael Kraich, The Chilling Realities of the Telecommuting Tax: Adapting Twenti- eth Century Policies for Twenty-First Century Technologies, 15 U. PITT. J. TECH. L. & POL’Y 224 (2015) (providing a comprehensive discussion of a “telecommuting tax”).
139 See DAVID RATNER, FED. RESERVE BD., UNEMPLOYMENT INSURANCE EXPERIENCE RAT- ING AND LABOR MARKET DYNAMICS 1 (2013), https://www.federalreserve.gov/pubs/feds/2013/ 201386/201386pap.pdf [https://perma.cc/T5W3-5YJD] (“The United States is the only OECD country to finance unemployment insurance (UI) through a tax system which penalizes layoffs. The original intent of this institution, known as ‘experience rating,’ was to apportion the costs of UI to the highest turnover firms and thereby stabilize employment. Experience rating can stabilize employment through a layoff cost. The layoff cost is levied when a firm lays off a worker and is assessed a higher tax rate in the future.”).
140 See id.
141 See generally Michael Keen & Kai A. Konrad, The Theory of International Tax Com- petition and Coordination, in 5 HANDBOOK OF PUBLIC ECONOMICS 257 (Alan Auerbach et al. eds., 2013), http://gabriel-zucman.eu/files/teaching/KeenKonrad13.pdf [https://perma.cc/ 3VC5-HLN3] (exploring models that suggest a country might prefer to raise its tax rate in response to lower tax rates in other countries); but see Bret N. Bogenschneider, Causation, Science & Taxation, 10 ELON L. REV. (forthcoming, Spring 2018) (“The hypothesis that tax cuts cause economic growth is a central tenet of neoclassical economic theory. Yet, it is not clear why economists hold this belief, as empirical evidence of any posited causal relation is conspicuously absent. . . . The available evidence indicates to the contrary of the hypotheses that tax cuts cause economic growth is that higher ratios of taxation to gross domestic product are associated with higher rates of national economic growth in most countries.”).
 
491
2018] Should Robots Pay Taxes? 171
avoid the tax by reducing the number of employees upon the effective date of the law. Accordingly, a retroactive effective date for measurement of em- ployment levels for the automation tax would be a practical necessity.
C. Grant Offsetting Tax Preferences for Human Workers
A third option is to attempt to grant offsetting tax preferences for firms that employ human workers for each category of tax benefit. To begin with wage taxation, the tax preference could entail a repeal of the employer con- tributions to the Social Security and Medicare systems. The result would be that both human and automated workers would be exempt for the employer in terms of wage taxes—not just automated workers. However, this would accelerate the insolvency of the Social Security system unless the resultant decrease in tax collections were otherwise offset.142
In terms of income taxation, an offsetting preference for human work- ers could be designed as an accelerated deduction for future wage compensa- tion expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for automated workers. In terms of indirect tax- ation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income. This would constitute an incentive for firms to employ human workers.
D. Levy of a Corporate Self-Employment Tax
A fourth option is to increase corporate level taxation for firms that produce outputs without using human labor. The additional taxes would be a substitute amount for Social Security and Medicare wage taxes avoided by the firm with automated labor.143 In part, this is the corollary to the individ- ual self-employment tax where a small-business owner is required to pay monies into the Social Security system approximating the Social Security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work.144 The corporate self- employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount deter- mined by the Treasury (in reference to industry standards), then backup
142 This would require a very significant offset. Federal Insurance Contributions and Self- Employment Insurance Contributions currently make up about 34.6% of net federal tax collec- tions. Federal Insurance Contributions include both employee and employer payments to fund Social Security and Medicare. See I.R.S., supra note 122.
143 MEISEL, supra note 32, at 222–23 (“Returning to the payroll tax analogy, companies that hire fewer people pay fewer payroll taxes. The payroll tax in the US helps fund social security, Medicare, and unemployment insurance. In Europe, payroll taxes are even higher than in the US.”).
144 Id. at 227 (“The automation tax might encourage companies to prefer productivity improvements achieved by using a combination of human and computer capabilities.”).
 
172 492 Harvard Law & Policy Review [Vol. 12
withholding could apply on corporate profits. The gross amount of the auto- mation tax could be designed to match the wage taxes avoided by the firm with automated workers.
William Meisel has similarly proposed an “automation tax” which he referred to in lay terms as a “payroll tax on computers.”145 This would be like the corporate self-employment tax described here. Meisel wrote:
I propose that a national automation tax be based on the ratio of a company’s revenues (total sales) to their number of employ- ees. . . .[T]he automation tax should increase as a percentage as the revenue-per-employee [ratio] grows, making it more attractive to create jobs than to replace them with automation. . . . I prefer applying the percentage to revenues. . . . Profits can be manipu- lated with deductions and other accounting complexities much more than revenues.146
Meisel’s “automation tax” differs from our proposed corporate self-em- ployment tax in that the former uses a sales ratio as opposed to a profit ratio. A sales ratio may be unworkable in practice since the tax would prohibi- tively fall on firms with high sales but low profit margins, such as dis- counted retailers. Since automation often occurs in the high-tech industry among companies with high profit margins, it seems preferable that a viable “automation tax” using a ratio to employee expense should be premised on profits, not sales.
E. Increase the Corporate Tax Rate
A fifth option would be to significantly increase the corporate tax rate, with the intent of increasing the relative portion of the tax base borne by capital and decreasing that borne by labor. The counter-intuitive advantage of this approach is that higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment re- fers to incremental investment made only because of the tax system.147 As one of us has explained, “[t]he experienced tax attorney always counsels the client that marginal capital investment is tax deductible.”148 Thus, mul- tinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions to claim tax deductions of relatively higher value. In part for this reason, for smaller and growing firms that are reinvest- ing profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
145 Id. at 220 (“If software is to take over many jobs, why not have an income tax on software? We could perhaps think of it as a payroll tax on computers.”).
146 Id. at 221–23.
147 See Bret N. Bogenschneider, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74 (2015).
148 Id. at 61 (second emphasis added).
 
493
2018] Should Robots Pay Taxes? 173
The drawbacks to increasing the corporate tax rate are well-known and may be summarized as follows: First, the corporate tax rate might be a signal to firms about the tax climate of a jurisdiction, so higher tax rates could have a negative psychological effect on capital investment decision making.149 Second, accelerated tax deductions would be a stronger automation incen- tive with a higher corporate tax rate as the deduction would have greater value. This means that an increase in the corporate tax rate should be taken in combination with our other proposals. Third, the increase in corporate tax rates would affect all firms, even those not engaged in worker automation. Hence, the increase in corporate tax rate option might be viewed as one version of zero-sum analysis, in which tax policy is designed not to allow a shift of the tax burden from capital to other taxpayers. Further, any increase in corporate tax rates may prompt firms to attempt to shift the tax incidence to workers or consumers.150 Finally, increasing corporate tax rates may be politically unfeasible. As Meisel notes in an understated fashion, “[c]orporations might instinctively fight a corporate tax.”151
F. Issues in Economic Efficiency Relevant to Automation Tax Policy Proposals
The tax policy analysis developed here comes from the perspective of average effective tax rates as opposed to solely marginal rates.152 Any margi- nal tax rate methodology excludes an analysis of taxation relative to the overall share of the tax base. For example, technology and pharmaceutical companies often pay a very low average effective tax rate (e.g., less than 10%) but could also be correctly found to simultaneously have a high margi- nal effective tax rate (e.g., about 35%). A corporate taxpayer which pays very little tax relative to its level of taxable income could correctly describe its marginal tax rate as “high.” Accordingly, the last dollar of income may nearly always be found to be taxed at a “high” marginal tax rate, even where the average effective tax rate is relatively low.153
Economic models of taxation are typically designed by modeling the hypothetical effects of changes in marginal tax rates.154 Marginal tax rates
149 Id. at 60–61 (“Any income tax system is designed initially to favor active investors. This is because no matter how high the actual tax rate, it is levied only on what is referred to as ‘taxable income.’ Of course, ‘taxable income’ means the amount of profits less deductions. Every tax professional is aware of this feature of an income tax system and counsels the client accordingly.”).
150 See Kimberly Clausing, In Search of Corporate Tax Incidence, 65 TAX L. REV. 433, 468 (2012). Firms, however, behave as if they bear the incidence of corporate taxation.
151 MEISEL, supra note 32, at 225.
152 The calculation of a marginal tax rate is essentially the theoretical opposite of the calculation of taxation as a percentage of the share of the overall tax base.
153 For example, a firm may have an overall tax rate of 20% on all of its earnings; how- ever, with respect to a hypothetical decision of whether to earn incremental income, the margi- nal tax rate might be 35%.
154 For a discussion of marginal tax rates in economic analysis, see David Madden, The Poverty Effects of a ‘Fat Tax’ in Ireland, 24 HEALTH ECON. 104, 106 (2015) (“The difficulties
 
174 494 Harvard Law & Policy Review [Vol. 12
again represent incremental changes to the statutory tax rate on the last dol- lar of income.155 For example, a change in the statutory corporate tax rate from 35% to 30% would be reflected in economic models premised on mar- ginal rate analysis. The trouble with this form of economic modeling is that its validity relies on the presumption that firm decisions are made based on tax effects on the marginal investment and not based on an average. This approach has major implications for tax policy design as tax cuts to the stat- utory rate are nearly certain to have a marginal effect even where the firm does not pay a high tax rate overall. Thus, business and investment decisions are presumed not to proceed at the average tax rate for all earned income, but only with respect to incremental tax changes relevant to marginal income.
Other economic modeling proceeds on a marginal effective tax rate ba- sis (i.e., reflecting that corporate taxpayers do not pay the statutory rate). For example, the granting of an additional deduction for manufacturing activity to corporations could reduce the marginal effective tax rate on the last dollar of income from 30% to 27% where the statutory rate is 35%. By this method, the firm would be presumed to make an investment decision based on the average tax rate at the margin. Both approaches are distinguishable from analysis using simply an average effective tax rate, which for large corporations is now calculated at approximately 20% (including permanent deferrals) and trending downward.156 However, for many tech companies, the effective tax rate is below 10%. At such very low average effective tax rates, it is not clear that economic analysis of marginal effects of tax cuts is a realistic method of tax policy analysis. By such methods, significant macroeconomic benefits can be posited where corporate effective tax rates are reduced from very low levels to even lower levels (e.g., from 2% to 1%), but where it is likely that factors other than marginal taxation are likely to drive firm investment decisions. Also, the positing of economic growth from marginal tax cuts does not consider the effect changes in the composition of the overall tax base, where the taxation of one factor is substantially re- duced, namely capital, and the taxation of another factor is increased (or overall borrowing is increased). Further, multinational firms do not engage in tax avoidance planning to reduce income which they do not intend to earn.
associated with non-marginal tax reforms have led a number of analysts to concentrate on marginal tax reforms. This approach has the advantage of not requiring estimates of individual demand and utility functions.”) (internal citation omitted).
155 The U.S. federal statutory corporate tax rate is thirty-five percent for corporate income in excess of ten million. See I.R.C. § 11 (2012). Various individual U.S. states also levy an incremental state-level corporate tax. See generally NICOLE KAEDING, TAX FOUND., STATE INCOME CORPORATE TAX RATES AND BRACKETS FOR 2016 (2016), https://files.taxfoundation .org/legacy/docs/TaxFoundation-FF497.pdf [https://perma.cc/J37Z-Y8X6].
156 For effective tax rates on multinational firms including the delay in taxation of foreign earnings for U.S. multinationals, see generally Bret N. Bogenschneider, The Effective Tax Rates of U.S. Firms with Permanent Deferral, 145 TAX NOTES 1391 (2015).
 
495
2018] Should Robots Pay Taxes? 175
In summary, notwithstanding that the statutory corporate tax rate, or marginal corporate effective tax rates, might be correctly described as “high” in the economic theory of taxation, such analysis is also subject to a relative or zero-sum form of analysis, where tax cuts for one party are trans- ferred as tax increases to another party. The average effective tax rate on workers is relatively “high” where all types of taxation are taken into ac- count.157 The taxation of workers comprises the bulk of the tax base in the United States and that of most developed countries. As workers are substi- tuted or replaced by automation, follow-on effects are possible not only from the direct reduction in the tax base, but also indirectly where the relative taxes are transferred to other workers in the economy.
CONCLUSION
Automation promises to be one of the great social challenges of our generation. It can benefit everyone, or it can benefit the select few at the expense of the many. Tax is a critical component of any automation policy. Existing tax policies both encourage automation and dramatically reduce the government’s tax revenue. This means that attempts to craft policy solutions to deal with automation will be inadequate if they fail to take taxation into account. In this article, we have proposed a series of tax policy changes that could level the playing field for human workers. Whether these proposals are adopted may depend on whether policy makers are prepared to make politically challenging decisions about how to deal with automation.
 157 See Bogenschneider, supra note 27.

496
The Reasonable Computer: Disrupting the Paradigm of Tort Liability
Ryan Abbott* ABSTRACT
Artificial intelligence is part of our daily lives. Whether working as chauf- feurs, accountants, or police, computers are taking over a growing number of tasks once performed by people. As this occurs, computers will also cause the injuries inevitably associated with these activities. Accidents happen, and now computer-generated accidents happen. The recent fatality involving Tesla’s au- tonomous driving software is just one example in a long series of “computer- generated torts.”
Yet hysteria over such injuries is misplaced. In fact, machines are, or at least have the potential to be, substantially safer than people. Self-driving cars will cause accidents, but they will cause fewer accidents than human drivers. Because automation will result in substantial safety benefits, tort law should encourage its adoption as a means of accident prevention.
Under current legal frameworks, suppliers of computer tortfeasors are likely strictly responsible for their harms. This Article argues that where a sup- plier can show that an autonomous computer, robot, or machine is safer than a reasonable person, the supplier should be liable in negligence rather than strict liability. The negligence test would focus on the computer’s act instead of its design, and in a sense, it would treat a computer tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, and it would continue to reward sup- pliers for improving safety.
More importantly, principles of harm avoidance suggest that once com- puters become safer than people, human tortfeasors should no longer be mea- sured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against computers. To appropriate the immortal words of Justice Holmes, we are all “hasty and awkward” compared to the reasonable computer.
TABLE OF CONTENTS
INTRODUCTION ................................................. 2 I. LIABILITY FOR MACHINE INJURIES ..................... 8 A. A Brief History ..................................... 8
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at UCLA. Thanks to Hrafn Asgeirsson, Bret Bogenschneider, Richard Epstein, Marie Newhouse, Alexander Sarch, and Christopher Taggart for their insightful comments.
 January 2018 Vol. 86 No. 1
1

2
497 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
B. Tort Law as a Mechanism for Accident Prevention . 11 C. Negligence .......................................... 12 D. Strict and Product Liability ......................... 13
II. COMPUTER-GENERATED TORTS......................... 16
A. Automation Will Prevent Accidents ................. 16
B. Tort Liability Discourages Automation.............. 19
C. Computer-Generated Torts Should Be Negligence
Based ............................................... 22
D. Computer-Generated Torts as a Type of Machine
Injury............................................... 24
E. Implementation ..................................... 26
F. Financial Liability................................... 30
G. Alternatives to Negligence ........................... 32
III. THE REASONABLE ROBOT .............................. 35
A. When Negligence Is Strict ........................... 35
B. The New Hasty and Awkward ...................... 36
C. Reasonable People Use Autonomous Computers . . . . 39
D. The Reasonable Computer Standard for Computer
Tortfeasors.......................................... 41
E. The Automation Problem ........................... 42
CONCLUSION ................................................... 44
INTRODUCTION
An automation revolution is coming, and it is going to be hugely disruptive.1 Ever cheaper, faster, and more sophisticated computers are able to do the work of people in a wide variety of fields and on an unprecedented scale. They may do this at a fraction of the cost of existing workers, and in some instances, they already outperform their human competition.2 Today’s automation is not limited to manual la- bor; modern machines are already diagnosing disease,3 conducting le-
1 See generally JAMES MANYIKA ET AL., MCKINSEY & CO., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013).
2 See, e.g., Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible Are Jobs to Computerisation?, 114 TECHNOLOGICAL FORECASTING & SOC. CHANGE 254, 265–66 (2017) (reporting in a seminal paper that “47 percent of total US employment is [at] high risk” of automation, and stating that “recent developments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future”).
3 See Roger Parloff, Why Deep Learning Is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning [https://perma.cc/E3UA-N2TZ]. Several artificial intelligence systems are already capable of au- tomating medical diagnoses. See id. For instance, Freenome has a system for diagnosing cancer from blood samples that is competitive with pathologists. See id.; see also FREENOME, http:// www.freenome.com (last visited Jan. 4, 2018).
 
2018] 498 THE REASONABLE COMPUTER 3
gal due diligence,4 and providing translation services.5 For better or worse, automation is the way of the future—the economics are simply too compelling for any other outcome.6 But what of the injuries these automatons will inevitably cause? What happens when a machine fails to diagnose a cancer, ignores an incriminating email, or inadvertently starts a war?7 How should the law respond to computer-generated torts?
Tort law has answers to these questions based on a system of common law that has evolved over centuries to deal with unintended harms.8 The goals of this body of law are many: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.9 Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law.10 By creating a framework for loss shifting from injured victims to tortfeasors, tort law deters unsafe conduct.11 A purely financially motivated rational
4 See Jane Croft, Legal Firms Unleash Office Automatons, FIN. TIMES (May 16, 2016), https://www.ft.com/content/19807d3e-1765-11e6-9d98-00386a18e39d (discussing various software programs that can outperform attorneys and paralegals in document review); cf. Dana Remus & Frank S. Levy, Can Robots Be Lawyers? Computers, Lawyers, and the Practice of Law (Nov. 27, 2016), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2701092 (unpublished manuscript) (arguing that artificial intelligence will refocus rather than replace attorneys).
5 See Yonghui Wu et al., Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation (Sept. 26, 2016), https://arxiv.org/pdf/1609.08144.pdf (unpublished manuscript). Google now claims its Google Neural Machine Translation system is approaching human-level translation accuracy. Id. at 2.
6 See, e.g., DELOITTE, FROM BRAWN TO BRAINS: THE IMPACT OF TECHNOLOGY ON JOBS IN THE UK 4 (2015), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Growth/ deloitte-uk-insights-from-brawns-to-brain.pdf (suggesting that every nation and region of the U.K. has benefitted from automation and that automation has resulted in £140 billion to the U.K.’s economy in new wages).
7 See, e.g., Fiona Macdonald, The Greatest Mistranslations Ever, BBC (Feb. 2, 2015), http://www.bbc.co.uk/culture/story/20150202-the-greatest-mistranslations-ever (describing some of the unfortunate outcomes associated with mistranslation).
8 See generally MORTON J. HORWITZ, THE TRANSFORMATION OF AMERICAN LAW, 1780–1860 (1977) [hereinafter HORWITZ, 1780–1860]; MORTON J. HORWITZ, THE TRANSFORMA- TION OF AMERICAN LAW 1870–1960 (1992).
9 See George L. Priest, Satisfying the Multiple Goals of Tort Law, 22 VAL. U. L. REV. 643, 648 (1988).
10 See, e.g., George L. Priest, The Invention of Enterprise Liability: A Critical History of the Intellectual Foundations of Modern Tort Law, 14 J. LEGAL STUD. 461 (1985); see also Robert F. Blomquist, Goals, Means, and Problems for Modern Tort Law: A Reply to Professor Priest, 22 VAL. U. L. REV. 621 (1988) (arguing that economic theory and moral philosophy both require accident reduction to be the primary aim of tort law).
 11 See George L. Priest, Modern Tort Law and Its Reform, 22 VAL. U. L. REV. 1, 7 (1987).

4 499 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.12 This liability framework has far-reaching and sometimes complex impacts on be- havior. It can either accelerate or impede the introduction of new technologies.13
Most injuries people cause are evaluated under a negligence stan- dard where unreasonable conduct establishes liability.14 When com- puters cause the same injuries, however, a strict liability standard applies.15 This distinction has financial consequences and a corre- sponding impact on the rate of technology adoption.16 It discourages automation, because machines incur greater liability than people. It also means that in cases where automation will improve safety, the current framework to prevent accidents now has the opposite effect.
This Article argues that the acts of autonomous computer tortfeasors should be evaluated under a negligence standard, rather than a strict liability standard, in cases where an autonomous com- puter is occupying the position of a reasonable person in the tradi- tional negligence paradigm and where automation is likely to improve safety. For the purposes of ultimate financial liability, the computer’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments under standard principles of product liability law.
This Article employs a functional approach to distinguish an au- tonomous computer, robot, or machine from an ordinary product.17
12 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (applying rule that balances the burden of additional protections on the actor with the probability and gravity of an injury).
13 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Gideon Parchomovsky & Alex Stein, Torts and Innovation, 107 MICH. L. REV. 285, 286 (2008) (discussing how the role of custom in tort law impedes innovation). Nor is the idea that tort liability is a barrier to developments in machine intelligence new. See Steven J. Frank, Tort Adjudication and the Emergence of Artificial Intelligence Software, 21 SUFFOLK U. L. REV. 623, 639 (1987).
14 See infra text accompanying notes 62–71.
15 See infra text accompanying notes 93–100.
16 See, e.g., Amy Finkelstein, Static and Dynamic Effects of Health Policy: Evidence from
the Vaccine Industry, 119 Q.J. ECON. 527, 535 (2004) (explaining that establishment of the Vac- cine Injury Compensation Fund encouraged vaccine development by indemnifying manufactur- ers from liability).
17 Terms such as “robot,” “machine,” “artificial intelligence,” “machine intelligence,” and even “computer” are not used consistently even in the scientific literature. See, e.g., NEIL JOHN- SON ET AL., ABRUPT RISE OF NEW MACHINE ECOLOGY BEYOND HUMAN RESPONSE TIME 2 (2013), https://www.nature.com/articles/srep02627.pdf (discussing autonomy in the context of ar- tificial intelligence); Matthew U. Scherer, Regulating Artificial Intelligence Systems: Risks, Chal-
 
2018] 500 THE REASONABLE COMPUTER 5
Society’s relationship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are given tasks to complete and determine for themselves how to complete those tasks. For instance, a person could instruct a self-driving car to take them from point A to point B, but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. This distinction is analogous to the distinction be- tween employees and independent contractors, which centers on the degree of control and independence.18 As this Article uses such terms, autonomous machines or computer tortfeasors control the means of completing tasks, regardless of their programming.19
The most important implication of this line of reasoning is that just as computer tortfeasors should be compared to human tortfeasors, so too should humans be compared to computers. Once computers become safer than people and practical to substitute, com- puters should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical, reasonable person would have done in their situation, but what a computer would have done. In time, as computers come to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy,20 but people would engage in certain activi- ties at their own peril. Such a rule is entirely consistent with the ratio- nale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving
lenges, Competencies, and Strategies, 29 HARV. J.L. & TECH. 353, 359–61 (2016) (discussing difficulties with defining artificial intelligence); John McCarthy, What Is Artificial Intelligence? 2–3 (Nov. 12, 2007), http://jmc.stanford.edu/articles/whatisai/whatisai.pdf (discussing the lack of a standardized definition of artificial intelligence by the scientist who coined the term).
18 See Yewens v. Noakes [1880] 6 QB 530 at 532–33 (Eng.) (“A servant is a person subject to the command of his master as to the manner in which he shall do his work.”). Also see O’Connor v. Uber Technologies, Inc., No. 14-16078 (9th Cir. argued Sept. 20, 2017), for one of the many ongoing lawsuits against Uber highlighting modern challenges distinguishing between employees and independent contractors.
19 See, e.g., Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) (discussing types of machine architectures, including conventional knowledge-based systems with expert rules as well as types of machine intelligence algorithms that result in unexpected machine behavior).
20 See generally Richard M. Ryan & Edward L. Deci, Overview of Self-Determination The- ory: An Organismic Dialectical Perspective, in HANDBOOK OF SELF-DETERMINATION RESEARCH 3, 6 (Edward L. Deci & Richard M. Ryan eds., 2002) (arguing that people have three basic psychological needs: connectedness, autonomy, and feeling competent).
 
6 501 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
“reasonable computer” standard should even apply to computer tortfeasors, such that computers will be held to the standard of other computers. By this time, computers will cause so little harm that the primary effect of the standard would be to make human tortfeasors essentially strictly liable for their harms.
This Article uses self-driving cars as a case study to demonstrate the need for a new torts paradigm.21 There is public concern over the safety of self-driving cars, but a staggering ninety-four percent of crashes involve human error.22 These contribute to over 37,000 fatali- ties a year in the United States at a cost of about $242 billion.23 Auto- mated vehicles may already be safer than human drivers, but if not, they will be soon.24 Shifting to negligence would accelerate the adop- tion of driverless technologies, which, according to a report by the consulting firm McKinsey & Company, may otherwise not be wide- spread until the middle of the century.25
Automated vehicles may be the most prominent and disruptive upcoming example of robots changing society, but this analysis applies to any context with computer tortfeasors. For instance, IBM’s flagship artificial intelligence system, Watson, is working with clinicians at Me- morial Sloan Kettering to analyze patient medical records and provide
21 Others have written about tort liability and self-driving vehicles, although primarily dealing with how existing law deals with accidents involving autonomous vehicles. See, e.g., Jef- frey K. Gurney, Sue My Car Not Me: Products Liability and Accidents Involving Autonomous Vehicles, 2013 U. ILL. J.L. TECH. & POL’Y 247; F. Patrick Hubbard, “Sophisticated Robots”: Balancing Liability, Regulation, and Innovation, 66 FLA. L. REV. 1803, 1803 (2014) (arguing, using the example of self-driving vehicles, that the current framework “provides an appropriate balance of innovation and liability for personal injury”); Gary E. Marchant & Rachel A. Lindor, The Coming Collision Between Autonomous Vehicles and the Liability System, 52 SANTA CLARA L. REV. 1321 (2012).
22 See NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 115, CRITICAL REASONS FOR CRASHES INVESTIGATED IN THE NATIONAL MOTOR VEHICLE CRASH CAUSATION SURVEY 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublica- tion/812115.
23 General Statistics, INS. INST. FOR HIGHWAY SAFETY (Dec. 2017), http://www.iihs.org/ iihs/topics/t/general-statistics/fatalityfacts/overview-of-fatality-facts [https://perma.cc/2J5P- Y27C]; see NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 013, THE ECONOMIC AND SOCIETAL IMPACT OF MOTOR VEHICLE CRASHES, 2010 (REVISED) 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812013.
24 See Cadie Thompson, Why Driverless Cars Will Be Safer Than Human Drivers, BUS. INSIDER (Nov. 16, 2016, 9:24 PM), http://www.businessinsider.com/why-driverless-cars-will-be- safer-than-human-drivers-2016-11.
25 Michele Bertoncello & Dominik Wee, Ten Ways Autonomous Driving Could Redefine the Automotive World, MCKINSEY & CO. (June 2015), http://www.mckinsey.com/industries/auto- motive-and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
 
2018] 502 THE REASONABLE COMPUTER 7
evidence-based cancer treatment options.26 It even provides support- ing literature to human physicians to support its recommendations.27 Like self-driving cars, Watson does not need to be perfect to improve safety—it just needs to be better than people. In that respect, the bar is unfortunately low. Medical error is one of the leading causes of death.28 A 2016 study in the British Medical Journal reported that it is the third leading cause of death in the United States, ranking just be- hind cardiovascular disease and cancer.29 Some companies already claim their artificial intelligence systems outperform doctors, and that claim is not hard to swallow.30 Why should a computer not be able to outperform doctors when the computer can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?31
This Article is divided into three Parts. Part I provides back- ground on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negli- gence and strict product liability. Part II argues that while some forms of automation should prevent accidents, tort law may act as a deter- rent to adopting safer technologies. To encourage automation and im- prove safety, this Part proposes a new categorization of “computer- generated torts” for a subset of machine injuries. This would apply to cases in which an autonomous computer, robot, or machine is occupy- ing the position of a reasonable person in the traditional negligence paradigm and where automation is likely to improve safety. This Part contends that the acts of computer tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and it goes on to propose rules for implementing the system.
26 Oncology and Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-ge- nomics [https://perma.cc/Z6H7-S5W4].
27 Id.
28 See INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER HEALTH SYSTEM (Linda T. Kohn et al. eds., 2000); Martin A. Makary & Michael Daniel, Medical Error—The Third Leading Cause of Death in the US, 353 BMJ 2139, 2139 (2016). The landmark report published by the Institute of Medicine in 2000 was a wake-up call to the medical profession about the harmful effects of medical error. See INST. OF MED., supra. Yet the report was based on studies conducted in 1984 and 1992. See id.
29 Makary & Daniel, supra note 28, at 2143 fig.1.
30 Parloff, supra note 3. For example, Enlitic has a program for detecting and classifying
lung cancers which the company claims has already outperformed human radiologists. Id.
31 See, e.g., Saul N. Weingart et al., Epidemiology of Medical Error, 320 BMJ 774, 775 (2010) (discussing some of the causes of human medical error).
 
8 503 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
Finally, Part III argues that once computer operators become safer than people and automation is practical, the “reasonable computer” should become the new standard of care. It explains how this standard would work, argues the reasonable computer standard works better than a reasonable person using an autonomous machine, and consid- ers when the standard should apply to computer tortfeasors. At some point, computers will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
This Article is focused on the effects of automation on accidents, but automation implicates a host of social concerns. It is important that policymakers act to ensure that automation benefits everyone. Automation may increase productivity and wealth, but it may also contribute to unemployment, financial disparities, and decreased so- cial mobility. These and other concerns are certainly important to con- sider in the automation discussion, but tort liability may not be the best mechanism to address every issue related to automation.32
I. LIABILITY FOR MACHINE INJURIES
A. A Brief History
Injuries caused by machines are nothing new. For as long as peo- ple have used machines, injuries have resulted—and machines have been with us for quite some time. The earliest evidence of simple ma- chines—tools that redirect force to make work easier, like axes— dates back millions of years to the beginning of the Stone Age.33 In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as hand axes.34 The pri- mary function of these tools was to hunt and cut meat,35 but they were also used to facilitate violence against people.36 Machines used in the furtherance of intentional torts were likely used negligently as well.
32 See, e.g., Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL’Y REV. 145 (2018) (arguing that the tax system incentivizes automation even in cases where it is not otherwise efficient and that automation decreases government tax revenue, and proposing changes to existing tax policies as a solution).
33 Kate Wong, Ancient Cut Marks Reveal Far Earlier Origin of Butchery, SCI. AM. (Aug. 11, 2010), https://www.scientificamerican.com/article/ancient-cutmarks-reveal-butchery/.
34 See Stone Age, MERRIAM-WEBSTER, https://www.merriam-webster.com/dictionary/ Stone%20Age [https://perma.cc/U6W4-M4M8]. See generally Sonia Harmand et al., 3.3-Million- Year-Old Stone Tools from Lomekwi 3, West Turkana, Kenya, 521 NATURE 310 (2015).
35 Wong, supra note 33.
36 M. Mirazo ́n Lahr et al., Inter-group Violence Among Early Holocene Hunter-Gatherers
of West Turkana, Kenya, 529 NATURE 394, 396 (2016).
 
2018] 504 THE REASONABLE COMPUTER 9
Given that home knife accidents led to about a third of a million emergency room visits in the United States in 2011 alone, it is not difficult to imagine that during the Stone Age these simple machines caused accidents.37
As history progressed, and the use and complexity of simple ma- chines grew, so too did the resultant injuries38: Mesopotamian sur- geons botched procedures,39 Greek construction zones were so dangerous they required physicians on site,40 and Egyptian embalmers accidently left instruments in their subjects.41 Such injuries continued unabated from the time complex machines were invented by the an- cient Chinese and Greeks to the time of the first modern industrial machines.42
The Industrial Revolution marked a turning point in the role of machines in society.43 Major technological advances occurred during
37 See Joe Yonan, Knife Injuries and Other Kitchen Mishaps Afflict Both Top Chefs and Everyday Cooks, WASH. POST (Jan. 7, 2013), https://www.washingtonpost.com/national/health- science/knife-injuries-and-other-kitchen-mishaps-afflict-both-top-chefs-and-everyday-cooks/ 2013/01/07/92e191f8-4af0-11e2-b709-667035ff9029_story.html.
38 See generally Y.C. CHIU, AN INTRODUCTION TO THE HISTORY OF PROJECT MANAGE- MENT 19–115 (2010) (discussing the use of technology in industrial activities). For example, al- most half a million people died building the Great Wall of China, although the number of these deaths due to machine injuries is unknown. Great Wall of China, HISTORY.COM, http:// www.history.com/topics/great-wall-of-china [https://perma.cc/7MP5-FJX5]. So common were machine and industrial injuries in the ancient world that ancient Greek, Roman, Arab, and Chi- nese laws provided for compensation schedules for accidents. See Gregory P. Guyton, A Brief History of Workers’ Compensation, 19 IOWA ORTHOPAEDIC J. 106, 106 (1999). Under ancient Arab law, “loss of a joint of the thumb was worth one-half the value of a finger. The loss of a penis was compensated by the amount of length lost, and the value an ear was based on its surface area.” Id.
39 See Emily K. Teall, Medicine and Doctoring in Ancient Mesopotamia, 3 GRAND VALLEY J. HIST. 1, 5 (2014). Unfortunately for these doctors, medical malpractice in Babylon was corpo- rally punishable. Allen D. Spiegel & Christopher R. Springer, Babylonian Medicine, Managed Care and Codex Hammurabi, Circa 1700 B.C., 22 J. COMMUNITY HEALTH 69, 81 (1997); see also GUIDO MAJNO, THE HEALING HAND: MAN AND WOUND IN THE ANCIENT WORLD 53 (1975).
40 DAVID MATZ, VOICES OF ANCIENT GREECE AND ROME: CONTEMPORARY ACCOUNTS OF DAILY LIFE 58 (2012).
41 Granted, this example involves cadavers rather than living patients, or so one hopes. Owen Jarus, Oops! Brain-Removal Tool Left in Mummy’s Skull, LIVE SCI. (Dec. 14, 2012, 8:03 AM), http://www.livescience.com/25536-mummy-brain-removal-tool.html/. It certainly portends modern medical malpractice cases involving retained surgical instruments. See, e.g., Atul A. Ga- wande et al., Risk Factors for Retained Instruments and Sponges After Surgery, 348 NEW ENG. J. MED. 229, 230 (2003).
42 Peter J. Lu, Early Precision Compound Machine from Ancient China, 304 SCIENCE 1638 (2004); cf. Russell Fowler, The Deep Roots of Workers’ Comp, 49 TENN. B.J. 10, 10–12 (2013) (discussing historical development of workers’ compensation schemes from the medieval through the modern era).
 43 Economists have argued the Industrial Revolution was “certainly the most important

10 505 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
this period in textiles, transportation, and iron making, which resulted in the development of machines for shaping materials and the rise of the factory system.44 It also resulted in a dramatic increase in the num- ber and severity of machine injuries.45 Working in industrial settings was a dangerous business, in part because employers often had mini- mal liability for employee harms.46 These dangerous working condi- tions persisted well into the twentieth century before the U.S. government began collecting data on work-related injuries in a sys- tematic way.47 In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of 61 deaths per 100,000 workers.48
In the modern era, the rate of work-related injuries has declined significantly. In 2016, for example, the U.S. Bureau of Labor reported 5190 fatal work injuries, a rate of 3.6 per 100,000 workers.49 The rea- son for this decline is multifactorial: changes to tort liability, evolved societal and ethics norms that place a greater priority on human wel- fare, a modern system of regulations and criminal liability that pro- tects worker wellbeing, as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Workplace accidents were responsible for
event in the history of humanity since the domestication of animals and plants, perhaps the most important since the invention of language. It bids fair to free us all, eventually.” Deirdre Mc- Closkey, Review of The Cambridge Economic History of Modern Britain, PRUDENTIA (Jan. 15, 2004), http://www.deirdremccloskey.com/articles/floud.php [https://perma.cc/UAP4-6ZZ3].
44 See generally History of Technology: The Industrial Revolution (1750–1900), EN- CYCLOPæDIA BRITANNICA, https://www.britannica.com/technology/history-of-technology/The-In- dustrial-Revolution-1750-1900 [https://perma.cc/QV7K-LKLK].
45 See generally HENRY ROGERS SEAGER, SOCIAL INSURANCE: A PROGRAM OF SOCIAL REFORM 24–52 (1910) (including a chapter on industrial accidents in a classic exposition of the philosophical movement for social insurance).
46 John S. Haller, Jr., Industrial Accidents—Worker Compensation Laws and the Medical Response, 148 WEST J. MED. 341–48 (1988); see also HORWITZ, 1780–1860, supra note 8, at 90. 47 See Progressive Era Investigations, U.S. DEP’T LAB., https://www.dol.gov/dol/aboutdol/ history/mono-regsafepart05.htm [https://perma.cc/HUT4-5WQE]. The first systematic U.S. sur- vey of workplace fatalities found that 526 workers died in “work accidents” in Allegheny County from July 1906 to June 1907. Improvements in Workplace Safety—United States, 1900–1999, 48 CDC MORBIDITY & MORTALITY WKLY. REP. 461, 461 (1999). Of those fatalities, 195 were steel-
workers. Id. Contrast that with 17 national steelworker fatalities in 1997. Id.
48 Improvements in Workplace Safety—United States, 1900–1999, supra note 47, at 461. The National Safety Council estimated that 18,000–21,000 workers died from work-related inju-
ries in 1912. Id.
49 BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR, NATIONAL CENSUS OF FATAL
OCCUPATIONAL INJURIES IN 2016, at 1 (2017), http://www.bls.gov/news.release/pdf/cfoi.pdf [https://perma.cc/2YMS-RA8F].
 
2018] 506 THE REASONABLE COMPUTER 11
approximately 4000 deaths in the United States in 2014 and a total cost of about $140 billion.50 More broadly, there were a total of almost 200,000 injury-related deaths in 2014 in the United States, with all un- intentional injuries costing some $850 billion.51 Unintentional injuries are the fourth leading cause of death.52
B. Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law now provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health.53
A tort is a harmful civil act, other than under contract, where one person is damaged by another, and it gives way to a right to sue.54 A variety of goals have been proposed for tort law: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.55 Whether tort law is the best means for achieving all of these goals is a matter of endless dispute.56 Jurists are united, however, in considering accident reduction as one of the central goals of tort law, if not the primary goal.57 By creating a framework for loss shifting from injured
50 NAT’L SAFETY COUNCIL, INJURY FACTS: 2016 EDITION 3, 8 (2016).
51 Id. Lost quality of life from those injuries is valued at an additional $3345.5 billion. Id. at
8.
52 Id. at 2.
53 Tort law primarily grew out of a focus on bodily injury and physical property damage,
but protection in modern times has been extended beyond the physical to include harm to emo- tional well-being, and economic loss.
The range of torts is as broad as human experience and includes such wrongful conduct as negligence (personal injury law for unintentional harm), intentional torts (e.g., assault, battery, trespass to land), products liability (defective products), abnormally dangerous activities liability (e.g., blasting, aerial pesticide spraying), nuisance (e.g., air, water, and noise pollution), defamation (libel and slander), pri- vacy invasion (private area intrusion and personal autonomy interference), and fraud (misrepresentation). Tort law study also includes consideration of legislative measures related to torts and alternatives to tort liability, for example, automobile no-fault compensation systems.
DOMINICK VETRI ET AL., TORT LAW AND PRACTICE 3 (5th ed. 2016).
54 See id. at 2. A tort governs loss shifting from injured victims to tortfeasors, and it dic-
tates who can sue and what they can sue for. See id. It is “the set of legal rules establishing liability and compensation for personal injury and death caused by the intentional or careless conduct of a third party.” Id.
55 See, e.g., Priest, supra note 9, at 645 n.23, 648.
56 See, e.g., Priest, supra note 10.
57 See Blomquist, supra note 10, at 628–29 (arguing that economic theory and moral phi-
 losophy both require accident reduction to be the primary aim of tort law).

12 507 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
victims to tortfeasors, tort law deters unsafe conduct.58 A purely finan- cially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.59
On a broader level, the law of torts is one of the primary ways in which society choses to allocate liability. And allocating liability has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing and pulse oximeters, or it can discourage the use of new technologies, as is usually the case where the standard of care is based on custom.60
Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts which do not require fault.61 Covering the “great mass of cases” in the middle are harms involving negligence.62
C. Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms.63 In practice, to pre- vail in most personal injury cases, a plaintiff must prove by a prepon- derance of the evidence that the defendant owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages.64 This generally requires proof that the defendant acted neg- ligently, which is to say, acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circum-
58 See Priest, supra note 9, at 648.
59 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (stating that
liability calculations should consider whether the probability of injury times potential damages is lower than the burden imposed).
60 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Parchomovsky & Stein, supra note 13, at 306 (discussing how the role of custom in
 tort law 61
62 63 64
impedes innovation).
Oliver Wendell Holmes, Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873). Id.
See Thomas C. Grey, Accidental Torts, 54 VAND. L. REV. 1225, 1283–84 (2001). See RESTATEMENT (SECOND) OF TORTS § 281 (AM. LAW INST. 1965).

2018] 508 THE REASONABLE COMPUTER 13
stances.65 Thus, if the courts determined that a reasonable person would not have headed out to sea without a radio to warn of storm conditions,66 manufactured a ginger beer with a snail inside,67 or dropped heavy objects off the side of a building,68 then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compen- sating victims as well as encouraging economic growth and progress.69 One way that tort law attempts to achieve this balance is by permit- ting recovery in negligence only where there has been socially blame- worthy conduct.70 Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will gen- erally be no liability. Juries play a key role in determining the reasona- ble person standard as applied to the facts of a case.71
D. Strict and Product Liability
While negligence governs virtually all accidents, there are excep- tions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities such as hazardous waste disposal or blasting.72 Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy.73 Thus, a defendant cor-
65 The idea that negligence involves conduct that falls below an objective standard was first articulated by Baron Alderson in the case of Blyth v. Birmingham Waterworks Co.:
Negligence is the omission to do something which a reasonable man, guided upon those considerations which ordinarily regulate the conduct of human affairs, would do, or doing something which a prudent and reasonable man would not do. The defendants might have been liable for negligence, if, unintentionally, they omitted to do that which a reasonable person would have done, or did that which a person taking reasonable precautions would not have done.
(1856) 156 Eng. Rep. 1047, 1049; 11 Ex. 781, 784.
66 See The T.J. Hooper, 53 F.2d 107 (S.D.N.Y. 1931).
67 See Donoghue v. Stevenson [1932] AC 562 (HL) (appeal taken from Scot.).
68 See Byrne v. Boadle (1863) 159 Eng. Rep. 299.
69 VETRI ET AL., supra note 53, at 12.
70 See James Barr Ames, Law and Morals, 22 HARV. L. REV. 97, 99 (1908).
71 VETRI ET AL., supra note 53, at 10.
72 Id. at 11.
73 See Frederick Pollock, Duties of Insuring Safety: The Rule in Rylands v. Fletcher, 2 L.Q.
REV. 52, 53 (1886). While early English common law imposed strict liability for certain wrongs such as trespass, Rylands v. Fletcher (1868) 3 LRE & I App. 330 (HL), was the progenitor of the doctrine of strict liability for abnormally dangerous activities, and its ruling had a major impact on the development of tort law. Pollock, supra, at 52, 59. In the case, Fletcher’s reservoir burst and flooded a neighboring mine run by Rylands through no fault of Fletcher. Id. at 53. This court held that “the person who for his own purposes brings on his lands and collects and keeps there,
 
14 509 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
poration that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defec- tive or because its properties are falsely represented.74 Product inju- ries cause upwards of 200 million injuries a year in the United States.75 In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products.76 The bulk of product liability cases involve claims for damages against a manufac- turer or retailer by a person injured while using a product.77 Typically, a plaintiff will try to prove that an injury was the result of some inher- ent defect of a product or its marketing and that the product was flawed or falsely advertised.78 Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and ac- curately marketed.79 Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product or that something other than the product caused the harm.80
Product liability was not always governed by strict liability. Origi- nally, American courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise.81 Toward the end of the nineteenth century, however, states began increasingly em- ploying the doctrine of caveat venditor and an implied warranty of merchantable quality.82 Under this doctrine, “[s]elling for a sound price raises an implied warranty that the thing sold is free from de-
anything likely to do mischief if it escapes, must keep it in at his peril, and, if he does not do so, is prima facie answerable for all the damage which is the natural consequence of its escape.” Id. at 54. Critics of the case objected to its potential impact on economic activity. See, e.g., THOMAS C. GREY, FORMALISM AND PRAGMATISM IN AMERICAN LAW 248 (2014) (noting that many “prestig- ious judges and commentators” repudiated Rylands on the basis that “liberal principles of formal equality and economic freedom, or a devotion to economic development, required rejection of tort liability without fault”).
74 DAVID G. OWEN, PRODUCTS LIABILITY LAW 1 (3d ed. 2014).
75 Id. at 1.
76 Id. at 3.
77 Id.
78 Id.
79 Id.
80 Id.
81 Id. at 17–18.
82 Id. at 18.
 
2018] 510 THE REASONABLE COMPUTER 15
fects, known and unknown (to the seller).”83 Ultimately, the doctrine of implied warranty of merchantable quality was reduced to statutory form in the Uniform Sales Act of 1906.84 Yet even so, manufacturers were in large part able to avoid liability for defective products by ar- guing they lacked privity of contract with consumers.85 This was possi- ble because in most cases consumers purchased products from third- party retailers rather than directly from manufacturers.86
That changed in 1916 with the New York Court of Appeals deci- sion in MacPherson v. Buick Motor Co.87 The case involved a motorist who was injured when one of the wooden wheels of his Buick col- lapsed.88 He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if the manufacturer of such a foreseeably dangerous product knows that it “will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”89 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted MacPherson’s holding.90 This shift was ac- companied by growing public support for consumer protection to- gether with the understanding that liability would not unduly burden economic activity.91 Businesses are often in the best position to pre- vent product injuries and can distribute liability through insurance.92
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products, Inc.,93 which held that manufacturers of defec- tive products are strictly liable for injuries caused by such products.94
83 Id. (quoting S. Iron & Equip. Co. v. Bamberg, E. & W. Ry. Co., 149 S.E. 271, 278 (S.C. 1929)).
84 Id.; see U.C.C. § 2-314 (AM. LAW INST. & UNIF. LAW COMM’N 2014). See generally Friedrich Kessler, The Protection of the Consumer Under Modern Sales Law, Part 1, 74 YALE L.J. 262 (1964).
85 OWEN, supra note 74, at 18.
86 Id.
87 111 N.E. 1050 (N.Y. 1916).
88 Id. at 1051.
89 Id. at 1053.
90 OWEN, supra note 74, at 22. Maine was the last state to abolish the privity requirement
in negligence actions in 1982. Id.
91 See id. at 22–23.
92 See id.
93 377 P.2d 897 (Cal. 1963) (in bank).
94 Id. at 900. Of note, Justice Roger Traynor, who wrote the majority opinion in the case,
had suggested this strict liability rule nineteen years earlier in a concurring opinion in Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436 (Cal. 1944). He argued responsibility should “be
 
16 511 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
This case represents the birth of modern products liability law in America.95 After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s, with the American Law Institute memorializing the rule in Section 402A of the Restatement (Second) of Torts.96
Of course, today’s products liability law is not as simple as this brief narrative suggests.97 It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts.98 Since the 1960s, a variety of state statutes have attempted to reform prod- ucts liability law, often to limit the rights of consumers in order to protect manufacturers.99 For our purposes, however, it suffices to say that as a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.100
II. COMPUTER-GENERATED TORTS
A. Automation Will Prevent Accidents
On May 7, 2016, a Tesla driver was killed in the first known fatal crash of a self-driving car.101 Tesla reported that the autopilot system
fixed wherever it will most effectively reduce the hazards to life and health inherent in defective products that reach the market.” Id. at 440 (Traynor, J., concurring). A few years before this case, the Supreme Court of New Jersey found manufacturers strictly liable in warrantee to re- mote consumers in Henningsen v. Bloomfield Motors, Inc., 161 A.2d 69, 77, 84 (N.J. 1960).
95 OWEN, supra note 74, at 23.
96 RESTATEMENT (SECOND) OF TORTS § 402A (AM. LAW INST. 1965); see OWEN, supra note 74, at 23.
97 For a more comprehensive view on products liability, the American Law Institute pub- lished a Restatement specifically on products liability in 1998. RESTATEMENT (THIRD) OF TORTS: PRODUCTS LIABILITY (AM. LAW INST. 1998).
98 OWEN, supra note 74, at 4.
99 Id. at 23.
100 See Vandermark v. Ford Motor Co., 391 P.2d 168, 171–72 (Cal. 1964) (in bank) (“Retail-
ers like manufacturers are engaged in the business of distributing goods to the public. They are an integral part of the overall producing and marketing enterprise that should bear the cost of injuries resulting from defective products. In some cases the retailer may be the only member of that enterprise reasonably available to the injured plaintiff. In other cases the retailer himself may play a substantial part in insuring that the product is safe or may be in a position to exert pressure on the manufacturer to that end; the retailer’s strict liability thus serves as an added incentive to safety. Strict liability on the manufacturer and retailer alike affords maximum pro- tection to the injured plaintiff and works no injustice to the defendants, for they can adjust the costs of such protection between them in the course of their continuing business relationship.” (citation omitted)).
101 Sam Levin & Nicky Woolf, Tesla Driver Killed While Using Autopilot Was Watching Harry Potter, Witness Says, GUARDIAN (July 1, 2016, 1:43 PM), https://www.theguardian.com/
 
2018] 512 THE REASONABLE COMPUTER 17
did not apply the brakes after the car’s sensor system failed to detect an eighteen-wheel truck and trailer.102 The car attempted to drive full speed under the trailer and the bottom of the trailer impacted the car’s windshield.103 The driver, whom Tesla claims should have re- mained alert and who also failed to apply the brakes, may have been watching a Harry Potter movie at the time.104
Surveys of attitudes toward self-driving cars have produced mixed results, but they have often uncovered negative opinions.105 A survey by the American Automobile Association in March 2016 re- ported that three out of four U.S. drivers surveyed said they would feel “afraid” to ride in a self-driving car.106 Only one in five said they would trust a driverless car to drive itself while they were inside.107 Another recent survey found that most U.K. citizens would feel un- comfortable with self-driving vehicles on the road, and more than
technology/2016/jul/01/tesla-driver-killed-autopilot-self-driving-car-harry-potter; see Anjali Singhvi & Karl Russell, Inside the Self-Driving Tesla Fatal Accident, N.Y. TIMES (July 12, 2016), http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html?_r=0. This has been the first reported fatality, but not the only reported crash for which a self-driving vehicle has been at fault. See, e.g., Tan Weizhen, Self-Driving Car in Accident with Lorry at One-North, TODAY (Oct. 18, 2016), http://www.todayonline.com/singapore/self-driving-car-involved-acci- dent-one-north. Other, nonfatal accidents have been attributed to self-driving vehicles. See Dave Lee, Google Self-Driving Car Hits a Bus, BBC NEWS (Feb. 29, 2016), http://www.bbc.co.uk/news/ technology-35692845. The National Highway Traffic Safety Administration (“NHTSA”) investi- gated this accident and issued a report in January 2017 stating that “[a] safety-related defect trend has not been identified at this time and further examination of this issue does not appear to be warranted.” NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., INVESTI- GATION PE 16-007 (2017), https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF. The NHTSA found the accident was beyond the capabilities of the vehicle’s Autopilot and Auto- matic Emergency Breaking systems. Id. The report went on to state that overall crash rates decreased by nearly forty percent after installation of Tesla’s Autosteer technology. Id. at 10.
102 Levin & Woolf, supra note 101.
103 Id.
104 Id.
105 Similarly, a poll of 1869 registered voters in January 2016 by Morning Consult found
that forty-three percent of registered voters said self-driving cars were unsafe, while only thirty- two percent said they were safe. Amir Nasr & Fawn Johnson, Voters Aren’t Ready for Driverless Cars, Poll Shows, MORNING CONSULT (Feb. 8, 2016), https://morningconsult.com/2016/02/08/vot- ers-arent-ready-for-driverless-cars-poll-shows/. Fifty-one percent of respondents said they would not ride in a driverless car, while twenty-five percent said they would. Id.; see Paul Lienert, Tesla Crash Does Little to Sway Public Opinion on Self-Driving Cars, AUTOMOTIVE NEWS (July 29, 2016, 2:21 PM), http://www.autonews.com/article/20160729/OEM06/160729812/tesla-crash-does- little-to-sway-public-opinion-on-self-driving-cars (discussing the results of other surveys).
106 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM (Mar. 1, 2016), http://newsroom.aaa.com/2016/03/three-quarters-of-ameri- cans-afraid-to-ride-in-a-self-driving-vehicle/.
 107 Id.

18 513 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
three-quarters would want to retain a steering wheel.108 Regulators are more optimistic than the public, but they are still cautious.109 Until very recently, California required human drivers to be present in all self-driving cars being tested on public roads.110 Two laws passed in 2016, however, now permit unmanned vehicles to operate on public roads under certain circumstances.111
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether computers are perfect (they are not), but whether they are safer than people (they are). Nearly all crashes involve human error.112 A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs.113 The U.S. Department of Transportation reports that more than 35,000 people died from motor vehicle accidents in the United States in 2015.114 It estimates the economic costs of those accidents at over $240 billion.115
By contrast, the Tesla fatality was the first known autopilot death in about 130 million miles driven by the system.116 It is also important to note that driverless technologies are in their infancy. Imagine how improved such technologies will be in ten years. One academic expert predicted in September 2016 that self-driving cars will be ten times safer than human drivers in three years, and one hundred times safer in ten years.117 At the point where automated cars are ten times safer
108 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, INQUIRER (Oct. 17, 2016), http://www.theinquirer.net/inquirer/news/2474351/over-half-of-brits- wont-feel-safe-using-the-streets-with-driverless-cars.
109 This caution is reflected, for example, in guidelines released in September 2016 by the Department of Transportation for safe design, development, and testing of self-driving cars. NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., FEDERAL AUTOMATED VE- HICLES POLICY: ACCELERATING THE NEXT REVOLUTION IN ROADWAY SAFETY 5–7 (2016), https://www.transportation.gov/sites/dot.gov/files/docs/AV%20policy%20guidance%20PDF.pdf.
110 Susmita Baral, Driverless Car Laws in California Get Major Changes in September, INT’L BUS. TIMES (Oct. 3, 2016, 5:40 PM), http://www.ibtimes.com/driverless-car-laws-california- get-major-changes-september-2425689.
111 Id.
112 NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., supra note 109, at 5.
113 ALEXANDER HARS, TOP MISCONCEPTIONS OF AUTONOMOUS CARS AND SELF-DRIVING
VEHICLES 1, 6 (2016), http://www.inventivio.com/innovationbriefs/2016-09/Top-misconceptions- of-self-driving-cars.pdf.
114 General Statistics, supra note 23.
115 Id.
116 A Tragic Loss, TESLA (June 30, 2016), https://www.tesla.com/en_GB/blog/tragic-loss
[https://perma.cc/LZ8X-UW2F].
117 Michael Belfiore, Self-Driving Cars Will Be 10x Safer Than Human Drivers in 3 Years,
MICHAEL BELFIORE BLOG (Sept. 20, 2016), http://michaelbelfiore.com/2016/09/20/self-driving- cars-will-be-10x-safer-than-human-drivers-in-3-years/ [https://perma.cc/4T78-CEWD]. Similarly,
 
2018] 514 THE REASONABLE COMPUTER 19
than human drivers, that could reduce the annual number of motor vehicle fatalities to about 3500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicted au- tonomous vehicles would reduce the number of auto deaths by about 30,000 a year.118 However, the report estimated that self-driving tech- nologies would not be adopted widely enough to permit this outcome until the middle of the century.119
B. Tort Liability Discourages Automation
To see why tort law discourages automation, it is important to look at the question of when it makes economic sense for a business to replace a human operator with a machine operator. In practice, it might be complex to calculate the cost of each operator. Human em- ployees have costs in excess of their salaries and wages, such as tax liability for employer portions of Social Security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; em- ployer portions of health insurance; paid holidays, vacations, and sick days; contributions toward retirement, pension, savings, and profit- sharing plans, etc.120 Computer costs may be simpler to estimate, but they may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision.121 For example, a person may require vocational training or be unable to work due to sickness; a computer may require software updates or be unable to work due to malfunction. Human operators may result in greater expenses for le- gal fees, administrative and overhead costs, as well as compliance with regulatory and employment requirements.122 Automation may provide
Bob Lutz, former General Motors (“GM”) vice chairman, predicted that GM’s first autonomous cars would have an accident rate about ten percent of those of human drivers. Michelle Fox, Self- Driving Cars Safer than Those Driven by Humans: Bob Lutz, CNBC (Sept. 8, 2014, 3:30 PM), http://www.cnbc.com/2014/09/08/self-driving-cars-safer-than-those-driven-by-humans-bob-lutz. html.
118 Bertoncello & Wee, supra note 25.
119 Id.
120 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level,
145 TAX NOTES 117, 118 (2014); see also WAYNE F. CASCIO, COSTING HUMAN RESOURCES (4th ed. 2000).
121 See ALFRED MARSHALL, PRINCIPLES OF ECONOMICS 368, 376 (8th ed. 1920).
122 See Cost of Small Business Employment, CTR. FOR ECON. & BUS. RES., www.cebr.com/
reports/cost-of-small-business-employment/ [https://perma.cc/V3F6-USE6].
 
20 515 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
tax benefits,123 but may infringe patents or result in negative public- ity.124 Whether to staff with a person or a machine may also take into account broader social policies. For instance, automation may pro- mote income inequality and unemployment. But businesses are re- quired to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to pro- mote social responsibility.125
The decision of whether to employ a computer or human opera- tor, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making—estimating uncertain future costs relatively accurately and making decisions as rational economic actors.126 Tort liability will only be one factor to consider when deciding whether to employ a computer or human op- erator. But, in the aggregate, tort liability will influence automation.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, businesses may not be directly liable for harms caused by autonomous computers.127 The computer’s manufacturer and other members of the supply chain will generally be liable. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability, for instance, by relying on independent contrac-
 123 See Abbott & Bogenschneider, supra note 32.
124 See, e.g., Kate Taylor, McDonald’s Ex-CEO Just Revealed a Terrifying Reality for Fast-
Food Workers, BUS. INSIDER (May 25, 2016, 10:05 AM), http://www.businessinsider.com/ mcdonalds-ex-ceo-takes-on-minimum-wage-2016-5 (discussing criticism of McDonald’s for re- placing workers with machines).
125 See generally Dodge v. Ford Motor Co., 170 N.W. 668, 682–84 (Mich. 1919). Of course, many companies argue they promote corporate social responsibility, and in some circumstances, there may be a business case for doing so. See, e.g., Archie B. Carroll & Kareem M. Shabana, The Business Case for Corporate Social Responsibility: A Review of Concepts, Research and Practice, 12 INT’L J. MGMT. REVS. 85 (2010).
126 See, e.g., Hugh Courtney, Jane Kirkland & Patrick Viguerie, Strategy Under Uncertainty, HARV. BUS. REV., Nov.–Dec. 1997.
127 See Mark A. Chinen, The Co-Evolution of Autonomous Machines and Legal Responsi- bility, 20 VA. J.L. & TECH. 338, 347–48 (2016).

2018] 516 THE REASONABLE COMPUTER 21
tors.128 Businesses are not usually liable for negligent harms caused by their independent contractors.129
Yet even in cases where liability rests with a business’s supplier or an independent contractor, such liability may indirectly impact a busi- ness. A manufacturer or retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent con- tractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product.130 Yet the fact that tort liability may be indirect and complex or that firms may purchase insurance to manage risk does not change the fact that tort liability has a financial cost which influences behavior.
Leaving aside tort liability, if both operators cost a business the same amount to employ, the decision of whether to utilize a person or computer should be neutral. If a business introduces the variable of tort liability into the decision, a human operator would be preferred. Harms caused by a person will be evaluated in negligence, but the same harms caused by a computer will be evaluated in strict liability. It is easier to establish strict liability than negligence.131 Strict liability does not require careless manufacturer behavior, only that a defect be present in a product.132 At least with regard to tort liability, the law
128 See, e.g., Kleeman v. Rheingold, 614 N.E.2d 712, 715 (N.Y. 1993). There are, however, limits on the extent to which businesses can rely on independent contractors or attempt to clas- sify employees as independent contractors. See, e.g., In re Morton, 30 N.E.2d 369, 371 (N.Y. 1940). As another example of how business can avoid tort liability for the actions of human operators, employers are not generally liable for intentional torts committed by employees. See, e.g., Ocana v. Am. Furniture Co., 91 P.3d 58, 70–71 (N.M. 2004).
129 See Kleeman, 614 N.E.2d at 715.
130 See generally RBB ECONOMICS, COST PASS-THROUGH: THEORY, MEASUREMENT, AND
POTENTIAL POLICY IMPLICATIONS (2014).
131 See Cronin v. J.B.E. Olson Corp., 501 P.2d 1153, 1162 (Cal. 1972) (in bank) (“[T]he very purpose of our pioneering efforts in [strict product liability] was to relieve the plaintiff from problems of proof inherent in pursuing negligence and warranty remedies, and thereby ‘to insure that the costs of injuries resulting from defective products are borne by the manufacturers . . . .’” (ellipsis in original) (citations omitted) (quoting Greenman v. Yuba Power Prods., Inc., 377 P.2d 897, 901 (Cal. 1963))); see also Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436, 441 (Cal. 1944) (Traynor, J., concurring) (“It is to the public interest to discourage the marketing of products having defects that are a menace to the public. If such products nevertheless find their way into the market it is to the public interest to place the responsibility for whatever injury they may cause upon the manufacturer, who, even if he is not negligent in the manufacture of the product, is responsible for its reaching the market. However intermittently such injuries may occur and however haphazardly they may strike, the risk of their occurrence is a constant risk and a general one. Against such a risk there should be general and constant protection and the manufacturer is best situated to afford such protection.”).
 132 See Cronin, 501 P.2d at 1162.

22 517 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
thus favors people over machines. This will hold true as long as com- puters are treated as “ordinary products” as to which strict liability is the default rule.
C. Computer-Generated Torts Should Be Negligence Based
Holding computer-generated torts to a negligence standard will result in an improved outcome: it will accelerate the adoption of auto- mation where doing so would reduce accidents. Of course, moving from a strict liability to a negligence standard would have some draw- backs. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to insure against loss. Indeed, this is why courts initially adopted strict product liability.133 Computer-gen- erated torts, however, differ from other product harms in that—once machines become safer than people—automation will result in net safety gains.
To illustrate this, imagine that with current technology a com- puter driver would be ten times safer than a human driver. In this case, it would be better that one human driver is replaced by a ma- chine than that the same machine becomes 100 times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for a computer driver (model C-A) is one fatality per 1 billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now imagine that we are able to improve C-A an additional ten- fold such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of 10 billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, how- ever, instead of focusing our efforts on improving C-A we simply re- place the human driver with another C-A, then over the course of 10 billion miles driven by C-A & C-A there will be a total of 20 fatalities. Once computers become safer than people, and particularly once computers become substantially safer than people, very significant re- ductions in accident rates will be gained by automation. Therefore—at some point—it is preferable to weaken the incentive to gain incremen-
 133 See, e.g., Greenman, 377 P.2d at 901.

2018] 518 THE REASONABLE COMPUTER 23
tal improvements in product safety to increase the adoption of safer technologies.
Also, even under a negligence standard, manufacturers will be incentivized to improve the safety of their computer systems because they may still be liable for accidents. Manufacturers will likely have the best information available to determine whether it would be bet- ter to pay to further reduce accident risks, e.g., whether an additional $10,000 per vehicle is worth a one percent reduction in accident risk, or whether to pay claims for additional accidents. Higher safety levels are not always better; inefficiently high safety levels may result in pro- hibitively high prices for consumers.134 To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, non-tort mechanisms could be brought to bear, such as regula- tory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.135
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and “computer tortfeasors” like Tesla’s autonomous driving software. Society’s rela- tionship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, computers are stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from a computer tortfeasor in this system are the concepts of independence and con- trol. Autonomous computers, robots, or machines are given tasks to complete, but they determine for themselves the means of completing those tasks.136 In some instances, machine learning can generate un- predictable behavior such that the means are not predictable either by those giving tasks to computers or even by the computer’s original programmers.137 But the difference between ordinary products and
134 David G. Owen, Rethinking the Policies of Strict Products Liability, 33 VAND. L. REV. 681, 710 (1980).
135 Id. at 694.
136 Curtis E.A. Karnow, The Application of Traditional Tort Theory to Embodied Machine
Intelligence, in ROBOT LAW 51, 52 (Ryan Calo et al. eds., 2016).
137 Id. Unlike Karnow, the author does not agree that the relevant distinction between autonomous and nonautonomous machines should be the degree to which they are unpredict- able. See id. at 55. Tort law should pursue functional solutions, and for the purposes of accident reduction, it should not matter whether a self-driving car operates per expert rules or per unpre-
 
24 519 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
autonomous computers should not be based on predictability, only on social and practical outcomes.138 It makes no difference to a person run over by a self-driving car what type of computer was operating the vehicle. Whether a computer acts according to fixed or expert rules created by a programmer or more complex machine-learning algo- rithms such as neural networks that generate new and sometimes un- foreseen behaviors, the physical outcome is the same.139 Leaving aside difficulties with courts attempting to distinguish between different types of computer architecture, ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.
D. Computer-Generated Torts as a Type of Machine Injury
Not all machine injuries would be computer-generated torts. To illustrate, consider two hypothetical accidents:
1) A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop off.
2) A crane operator is manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and one may assume) that the operator is at fault (al- though a creative plaintiff’s attorney might argue the crane was negli- gently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a fac- tual cause of the injury in torts vernacular, the machine did not inter- rupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an exten- sion of the operator, in the same way that the operator could commit a battery by throwing a rock at another person.140 In the second hypo- thetical, allocating fault is once again intuitively obvious. The machine
dictable machine-learning algorithms. See Abbott, supra note 19, at 1109 (arguing in the patent context that it would be impossible or impractical to distinguish between different computer architectures for determining whether a computer qualifies as an inventor and that the distinc- tion is irrelevant to promoting innovation).
138 Cf., e.g., David C. Vladeck, Machines Without Principals: Liability Rules and Artificial Intelligence, 89 WASH. L. REV. 117, 127 (2014) (arguing different liability rules may need to apply to injuries caused by computers that cannot be traced to a “design, manufacturing, or programming defect”).
139 See, e.g., Jack M. Balkin, The Path of Robotics Law, 6 CALIF. L. REV. CIR. 45, 45–46 (2015) (arguing against a focus on formalism and essentialism in the law).
140 See, e.g., R v. Day (1845) 1 Cox 207, 208 (holding that slashing a victim’s clothing with a knife constitutes battery).
 
2018] 520 THE REASONABLE COMPUTER 25
is at fault rather than the operator. The operator acted with reasona- ble care, and the injury was due to (one may assume) a flawed crane. These two scenarios would result in very different liability out- comes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby even if the manufacturer had exercised the utmost care in the design and con-
struction of the crane.
In both scenarios, an operator is using a crane in much the same
way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dy- namic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza Pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.141
Now imagine a third scenario:
3) A computer-operated, unmanned crane drops a steel frame on a passerby after incorrectly identifying the location for drop off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a ma- chine is being used as a tool. In Example 3, a computer has stepped into the shoes of the worker; it has replaced a person, and it is per- forming in essentially the same manner as a person. If the computer were a person, the computer would be liable in negligence and held to the standard of a reasonable person.142
Holding computer tortfeasors to a negligence standard requires rules for distinguishing between computer-generated torts and other
141 See J.J. Coulton, Lifting in Early Greek Architecture, 94 J. HELLENIC STUD. 1, 1, 12, 15–17 (1974).
142 The author has previously argued for a similar rule in the intellectual property context, where he proposed that computers should be recognized as authors and inventors if they inde- pendently perform creative acts. See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 187, 187 (Cassidy R. Sugimoto et al. eds., 2016); Abbott, supra note 19, at 1081. This rule would generate innovation by creating financial incentives for developing creative computers. See Abbott, supra; Abbott, supra note 19, at 1081.
 
26 521 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is a “computer tortfeasor” (Example 3).
Computer-generated torts could be those cases in which an au- tonomous computer occupies the position of a reasonable person in the negligence calculus and where automating promotes safety. It is only beneficial to encourage automation when doing so would reduce accidents. It would be harmful to encourage automation while human drivers outperform self-driving cars (though, it might still be beneficial to encourage automation for a subset of cases, for instance, the class of bad drivers). To shift from strict liability to negligence, manufactur- ers should have the burden to show by a preponderance of the evi- dence that a computer tortfeasor is safer on average than a person.
E. Implementation
Automation may occur on a more or less permanent basis, or it may be situational. For example, an autonomous vehicle may only permit machine control, or it may allow a person to switch between human and machine control. Where automation is all-or-nothing, the relevant inquiry should be whether a specific instance of automation would be expected to result in a net reduction in accidents, rather than to reduce the risk of the specific harm that occurred. For instance, if self-driving cars were better than people at avoiding collisions with other vehicles, but worse at avoiding collisions specifically with white cars, a computer driver might decrease the overall risk of accidents but increase the risk of colliding with white cars. In a case involving a collision with a white car, a negligence standard should still apply. Better that there should be more collisions with white cars so long as there are fewer collisions in total (assuming collisions with white cars do not result in disproportionate harm).
Even where automation is situational, it makes sense to apply a negligence standard. Hypothetically, if a self-driving car is on average ten times safer than a person, but only half as safe as a person in rainy conditions, a person should rely on autonomous driving software most of the time but operate the vehicle conventionally in the rain. If some- one instead uses self-driving software in the rain, the computer should still be evaluated under a negligence standard. It may be difficult for a user to know in advance what circumstances an autonomous computer is likely to encounter as well as when an autonomous computer will outperform a person. In addition, the manufacturer—as the liable

2018] 522 THE REASONABLE COMPUTER 27
party—may not have input into how its computers are used situation- ally. Manufacturers could utilize non-tort mechanisms to prevent un- safe uses, such as by warning users that self-driving cars may not be operated in the rain or by building in technological safeguards to pre- vent self-driving cars from operating in the rain. If self-driving cars prove to be less safe than human drivers in the rain, it is likely manu- facturers would still be liable for accidents in negligence.
Similarly, software used to diagnose disease based on medical imaging may outperform physicians generally, but underperform at detecting certain diseases. Ideally, this might result in human-machine collaborative review of imaging. If a machine were to underperform detecting lung cancer, for example, it should still be evaluated in negli- gence for its failures. The computer will likely be liable if a physician should have detected the lung cancer. In instances where a computer is generally safer than a person but underperforms in a certain area, it is likely to be liable in negligence when underperforming. This retains the ex ante incentive to improve an autonomous computer to reduce accidents and still allows victims to be compensated.
The basic inquiry about automation safety should focus on whether automation reduces, or is expected to reduce, overall acci- dents, not whether it did in fact reduce accidents in a specific instance. If Tesla can prove its self-driving cars are more likely safer overall than human drivers, this should be sufficient to shift to negligence even in a case where a particular substitution of a human driver with a self-driving car results in more accidents. Better that there should be fewer accidents in total even if one normal self-driving car gets in more accidents than the class average.
This new standard might sometimes involve complex problems of proof. A manufacturer would have the initial burden to prove its com- puters are safer than people, which creates an incentive to misrepre- sent a computer’s safety.143 Even when manufacturers are acting in good faith, it may be difficult to determine whether a computer is safer than a person. Research conducted to the highest scientific stan- dards sometimes fails to accurately predict real-world outcomes.144 It may be that Tesla has reason to believe its self-driving cars are signifi- cantly safer than human drivers, but once its cars enter the market-
143 Ryan Abbott, Big Data and Pharmacovigilance: Using Health Information Exchanges to Revolutionize Drug Safety, 99 IOWA L. REV. 225, 232–37 (2013) (discussing differences between premarket and postmarket data for evaluating safety in the pharmaceutical context and the in- centive for manufacturers to misrepresent safety profiles).
 144 Id.

28 523 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
place, they fail to meet expectations. For instance, Tesla’s research might fail to consider the reactions of drivers to self-driving vehicles in states other than California.145 In practice, automation may turn out to be safer or more dangerous than initially predicted. Decisions often must be made based on incomplete information, and waiting for per- fect knowledge risks sacrificing probable benefits at the altar of precaution.146
Adversarial legal proceedings are well suited for resolving such factual issues, and plaintiffs could use those proceedings to challenge manufacturer claims of safety.147 Thus, if Tesla presents evidence that its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs show that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles, that should shift the standard back to strict liability. It is worth noting that postmarket data is not always superior to premarket data; sometimes premarket data may be more predictive of future outcomes, particularly where postmarket data is limited or skewed.148
145 For example, although Google’s self-driving vehicles have been involved in accidents, nearly all accidents involving these vehicles have been the fault of human drivers. Chris Ziegler, A Google Self-Driving Car Caused a Crash for the First Time, VERGE (Feb. 29, 2016, 1:50 PM), http://www.theverge.com/2016/2/29/11134344/google-self-driving-car-crash-report. Pre-2017 monthly reports of accidents involving Google’s self-driving cars were originally available on Google’s website. See Steve Kovach, Google Quietly Stopped Publishing Monthly Accident Re- ports for Its Self Driving Cars, BUS. INSIDER (Jan. 18, 2017, 6:32 PM), http:// www.businessinsider.com/waymo-ends-publishing-self-driving-car-accident-reports-website- 2017-1. However, in 2017 the Google Self-Driving Car Project rebranded as Waymo, and Waymo no longer publishes monthly accident reports. See id.
146 Ryan Abbott & Ian Ayres, Evidence and Extrapolation: Mechanisms for Regulating Off- Label Uses of Drugs and Devices, 64 DUKE. L.J. 377, 380 (2014).
147 See Abbott, supra note 143, at 266 (discussing benefits of adversarial dispute resolu- tion). Alternately, manufacturers could have a duty to evaluate the safety of automation technol- ogies before sale and an ongoing duty to monitor their postmarket performance. This could mean that instead of plaintiffs and defendants engaging in a “battle of the experts” focused on objective safety outcomes, a manufacturer’s good faith belief that its computers were safe would be sufficient to give rise to a negligence standard. Plaintiffs could only rebut the presumption that a manufacturer acted in good faith. Thus, Tesla would remain liable in negligence if it could prove its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs could prove that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles. Unless plaintiffs could prove Tesla knew, or should have known, that its initial predictions were not accurate or prove that Tesla failed to monitor the performance of its cars, Tesla would not be liable. But this would create a greater risk that manufacturers would fail to aggressively monitor, or that manufacturers would fail to monitor appropriately despite their best efforts. Better to base the standard on objective evidence of safety than a manufacturer’s subjective knowledge. Better also to empower plaintiffs’ attorneys to hold manufacturers to account than to put foxes in charge of guarding henhouses.
148 See generally Ryan Abbott, The Sentinel Initiative as a Cultural Commons, in GOV- ERNING MEDICAL KNOWLEDGE COMMONS (Katherine J. Strandburg et al. eds., 2017), https://
 
2018] 524 THE REASONABLE COMPUTER 29
It should not be necessary for a computer tortfeasor to physically replace a human operator for negligence to apply. It should be suffi- cient that a computer is performing a task which a person could rea- sonably do. For example, if a new taxi company goes into business using a fleet of only self-driving vehicles, computers would not have replaced human operators, but they would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software, e.g., the engine, could not be reasona- bly substituted. A person could drive a taxi instead of a computer, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as a computer tortfeasor, but the other parts of the vehicle would not.
Once a manufacturer establishes that a computer tortfeasor is safer than a person, the negligence test should focus on whether the computer’s act was negligent, rather than whether the computer was negligently designed or marketed. Again, the computer is taking the place of a person in the traditional negligence paradigm, and this par- adigm would treat the computer more like a person than a product. It makes no difference to an accident victim what a computer was “thinking”; only how the computer acted.149 Accident victims have a right to demand careful conduct regardless of how well a computer tortfeasor may have been designed.150
Applying the above rules to the crane examples, Example 1 would result in human liability because the human operator acted carelessly and the crane did not interrupt a foreseeable chain of events. It would retain strict manufacturer liability for Example 2 be- cause a person could not reasonably be substituted for a crane. It would permit negligent manufacturer liability for Example 3 (because the computer was automating a task which a person could have per- formed), but only if the computer tortfeasor is on average safer than a human operator.
www.cambridge.org/core/books/governing-medical-knowledge-commons/sentinel-initiative-as-a- knowledge-commons/FE736CE30779C4FFE5BA740F2A0FBBFE/core-reader (discussing diffi- culties with using real-world data to predict safety outcomes in an example using the medication Dabigatran).
149 To appropriate criminal law terminology, we are interested in the actus reus rather than the mens rea. See generally DENNIS J. BAKER, TEXTBOOK OF CRIMINAL LAW 167 (3d ed. 2012) (explaining the concept of actus reus). There is no benefit to punishing computer tortfeasors for wrongful actions, even under civil law.
150 See Oliver Wendell Holmes, Lecture III: Torts—Trespass and Negligence, in 3 THE COL- LECTED WORKS OF JUSTICE HOLMES 154, 157–58 (Sheldon M. Novick ed., 1995).
 
30 525 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
In the context of automated driving, human drivers would be lia- ble for harms they cause due to their own driving decisions, while a manufacturer would be strictly liable for harms caused by defective machines that are not automating human functions (as would be the case for MacPherson’s Buick151), but manufacturers would be liable in negligence rather than strict liability for errors made by autonomous driving software if the software were proven safer on average than a person.
F. Financial Liability
Autonomy exists on a continuum. In practice, the divide between an ordinary product and an autonomous computer may not be clear cut. In the self-driving car context, for example, under one widely adopted framework, vehicles are categorized on a zero to five scale based on who does what, when.152 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction be- tween person and machine. When computers and people share deci- sionmaking, traditional principles of joint and several liability should apply.153 For instance, where a human driver and a computer driver are both at fault, as may be the case where a Tesla system fails to detect a truck while a human driver is watching a movie, both drivers could be liable for either the entire injury or in proportion to their wrongdoing.154
Whether in strict liability or negligence, computers could not be financially liable for their harms. Computers do not have property rights, are owned as chattel, and would not be influenced by the spec- ter of liability in the way a person might be influenced. For the pur- poses of financial liability, the computer’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product or where several parties are involved in the distribu-
151 See supra notes 87–89 and accompanying text.
152 See SAE INT’L, AUTOMATED DRIVING (2014) (on file with the Law Review) (describing
the SAE taxonomy).
153 See generally Richard W. Wright, The Logic and Fairness of Joint and Several Liability, 23 MEM. ST. U. L. REV. 45 (1992) (reviewing and analyzing the public policy debate over joint and several liability).
 154 Id. at 46.

2018] 526 THE REASONABLE COMPUTER 31
tion chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which Gen- eral Motors licenses and incorporates in its vehicles, and the vehicles are then leased by an independent retailer to Lyft. Common law liabil- ity rules could be altered by firms in the supply chain. That would be particularly likely to occur where manufacturers and retailers are large, sophisticated entities. For example, General Motors might in- demnify Apple, Delphi, and Lyft in return for more favorable licens- ing and leasing terms.
Alternately, the computer’s owner could be liable for its harms. That would be somewhat akin to treating computer tortfeasors as em- ployees and making owners liable under theories of vicarious liabil- ity.155 It is particularly easy to imagine owners purchasing insurance for harms caused by autonomous computers in the self-driving car context, where insurance policies may soon come with a rider (or dis- count) for autonomous software. Owner liability might further incen- tivize the production of autonomous computers given that manufacturers would have less liability, but it might reduce adoption because owners would be taking on that liability. These two effects might offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of computer tortfeasors, and because manufacturers are in the best position to im- prove product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self-driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars be- come significantly safer than human drivers, insurance rates will de- crease for self-driving cars and perhaps increase for human drivers.156 This should have a nudging effect on self-driving car adoption as fi- nancially sensitive individuals take auto premiums into account in de- ciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower pre- miums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsi- ble for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
155 See generally Fleming James, Jr., Vicarious Liability, 28 TUL. L. REV. 161 (1954).
156 See supra text accompanying notes 112–19.
 
32 527 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 G. Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the cre- ation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation.157 For exam- ple, incentives could take the form of grants for research and develop- ment,158 loans to build production facilities,159 enhanced intellectual property rights,160 prizes,161 preferential tax treatments,162 or govern- ment guarantees.163
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allow- ance Rebate System (“CARS”), better known as “cash for clunkers.”164 CARS provided consumers trading in old vehicles with
157 See generally Nancy Gallini & Suzanne Scotchmer, Intellectual Property: When Is It the Best Incentive System?, in 2 INNOVATION POLICY AND THE ECONOMY 51 (Adam B. Jaffe et al. eds., 2002).
158 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes De- bate, 92 TEX. L. REV. 303, 321 (2013) (discussing the role of government grants in innovation policy).
Today, direct federal R&D spending (which includes the very small amount cur- rently spent on prizes) is about $130–$140 billion per year—slightly more than half of which is defense-related. Many states also provide direct R&D support: in fiscal year 2009, states spent $3.6 billion on support for R&D at state universities and another $1.3 billion on other grants and facilities for in-state research.
Id. (footnote omitted).
159 See, e.g., Joe Stephens & Carol D. Leonnig, Solyndra: Politics Infused Obama Energy
Programs, WASH. POST (Dec. 25, 2011), https://www.washingtonpost.com/solyndra-politics-in fused-obama-energy-programs/2011/12/14/gIQA4HllHP_story.html?utm_term=.Bb171adb15da (providing background information on the billions in unexpected costs to taxpayers from contro- versial loans defaulted on by green technology programs).
160 See, e.g., Ryan Abbott, Treating the Health Care Crisis: Complementary and Alternative Medicine for PPACA, 14 DEPAUL J. HEALTH CARE L. 35, 62–98 (2011) (noting that pharmaceu- tical manufacturers can receive market exclusivity, extended patent terms, or even sui generis forms of intellectual property protection for preferred technologies).
161 See, e.g., Richard A. Posner, Intellectual Property: The Law and Economics Approach, 19 J. ECON. PERSP. 57, 58–59 (2005).
162 See, e.g., Nick Bloom et al., Do R&D Tax Credits Work? Evidence from a Panel of Countries 1979–1997, 85 J. PUB. ECON. 1, 2 (2002); Bronwyn Hall & John Van Reenen, How Effective Are Fiscal Incentives for R&D? A Review of the Evidence, 29 RES. POL’Y 449, 449 (2000).
163 See, e.g., Gunhild Berg & Michael Fuchs, Bank Financing of SMEs in Five Sub-Saharan African Countries: The Role of Competition, Innovation, and the Government (World Bank, Pol- icy Research Working Paper No. 6563, 2013).
164 TED GAYER & EMILY PARKER, CASH FOR CLUNKERS: AN EVALUATION OF THE CAR ALLOWANCE REBATE SYSTEM 1 (2013), https://www.brookings.edu/wp-content/uploads/2016/06/ cash_for_clunkers_evaluation_paper_gayer.pdf.
 
2018] 528 THE REASONABLE COMPUTER 33
vouchers of between $3500 and $4500 to purchase new cars.165 It was a nearly $3 billion U.S. federal program designed as a short-term eco- nomic stimulus and to benefit U.S. auto manufacturers.166 It was also intended to promote safer, cleaner, more fuel-efficient vehicles.167 Ul- timately, while critics dispute the effectiveness of the program at stim- ulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel efficiency and safety, and it was popular with consumers.168 In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alter- natives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that place caps on damages, limit contingency fees, eliminate joint and several liability, mandate periodic payments, or reduce the statute of limitations.169
Finally, the government could promote safety by means of regula- tion. This could involve requirements for industries to achieve mini- mum safety targets or direct requirements to adopt certain technologies.170 At the point where self-driving cars become ten or a
165 Id.
166 See id. at 1–2; $2 Billion More for Clunker Car Trade-Ins Passes Senate, N.Y. TIMES: CAUCUS (Aug. 6, 2009, 9:05 PM), https://thecaucus.blogs.nytimes.com/2009/08/06/2-billion-more- for-clunker-car-trade-ins-passes-senate/.
167 See GAYER & PARKER, supra note 164, at 1–2.
168 The Department of Transportation reported the program succeeded at boosting eco-
nomic growth and creating jobs. Press Release, Nat’l Highway Traffic Safety Admin., Secretary LaHood Touts Success of Cash for Clunkers; Responds to Reports by DOT Inspector General, GAO (Apr. 29, 2010), https://www.nhtsa.gov/press-releases/secretary-lahood-touts-success-cash- clunkers-responds-reports-dot-inspector-general. Others were less bullish. One study found that the total costs of the program outweighed the benefits by $1.4 billion. See Burton A. Abrams & George R. Parsons, Is CARS a Clunker?, ECONOMISTS’ VOICE, Aug. 2009, at 4. Another study argued that the program increased short-term spending, but decreased overall spending on new cars. Mark Hoekstra et al., Cash for Corollas: When Stimulus Reduces Spending 23 (Nat’l Bureau of Econ. Research, NBER Working Paper Series No. 20349, 2014), http://www.nber.org/papers/ w20349.pdf. With regard to fuel efficiency, one study found that the program improved the aver- age fuel economy of all vehicles purchased by 0.6 mpg in July 2009, and by 0.7 mpg in August 2009. MICHAEL SIVAK & BRANDON SCHOETTLE, U. MICH. TRANSP. RESEARCH INST., THE EF- FECT OF THE “CASH FOR CLUNKERS” PROGRAM ON THE OVERALL FUEL ECONOMY OF PUR- CHASED NEW VEHICLES 4 (2009), http://deepblue.lib.umich.edu/bitstream/2027.42/64025/1/ 102323.pdf.
169 These are some of the reforms created by the Medical Injury Compensation Reform Act of 1975 (“MICRA”) enacted by the California legislature to lower medical malpractice lia- bility insurance premiums. Cal. Civ. Code §§ 3333–3333.2 (West 2016).
170 See generally HEALTH & SAFETY EXEC., A GUIDE TO HEALTH AND SAFETY REGULA- TION IN GREAT BRITAIN 11 (2013), http://www.hse.gov.uk/pubns/hse49.pdf (outlining the occu-
 
34 529 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
hundred times safer than human drivers, nonautonomous driving could be prohibited.171 Regulatory solutions may be most appropriate where the benefits of automation are overwhelming and where it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer- and business-friendly so- lution. While consumers would have more difficulty seeking to re- cover for accidents, they would also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. For busi- nesses, it would lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not re- quire government funding, additional regulatory burdens on industry, or new administrative responsibilities. Additionally, it is an incremen- tal solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than more radical solutions.172 For all the above reasons, shifting to negli- gence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer cred- its, combined with direct regulations in certain instances.
Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable be- cause it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy.173 Indeed, automation
pational health and safety system in Great Britain and the various types of safety standards imposed on businesses).
171 See Stuart Dredge, Elon Musk: Self-Driving Cars Could Lead to Ban on Human Driv- ers, GUARDIAN (Mar. 18, 2015, 3:22 AM), https://www.theguardian.com/technology/2015/mar/18/ elon-musk-self-driving-cars-ban-human-drivers.
172 Indeed, some critics argued that CARS primarily subsidized Japanese auto manufactur- ers, while a similar Japanese stimulus program excluded American auto manufacturers. John Crawley, Japanese, Koreans Gain Most from Cash for Clunkers, REUTERS (Aug. 26, 2009, 5:34 PM), http://www.reuters.com/article/retire-us-usa-clunkers-sales-idUSTRE57P5C220090826; Douglas Stanglin, U.S. Cars Excluded from Japan’s Cash-for-Clunkers Program, USA TODAY (Dec. 11, 2009, 2:09 PM), http://content.usatoday.com/communities/ondeadline/post/2009/12/us- cars-excluded-from-japans-cash-for-clunkers-program-/1#.WDwOQXfc-t8.
173 See, e.g., Scherer, supra note 17, at 389–90 (discussing the reactionary nature of court proceedings); see also Bibb v. Navajo Freight Lines, Inc., 359 U.S. 520, 524 (1959) (“Policy deci- sions are for the . . . legislature . . . .”).
 
2018] 530 THE REASONABLE COMPUTER 35
to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could independently adopt these rules. Lawmakers would then have the option of modifying the common law.
III. THE REASONABLE ROBOT
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
—Oliver Wendell Holmes, Jr.174
A. When Negligence Is Strict
Negligence may function almost like strict liability for people with below average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.”175 This issue was at the heart of Vaughan v. Menlove176 in 1837, which con- cerned a defendant who lacked normal intelligence.177 The defense ar- gued that it would thus be unfair to hold him to the standard of an ordinary person and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, hold- ing that ordinary prudence should apply in every case of negligence.178 As Oliver Wendell Holmes, Jr., articulated in 1881, “The law consid- ers . . . what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”179 That re- mains the case today; a modern defendant cannot generally escape
174 O.W. HOLMES, JR., THE COMMON LAW 108 (1881).
175 Id. Holmes did distinguish between a lack of “intelligence and prudence” and “distinct
defect[s]” which he believed did not generally lead to strict liability. Id. at 108–10.
176 (1837) 132 Eng. Rep. 490, 492; 3 Bing. (N.C.) 468, 471.
177 Id. at 492.
178 Id. at 490, 492.
Instead, therefore, of saying that the liability for negligence should be co-extensive with the judgment of each individual, which would be as variable as the length of the foot of each individual, we ought rather to adhere to the rule which requires in all cases a regard to caution such as a man of ordinary prudence would observe. That was in substance the criterion presented to the jury in this case, and therefore the present rule must be discharged.
Id. at 493.
179 HOLMES, supra note 174, at 108.
 
36 531 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
liability for causing a motor vehicle accident because she has slow re- flexes, poor vision, or anxiety while driving.180
There are benefits to such a rule. Logistically, as Justice Tindal noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state.181 Better for administrative purposes to work with an external, objective stan- dard than to prove individual capacities and state of mind. Substan- tively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal con- duct of others.182 As Holmes articulated, damages caused by individu- als with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare, but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else perhaps “should refrain from operating an automobile” at all.183
B. The New Hasty and Awkward
Collectively, people are not the best drivers, even when they re- frain from drinking behind the wheel,184 falling asleep on the high- way,185 or colliding into police cars while playing Poke ́mon Go.186 But compared to computers? It will not be long until computers are safer than the average person and then safer than any human driver. Princi- ples of harm avoidance suggest that once it becomes practical to auto- mate, and once doing so is safer, a computer should become the new “reasonable person” or standard of care.
180 See, e.g., Roberts v. Ring, 173 N.W. 437, 437–38 (Minn. 1919).
181 Vaughan, 132 Eng. Rep. at 493.
182 See Holmes, supra note 150, at 154–55.
183 Roberts, 173 N.W. at 438. In this case, a seventy-seven-year-old defendant with defec-
tive sight and hearing was held liable for running over a seven-year-old boy when it was estab- lished that a reasonable driver could have stopped the car. Id.
184 See J. Michael Kennedy, Allowed in 26 States: Drinking and Driving: A Legal Mix, L.A. TIMES (Jan. 26, 1985), http://articles.latimes.com/1985-01-26/news/mn-13688_1_container-law (noting that until recently, it was even legal in many states to “sip[] on a Scotch and soda while cruising down the interstate”).
185 See David Boroff, Two Women Dead as Greyhound Bus Driver Falls Asleep at Wheel During California Crash; Driver was ‘Fatigued,’ Police Say, N.Y. DAILY NEWS (Jan. 19, 2016, 9:01 PM), http://www.nydailynews.com/news/national/greyhound-bus-crash-kills-2-injures-18-ar- ticle-1.2501658.
186 See Sarah Begley, Driver Hits Cop Car While Playing Poke ́mon Go. The Whole Thing Was Caught on Video, TIME (July 20, 2016), http://time.com/4414998/pokemon-go-hits-cop-body- cam/ (discussing a driver playing Poke ́mon Go who collided with a police car and had the inci- dent captured on video, and quoting the driver as saying, “That’s what I get for playing this dumb – game”).
 
2018] 532 THE REASONABLE COMPUTER 37
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defen- dant would be judged against what a computer would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an automated car would have prevented the injury. In fact, it may be that the automated vehicle is only able to prevent such an accident because it has superhuman abili- ties. It may have software capable of ultrafast decisionmaking, monitors that surpass human senses, and external cameras that ex- pand peripheral view beyond that of a person.187
With the reasonable person test, jurors are asked to put them- selves in the shoes of a reasonable person and decide what that person would have done.188 It may be a challenge for a juror to follow that reasoning in the case of a reasonable computer (or reasonable robot or machine). The reasonable computer, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasona- ble” in the context of a computer is an anthropomorphism to assist people conceptually. In fact, computers largely function according to fixed rules which—when all goes well—result in foreseeable behav- ior.189 Even those computers which can generate unpredictable behav- ior are still likely to be more predictable than people, particularly where such machines have been found to improve safety.190 It should be more or less possible to determine what a computer would have done in a particular situation.
To take a simple case, imagine an individual driving on dry pave- ment at forty miles per hour colliding with a child running into the road 150 feet ahead of the driver’s vehicle.191 To determine whether the driver is liable under the reasonable computer standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable computer would not have collided with
187 See supra notes 116–19 and accompanying text.
188 See supra notes 65, 71 and accompanying text.
189 THOMAS A. PETERS, COMPUTERIZED MONITORING AND ONLINE PRIVACY 97 (1999).
Malfunctioning computers would not be “reasonable” computers.
190 See id.
191 See Why Your Reaction Time Matters at Speed, NAT’L HIGHWAY TRAFFIC SAFETY AD-
MIN. (Aug. 2015), www.nhtsa.gov/nhtsa/Safety1nNum3ers/august2015/S1N_Aug15_Speeding_ 1.html.
 
38 533 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of self-driving software would be known, and the breaking distance could be standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software. Even in more com- plex cases, it should be easier to predict how a computer would have behaved than a person because computers are more predictable. Thus, it is possible to have a more objective test for the reasonable com- puter than for the reasonable person.
A defendant might argue that it is unfair for his best efforts to result in liability. A computer standard of care essentially makes peo- ple strictly liable for their accidental harms. That is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below- average driver due to computers. It may appear unfair to impose lia- bility on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or a computer.
Tort liability would not prohibit people from driving even at the point where computers become substantially safer than people. If that were a desired outcome it could be accomplished through command- and-control legislation.192 Instead, a computer standard of care would mean that people drive at their own risk. If a driver causes an acci- dent, he or she will be liable for the resultant damages. A tort-based incentive may be superior to an inflexible statutory mandate because there may be benefits to human driving unrelated to accidents, for instance, promoting freedom and autonomy.193 Individuals who partic- ularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, a computer standard of care is likely to have a significant impact on behavior. Making individ- uals and businesses essentially strictly liable for their harms will strongly discourage certain undertakings. In the self-driving car con- text, it would likely result in far fewer human drivers as insurance
192 See Orly Lobel, The Renew Deal: The Fall of Regulation and the Rise of Governance in Contemporary Legal Thought, 89 MINN. L. REV. 342, 371–404 (2004) (discussing the trend from regulations to incentive-based regimes).
193 See generally Ryan & Deci, supra note 20, at 6–7 (arguing that people have three basic psychological needs: (1) connectedness, (2) autonomy, and (3) feeling competent).
 
2018] 534 THE REASONABLE COMPUTER 39
rates for traditional vehicles become prohibitively expensive relative to rates for self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. Automatons may be prohibitively expensive or only available in limited quantities. That is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. There- fore, to introduce a computer standard of care, a plaintiff should have to show by a preponderance of the evidence that a person was per- forming a task that could be performed by a computer and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of a computer operator where 1) no such operator existed at the time of the accident, 2) no computer operator was available to the defendant, 3) a computer operator was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory require- ments for a human operator). If Tesla could manufacturer a com- pletely safe autonomous vehicle but at a cost of $1 million dollars, it would not be reasonable to require consumers to automate.
C. Reasonable People Use Autonomous Computers
As an alternative to the reasonable computer standard, the rea- sonable person could be a person using an autonomous computer. For example, once self-driving cars become safer than traditional vehicles, a jury might find that it is unreasonable to drive yourself rather than to use a self-driving car. Applying the “reasonable person using an autonomous computer” standard to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on her driving in the first place. A reasonable person would not have driven.
Under either the reasonable person or reasonable computer stan- dard, a human driver would be compared with a self-driving car, but in different ways. With the reasonable computer standard, courts would evaluate the human driver’s proximally harmful act, whereas with the reasonable person standard, courts would evaluate the human driver’s a priori decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable per- son standard would be more in line with the existing negligence re- gime, and it would be a less radical way to accomplish the goal of incentivizing automation to improve safety.

40 535 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
While keeping the reasonable person standard would be concep- tually easier, in practice it would be less desirable. The goal is to com- pare the harmful act of the person and computer, not to target the initial decision to automate. It is problematic to base liability on the decision to automate because it either must focus on the question of whether automation is generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform a machine. A situational focus must still compare the harmful act of a person versus a computer.
It is likely that as autonomous computers are introduced they will be safer at automating certain activities than others. For instance, au- tomated computers working to diagnose disease may be superior to physicians at detecting certain conditions, but not others. Self-driving cars may be safer than human drivers on average, but not safer than professional or above-average drivers. Autonomous vehicles may also be safer under most conditions, but might be relatively poor at, for example, driving off road. So, while automation may generally im- prove safety, optimal accident reduction may require a mix of com- puter and human activity.
Suppose a self-driving car is ten times safer than a human driver generally, but only half as safe as a human driver in icy conditions. Now suppose a human driver encounters a patch of black ice and causes an accident under circumstances in which she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable computer, she would es- cape liability if the computer would have been unable to avoid the accident (which is likely if the computer is half as safe in icy condi- tions). If the reasonable person using an autonomous computer test focuses on whether an autonomous computer is generally safer, how- ever, she would be liable. That test would conclude that it would have been unreasonable not to use a self-driving car because self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an autonomous com- puter evaluation could be situational. For instance, it could be reason- able not to use an autonomous computer, but only in icy conditions. However, this is just a more convoluted version of the reasonable computer test because it requires evaluating whether a computer would be safer than a person in a particular instance. That essentially asks how the computer would have acted in a situation—which is the

2018] 536 THE REASONABLE COMPUTER 41
reasonable computer standard.194 It would then require asking, based on that knowledge, which might be impractical for a person to have, whether an earlier decision to automate was reasonable. On top of that, it presupposes the ability to activate and deactivate automation as needed. In the black ice hypothetical, it could require the driver to know in advance of activating self-driving software whether there were icy conditions and how the computer would perform in icy con- ditions. It might require the driver to activate or deactivate automa- tion only during icy conditions or to understand whether the risk of using the computer in icy conditions outweighed the benefits of using the computer for other parts of the trip.
D. The Reasonable Computer Standard for Computer Tortfeasors
This Article proposes holding computer tortfeasors to a negli- gence standard and comparing their acts to the acts of a reasonable person after technology has advanced to the point that computers have been proven safer than people.195 It also proposes replacing the reasonable person standard with the reasonable computer standard, again, once this point has been reached.196 This means that computer tortfeasors would be held to the reasonable computer standard.
There will be instances in which it still makes sense to apply the reasonable person standard to computer tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of a computer, for instance, where auto- mation is prohibitively expensive or where computer operators are not widely available. We would not want to hold a computer tortfeasor to a higher standard than a human defendant. In some in- dustries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become customary or to meet the criteria proposed earlier for adopting the reasonable com- puter standard.
Eventually, once a reasonable computer becomes the standard of care, it would also be the standard for computer tortfeasors. For in- stance, if a self-driving Audi collided with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Volvo cars. There are a variety of ways to deter- mine the reasonable computer standard, for example, considering the industry customary, average, or safest technology. Under any stan-
194 See supra text accompanying notes 184–93.
195 See supra text accompanying notes 184–93.
196 See supra text accompanying notes 184–93.
 
42 537 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
dard, this is a different test than the current strict liability standard, in which the inquiry focuses on whether a product was defectively de- signed or its properties falsely represented.
As computers improve, the reasonable computer standard would grow stricter. That is alright, because once the reasonable computer is exponentially safer than a person, it is likely that computer tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable computer standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until computers are exponentially safer than people.
E. The Automation Problem
The impact of automation goes far beyond accident reduction. Just focusing on autonomous vehicles, the widespread adoption of this technology could have revolutionary benefits. It will allow people to be more productive and mobile, and it will reduce emissions and con- gestion.197 One autonomous vehicle could replace up to twelve normal cars.198 Given that the average automobile spends about ninety-five percent of its time sitting in place, self-driving cars may also eliminate the need for most parking.199 Getting rid of parking just in the United States would free up space the size of Connecticut and could allow redesigned, pedestrian-friendly urban areas.200 Automation will in- crease freedom for the disabled, blind, and unlicensed. It might elimi- nate traffic lights and the need for private car ownership.201 The net result of self-driving cars could be substantial environmental, eco- nomic, and social benefits.202
Driverless technologies may also result in the displacement of human workers, increased unemployment, greater wealth disparities, and a reduction of the tax base. Automation threatens the jobs of truck, bus, and taxi drivers who collectively make up about three per- cent of the working population.203 In other industries, automation has
197 DEP’T FOR TRANSPORT, THE PATHWAY TO DRIVERLESS CARS: SUMMARY REPORT AND ACTION PLAN 6 (2015).
198 Clive Thompson, No Parking Here, MOTHER JONES (Jan.–Feb. 2016), http:// www.motherjones.com/environment/2016/01/future-parking-self-driving-cars.
199 Id.
200 Id.
201 DEP’T FOR TRANSPORT, supra note 197, at 6.
202 Id.
203 RICHARD HENDERSON, INDUSTRY EMPLOYMENT AND OUTPUT PROJECTIONS TO 2024,
 
2018] 538 THE REASONABLE COMPUTER 43
resulted in reduced workforces.204 For instance, employment at com- puter and electronic companies decreased forty-five percent from 2001 to 2016.205 Employment at semiconductor makers decreased by half during the same period.206
These are all important issues to consider in formulating automa- tion policies, but tort law may not be the best mechanism to address these broader concerns.207 Ultimately, tort liability alone will not de- termine whether automation occurs. Consumer demand and the eco- nomics of automation will bring about increasing automation in the absence of laws prohibiting it.208 Tesla, for example, is planning to make all its cars self-driving, and Tesla is far from alone in automating vehicles.209 Billions of dollars have been invested in self-driving tech- nologies by at least forty-four corporations including Apple, Google, and General Motors.210
at 2 (2015); see AUSTL. BUREAU OF STATISTICS, 2011 CENSUS COMMUNITY PROFILES, http:// www.censusdata.abs.gov.au/census_services/getproduct/census/2011/communityprofile/0?open document&navpos=220 (last updated Jan. 12, 2017) (select “Working Population Profile”).
204 For example, WhatsApp had fifty-five employees when Facebook acquired it for $21.8 billion in 2014. Jon Swartz, Tech’s Gilded Glory Didn’t Mean Much to Trump’s Supporters, USA TODAY (Nov. 14, 2016), http://www.usatoday.com/story/tech/2016/11/14/techs-gilded-glory-didnt- mean-much-trumps-supporters/93598484/. Amazon, Tesla, and other companies have developed production lines that minimize the use of people. Id.
205 Id. 206 Id.
207 See, e.g., Priest, supra note 11, at 5–6.
208 See Brad Templeton, Robotaxi Economics, BRAD IDEAS (Sept. 8, 2016, 2:07 PM), http://
ideas.4brad.com/robotaxi-economics [https://perma.cc/T4JU-D866]; see also Who’s Self-Driving Your Car?, ECONOMIST (Sept. 22, 2016), http://www.economist.com/news/business/21707600-bat tle-driverless-cars-revs-up-whos-self-driving-your-car (noting a tight race between major tech- nology companies competing to make autonomous driving software due to financial expectations).
209 Tesla to Make All Its New Cars Self-Driving, BBC NEWS (Oct. 20, 2016), http:// www.bbc.co.uk/news/technology-37711489. Not all autonomous vehicles are created equal. A va- riety of technologies are in development to automate cars to a greater or lesser degree—ranging from driverless cars to self-parking vehicles. See generally SCIENCEWISE EXPERT RES. CTR., AU- TOMATED VEHICLES: WHAT THE PUBLIC THINKS (2014), http://www.sciencewise-erc.org.uk/cms/ assets/Uploads/Automated-Vehicles-Update-Jan-2015.pdf.
210 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehicles-corporations-list/ [https:// perma.cc/JM38-TR7D]; see Investment into Auto Tech on Pace to Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/blog/auto-tech-funding-h1-2016/ [https:// perma.cc/ZTE9-MH7E].
 
44 539 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 CONCLUSION
In the coming decades, as people and machines compete in an expanding array of activities, it is vital that appropriate legal and pol- icy frameworks be put in place to guide the development of technol- ogy and to ensure its widespread benefits.211 It is particularly important that tort liability be structured to optimize accident deterrence.
Technological advances present new challenges to existing frameworks. At some point in the future, there are likely to be few or no activities for which computers cannot outperform people.212 Self- driving cars may eventually be a thousand times safer than the best human driver.213 At some point, computers will cause so little harm that the economics of negligence versus strict liability will be irrele- vant. Autonomous computers will have become so ubiquitous that the constantly improving reasonable computer should be the benchmark for most or all areas of accident law. In fact, autonomous computers are likely to become so safe that regulatory mandates for automation will be desirable.
In the meantime, creating incentives for developing and adopting safer technologies could prevent countless accidents. It has become acceptable for more than a million people a year to die in traffic acci- dents worldwide, but only because there has not been a reasonable alternative until now.214 We could soon be living in a world where no one dies from unintended injury, or from medical error for that mat- ter. Once the third and fourth leading causes of death are eliminated, that would just leave us to deal with the leading two causes of death:
211 See, e.g., Press Release, European Parliament, Robots: Legal Affairs Committee Calls for EU-Wide Rules (Jan. 12, 2017), http://www.europarl.europa.eu/sides/getDoc.do?type=IM- PRESS&reference=20170110IPR57613&language=EN&format=XML (“EU rules for the fast- evolving field of robotics, to settle issues such as compliance with ethical standards and liability for accidents involving driverless cars, should be put forward by the EU Commission, urged the Legal Affairs Committee . . . .”).
212 See generally RAY KURZWEIL, THE SINGULARITY IS NEAR 7 (2005) (predicting that machines will be able to automate all human work in “a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed”).
213 See Dredge, supra note 171.
214 See Press Release, United Nations Secretary-General, Traffic Accidents Kill 1.3 Million
People Each Year, but with Commitment Roads Can Be Made Safer for All, Secretary General Says in Video Message (May 6, 2013), https://www.un.org/press/en/2013/sgsm15005.doc.htm [https://perma.cc/B2QQ-UN59].
 
2018] 540 THE REASONABLE COMPUTER 45 cardiovascular disease and cancer. Automation may eliminate those as
well.215
 215 See Abbott, supra note 19, at 1118 (hypothesizing about how artificial intelligence could cure cancer in an article about creative computers that are already independently generating patentable subject matter).

541
  Punishing Artificial Intelligence: Legal Fiction or Science Fiction
Ryan Abbott†* and Alex Sarch**
Whether causing flash crashes in financial markets, purchasing illegal drugs, or running over pedestrians, AI is increasingly engaging in activity that would be criminal for a natural person, or even an artificial person like a corporation. We argue that criminal law falls short in cases where an AI causes certain types of harm and there are no practically or legally identifiable upstream criminal actors. This Article explores potential solutions to this problem, focusing on holding AI directly criminally liable where it is acting autonomously and irreducibly. Conventional wisdom holds that punishing AI is incongruous with basic criminal law principles such as the capacity for culpability and the requirement of a guilty mind.
Drawing on analogies to corporate and strict criminal liability, as well as familiar imputation principles, we show how a coherent theoretical case can be constructed for AI punishment. AI punishment could result in general deterrence and expressive benefits, and it need not run afoul of negative limitations such as punishing in excess of culpability. Ultimately, however, punishing AI is not justified, because it might entail significant costs and it would certainly require radical legal changes. Modest changes to existing criminal laws that target persons, together with potentially expanded civil liability, are a better solution to AI crime.
TABLE OF CONTENTS
INTRODUCTION ................................................................................... 325
I.
†
*
** Alex Sarch, Reader (Associate Professor) in Legal Philosophy, University of Surrey School of Law. Thanks to Antony Duff, Sandra Marshall, Mark D’Souza, and Steve Bero for their insightful comments.
323
ARTIFICIAL INTELLIGENCE AND PUNISHMENT........................... 329 A. Introduction to Artificial Intelligence ................................. 329
Copyright © 2019 Ryan Abbott and Alex Sarch.
 Ryan Abbott, Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at University of California, Los Angeles.

542
  324
University of California, Davis [Vol. 53:323
A Framework for Understanding AI Crime ........................ 332 A Mainstream Theory of Punishment ................................. 337 1. Affirmative Reasons to Punish ................................... 338 2. Negative (Retributive) Limitations ............................ 341 3. Alternatives to Punishment........................................ 343 4. Putting the Pieces Together ....................................... 344
B. C.
II. THE AFFIRMATIVE CASE ........................................................... 344
A. Consequentialist Benefits ................................................... 344
B. Expressive Considerations ................................................. 346
III. RETRIBUTIVE AND CONCEPTUAL LIMITATIONS.......................... 349
A. The Eligibility Challenge ................................................... 349
1. Answer 1: Respondeat Superior ................................. 350
2. Answer 2: Strict Liability............................................ 352
3. Answer3:AFrameworkforDirectMensRea
Analysis for AI ............................................................ 354
B. Further Retributivist Challenges: Reducibility and
Spillover ............................................................................ 360 1. Reducibility................................................................361
2. Spillover ...................................................................... 362
C. Not Really Punishment?..................................................... 364 IV. FEASIBLE ALTERNATIVES ........................................................... 368
A. First Alternative: The Status Quo....................................... 369
1. WhattheAIcriminalgapisnot:reducibleharmful conduct by AI ............................................................. 369
2. WhattheAIcriminalgapis:irreduciblecriminal conduct by AI ............................................................. 373
B. The Costs of Punishing AI.................................................. 374
C. Second Alternative: Minimally Extending Criminal Law.... 378
D. Third Alternative: Moderate Changes to Civil Liability...... 381
E. Concluding Thoughts ......................................................... 383

543
  2019] Punishing Artificial Intelligence 325 INTRODUCTION
In 2015, an artist going by the moniker “Random Darknet Shopper” (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit.1 This was part of a performance project where RDS was given $100 in the cryptocurrency bitcoin each week to make a purchase from an online marketplace. The items were then shipped to a Swiss art gallery and put on exhibit. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.2
What makes this story interesting for our purposes is that RDS was an artificial intelligence (“AI”), and hardly the first to have a run-in with law enforcement.3 If RDS had been a natural person located in the United States, it could be criminally prosecuted under U.S. law.4 For that matter, entities involved in this activity other than RDS could also be criminally prosecuted, such as those supplying the bitcoin and hosting the exhibition.5 Luckily for RDS and crew, the Swiss authorities were art fans.6
Cases like this will pose new challenges, including for criminal law doctrine.7 The RDS case may be relatively straightforward, but programs exist that are autonomous, decentralized, and “unstoppable.”8 What if
1 Arjun Kharpal, Robot with $100 Bitcoin Buys Drugs, Gets Arrested, CNBC (Apr. 22, 2015, 5:09 AM), https://www.cnbc.com/2015/04/21/robot-with-100-bitcoin-buys- drugs-gets-arrested.html.
2 See id.
3 See Matt Novak, Was This the First Robot Ever Arrested?, GIZMODO (Feb. 18, 2014,
12:00 PM), https://paleofuture.gizmodo.com/was-this-the-first-robot-ever-arrested- 1524686968 (describing police confiscation in 1982 of a robot: “The police considered citing [its owner] for failing to obtain a permit for advertising . . . but no charges were filed and the robot was ultimately returned.”). Robot encounters with law enforcement are becoming more common. See, e.g., Peter Dockrill, A Robot Was Just ‘Arrested’ by Russian Police, SCI. ALERT (Sept. 20, 2016), https://www.sciencealert.com/a-robot-was- just-arrested-by-russian-police.
4 See 21 U.S.C. § 841(a)(1) (2019) (criminalizing distribution and possession with intent to distribute a controlled substance).
5 See 18 U.S.C. § 2(a) (2019) (criminalizing aiding and abetting offenses).
6 Random Darknet Shopper was eventually returned to its creators together with
all of the purchases except the Ecstasy. See Kharpal, supra note 1 (noting the prosecutor’s comment that “the possession of Ecstasy was indeed a reasonable means for the purpose of sparking public debate about questions related to the exhibition”). Apparently, the Hungarian passport was also returned. See id.
7 See Christopher Markou, We Could Soon Face a Robot Crimewave. . .The Law Needs to Be Ready, CONVERSATION (Apr. 11, 2017, 9:36 AM), https://theconversation. com/we-could-soon-face-a-robot-crimewave-the-law-needs-to-be-ready-75276.
8 See infra Part I.A (discussing The Decentralized Autonomous Organization (“The DAO”)).
 
544
  326 University of California, Davis [Vol. 53:323
RDS had been open source software that individuals from around the world independently helped program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike.9 Perhaps the best-known defender of punishing AI is Gabriel Hallevy. He contends that “[w]hen an AI entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”10 In his view, “[i]f all of its specific requirements are met, criminal liability may be imposed upon any entity — human, corporate or AI entity.”11 Drawing on the analogy to corporations,12 Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations and on AI entities.”13 “Modern times,” he contends, “warrant modern legal
9 See, e.g., Gabriel Hallevy, The Punishibility of Artificial Intelligence Technology, in LIABILITY FOR CRIMES INVOLVING ARTIFICIAL INTELLIGENCE SYSTEMS 185-229 (2014); J.K.C. Kingston, Artificial Intelligence and Legal Liability, in RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XXXIII: INCORPORATING APPLICATIONS AND INNOVATIONS IN INTELLIGENT SYSTEMS XXIV 269 (Max Bramer & Miltos Petridis eds., 2016), https://arxiv.org/pdf/1802.07782.pdf; Christina Mulligan, Revenge Against Robots, 69 S.C. L. REV. 579, 580 (2018); Jeffrey Wale & David Yuratich, Robot Law: What Happens If Intelligent Machines Commit Crimes?, CONVERSATION (July 1, 2015, 8:06 AM), http://theconversation.com/robot-law-what-happens-if-intelligent-machines-commit- crimes-44058; infra Part I.A (discussing The DAO).
10 Gabriel Hallevy, The Criminal Liability of Artificial Intelligence Entities — From Science Fiction to Legal Social Control, 4 AKRON INTELL. PROP. J. 171, 191 (2010).
11 Id. at 199.
12 See id. at 200 (asking why AI entities should be treated “different from corporations”).
13 Id. at 200-01.
 
545
  2019] Punishing Artificial Intelligence 327
measures.”14 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.15 Her arguments likewise draw heavily on the analogy to corporate criminal liability.16
In contrast to AI punishment expansionists like Hallevy and Hu, skeptics might be inclined to write off the idea of punishing AI from the start as conceptual confusion — akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental concepts of the criminal law like culpability — a “guilty mind” that is characterized by insufficient regard for legally protected values17 — would be misplaced. One might think the whole idea of punishing AI can be easily dispensed with as inconsistent with basic criminal law principles.
The idea of punishing AI is due for fresh consideration. This Article takes a measured look at the proposal, informed by theory and practice alike. We argue punishment of AI cannot be categorically ruled out. Harm caused by a sophisticated AI may be more than a mere accident where no wrongdoing is implicated. Some AI-generated harms may stem from difficult-to-reduce behaviors of an autonomous system,
14 Id. at 199.
15 Ying Hu, Robot Criminals, 52 MICH. J.L. REFORM 487, 531 (2019); see also id. at
490 (“[A]n argument can be made for robot criminal liability, provided that the robot satisfies three threshold conditions . . . . [T]he robot must be (1) equipped with algorithms that can make nontrivial morally relevant decisions; (2) capable of communicating its moral decisions to humans; and (3) permitted to act on its environment without immediate human supervision.”).
16 See Ying Hu, Robot Criminal Liability Revisited, in DANGEROUS IDEAS IN LAW 494, 497-98 (Jin Soo Yoon, Sang Hoon Han & Seong Jo Ahn eds., 2018) (arguing that corporations are “structurally similar” to “robots that are equipped with machine learning algorithms to determine the appropriate course of actions in specific circumstances,” and concluding that “if there is reason to treat corporations as moral agents, there is reason to treat sophisticated robots as moral agents as well”); Hu, supra note 15, at 520-21 (“One may argue that a smart robot can act intentionally in the same way that a corporation can. A robot’s moral algorithms are functionally similar to a corporation’s internal decision structure . . . . By analogy, . . . any act made pursuant to a smart robot’s moral algorithms is an act done for the robot’s own reasons and would therefore amount to an intentional action.”). Unlike Hu, we do not argue that AIs have genuine moral responsibility. We focus on the legal notion of culpability, which involves institutional design constraints that allow it to diverge from moral responsibility or blameworthiness.
17 Alexander Sarch, Who Cares What You Think? Criminal Culpability and the Irrelevance of Unmanifested Mental States, 36 L. & PHIL. 707, 709 (2017) [hereinafter Who Cares].
 
546
  328 University of California, Davis [Vol. 53:323
whose actions resemble those of other subjects of the criminal law, especially corporations. These harms may be irreducible where, for a variety of reasons, they are not directly attributable to the activity of a particular person or persons.18 Corporations similarly can directly face criminal charges when their defective procedures generate condemnable harms19 — particularly in scenarios where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals.20
It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented into criminal law doctrine, and to consider the alternatives. Our primary focus is not what form AI punishment would take, which could directly target AIs through censure, deactivation, or reprogramming, or could involve negative outcomes directed at natural persons or companies involved in the use or creation of AI.21 Rather, our focus is the prior question of whether the doctrinal and theoretical commitments of the criminal law can be reconciled with criminal liability for AI.
Our inquiry focuses on the strongest case for punishing AI: scenarios where crimes are functionally committed by machines and there is no identifiable person who has acted with criminal culpability. We call these Hard AI Crimes. This can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important negative, retributivist-style limitations that apply to persons need not prohibit AI punishment. On the other hand, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep.22 In
18 See infra Part II.B.
19 See MODEL PENAL CODE § 2.07 (AM. LAW INST. 1962) (outlining conditions under
which a corporation could be convicted of an offense).
20 See William S. Laufer, Corporate Bodies and Guilty Minds, 43 EMORY L.J. 647, 664- 68 (1994) (outlining prevalent models of “genuine corporate culpability” including proactive fault, reactive fault, corporate ethos, and corporate policy); infra notes 166– 168 and accompanying text (discussing ways to defend the irreducibility of corporate culpability).
21 See Hu, supra note 15, at 529-30 (discussing the question of how a robot should be punished, and proposing “a range of measures [that] might be taken to ensure that the robot commits fewer offenses in the future”);
  “robot death penalty”). 22 See infra Part III.
Mark A. Lemley & Bryan Casey,
 Remedies for Robots 86 U. CHI. L. REV. 1311, 1316, 1389-93 (2019)
(discussing the

547
  2019] Punishing Artificial Intelligence 329
the end, our conclusion is this: While a coherent theoretical case can be made for punishing AI, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This Article proceeds as follows. Part I provides a brief background of AI and “AI crime.” It then provides a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Part II considers potential benefits to AI punishment, and argues it could provide general deterrence and expressive benefits. Part III examines whether punishment of AI would violate any of the negative limitations on punishment that relate to desert, fairness, and the capacity for culpability. It finds that the most important constraints on punishment, such as requiring a capacity for culpability for it to be appropriately imposed, would not be violated by AI punishment.
Finally, Part IV considers feasible alternatives to AI punishment. It argues the status quo is or will be inadequate for properly addressing AI crime. While direct AI punishment is a solution, this would require problematic changes to criminal law. Alternately, AI crime could be addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. We argue that civil liability is generally preferable to criminal liability for AI activity as it is proportionate to the scope of the current problem and a less significant departure from existing practice with fewer costs. In this way, the Article aims to map out the possible responses to the problem of harmful AI activity and makes the case for approaching AI punishment with extreme caution.
I. ARTIFICIAL INTELLIGENCE AND PUNISHMENT A. Introduction to Artificial Intelligence
We use the term “AI” to refer to a machine that is capable of completing tasks otherwise typically requiring human cognition.23 AI only sometimes has the ability to directly act physically, as in the case of a “robot,” but it is not necessary for an AI to directly affect physical activity to cause harm (as the RDS case demonstrates).
23 AI lacks a standard definition, but its very first definition in 1955 holds up reasonably well: “[T]he artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” J. MCCARTHY ET AL., A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE (1955), http://www-formal.stanford.edu/jmc/history/dartmouth/ dartmouth.html.
 
548
  330 University of California, Davis [Vol. 53:323
AI is rapidly improving, driven by advances in software, computing power, and big data.24 Hardly a day goes by without a new report of some impressive feat achieved by AI. In 2017, Alphabet’s flagship DeepMind AI beat the world champion of the board game Go.25 This was considered an important feat in the AI community, because of the sheer complexity of the game.26 There are more possible Go board configurations than there are atoms in the universe.27 Thus, a machine designed to play Go cannot simply be preprogrammed with optimal predetermined moves, or solely rely on a brute force approach to considering a large number of future moves.28 Go was the last traditional board game in which people had been able to outperform machines.29
In some areas, AI already makes significant practical contributions. For instance, Google Translate supports more than 100 languages, including 37 by photo input, 32 by voice input, and 27 in “augmented reality mode.”30 The increasing prevalence and capability of AI will lead to widespread social benefit, but will also cause harm. Virtually all activity involves a risk of harm, and as AI comes to do more it will inevitably cause more harm.31
A few features of AI are important to highlight. First, AI has the potential to act unpredictably.32 Some leading AIs rely on machine learning or similar technologies which involve a computer program, initially created by individuals, further developing in response to data without explicit programming.33 This is one means by which AI can
24 See Ryan Abbott, Everything Is Obvious, 66 UCLA L. REV. 2, 23-28 (2019).
25 See id. at 24.
26 See id.
27 See id.
28 See id.
29 See id.
30 GOOGLE TRANSLATE, https://translate.google.com/intl/en/about/languages/ (last
visited Oct. 9, 2019).
31 See, e.g., Daisuke Wakabayashi, Self-Driving Uber Car Kills Pedestrian in Arizona,
Where Robots Roam, N.Y. TIMES (Mar. 19, 2018), https://www.nytimes.com/2018/03/19/ technology/uber-driverless-fatality.html.
32 See, e.g., Taha Yasseri, Never Mind Killer Robots — Even the Good Ones Are Scarily Unpredictable, PHYS.ORG (Aug. 25, 2017), https://phys.org/news/2017-08-mind-killer- robots-good-scarily.html; Why Did the Neural Network Cross the Road?, AI WEIRDNESS (2018), http://aiweirdness.com/post/174691534037/why-did-the-neural-network-cross- the-road (describing a programmer who made her machine learning algorithm attempt to tell jokes).
33 See, e.g., Davide Castelvecchi, Can We Open the Black Box of AI?, NATURE (Oct. 5, 2016), https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731.
 
549
  2019] Punishing Artificial Intelligence 331
engage in activities its original programmers may not have intended or foreseen.34
Second, AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.35 This has led to some AIs being described as “black box” systems.36 For instance, an algorithm may refuse a credit application but not be able to articulate why the application was rejected.37 That is particularly likely in the case of AIs that learn from data, and which may have been exposed to millions or billions of data points.38 Even if it is theoretically possible to explain an AI outcome, it may be impracticable given the potentially resource intensive nature of such inquiries, and the need to maintain earlier iterative versions of AI and specific data.
Third, AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual. Suppose an individual creates an AI to steal financial information by mimicking a bank’s website, stealing user information, and posting that information online. While the theft may be entirely reducible to an individual who is using the AI as a tool, the AI may continue to act in harmful ways without further human involvement. It may even be the case that the individual who sets an AI in motion is not able to regain control of the AI, which could be by design.39
Fourth, while AI can already outperform people in spectacular fashion in some domains, like playing board games, in other domains AI is not even competitive with toddlers.40 That is because all AI is
34 There has been a recent focus on biased decisions by machine learning algorithms — sometimes due to a programmer’s implicit bias, sometimes due to biased training data. See, e.g., Chris DeBrusk, The Risk of Machine-Learning Bias (and How to Prevent It), MIT SLOAN MGMT. REV. (Mar. 26, 2018), https://sloanreview.mit.edu/article/the-risk-of- machine-learning-bias-and-how-to-prevent-it/.
35 See, e.g., Castelvecchi, supra note 33.
36 Id.
37 See id.
38 See id.
39 “The DAO” was the most famous attempt to create a decentralized autonomous
organization. See Samuel Falkon, The Story of the DAO