ve activity, but it is not innovative. It applies a known body of knowledge to solve problems with known solutions in a predictable fashion (for example, multiplying values together). However, while Excel may sometimes solve problems that a person could not easily solve without the use of technology, it lacks the ability to engage in almost any inventive activity.160 Excel is not the equivalent of a skilled machine—it is an automaton incapable of ordinary creativity.
Watson in clinical practice may be a better analogy for a skilled worker. Watson analyzes patients’ genomes and provides treatment recommendations.161 Yet as with Excel, this activity is not innovative. The problem Watson is solving may be more complex than multiplying a series of numbers, but it has a known solution. Watson is identifying known genetic mutations from a patient’s genome. Watson is then suggesting known treatments based on existing medical literature. Watson is not innovating
158. Kimberly-Clark Corp. v. Johnson & Johnson, 745 F.2d 1437, 1454 (Fed. Cir. 1984) (“[The] hypothetical person is not the inventor, but an imaginary being possessing ‘ordinary skill in the art’ created by Congress to provide a standard of patentability.”).
159. See I Think, supra note 1 (arguing against a subjective standard for computational invention).
160. Some behaviors like correcting a rogue formula may have a functionally creative aspect, but this is a minimal amount that would not rise to the level of patent conception if performed by a person.
161. See Wrzeszczynski et al., supra note 107.
  
445
Everything Is Obvious 33
because it is being applied to solve problems with known solutions, adhering to conventional wisdom.
Unlike Excel, however, Watson could be inventive. For instance, Watson could be given unpublished clinical data on patient genetics and actual drug responses and tasked with determining whether a drug works for a genetic mutation in a way that has not yet been recognized. Traditionally, such findings have been patentable. Watson may be situationally inventive depending on the problem it is solving.
It may be difficult to identify an actual computer program now which has a “skilled” level of creativity. To the extent a computer is creative, in the right circumstances, any degree of creativity might result in inventive output. To be sure, thisissimilartotheskilledperson. Apersonofordinaryskill,oralmostanyone,may haveaninventiveinsight. Characteristicscanbeimputedtoaskilledperson,butitis not possible the way the test is applied to identify an actual skilled person or to definitivelysaywhatshewouldhavefoundobvious. Theskilledpersontestissimply a theoretical device for a decisionmaker.
Assuming a useful characterization of a skilled machine, to determine that a skilled machine now represents the average worker in a field, decisionmakers would need information about the extent to which such machines are used. Obtaining this information may not be practical. Patent applicants could be asked generally about the use and prevalence of computer software in their fields, but it would be unreasonable to expect applicants to already have, or to obtain, accurate information about general industry conditions. The Patent Office, or another government agency, could attempt to proactively research the use of computers in different fields, but this would not be a workable solution. Such efforts would be costly, the Patent Office lacks expertise in this activity, and its findings would inevitably lag behind rapidly changing conditions. Ultimately, there may not be a reliable and low-cost source of information about skilled machines right now.
D. Inventive Is the New Skilled
Having inventive machines replace the skilled person may better correspond with real world conditions. Right now, there are inherent limits to the number and capabilities of human workers. The cost to train and recruit new researchers is significant, and there are a limited number of people with the ability to perform this work. By contrast, inventive machines are software
 
446
34 66 UCLA L. REV. 2 (2019)
programs which may be copied without additional cost.162 Once Watson outperforms the average industry researcher, IBM may be able to simply copy Watson and have it replace most of an existing workforce. Copies of Watson could replace individual workers, or a single Watson could do the work of a large team of researchers.
Indeed, as mentioned earlier, in a non-inventive setting, Watson can interpret a patient’s entire genome and prepare a clinically actionable report in ten minutes, as opposed to a team of human experts, which takes around one- hundred and sixty hours.163 Once Watson is proven to produce better patient outcomes than the human team, it may be unethical to have people underperform a task which Watson can automate. When that occurs, Watson should not only replace the human team at its current facility—it should replace every comparable human team. Watson could similarly automate in an inventive capacity.
Thus, inventive machines change the skilled paradigm because once they become the average worker, the average worker becomes inventive. As the outputs of these inventive machines become routinized, however, they should no longer be inventive by definition. The widespread use of these machines should raise the bar for obviousness, so that these machines no longer qualify as inventive but shift to become skilled machines—machines which now represent the average worker and are no longer capable of routine invention.164
Regardless of the terminology, as machines continue to improve, the bar for nonobviousness should rise. To generate patentable output, it may be necessary to use an advanced machine that can outperform standard machines, or a person or machine will need to have an unusual insight that standard machines cannot easily recreate. Inventiveness might also depend on the data supplied to a machine, such that only certain data would result in inventive output. Taken to its logical extreme, and given there is no limit to how sophisticated computers can become, it may be that everything will one day be obvious to commonly used computers.
It is possible to generate reasonably low-cost and accurate information about the use of inventive machines. The Patent Office should institute a requirement for patent applicants to disclose the role of computers in the
162. ANDREAS KEMPER, VALUATION OF NETWORK EFFECTS IN SOFTWARE MARKETS: A COMPLEX NETWORKS APPROACH 37 (2010).
163. See Wrzeszczynski et al., supra note 107.
164. See Enzo Biochem, Inc. v. Calgene, Inc., 188 F.3d 1362, 1374 n.10 (Fed. Cir. 1999) (“In view
of the rapid advances in science, we recognize that what may be unpredictable at one point in time may become predictable at a later time.”).
  
447
Everything Is Obvious 35
inventive process.165 This disclosure could be structured along the lines of current inventorship disclosure. Right now, applicants must disclose all patent inventors.166 Failure to do so can invalidate a patent or render it unenforceable.167 Similarly, applicants should have to disclose when a machine autonomously meets inventorship criteria.
These disclosures would only apply to an individual invention. However, the Patent Office could aggregate responses to see whether most inventors in a field (for example, a class or subclass) are human or machine. These disclosures would have a minimal burden on applicants compared to existing disclosure requirements and the numerous procedural requirements of a patent application. In addition to helping the Patent Office with determinations of nonobviousness, these disclosures would provide valuable information for purposes of attributing inventorship.168 It might also be used to develop appropriate innovation policies in other areas.169
E. Skilled People Use Machines
The current standard neglects to appropriately take into account the modern importance of machines in innovation. Instead of now replacing the skilled person with the skilled machine, it would be less of a conceptual change, and administratively easier, to characterize the skilled person as an average worker facilitated by technology. Recall the factor test for the skilled person includes: (1) “type[s] of problems encountered in the art,” (2) “prior art solutions to those problems,” (3) “rapidity with which innovations are made,” (4) “sophistication of the technology,” and (5) “educational level of active workers in the field.”170 This test could be amended to include, (6) “technologies used by
165. It may also be beneficial for applicants to disclose the use of computers when they have been part of the inventive process but where their contributions have not risen to the level of inventorship. Ideally, a detailed disclosure should be provided: Applicants should need to disclose the specific software used and the task it performed. In most cases, this would be as simple as noting a program like Excel was used to perform calculations. However, while this information would have value for policy making, it might involve a significant burden to patent applicants.
166. Duty to Disclose Information Material to Patentability, 37 C.F.R. § 1.56 (2018), https://www.uspto.gov/web/offices/pac/mpep/s2001.html [https://perma.cc/4DE9-ZRWE].
167. See, e.g., Advanced Magnetic Closures, Inc. v. Rome Fastener Corp., 607 F.3d 817, 829–30 (Fed. Cir. 2010) (upholding a district court decision to render a patent unenforceable on the grounds of inequitable conduct for misrepresenting inventorship).
168. See I Think, supra note 1 (advocating for acknowledging machines as inventors).
169. See Should Robots Pay Taxes?, supra note 6 (arguing the need to monitor automation for
adjusting tax incentives).
170. In re GPAC Inc., 57 F.3d 1573, 1579 (Fed. Cir. 1995).
  
448
36 66 UCLA L. REV. 2 (2019)
active workers.” This would more explicitly take into account the fact that human researchers’ capabilities are augmented with computers.
Moving forward in time, once the use of inventive machines is standard, instead of a skilled person being an inventive machine, the skilled person standard could incorporate the fact that technologies used by active workers includes inventive machines. In future research, the standard practice may be for a worker to ask an inventive machine to solve a problem. This could be conceptualized as the inventive machine doing the work, or the person doing the work using an inventive machine.
Granted, in some instances, using an inventive machine may require significant skill, for instance, if the machine is only able to generate a certain output by virtue of being supplied with certain data. Determining which data to provide a machine, and obtaining that data, may be a technical challenge. Also, it may be the case that significant skill is required to formulate the precise problem to put to a machine. In such instances, a person might have a claim to inventorship independent of the machine, or a claim to joint inventorship. This is analogous to collaborative human invention where one person directs another to solve a problem. Depending on details of their interaction, and who “conceived” of the invention, one person or the other may qualify as an inventor, or they may qualify as joint inventors.171 Generally, however, directing another partytosolveaproblemdoesnotqualifyforinventorship.172 Moreover,afterthe development of AGI, there may not be a person instructing a computer to solve a specific problem.
Whether the future standard becomes an inventive machine or a skilled person using an inventive machine, the result will be the same: The average worker will be capable of inventive activity. Replacing the skilled person with the inventive machine may be preferable doctrinally, because it emphasizes that it is the machine which is engaging in inventive activity, rather than the human worker.
The changing use of machines also suggests a change to the scope of prior art. The analogous art test was implemented because it is unrealistic to expect inventors to be familiar with anything more than the prior art in their field, and
171. “[C]onception is established when the invention is made sufficiently clear to enable one skilled in the art to reduce it to practice without the exercise of extensive experimentation or the exercise of inventive skill.” Hiatt v. Ziegler & Kilgour, 179 U.S.P.Q. 757, 763 (Bd. Pat. Interferences 1973); see also Gunter v. Stream, 573 F.2d 77, 79 (C.C.P.A. 1978).
172. Ex parte Smernoff, 215 U.S.P.Q. at 547 (“[O]ne who suggests an idea of a result to be accomplished, rather than the means of accomplishing it, is not a coinventor.”).
  
449
Everything Is Obvious 37
the prior art relevant to the problem they are trying to solve.173 However, a machine is capable of accessing a virtually unlimited amount of prior art. Advances in medicine, physics, or even culinary science may be relevant to solving a problem in electrical engineering. Machine augmentation suggests that the analogous arts test should be modified or abolished once inventive machines are common, and that there should be no difference in prior art for purposes of novelty and obviousness.174 The scope of analogous prior art has consistently expanded in patent law jurisprudence, and this would complete that expansion.175
F. The Evolving Standard
The skilled person standard should be amended as follows:
1) The test should now incorporate the fact that skilled persons are already augmented by machines. This could be done by adding “technologies used by active workers” as a sixth factor to the Federal Circuit’s factor test for the skilled person.
2) Once inventive machines become the standard means of research in a field, the skilled person should be an inventive machine when the standard approach to research in a field or with respect to a particular problem is to use an inventive machine.
3) When and if artificial general intelligence is developed, inventive machines should become the skilled person in all areas, taking into account that artificial general intelligence may also be augmented by specific artificial intelligence.
III. A POST-SKILLED WORLD
This Part provides examples of how the Inventive Machine Standard could work in practice, such as by focusing on reproducibility or secondary factors. It then goes on to consider some of the implications of the new standard. Once the average worker is inventive, there may no longer be a need for patents to function
173. In 1966, in Graham, the Court recognized that “the ambit of applicable art in given fields of science has widened by disciplines unheard of a half century ago . . . . [T]hose persons granted the benefit of a patent monopoly [must] be charged with an awareness of these changed conditions.” Graham v. John Deere Co., 383 U.S. 1, 19 (1966).
174. See supra Subpart I.E.
175. Innovative Scuba Concepts, Inc., v. Feder Indus., Inc., 819 F. Supp. 1487, 1503 (D. Colo.
1993) (discussing the expansion of analogous art); see also, e.g., George. J. Meyer Mfg. Co. v. San Marino Elec. Corp., 422 F.2d 1285, 1288 (9th Cir. 1970) (discussing the expansion of analogous art).
  
450
38 66 UCLA L. REV. 2 (2019)
as innovation incentives. To the extent patents accomplish other goals such as promoting commercialization and disclosure of information or validating moral rights, other mechanisms may be found to accomplish these goals with fewer costs.
A. Application
Mobil Oil Corp. v. Amoco Chemicals Corp. concerned complex technology involving compounds known as Zeolites used in various industrial applications.176 Mobil had developed new compositions known as ZSM-5 zeolites and a process for using these zeolites as catalysts in petroleum refining to help produce certain valuable compounds. The company received patent protection for these zeolites and for the catalytic process.177 Mobil subsequently sued Amoco, which was using zeolites as catalysts in its own refining operations, alleging patent infringement. Amoco counterclaimed seeking a declaration of noninfringement, invalidity, and unenforceability with respect to the two patents at issue. The case involved complex scientific issues. The three-week trial transcript exceeds 3300 pages, and more than 800 exhibits were admitted into evidence.
One of the issues in the case was the level of ordinary skill. An expert for Mobil testified that the skilled person would have “a bachelor’s degree in chemistry or engineering and two to three years of experience.”178 An expert for Amoco argued the skilled person would have a doctorate in chemistry and several years of experience.179 The District Court for the District of Delaware ultimately decided that the skilled person “should be someone with at least a Masters degree in chemistry or chemical engineering or its equivalent, [and] two or three years of experience working in the field.”180
If a similar invention and subsequent fact pattern happened today, to apply the obviousness standard proposed in this Article a decisionmaker would need to: (1) determine the extent to which inventive technologies are used in the field, (2) characterize the inventive machine(s) that best represents the average worker if inventive machines are the standard, and (3) determine whether the machine(s) would find an invention obvious. The decisionmaker is a patent
176. Mobil Oil Corp. v. Amoco Chems. Corp.,779 F. Supp. 1429, 1442–43 (D. Del. 1991).
177. Id.
178. Id. at 1443.
179. Id.
180. Id.
  
451
Everything Is Obvious 39
examiner in the first instance,181 and potentially a judge or jury in the event the validity of a patent is at issue in trial.182 For the first step, determining the extent to which inventive technologies are used in a field, evidence from disclosures to the Patent Office could be used. That may be the best source of information for patent examiners, but evidence may also be available in the litigation context.
Assume that today most petroleum researchers are human, and that if machines are autonomously inventive in this field, it is happening on a small scale. Thus, the court would apply the skilled person standard. However, the court would now also consider “technologies used by active workers.” For instance, experts might testify that the average industry researcher has access to a computer like Watson. They further testify that while Watson cannot autonomously develop a new catalyst, it can significantly assist an inventor. The computer provides a researcher with a database containing detailed information about every catalyst used not only in petroleum research, but in all fields of scientific inquiry. Once a human researcher creates a catalyst design, Watson can also test it for fitness together with a predetermined series of variations on any proposed design.
The question for the court will thus be whether the hypothetical person who holds at least a Master’s degree in chemistry or chemical engineering or its equivalent, has two or three years of experience working in the field, and is using Watson, would find the invention obvious. It may be obvious, for instance, if experts convincingly testify that the particular catalyst at issue were very closely related to an existing catalyst used outside of the petroleum industry in ammonia synthesis, that any variation was minor, and that a computer could do all the work of determining if it were fit for purpose.183 It might thus have been an obvious design to investigate, and it did not require undue experimentation in order to prove its effectiveness.
Now imagine the same invention and fact pattern occurring approximately ten years into the future, at which point DeepMind, together with Watson and a competing host of AI systems, have been set to the task of developing new
181. See U.S. PAT. & TRADEMARK OFF., supra note 24 (at the Patent Office, applications are initially considered by a patent examiner, and examiner decisions can be appealed to the Patent Trial and Appeal Board (PTAB)).
182. Mark A. Lemley, Why Do Juries Decide if Patents Are Valid? (Stanford Law Sch., Pub. Law & Legal Theory Research Paper Series, Working Paper No. 2306152, 2013), https://ssrn.com/abstract=2306152.
183. See Daiichi Sankyo Co. v. Matrix Labs., Ltd., 619 F.3d 1346, 1352 (Fed. Cir. 2010) (finding that a “chemist of ordinary skill would have been motivated to select and then to modify a prior art compound (e.g., a lead compound) to arrive at a claimed compound with a reasonable expectation that the new compound would have similar or improved properties compared with the old”).
  
452
40 66 UCLA L. REV. 2 (2019)
compounds to be used as catalysts in petroleum refining. Experts testify that the standard practice is for a person to provide data to a computer like DeepMind, specify desired criteria (for example, activity, stability, perhaps even designing around existing patents), and ask the computer to develop a new catalyst. From this interaction, the computer will produce a new design. As most research in this field is now performed by inventive machines, a machine would be the standard for judging obviousness.
The decisionmaker would then need to characterize the inventive machine(s). It could be a hypothetical machine based on general capabilities of inventive machines, or a specific computer. Using the standard of a hypothetical machine would be similar to using the skilled person test, but this test could be difficult to implement. A decisionmaker would need to reason what the machine would have found obvious, perhaps with expert guidance. It is already challenging for a person to predict what a hypothetical person would find obvious; it would be even more difficult to do so with a machine. Computers may excel at tasks people find difficult (like multiplying a thousand different numbers together), but even supercomputers struggle with visual intuition, which is mastered by most toddlers.
In contrast, using a specific computer should result in a more objective test. This computer might be the most commonly used computer in a field. For instance, if DeepMind and Watson are the two most commonly used AI systems for research on petroleum catalysts, and DeepMind accounts for 35 percent of the market while Watson accounts for 20 percent, then DeepMind could represent the standard. However, this potentially creates a problem—if DeepMind is the standard, then it would be more likely that DeepMind’s own inventions would appear obvious as opposed to the inventions of another machine. This might give an unfair advantage to non-market leaders, simply because of their size.
To avoid unfairness, the test could be based on more than one specific computer. For instance, both DeepMind and Watson could be selected to represent the standard. This test could be implemented in two different ways. In the first case, if a patent application would be obvious to DeepMind or Watson, then the application would fail. In the second case, the application would have to be obvious to both DeepMind and Watson to fail. The first option would result in fewer patents being granted, with those patents presumably going mainly to disruptive inventive machines with limited market penetration, or to inventions made using specialized non-public data. The second option would permit patents where a machine is able to outperform its competitors in some
 
453
Everything Is Obvious 41
material respect. The second option could continue to reward advances in inventive machines, and therefore seems preferable.
It may be that relatively few AI systems, such as DeepMind and Watson, end up dominating the research market in a field. Alternately, many different machines may each occupy a small share of the market. There is no need to limit the test to two computers. To avoid discriminating on the basis of size, all inventive machines being routinely used in a field or to solve a particular problem might be considered. However, allowing any machine to be considered could allow an underperforming machine to lower the standard, and too many machines might result in an unmanageable standard. An arbitrary cutoff may be applied based on some percentage of market share. That might still give some advantage to very small entities, but it should be a minor disparity.
After characterizing the inventive machine(s), a decisionmaker would need to determine whether the inventive machine(s) would find an invention obvious. This could broadly be accomplished in one of two ways: either with abstract knowledge of what the machines would find obvious, perhaps through expert testimony, or through querying the machines. The former would be the morepracticaloption.184 Forexample,apetroleumresearcherexperiencedwith DeepMind might be an expert, or a computer science expert in DeepMind and neural networks. This inquiry could focus on reproducibility.
Finally, a decisionmaker will have to go through a similar process if the same invention and fact pattern occurs twenty-five years from now, at which point artificial general intelligence has theoretically taken over in all fields of research. AGI should have the ability to respond directly to queries about whether it finds an invention obvious. Once AGI has taken over from the average researcher in all inventive fields, it may be widely enough available that the Patent Office could arrange to use it for obviousness queries. In the litigation context, it may be available from opposing parties. If courts cannot somehow access AGI, they may still have to rely on expert evidence.
184. Alternatively, the machine could be asked to solve the problem at question and given the relevant prior art. If the machine generates the substance of the patent, the invention would be considered obvious. However, this would require a decisionmaker to have access to the inventive machine. At the application stage, the Patent Office would need to contract with, say, Google to use DeepMind in such a fashion. For that matter, the Patent Office might use DeepMind not only to decide whether inventions are obvious, but to automate the entire patent examination process. At trial, if Google is party to a lawsuit, an opposing party might subpoena use of the computer. However, if Google is not a party, it might be unreasonable to impose on Google for access to DeepMind.
  
454
42 66 UCLA L. REV. 2 (2019)
B. Reproducibility
Even if an inventive machine standard is the appropriate theoretical tool for nonobviousness, it still requires certain somewhat subjective limitations, and decisionmakers may still have difficulty with administration. Still, the new standard only needs to be slightly better than the existing standard to be an administrative success.
A test focused on reproducibility, based on the ability of the machine selected to represent the standard being able to independently reproduce the invention, offers some clear advantages over the current skilled person standard, which results in inconsistent and unpredictable outcomes.185 Courts have “provided almost no guidance concerning either what degree of ingenuity is necessary to meet the standard or how a decisionmaker is supposed to evaluate whether the differences between the invention and prior art meet this degree.”186 This leaves decisionmakers in the unenviable position of trying to subjectively establish what another person would have found obvious. Worse, this determination is to be made in hindsight with the benefit of a patent application. On top of that, judges and juries lack scientific expertise.187 In practice, decisionmakers may read a patent application, decide that they know
185. See FED. TRADE COMM’N, supra note 16 (discussing objections to the skilled person standard).
186. Mandel, The Non-Obvious Problem, supra note 19, at 64.
187. As Judge Learned Hand wrote:
I cannot stop without calling attention to the extraordinary condition of the law which makes it possible for a man without any knowledge of even the rudiments of chemistry to pass upon such questions as these. The inordinate expense of time is the least of the resulting evils, for only a trained chemist is really capable of passing upon such facts . . . . How long we shall continue to blunder along without the aid of unpartisan and authoritative scientific assistance in the administration of justice, no one knows; but all fair persons not conventionalized by provincial legal habits of mind ought, I should think, unite to effect some such advance.
Parke-Davis & Co. v. H.K. Mulford Co., 189 F. 95, 115 (S.D.N.Y. 1911). See also Safety Car Heating & Lighting Co. v. Gen. Elec. Co., 155 F.2d 937, 939 (1946) (“Courts, made up of laymen as they must be, are likely either to underrate, or to overrate, the difficulties in making new and profitable discoveries in fields with which they cannot be familiar . . . .”); see also Doug Lichtman & Mark A. Lemley, Rethinking Patent Law’s Presumption of Validity, 60 STAN. L. REV. 45, 67 (2007) (“District Court judges are poorly equipped to read patent documents and construe technical patent claims. Lay juries have no skill when it comes to evaluating competing testimony about the originality of a technical accomplishment.”).
  
455
Everything Is Obvious 43
obviousness when they see it, and then reason backward to justify their findings.188
This is problematic because patents play a critical role in the development and commercialization of products, and patent holders and potential infringers should have a reasonable degree of certainty about whether patents are valid. A more determinate standard would make it more likely the Patent Office would apply a single standard consistently and result in fewer judicially invalidated patents. To the extent machine reproducibility is a more objective standard, this would seem to address many of the problems inherent in the current standard.
On the other hand, reproducibility comes with its own baggage. Decisionmakers have difficulty imagining what another person would find obvious, and it would probably be even more difficult to imagine in the abstract what a machine could reproduce. More evidence might need to be supplied in patent prosecution and during litigation, perhaps in the format of analyses performed by inventive machines, to demonstrate whether particular output was reproducible. This might also result in a greater administrative burden.
In some instances, reproducibility may be dependent on access to data. A large health insurer might be able to use Watson to find new uses for existing drugs by giving Watson access to proprietary information on its millions of members. Or, the insurer might license its data to drug discovery companies using Watson for this purpose. Without that information, another inventive computer might not able to recreate Watson’s analysis.
This too is analogous to the way data is used now in patent applications: Obviousness is viewed in light of the prior art, which does not include non- public data relied upon in a patent application. The rationale here is that this rule incentivizes research to produce and analyze new data. Yet as machines become highly advanced, it is likely that the importance of proprietary data will decrease. More advanced machines may be able to do more with less.
Finally, reproducibility would require limits. For instance, a computer which generates semi-random output might eventually recreate the inventive concept of a patent application if it were given unlimited resources. However, it would be unreasonable to base a test on what a computer would reproduce given, say, 7.5 million years.189 The precise limits that should be placed on
188. Jacobellis v. Ohio, 378 U.S. 184, 197 (1964) (Stewart, J., dissenting). This was later recognized as a failed standard. Miller v. California, 413 U.S. 15, 47–48 (1973) (Brennan, J., dissenting) (obscenity cases similarly relying on the Elephant Test).
189. This brings to mind a super intelligent artificial intelligence system, “Deep Thought,” which famously, and fictionally, took 7.5 million years to arrive at the “Answer to the Ultimate Question of Life, the Universe, and Everything.” DOUGLAS ADAMS, THE HITCHHIKER’S GUIDE TO THE GALAXY 180 (rev. ed. 2001) (1979). The answer was 42. Id. at 188.
  
456
44 66 UCLA L. REV. 2 (2019)
reproducibility might depend on the field in question, and what best reflected the actual use of inventive machines in research. For instance, when asked to design a new catalyst in the petroleum industry, Watson might be given access to all prior art and publicly available data, and then given a day to generate output.
C. An Economic vs. Cognitive Standard
The skilled person standard received its share of criticism even before the arrival of inventive machines.190 The inquiry focuses on the degree of cognitive difficulty in conceiving an invention but fails to explain what it actually means for differences to be obvious to an average worker. The approach lacks both a normative foundation and a clear application.191
In Graham, the Supreme Court’s seminal opinion on nonobviousness, the Court attempted to supplement the test with more “objective” measures by looking to real-world evidence about how an invention was received in the marketplace.192 Rather than technological features, these “secondary” considerations focus on “economic and motivational” features, such as commercial success, unexpected results, long-felt but unsolved needs, and the failure of others.193 Since Graham, courts have also considered, among other
190. See, e.g., Chiang, supra note 19, at 49 (as one commentator noted about the test as articulated by the Supreme Court in Graham, it gives “all the appearance of expecting a solution to appear out of thin air once the formula was followed. The lack of an articulable rule meant that determinations of obviousness took the appearance—and arguably the reality—of resting on judicial whim . . . .” (footnote omitted)); Abramowicz & Duffy, supra note 16, at 1598; Gregory N. Mandel, Patently Non-Obvious: Empirical Demonstration That the Hindsight Bias Renders Patent Decisions Irrational, 67 OHIO ST. L.J. 1391 (2006) (discussing problems with hindsight in nonobviousness inquiries); Gregory N. Mandel, Another Missed Opportunity: The Supreme Court’s Failure to Define Nonobviousness or Combat Hindsight Bias in KSR v. Teleflex, 12 LEWIS & CLARK L. REV. 323 (2008).
191. See Abramowicz & Duffy, supra note 16, at 1603 (“[N]either Graham nor in subsequent cases has the Supreme Court attempted either to reconcile the inducement standard with the statutory text or to provide a general theoretical or doctrinal foundation for the inducement standard.”).
192. See Graham v. John Deere Co., 383 U.S. 1, 17; MPEP § 2144.
193. Graham, 383 U.S. at 17; MPEP § 2144. Additional secondary considerations have since been
proposed. See, e.g., Andrew Blair-Stanek, Increased Market Power as a New Secondary Consideration in Patent Law, 58 AM. U. L. REV. 707 (2009) (arguing for whether an invention provides an inventor with market power); Abramowicz & Duffy, supra note 16, at 1656 (proposing changing commercial success to “unexpected commercial success,” adding as a consideration of the “cost of the experimentation leading to the invention,” and a few additional considerations).
  
457
Everything Is Obvious 45
things, patent licensing,194 professional approval,195 initial skepticism,196 near- simultaneous invention,197 and copying.198 Today, while decisionmakers are required to consider secondary evidence when available, the importance of these factors varies significantly.199 Graham endorsed the use of secondary considerations, but their precise use and relative importance have never been made clear.200
An existing vein of critical scholarship has advocated for adopting a more economic than cognitive nonobviousness inquiry, for example through greater reliance on secondary considerations.201 This would reduce the need for decisionmakers to try and make sense of complex technologies, and it could reduce hindsight bias.202
Theoretically, in Graham, the Court articulated an inducement standard, which dictates that patents should only be granted to “those inventions which would not be disclosed or devised but for the inducement of a patent.”203 But in practice, the inducement standard has been largely ignored due to concerns over application.204 For instance, few, if any, inventions would never be disclosed or devised given an unlimited time frame. Patent incentives may not increase, so
194. See, e.g., SIBIA Neurosciences, Inc. v. Cadus Pharm. Corp., 225 F.3d 1349, 1358 (Fed. Cir. 2000).
195. See, e.g., Vulcan Eng’g Co. v. Fata Aluminum, Inc., 278 F.3d 1366, 1373 (Fed. Cir. 2002).
196. See, e.g., Metabolite Labs., Inc. v. Lab. Corp. of Am. Holdings, 370 F.3d 1354, 1368 (Fed. Cir.
2004).
197. See, e.g., Ecolochem, Inc. v. S. Cal. Edison Co., 227 F.3d 1361, 1379 (Fed. Cir. 2000).
198. See, e.g., id. at 1377. See also Mark A. Lemley, Should Patent Infringement Require Proof of
Copying?, 105 MICH. L. REV. 1525, 1534–35 (2007).
199. See MPEP § 2144; Durie & Lemley, supra note 19, at 996–97.
200. See, e.g., Dorothy Whelan, A Critique of the Use of Secondary Considerations in Applying the
Section 103 Nonobviousness Test for Patentability, 28 B.C. L. REV. 357 (1987).
201. See, e.g., Merges, supra note 19, at 19 (arguing for patentability to be based on an a priori degree of uncertainty, that “rewards one who successfully invents when the uncertainty facing her prior to the invention makes it more likely than not that the invention won’t succeed” (emphasis omitted)); Chiang, supra note 19, at 42 (arguing for a utilitarian standard, such that “[a]n invention should receive a patent if the accrued benefits before independent invention outweigh the costs after independent invention”); Mandel, The Non- Obvious Problem, supra note 19, at 62 (arguing for nonobviousness to be based on “how probable the invention would have been for a person having ordinary skill in the art working on the problem that the invention solves”); Durie & Lemley, supra note 19, at 1004–07 (arguing for a greater reliance on secondary considerations); Duffy, supra note 19, at 343 (arguing a timing approach to determining obviousness); Devlin & Sukhatme, supra note
19; Abramowicz & Duffy, supra note 16, at 1598 (arguing for an inducement standard).
202. Graham, 383 U.S. at 36 (“[Secondary considerations] may also serve to ‘guard against slipping into use of hindsight.’” (citation omitted)). See also HERBERT F. SCHWARTZ &
ROBERT J. GOLDMAN, PATENT LAW AND PRACTICE 90–91 (6th ed. 2008).
203. Graham, 383 U.S. at 11.
204. See Abramowicz & Duffy, supra note 16, at 1594–95.
  
458
46 66 UCLA L. REV. 2 (2019)
much as accelerate, invention.205 This suggests that an inducement standard would at least need to be modified to include some threshold for the quantum of acceleration needed for patentability. Too high a threshold would fail to provide adequate innovation incentives, but too low a threshold would be similarly problematic. Just as inventions will be eventually disclosed without patents given enough time, patents on all inventions could marginally speed the disclosure of just about everything, but a trivial acceleration would not justify the costs of patents. An inducement standard would thus require a somewhat arbitrary threshold in relation to how much patents should accelerate the disclosure of information, as well as a workable test to measure acceleration.206 To be sure, an economic test based on the inducement standard would have challenges, but it might be an improvement over the current cognitive standard.207
The widespread use of inventive machines may provide the impetus for an economic focus. After inventive machines become the standard way that R&D is conducted in a field, courts could increase reliance on secondary factors. For instance, patentability may depend on how costly it was to develop an invention, andtheexanteprobabilityofsuccess.208 Thereisnoreasonaninventivemachine cannot be thought of, functionally, as an economically motivated rational actor. The test would raise the bar to patentability in fields where the cost of invention decreases over time due to inventive machines.
D. Other Alternatives
Courts may maintain the current skilled person standard and decline to consider the use of machines in obviousness determinations. However, this means that as research is augmented and then automated by machines, the average worker will routinely generate patentable output. The dangers of such a
205. See, e.g., Yoram Barzel, Optimal Timing of Innovations, 50 REV. ECON. & STATS. 348, 348 (1968); John F. Duffy, Rethinking the Prospect Theory of Patents, 71 U. CHI. L. REV. 439, 444 (2004).
206. Abramowicz & Duffy, supra note 16, at 1599 (proposing a “substantial period of time”).
207. See Abramowicz & Duffy, supra note 16, at 1663.
208. Id.
  
459
Everything Is Obvious 47
standard for patentability are well-recognized.209 A low obviousness requirement can “stifle, rather than promote, the progress of the useful arts.”210
Concerns already exist that the current bar to patentability is too low, and that a patent “anticommons” with excessive private property is resulting in “potential economic value . . . disappear[ing] into the ‘black hole’ of resource underutilization.”211 It is expensive for firms interested in making new products to determine whether patents cover a particular innovation, evaluate those patents, contact patent owners, and negotiate licenses.212 In many cases, patent owners may not wish to license their patents, even if they are non-practicing entities that do not manufacture products themselves.213 Firms that want to make a product may thus be unable to find and license all the rights they need to avoid infringing. Adding to this morass, most patents turn out to be invalid or not infringed in litigation.214 Excessive patenting can thus slow innovation, destroy markets, and, in the case of patents on some essential medicines, even cost lives.215 Failing to raise the bar to patentability once the use of inventive machines is widespread would significantly exacerbate this anticommons effect.
Instead of updating the skilled person standard, courts might determine that inventive machines are incapable of inventive activity, much as the U.S. Copyright Office has determined that nonhuman authors cannot generate copyrightable output.216 In this case, otherwise patentable inventions might not
209. See, e.g., ADAM B. JAFFE & JOSH LERNER, INNOVATION AND ITS DISCONTENTS: HOW OUR BROKEN PATENT SYSTEM IS ENDANGERING INNOVATION AND PROGRESS, AND WHAT TO DO ABOUT IT 32–35, 75, 119–23, 145–49 (2004) (criticizing the Patent Office for granting patents on obvious inventions); NATIONAL RESEARCH COUNCIL, A PATENT SYSTEM FOR THE 21ST CENTURY 87–95 (2004) (criticizing lenient nonobviousness standards); Matthew Sag & Kurt Rohde, Patent Reform and Differential Impact, 8 MINN. J.L. SCI. & TECH. 1, 2 (2007) (“Academics, business leaders, and government officials have all expressed concern that too many patents are issued for [obvious] inventions.” ).
210. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 427 (2007).
211. James M. Buchanan & Yong J. Yoon, Symmetric Tragedies: Commons and Anticommons, 43
J.L. & ECON. 1, 2; accord DAN L. BURK & MARK A. LEMLEY, THE PATENT CRISIS AND HOW THE
COURTS CAN SOLVE IT (2009) (arguing for a heightened bar to patentability).
212. See generally Mark A. Lemley, Ignoring Patents, 2008 MICH. ST. L. REV. 19, 25–26 (2008)
(describing various costs associated with innovation in patent heavy industries).
213. See David L. Schwartz & Jay P. Kesan, Analyzing the Role of Non-Practicing Entities in the
Patent System, 99 CORNELL L. REV. 425 (2014).
214. See Mark A. Lemley & Carl Shapiro, Probabilistic Patents, 19 J. ECON. PERSP. 75, 80 (2005).
215. See Michael A. Heller, The Tragedy of the Anticommons: Property in the Transition From
Marx to Markets, 111 HARV. L. REV. 621 (1998); see also MICHAEL HELLER, THE GRIDLOCK ECONOMY: HOW TOO MUCH OWNERSHIP WRECKS MARKETS, STOPS INNOVATION AND COSTS LIVES (2008); see also Michael A. Heller & Rebecca S. Eisenberg, Can Patents Deter Innovation? The Anticommons in Biomedical Research, 280 SCIENCE 698 (1998).
216. This has been a policy of the Copyright Office since at least 1984. See U.S. COPYRIGHT OFFICE, COMPENDIUM OF U.S. COPYRIGHT OFFICE PRACTICES § 306 (3d ed. 2014). The
  
460
48 66 UCLA L. REV. 2 (2019)
be eligible for patent protection, unless provisions were made for the inventor to be the first person to recognize the machine output as patentable. However, this would not be a desirable outcome. As I have argued elsewhere, providing intellectual property protection for computer-generated inventions would incentivize the development of inventive machines, which would ultimately result in additional invention.217 This is most consistent with the constitutional rationale for patent protection “[t]o promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.”218
E. Incentives Without Patents?
Today, there are strong incentives to develop inventive machines. Inventions by these machines have value independent of intellectual property protection, but they should also be eligible for patent protection. People may apply as inventors for recognizing the inventive nature of a machine’s output,219 or more ambitiously, inventive machines may be recognized as inventors, resulting in stronger and fairer incentives.
Once inventive machines set the baseline for patentability, standard inventive machines, as well as people, should have difficulty obtaining patents. It is widely thought that setting a nonobviousness standard too high would reduce the incentives for innovators to invent and disclose. Yet once inventive machinesarenormal,thereshouldbelessneedforpatentincentives.220 Oncethe
Compendium of U.S. Copyright Office Practices elaborates on the “human authorship” requirement by stating: “The term ‘authorship’ implies that, for a work to be copyrightable, it must owe its origin to a human being.” Id. It further elaborates on the phrase “[w]orks not originated by a human author” by stating: “In order to be entitled to copyright registration, a work must be the product of human authorship. Works produced by mechanical processes or random selection without any contribution by a human author are not registrable.” Id. § 503.03(a).
217. See generally I Think, supra note 1.
218. U.S. CONST. art. I, § 8, cl. 8.
219. Conception requires contemporaneous recognition and appreciation of the invention. See
Invitrogen Corp. v. Clontech Labs., Inc., 429 F.3d 1052, 1064 (Fed. Cir. 2005) (noting that the inventor must have actually made the invention and understood the invention to have the features that comprise the inventive subject matter at issue); see also, e.g., Silvestri v. Grant, 496 F.2d 593, 597 (C.C.P.A. 1974) (“[A]n accidental and unappreciated duplication of an invention does not defeat the patent right of one who, though later in time, was the first to recognize that which constitutes the inventive subject matter.”).
220. See generally, Mark A. Lemley, IP in a World Without Scarcity (Stanford Public Law, Working Paper No. 2413974, 2014), http://dx.doi.org/10.2139/ssrn.2413974 (arguing new technologies that reduce costs will weaken the case for IP).
  
461
Everything Is Obvious 49
average worker is inventive, inventions will “occur in the ordinary course.”221 Machine inventions will be self-sustaining. In addition, the heightened bar might result in a technological arms race to create ever more intelligent computers capable of outdoing the standard. That would be a desirable outcome in terms of incentivizing innovation.
Even after the widespread use of inventive machines, patents may still be desirable. For instance, patents may be needed in the biotechnology and pharmaceutical industries to commercialize new technologies. The biopharma industry claims that new drug approvals cost around 2.2 billion dollars and take an average of eight years.222 This cost is largely due to resource intensive clinical trials required to prove safety and efficacy. Once a drug is approved, it is often relatively easy for another company to recreate the approved drug. Patents thus incentivize the necessary levels of investment to commercialize a product given that patent holders can charge monopoly prices for their approved products during the term of a patent.
Yet patents are not the only means of promoting product commercialization. Newly approved drugs and biologics, for example, receive a period of market exclusivity during which time no other party can sell a generic or biosimilar version of the product. Newly approved biologics, for instance, receive a twelve-year exclusivity period in the United States. Because of the length of time it takes to get a new biologic approved, the market exclusivity period may exceed the term of any patent an originator company has on its product. A heightened bar to patentability may lead to greater reliance on alternative forms of intellectual property protection such as market exclusivity, prizes, grants, or tax incentives.223
With regards to disclosure, without the ability to receive patent protection, owners of inventive machines may choose not to disclose their discoveries and rely on trade secret protection. However, with an accelerated rate of technological progress, intellectual property holders would run a significant risk that their inventions would be independently recreated by inventive machines.
Depending on the type of innovation, industry, and competitive landscape, business ventures may be successful without patents, and patent protection is
221. KSR Int’l Co. v. Teleflex Inc., 550 U.S. 398, 402 (2007).
222. Joseph A. DiMasi, Henry G. Grabowski, & Ronald W. Hansen, Innovation in the
Pharmaceutical Industry: New Estimates of R&D Costs, 47 J. OF HEALTH ECON. 20–33 (2016).
223. See generally Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes Debate, 92 TEX. L. REV. 303 (2013) (describing various nontraditional intellectual property
incentives).
  
462
50 66 UCLA L. REV. 2 (2019)
not sought for all potentially patentable inventions.224 In fact, “few industries consider patents essential.”225 For instance, patents are often considered a critical part of biotechnology corporate strategy, but often ignored in the software industry.226 On the whole, a relatively small percentage of firms patent, evenamongfirmsconductingR&D.227 Mostcompaniesdonotconsiderpatents crucial to business success.228 Other types of intellectual property such as trademark, copyright, and trade secret protection, combined with “alternative” mechanisms such as first mover advantage and design complexity may protect innovation even in the absence of patents.229
F. A Changing Innovation Landscape
Inventive machines may result in further consolidation of wealth and intellectual property in the hands of large corporations like Google and IBM. Large enterprises may be the most likely developers of inventive machines due to their high development costs.230 A counterbalance to additional wealth disparity could be broad societal gains. The public would stand to gain access to a tremendous amount of innovation—innovation which might be significantly delayed or never come about without inventive machines. In fact, concerns about industry consolidation are another basis for revising the obviousness inquiry. The widespread use of inventive machines may be inevitable, but raising the bar to patentability would make it so that inventions which would
224. BRONWYN HALL ET AL., INTELLECTUAL PROPERTY OFFICE, THE USE OF ALTERNATIVES TO PATENTS AND LIMITS TO INCENTIVES, 2 (2012), http://webarchive.nationalarchives.gov.uk/ 20140603121456/http://www.ipo.gov.uk/ipresearch-patalternative.pdf; see also, Rochelle Cooper Dreyfuss, Does IP Need IP? Accommodating Intellectual Production Outside the Intellectual Property Paradigm, 31 CARDOZO L. REV. 1437, 1439 (2010); see also David Fagundes, Talk Derby to Me: Intellectual Property Norms Governing Roller Derby Pseudonyms, 90 TEX. L. REV. 1094, 1146 (2012) (describing norm-based protections that function effectively in the absence of traditional IP). Patent holders are only successful in about a quarter of cases that are litigated to a final disposition and appealed. Paul M. Janicke & LiLan Ren, Who Wins Patent Infringement Cases?, 34 AIPLA Q.J. 1, 8 (2006). Fewer than two percent of patents are ever litigated, and only about 0.1 percent go to trial. Lemley & Shapiro, supra note 214, at 79. In cases where the validity of a patent is challenged, about half of the time the patent is invalidated. Allison & Lemley, supra note 20, at 205 (1998).
225. Merges, supra note 19, at 19.
226. See generally, Lemley & Shapiro, supra note 214.
227. Id.
228. Id.
229. Id.
230. See Jamie Carter, The Most Powerful Supercomputers in the World—and What They Do,
TECHRADAR (Dec. 13, 2014), http://www.techradar.com/us/news/computing/the-most- powerfulsupercomputers-in-the-world-and-what-they-do-1276865 (noting that most advanced computer systems are owned by governments and large businesses).
  
463
Everything Is Obvious 51
naturally occur would be less likely to receive protection. To the extent market abuses such as price gouging and supply shortages are a concern, protections are, at least theoretically, built into patent law to protect consumers against such problems.231 For example, the government could exercise its march in rights or issue compulsory licenses.232
Inventive machines may ultimately automate knowledge work and render human researchers redundant. While past technological advances have resulted in increased rather than decreased employment, the technological advances of the near future may be different.233 There will be fewer limits to what machines will be able to do, and greater access to machines. Automation should generate innovation with net societal gains, but it may also contribute to unemployment, financial disparities, and decreased social mobility.234 It is important that policymakers act to ensure that automation benefits everyone, for instance by investing in retraining and social benefits for workers rendered technologically unemployed.235 Ultimately, patent law alone will not determine whether automation occurs. Even without the ability to receive patent protection, once inventive machines are significantly more efficient than human researchers, they will replace people.
CONCLUSION
Prediction is very difficult, especially about the future.236
In the past, patent law has reacted slowly to technological change. For instance, it was not until 2013 that the Supreme Court decided human genes should be unpatentable.237 By then, the Patent Office had been granting patents on human genes for decades,238 and more than 50,000 gene-related patents had been issued.239
231. See Balancing Access, supra note 27 (discussing patent law protections against practices including “evergreening”).
232. See id. at 345 (explaining India’s issuance of a compulsory license).
233. See Should Robots Pay Taxes?, supra note 6; see supra Part I.
234. Id.
235. Id.
236. ARTHUR K. ELLIS, TEACHING AND LEARNING ELEMENTARY SOCIAL STUDIES 56, (1970) (quoting physicist Niels Bohr).
237. Ass’n for Molecular Pathology v. Myriad Genetics, Inc., 133 S. Ct. 2107 (2013).
238. See, e.g., U.S. Patent No. 4,447,538 (filed Feb. 5, 1982) (a patent issued in 1984 which claims
the human Chorionic Somatomammotropin gene).
239. Robert Cook-Deegan & Christopher Heaney, Patents in Genomics and Human Genetics, 11
ANN. REV. OF GENOMICS & HUM. GENETICS 383, 384 (2010) (“In April 2009, the U.S. Patent
  
464
52 66 UCLA L. REV. 2 (2019)
Eminent technologists now predict that artificial intelligence is going to revolutionize the way innovation occurs in the near to medium term. Much of what we know about intellectual property law, while it might not be wrong, has not been adapted to where we are headed. The principles that guide patent law need to be, if not rethought, then at least retooled in respect of inventive machines. We should be asking what our goals are for these new technologies, what we want our world to look like, and how the law can help make it so.
  and Trademark Office (USPTO) granted the 50,000th U.S. patent that entered the DNA Patent Database at Georgetown University. That database includes patents that make claims mentioning terms specific to nucleic acids (e.g., DNA, RNA, nucleotide, plasmid, etc.).”).

465
Should Robots Pay Taxes? Tax Policy in the Age of Automation
Ryan Abbott* & Bret Bogenschneider**
Existing technologies can already automate most work functions, and the cost of these technologies is decreasing at a time when human labor costs are increasing. This, combined with ongoing advances in computing, artificial intelligence, and robotics, has led experts to predict that automation will lead to significant job losses and worsening income inequality. Policy makers are actively debating how to deal with these problems, with most proposals focusing on investing in education to train workers in new job types, or investing in social benefits to distribute the gains of automation.
The importance of tax policy has been neglected in this debate, which is unfor- tunate because such policies are critically important. The tax system incentivizes automation even in cases where it is not otherwise efficient. This is because the vast majority of tax revenues are now derived from labor income, so firms avoid taxes by eliminating employees. Also, when a machine replaces a person, the government loses a substantial amount of tax revenue—potentially hundreds of billions of dol- lars a year in the aggregate. All of this is the unintended result of a system designed to tax labor rather than capital. Such a system no longer works once the labor is capital. Robots are not good taxpayers.
We argue that existing tax policies must be changed. The system should be at least “neutral” as between robot and human workers, and automation should not be allowed to reduce tax revenue. This could be achieved through some combination of disallowing corporate tax deductions for automated workers, creating an “automa- tion tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate.
INTRODUCTION
An automation revolution is underway.1 Current technologies can al- ready mechanize most work activities, and the cost of these technologies is
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine at the David Geffen School of Medicine at University of California, Los Angeles.
** Senior Lecturer (Associate Professor), Finance Law & Ethics, University of Surrey School of Law. Thanks to Daniel Hemel for his insightful comments.
1 See, e.g., BANK OF AMERICA MERRILL LYNCH, ROBOT REVOLUTION: GLOBAL ROBOT & AI PRIMER 3 (Dec. 16, 2015) (on file with the Harvard Law School Library) (“The pace of disruptive technological innovation has gone from linear to parabolic in recent years. Penetra- tion of robots and artificial intelligence (AI) has hit every industry sector, and has become an integral part of our daily lives. Technology has also expanded beyond routine work, and moved into complex problem-solving, and replicating human perception, tasks that only people were capable of.”); see also Relating to the Training and Utilization of the Manpower Resources of the Nation: Hearing Before the Subcomm. on Emp’t and Manpower of the Comm. on Labor and Pub. Welfare, 88th Cong. 1659 (1963) (statement of Isaac L. Auerbach, President, Interna- tional Federation for Information Processing) (“The word ‘automation’ was coined by Delmar S. Harder, then executive vice president of the Ford Motor Co., in attempting to describe the latest kind of assembly line technique involving engine-block transfer machines then being installed at Ford’s River Rouge and Cleveland plants.”). For a definition of the term “automa- tion,” see Meg Leta Jones, The Ironies of Automation Law: Tying Policy Knots with Fair Auto-
  
146 466 Harvard Law & Policy Review [Vol. 12
decreasing at a time when human labor costs are increasing.2 On top of that, ongoing and exponential improvements in computing, artificial intelligence (AI), and robotics are permitting automation in an ever-increasing number of fields.3 As a result, academic and industry experts are widely predicting that automation will result in substantial “technological unemployment” in the near future.4 For instance, the McKinsey Global Institute has claimed that the disruption caused by AI will “happ[en] ten times faster and at 300 times the scale, or roughly 3,000 times the impact,” of the Industrial Revolution.5 We
mation Practices Principles, 18 VAND. J. ENT. & TECH. L. 77, 84 (2015) (“Broadly, automation includes all the ways computers and machines help people perform tasks more quickly, accurately, and efficiently. The term ‘automation’ refers to: (1) the mechanization and integration of the sensing of environmental variables through artificial sensors, (2) data processing and decision making by computers, and (3) mechanical action by devices that apply forces on the environment or information action through communication to people of informa- tion processed. The term encompasses open-loop operations and closed-loop control, as well as intelligent systems.”) (citations omitted). One of the most cited studies on technological unemployment claims that forty-seven percent of American jobs are at high risk of loss due to automation. See Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible are Jobs to Computerisation?, 114 TECH. FORECASTING & SOC. CHANGE 254, 265–66 (2017), https://ac.els-cdn.com/S0040162516302244/1-s2.0-S0040162516302244-main .pdf?_tid=14d233e0-c236-11e7-a741-00000aacb362&acdnat=1509892499_c57668bde931faf 6de11b39073cccfc5 [https://perma.cc/LFE2-2T7A] (“[O]ur findings suggest recent develop- ments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future.”).
2 See Frey & Osborne, supra note 1, at 265–68; but see JAMES MANYIKA ET AL., MCKIN- SEY GLOBAL INST., A FUTURE THAT WORKS: AUTOMATION, EMPLOYMENT, AND PRODUCTIVITY 21 (2017), https://www.mckinsey.com/~/media/McKinsey/Global%20Themes/Digital%20Dis ruption/Harnessing%20automation%20for%20a%20future%20that%20works/MGI-A-future- that-works_Full-report.ashx [https://perma.cc/2F6U-U259] (predicting that fewer than five percent of occupations could be entirely automated with existing technologies).
3 For examples of automation in white-collar and professional settings, see Roger Parloff, Why Deep Learning is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning/ [https://perma.cc/5A6Q- 5U4T]. Of particular concern to future attorneys is that AI is already automating work func- tions in the legal services industry. See, e.g., Jane Croft, Legal Firms Unleash Office Auto- matons, FIN. TIMES, May 16, 2016 at 4 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Dana Remus & Frank Levy, Can Robots Be Lawyers?: Computers, Lawyers, and the Practice of Law, 30 GEO. J. LEGAL ETHICS 501 (2017) (arguing that AI will refocus rather than replace attorneys).
4 See supra note 1. In the 1930s, the economist John Maynard Keynes popularized the term “technological unemployment” to refer to “unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour.” The Future of Jobs: The Onrushing Wave, ECONOMIST (Jan. 18, 2014), https://www .economist.com/news/briefing/21594264-previous-technological-innovation-has-always-deliv- ered-more-long-run-employment-not-less [https://perma.cc/QQ3N-9AWN].
5 Richard Dobbs et al., The Four Global Forces Breaking All the Trends, MCKINSEY GLOBAL INST. (Apr. 2015), https://www.mckinsey.com/business-functions/strategy-and-corpo- rate-finance/our-insights/the-four-global-forces-breaking-all-the-trends [https://perma.cc/ LC89-B23C] (excerpting RICHARD DOBBS ET AL., NO ORDINARY DISRUPTION: THE FOUR GLOBAL FORCES BREAKING ALL THE TRENDS (2015)); see also JAMES MANYIKA ET AL., MCK- INSEY GLOBAL INST., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013), https://www.mckinsey.com/~/media/McKin sey/Business%20Functions/McKinsey%20Digital/Our%20Insights/Disruptive%20technolo gies/MGI_Disruptive_technologies_Full_report_May2013.ashx [https://perma.cc/EV85- WHVG] (predicting also trillions of dollars in economic impact by 2025 from advanced robot- ics, 3D printing and autonomous vehicles).
 
467
2018] Should Robots Pay Taxes? 147
are entering an era in which the combined impact of technological improve- ments in many different areas is going to be profoundly transformative—and disruptive.6
Automation has the potential to create widespread benefits. Not only will automation increase productivity, it will also improve safety and lead to new scientific breakthroughs.7 But without oversight, automation will also exacerbate unemployment and economic inequality.8 Even if workers ren- dered technologically unemployed are able to transition to new jobs, as has been the case during previous eras of rapid change, there will still be signifi- cant short-term disruptions. Moreover, many experts are predicting that to- day’s technological advances are different in kind from those of the past, and that large-scale permanent increases in unemployment are inevitable.9 In 1990, the three largest companies in Detroit with a combined market capital- ization of $36 billion employed 1.2 million workers.10 In 2014, the three
6 See, e.g., HERRING KAGERMANN, ET AL., INDUSTRIE 4.0 WORKING GRP., RECOMMENDA- TIONS FOR IMPLEMENTING THE STRATEGIC INITIATIVE INDUSTRIE 4.0, at 5 (2013), http://www .acatech.de/fileadmin/user_upload/Baumstruktur_nach_Website/Acatech/root/de/Material_fuer _Sonderseiten/Industrie_4.0/Final_report__Industrie_4.0_accessible.pdf [https://perma.cc/ PA7X-YSRE] (“The first three industrial revolutions came about as a result of mechanisation, electricity and IT. Now, the introduction of the Internet of Things and Services into the manu- facturing environment is ushering in a fourth industrial revolution.”); see also VERNOR VINGE, THE COMING TECHNOLOGICAL SINGULARITY: HOW TO SURVIVE IN THE POST-HUMAN ERA (1993) https://edoras.sdsu.edu/~vinge/misc/singularity.html [https://perma.cc/K4C9-LRDE] (coining the term “singularity” to refer to the argument that “we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence.”).
7 See generally Ryan Abbott, The Reasonable Computer: Disrupting the Paradigm of Tort Liability, 86 GEO. WASH. L. REV. (forthcoming 2018) (discussing the potential of automation to result in substantial safety benefits, for instance in the transportation industry); see also Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 1079 B.C. L. REV. 1083–91 (2016) (discussing examples in which AI has generated patentable subject matter under circumstances in which the computer rather than a person has qualified for inventorship).
8 See COMM. ON TECH., NAT’L SCI. & TECH. COUNCIL, PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE 2 (2016) [hereinafter COMM. ON TECH.], https://obamawhitehouse .archives.gov/sites/default/files/whitehouse_files/microsites/ostp/NSTC/preparing_for_the_fut ure_of_ai.pdf [https://perma.cc/DEH5-AQBK].
9 See Klaus Schwab & Richard Samans, Preface to WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION, at v–vi (2016), http://www3.weforum.org/docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL]; see also Brian Dorini, The End of Work: The Decline of the Global Labor Force and the Dawn of the Post-Market Era, 9 HARV. J.L. & TECH. 231, 232–33 (1995) (reviewing JEREMY RIFKIN, THE END OF WORK: THE DECLINE OF THE GLOBAL LABOR FORCE AND THE DAWN OF THE POST-MARKET ERA 136–43 (1995)) (“The ranks of the unem- ployed are swelling with former service sector workers, such as secretaries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recognition devices, elec- tronic mail, and inventory control and monitoring devices.”) (citation omitted).
10 See Michael Chui & James Manyika, Digital Era Brings Hyperscale Challenges, FIN. TIMES (Aug. 13, 2014), https://www.ft.com/content/f30051b2-1e36-11e4-bb68-00144feabdc0 [http://perma.cc/4QHG-ZKDL].
 
148 468 Harvard Law & Policy Review [Vol. 12
largest companies in Silicon Valley with a combined market capitalization of $1.09 trillion employed 137,000 workers.11
These are not new problems.12 In 1962, President Kennedy stated, “I regard it as the major domestic challenge, really, of the sixties, to maintain full employment at a time when automation, of course, is replacing men.”13 His solution was to pass the nation’s first and most sweeping federal pro- gram to train workers unemployed due to technological advances.14 More recently, in December 2016, the Executive Office of the President issued a report which outlined a three-pronged policy response to automation and AI, namely, to: (i) “[i]nvest in and develop AI for its many benefits,” (ii) “[e]ducate and train Americans for jobs of the future,” and, (iii) “[a]id workers in the transition and empower workers to ensure broadly shared growth.”15 These and other proposals for dealing with automation have fo- cused on improving education and improving social benefit systems. Con- cerns about technological unemployment have even breathed new life into an old social benefit proposal—guaranteed minimum income, which could involve the government making fixed payments to each of its citizens re- gardless of their circumstances.16 While education reform often enjoys bipar- tisan support, enhanced social benefits are a politically challenging goal since liberals and conservatives often disagree on their desirability.17 In any
11 See id.
12 See generally JOHN FORBES DOUGLAS, SOME EVIDENCES OF TECHNOLOGICAL UNEM- PLOYMENT IN ANCIENT ATHENS AND ROME (1932). For instance, the Roman Emperor Vespa- sian once refused to use a labor-saving transportation machine, famously stating, “You must allow my poor hauliers to earn their bread.” See Steve Welch, The Real Political Divide is Education, TECH CRUNCH (Dec. 30, 2016), https://techcrunch.com/2016/12/30/the-real-politi- cal-divide-is-education/ [https://perma.cc/EL6C-JKAQ].
13 John F. Kennedy, The President’s News Conference, AM. PRESIDENCY PROJECT (Feb. 14, 1962), http://www.presidency.ucsb.edu/ws/index.php?pid=9003 [https://perma.cc/2L35- QTT7].
14 See Gladys Roth Kremen, MDTA: The Origins of the Manpower Development and Training Act of 1962, U.S. DEP’T OF LAB. (1974), www.dol.gov/general/aboutdol/history/ mono-mdtatext [https://perma.cc/KFC7-MPCV] (describing the law’s origins). Also of note, in 1961 (a year before the MDTA), the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. See id. For reviews of automation issues in the 1960s, see JAMES L. SUNDQUIST, POLITICS AND POLICY: THE EISENHOWER, KENNEDY, AND JOHNSON YEARS 77 (1968).
15 EXEC. OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY 3 (2016) [hereinafter ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECON- OMY], https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial- Intelligence-Automation-Economy.pdf [https://perma.cc/LK89-E5RG].
16 Guaranteed minimum income was proposed during the Industrial Revolution by Charles Fourier, and then later Joseph Charlier, before being adopted by John Stuart Mill. See Philippe Van Parijs, A Basic Income for All, BOS. REV. (2000), bostonreview.net/forum/ubi-van-parijs [https://perma.cc/6E52-Q63K]. (According to Mill’s proposal, “[A] certain minimum is first assigned for the subsistence of every member of the community, whether capable or not of labour.”).
17 See Yvonne A. Stevens, The Future: Innovation and Jobs, 56 JURIMETRICS J. 367, 373 (2016) (“One of the most commonly considered government payout schemes is what is re- ferred to as a basic income guarantee (BIG). Generally speaking, BIG is a monetary govern- ment-backed and issued guarantee such that all adults have access to an amount of money necessary to meet basic needs.”). President Richard Nixon also once proposed a guaranteed
 
469
2018] Should Robots Pay Taxes? 149
event, both education and social benefit reforms to deal with automation would require significant financial support.18
While there has been a lively public discourse on technological unem- ployment and income disparity, the automation debate has historically ig- nored the issue of taxation. That has very recently started to change. In February 2017, the European Parliament rejected a proposal to impose a “robot tax” on owners to fund support for displaced workers, citing con- cerns of stifling innovation.19 The next day, Bill Gates stated that he thought governments should tax companies’ use of robots to slow the spread of auto- mation and to fund other types of employment.20 Former U.S. Secretary of the Treasury Lawrence Summers then claimed Gates’s argument was “pro- foundly misguided.”21 In August 2017, South Korea announced plans for the world’s first “tax on robots” by limiting tax incentives for automated ma- chines.22 Currently, Korean businesses may deduct three to seven percent of an investment in automation equipment from their corporate taxes, depend- ing on the size of their operation.23 The announced reform would decrease the deduction rate by up to two percent.24
basic income of about $10,000 in today’s dollars for families of four. This proposal, the Family Assistance Plan, passed through the House before it was voted down by Senate Democrats. See Whitney Mallett, The Town Where Everyone Got Free Money, VICE: MOTHERBOARD (Feb. 4, 2015), https://motherboard.vice.com/en_us/article/nze99z/the-mincome-experiment-dauphin [https://perma.cc/R7E7-3NTL].
18 For example, in 2016, Switzerland voted down proposed guaranteed minimum income legislation that would have provided each citizen with about $30,000 a year. The cost of the legislation was estimated at about $200 billion, about three times Switzerland’s current annual federal spending. See John Thornhill & Ralph Atkins, Universal Basic Income: Money for Nothing, FIN. TIMES.COM (May 26, 2016), https://www.ft.com/content/7c7ba87e-229f-11e6- 9d4d-c11776a5124d [https://perma.cc/GR78-WG5H]. In the United Kingdom, it was esti- mated that distributing the current total welfare spending of £251 billion to 64.5 million per- sons as a universal basic income would result in a monthly payment to all residents of just £324. See Jim Edwards & Will Heilpern, Here’s How Much We’d All Get if the UK Introduced a ‘Fiscally Neutral’ Universal Basic Income Scheme, BUS. INSIDER (June 6, 2016, 10:25 AM), http://www.businessinsider.com/universal-basic-income-scheme-for-the-uk-2016- 6?r=UK&IR=T [https://perma.cc/4YRW-35G9]. This analysis is overly simplified, but dem- onstrates that providing a meaningful level of social benefits on a widespread basis requires significant funding.
19 See Reuters Staff, European Parliament Calls for Robot Law, Rejects Robot Tax, REUTERS (Feb. 16, 2007, 2:03 PM), http://ca.reuters.com/article/technologyNews/idCAKBN15 V2KM [https://perma.cc/5KTN-6VTJ].
20 See Kevin J. Delaney, The Robot That Takes Your Job Should Pay Taxes, Says Bill Gates, QUARTZ (Feb. 17, 2017), https://qz.com/911968/bill-gates-the-robot-that-takes-your- job-should-pay-taxes/ [https://perma.cc/6SHD-L7WY] (“Exactly how you’d do it, measure it, you know, it’s interesting for people to start talking about now.”).
21 Sarah Kessler, Lawrence Summers Says Bill Gates’ Idea for a Robot Tax is “Profoundly Misguided”, QUARTZ (Mar. 6, 2017), https://qz.com/925412/lawrence-summers-says-bill- gates-idea-for-a-robot-tax-is-profoundly-misguided/ [https://perma.cc/ATV3-DXEG].
22 See Cara McGoogan, South Korea Introduces World’s First ‘Robot Tax’, TELEGRAPH: TECH (Aug. 9, 2017, 12:54 PM), http://www.telegraph.co.uk/technology/2017/08/09/south-ko- rea-introduces-worlds-first-robot-tax/ [https://perma.cc/H93H-RPMC].
23 See Yoon Sung-won, Korea Takes First Step to Introduce ‘Robot Tax’, KOREA TIMES (Aug. 7, 2017, 8:47 PM), http://www.koreatimes.co.kr/www/news/tech/2017/08/133_234312 .html [https://perma.cc/82WW-B4QL].
24 See id.
 
150 470 Harvard Law & Policy Review [Vol. 12
The critical importance of tax policies on automation has not been ap- preciated. The current system encourages automation by providing employ- ers with preferential tax treatment for robot workers. Automation allows firms to avoid employee and employer wage taxes levied by federal, state, and local taxing authorities. It also permits firms to claim accelerated tax depreciation on capital costs for automated workers, and it creates a variety of indirect incentives for machine workers. All of this is the unintended re- sult of a tax system designed to tax labor rather than capital. Tax policies may thus result in automation in some cases in which a firm would other- wise choose a human worker.
Even more concerning, automation significantly reduces the govern- ment’s tax revenue since most tax revenue comes from labor-related taxes.25 When firms replace employees with machines, the government loses income due to taxation. A very rough estimate of revenue loss can be arrived at by multiplying an effective tax rate by the gross salary loss due to automation. In January 2017, the McKinsey Global Institute claimed that about half of current work activities could be automated using currently demonstrated technologies, which would eliminate $2.7 trillion in annual wages in the United States alone.26 Workers pay high effective tax rates ranging from twenty-five percent to fifty-five percent when all tax types are taken into account.27 This suggests that worker automation could result in hundreds of billions or even trillions of dollars in tax revenue lost per year at various levels of government.28
In the United States and most other developed nations, the bulk of taxes are currently remitted by workers either through wage withholding, taxation of labor income, or indirect taxation of workers as consumers.29 Since robots are not subject to these types of tax regimes, automation reduces the overall tax base. Robots are simply not taxpayers, at least not to the same extent as human workers. If all workers were to be replaced by machines tomorrow,
25 See OFFICE OF MGMT. & BUDGET, EXEC. OFFICE OF THE PRESIDENT, FISCAL YEAR 2015 HISTORICAL TABLES: BUDGET OF THE U.S. GOVERNMENT 32–33 tbl.2.1 (2015), https://www .gpo.gov/fdsys/pkg/BUDGET-2015-TAB/pdf/BUDGET-2015-TAB.pdf [https://perma.cc/ TT33-T3HA] (showing that individual income taxes, Social Security taxes, Medicare taxes, and other taxes assessed on labor wages comprised more than fifty percent of overall revenue); I.R.S., PUB. NO. 55B, DATA BOOK, 2014, at 3 tbl.1 (2015), https://www.irs.gov/pub/irs-pdf/ p55b.pdf [https://perma.cc/Q4YU-GUFD]; CONG. BUDGET OFFICE, DISTRIBUTION OF HOUSE- HOLD INCOME AND FEDERAL TAXES, 2010 (2013), https://www.cbo.gov/sites/default/files/ 113th-congress-2013-2014/reports/44604-AverageTaxRates.pdf [https://perma.cc/GH9J- YNQW]; see also Lester B. Snyder & Marianne Gallegos, Redefining the Role of the Federal Income Tax: Taking the Tax Law “Private” Through the Flat Tax and Other Consumption Taxes, 13 AM. J. TAX POL’Y 1, 86 (1996).
26 MANYIKA ET AL., supra note 2, at 6 exhibit E3.
27 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level, 145 TAX NOTES 117, 117 tbl.1 (2014).
28 See MANYIKA ET AL., supra note 2, at 5; see also Frey & Osborne, supra note 1, at 267 (asserting that many creative science, engineering, and general knowledge work jobs will be done by computers in the long run).
29 See REVENUE STATISTICS - OECD COUNTRIES: COMPARATIVE TABLES, ORG. FOR ECON. CO-OPERATION & DEV. (2016), http://stats.oecd.org/Index.aspx?DataSetCode=REV [https:// perma.cc/74EJ-2KS8].
 
471
2018] Should Robots Pay Taxes? 151
most of the tax base would immediately disappear. As a matter of taxation, automated workers represent a type of capital investment, and capital in- come is currently taxed at much lower rates than labor income.30 This is not accidental; it is based on the historic belief that the taxation of labor income is more efficient than the taxation of capital income. This concept is dis- cussed in tax policy analysis as the “tax incidence” of capital taxation.31
Tax is thus critically important to the automation debate. Tax policies should not encourage automation unless it is part of a deliberate strategy based on sound public policy. We believe the solution is to adjust the tax system to be at least neutral as between robot and human workers.32 More ambitiously, changes to tax policies are necessary to account for the loss of government tax revenue due to automation. This is particularly critical be- cause the education and social benefit reform necessitated by automation will only be possible with more, not less, tax revenue.
This article outlines several potential tax policy solutions to address the automation revolution. Tax “neutrality” between human and automated workers could be achieved through some combination of disallowing corpo- rate tax deductions for automated workers, creating an “automation tax” which mirrors existing unemployment schemes, granting offsetting tax pref- erences for human workers, levying a corporate self-employment tax, and increasing the corporate tax rate. Neutrality in this setting refers to a system in which various alternatives are taxed equally, and so actors make decisions based on non-tax reasons.
Tax neutrality is widely accepted as an economically efficient principle for organizing a tax system.33 Neutral taxes are more likely to have fewer negative effects, lower administration and compliance costs, promote distri-
30 The term “capital taxation” refers here to corporate income taxation. For a comparison of effective tax rates between U.S. and EU multinationals, see Reuven S. Avi-Yonah & Yaron Lahav, The Effective Tax Rate of the Largest U.S. and EU Multinationals, 65 TAX L. REV. 375 (2012).
31 In a strange twist of economic theory, the ultimate cost of wage taxation paid by work- ers is generally thought to be borne by capital. See Arnold C. Harberger, Tax Policy in a Small, Open Developing Economy, in THE ECONOMICS OF THE CARIBBEAN BASIN 1 (Michael B. Con- nolly & John McDermott eds., 1985). For the extension of the “small open economy” model beyond the small open economy context, see A. Lans Bovenberg, Capital Income Taxation in Growing Open Economies, 31 J. PUB. ECON. 347 (1986); Anne Sibert, Taxing Capital in a Large, Open Economy, 41 J. PUB. ECON. 297 (1990); Alan J. Auerbach, Who Bears the Corpo- rate Tax? A Review of What We Know, 20 TAX POL’Y & ECON. 1 (2006).
32 See WILLIAM MEISEL, THE SOFTWARE SOCIETY: CULTURAL AND ECONOMIC IMPACT 226 (2013) (“There are other alternatives using the tax code. One option suggested by Martin Ford in The Lights in the Tunnel is modification of the payroll tax, a tax that discourages hiring people and encourages automation since it makes the use of people more expensive. He sug- gests a reform of the tax system where we get away from taxing based on workers to reduce the disincentive to hiring.”) (citing MARTIN FORD, THE LIGHTS IN THE TUNNEL: AUTOMATION, ACCELERATING TECHNOLOGY, AND THE ECONOMY (2009)).
33 See Tax: Fundamentals in Advance of Reform: Hearing Before the S. Comm. on Fin., 110th Cong. 41–50 (2008) (prepared statement of Jason Furman, Senior Fellow and Director of The Hamilton Project, The Brookings Institution) [hereinafter Prepared Statement of Jason Furman], https://www.finance.senate.gov/imo/media/doc/56020.pdf [https://perma.cc/Y98J- RN8K].
 
152 472 Harvard Law & Policy Review [Vol. 12
butional fairness, and increase transparency.34 Tax neutrality can thus result in a broader tax base with lower rates.35 Non-neutralities in the tax system distort choices and behavior other than for economic reasons, and encourage socially wasteful efforts to reduce tax payments.36 They can thus “create complexity, encourage avoidance, and add costs for both taxpayers and governments.”37
However, non-neutral taxes can be used deliberately to advance social policy—for instance, incentivizing activities like medical research, educa- tion, and homeownership.38 Taxes may also be used to disincentivize certain activities, as so-called “Pigouvian” taxes. For instance, consumer goods such as alcoholic beverages and tobacco products bear an exceptional tax burden. In turn, this results in increased consumer costs, with the goal of decreasing consumption—but due to taxes rather than to other market and economic factors.
The advantage of tax neutrality as between human and automated work- ers is that it permits the marketplace to adjust without tax distortions. With a level playing field, firms should only automate if it will be more efficient, without taking taxes into account. Since the current tax system favors auto- mated workers, a move toward a neutral tax system could increase the ap- peal of human workers. Policy solutions could even be implemented to make human workers more appealing than machines in terms of tax costs and ben- efits, to the extent policy makers choose to discourage automation.
The remainder of this article is divided into three parts. Part I discusses the phenomenon of automation and provides historical background on ef- forts to deal with its harmful effects. Part II analyzes current tax policies and contends that they promote automation even where it would not otherwise be efficient. Finally, Part III argues that changes to tax policy are needed to prevent the unintended consequences of encouraging automation and to off- set the government’s loss of tax revenue. We provide several potential solu- tions for achieving these goals.
The increased tax revenue from our proposal could be used to provide improved education and training for workers rendered unemployed by robots and computers. Should the pessimistic prediction of a near future with sub- stantially increased unemployment due to automation manifest, these taxes could also support social benefit programs such as a guaranteed minimum income. Automation will likely generate more wealth than has ever been possible. It should not come at the expense of the most vulnerable.
34 See JAMES MIRRLEES ET AL., INST. FOR FISCAL STUDIES, TAX BY DESIGN 22–23 (2011), https://www.ifs.org.uk/docs/taxbydesign.pdf [https://perma.cc/JSU8-KS5Q].
35 See Prepared Statement of Jason Furman, supra note 33, at 33.
36 See MIRRLEES ET AL., supra note 34, at 40.
37 Id. at 41.
38 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents–Prizes De-
bate, 92 TEX. L. REV. 303 (2013).
 
473
2018] Should Robots Pay Taxes? 153
I. THE PROBLEM WITH AUTOMATION
A. Automation is Coming
Experts are widely predicting that automation is going to have a sub- stantial impact on employment even in the near term. Bank of America Mer- rill Lynch argues that by 2025, AI may eliminate $9 trillion in employment costs by automating knowledge work.39 A report by the World Economic Forum estimates that automation could result in the net loss of 5.1 million jobs by 2020.40 The consulting firm Deloitte claims that thirty-five percent of jobs in the United Kingdom are at high risk of redundancy due to automation in the next ten to twenty years.41 This is due to a combination of factors: improvements in automation technologies, decreased costs for such technol- ogies, and increased labor costs. Whereas it was previously possible to auto- mate a large number of work processes, it has now become practicable. As automation technologies continue to both improve and decrease in cost, it is difficult to think of work functions that will not eventually be susceptible to automation.42
1. The Good: Increased Productivity and New Jobs
Automation increases productivity, which generates value and creates wealth.43 Partly due to technological advances and automation, the U.S. Gross Domestic Product (GDP) has steadily risen from $1.37 trillion in 1960 to $73.5 trillion in 2015.44 Despite academic criticism, GDP has remained the dominant economic indicator of welfare and standard of living for half a century.45
39 BANK OF AMERICA MERRILL LYNCH, supra note 1, at 1 (noting also that AI will yield $14–33 trillion in annual economic impact).
40 See WORLD ECON. F., THE FUTURE OF JOBS: EMPLOYMENT, SKILLS AND WORKFORCE STRATEGY FOR THE FOURTH INDUSTRIAL REVOLUTION 13 (2016), http://www3.weforum.org/ docs/WEF_Future_of_Jobs.pdf [https://perma.cc/K6B4-2EDL].
41 DELOITTE, AGILETOWN: THE RELENTLESS MARCH OF TECHNOLOGY AND LONDON’S RE- SPONSE 5 (2014), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/uk-futures/ london-futures-agiletown.pdf [https://perma.cc/Z5HU-PY25].
42 See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 188–91 (Cassidy R. Sugimoto et al. eds., 2016) (noting the ways in which automation technologies could replace workers in the pharmaceutical sciences).
43 See generally Joel Mokyr et al., The History of Technological Anxiety and the Future of Economic Growth: Is This Time Different?, 29 J. ECON. PERSP. 31 (2015).
44 See GDP (Current US$), WORLD BANK: DATA (2016), https://data.worldbank.org/indi- cator/NY.GDP.MKTP.CD [https://perma.cc/C34J-U67E].
45 See, e.g., Jeroen C.J.M. van den Bergh, The GDP Paradox, 30 J. ECON. PSYCHOL. 117, 117–18 (2008) (“Gross domestic product (GDP) is the monetary, market value of all final goods and services produced in a country over a period of a year. The real GDP per capita (corrected for inflation) is generally used as the core indicator in judging the position of the economy of a country over time or relative to that of other countries. The GDP is thus implic- itly, and often even explicitly, identified with social welfare—witness the common substituting phrase ‘standard of living’. . . . For over half a century now, the GDP (per capita) has been
 
154 474 Harvard Law & Policy Review [Vol. 12
Automation can also create new jobs.46 Human workers may be needed to build and maintain automation technologies. Automation may free up cap- ital for investments in new enterprises, result in the creation of new prod- ucts, or decrease production costs for existing products. Decreased production costs may result in lower consumer prices and thus greater con- sumer demand. All of this may increase employment. Technological ad- vances have also historically upgraded the labor force: automation has reduced the need for unskilled workers but increased the need for skilled workers.47 For instance, some of today’s most in-demand occupations did not exist even five years ago.48
2. The Bad: Unemployment and Inequality
Automation can cause under- and un-employment. While worker pro- ductivity has risen robustly since 2000, employment has stagnated.49 This may be due in part to technological advances.50 When a company like Mc- Donald’s introduces computer cashiers, the company may save money and consumers may enjoy lower prices.51 But human cashiers now find them- selves in a more competitive labor market. The enhanced competition may result in lower wages, less favorable employment terms, fewer working
severely criticized as not adequately capturing human welfare and progress. All the same, the GDP has maintained a firm position as a dominant economic indicator. . . .”).
46 The following arguments were referred to as “compensation theory” by Karl Marx, who argued none of these effects were guaranteed and that automation could result in forcing workers into lower paying jobs. See KARL MARX, CAPITAL, VOLUME I: THE PROCESS OF PRO- DUCTION OF CAPITAL 570 (1867).
47 See Automation and Technological Change: Hearing Before the Subcomm. on Econ. Stabilization of the J. Comm. on the Econ. Rep., 84th Cong. 29, 34–35 (Statement of Walter S. Buckingham, Jr., Associate Professor, Georgia Institute of Technology), https://www.jec.sen ate.gov/reports/84th%20Congress/Automation%20and%20Technological%20Change%20-% 20Hearings%20%2875%29.pdf [https://perma.cc/B348-CT38].
48 See WORLD ECON. F., supra note 40, at 3.
49 See, e.g., STEVEN GREENHOUSE, THE BIG SQUEEZE: TOUGH TIMES FOR THE AMERICAN WORKER 3 (2008).
50 See id. at 9.
51 Cf. Ted Goodman, Fight for $15? McDonald’s To Place Automated Ordering Stations At All US Locations, DAILY CALLER (Nov. 18, 2016, 6:44 PM), http://dailycaller.com/2016/11/ 18/fight-for-15-mcdonalds-to-place-automated-ordering-stations-at-all-us-locations [https:// perma.cc/VS4R-X35Y]. Standard economic principles suggest that in a competitive market lower business costs will result in lower consumer prices. See, e.g., Arthur A. Thompson, Jr., Strategies for Staying Cost Competitive, HARV. BUS. REV. (Jan. 1984), https://hbr.org/1984/01/ strategies-for-staying-cost-competitive [https://perma.cc/Y3SR-2WQC]. In fairness, fast food automation has been around since the nineteenth century. See Angelika Epple, The “Automat”: A History of Technological Transfer and the Process of Global Standardization in Modern Fast Food around 1900, 7 FOOD & HISTORY 97, 98 (2009), http://wwwhomes.uni-bielefeld.de/aep- ple/Aufsatz12TheAutomat2009.pdf [https://perma.cc/LZ7F-MSXS] (discussing the restaurant chain “Automat” which opened its first location in 1896). (“One of [Automat’s] highly unique selling features around 1900 was that no waiters were to be seen in the guest room. The Automat of that time was—at first sight—operated by vending machines only. ‘You absolutely help yourself’ was one of its most prominent marketing slogans.”) Id. at 99. The Automat’s technology transferred around the U.S. and Europe and eventually developed into the world’s largest restaurant chain: Horn & Hardart. Id. at 97.
 
475
2018] Should Robots Pay Taxes? 155
hours, reduced hiring, or layoffs.52 As the former CEO of McDonald’s USA famously quipped, “[i]t’s cheaper to buy a $35,000 robotic arm than it is to hire an employee who’s inefficient making $15 an hour bagging French fries. . . .”53 McDonald’s is now expanding its use of automated cashiers throughout the United States and in other countries.54
Also, while automation generates wealth, it does so unevenly. Over the past twenty-five years, partly due to automation technologies, the income share of the top 0.1% has increased substantially.55 The top 0.1% of the U.S. population is now worth about as much as the bottom 90%.56 CEO-to-worker pay ratios have increased a thousand-fold since 1950,57 but overall wages have been stagnant for thirty-five years.58 Increased automation is likely to accelerate these trends. The White House Council of Economic Advisers has predicted that future automation will disproportionately affect lower-wage jobs and less educated workers, causing greater economic inequality.59
Worsening employment coupled with growing income inequality is a recipe for social unrest.60 As physicist Stephen Hawking has warned,
52 See Simon Neville, McDonald’s ties nine out of 10 workers to zero-hours contracts, GUARDIAN (Aug. 5, 2013, 4:13 PM), https://www.theguardian.com/business/2013/aug/05/ mcdonalds-workers-zero-hour-contracts [https://perma.cc/UF3D-D4S4] (noting that 90% of McDonald’s UK workers have no guaranteed hours); see also Stephanie Strom, McDonald’s Introduces Screen Ordering and Table Service, N.Y. TIMES (Nov. 17, 2016), https://www.ny- times.com/2016/11/18/business/mcdonalds-introduces-screen-ordering-and-table-service .html?_r=0 [https://perma.cc/3DZ7-R68J] (reporting that the cost of purchasing and installing eight touch order screens is $56,000).
53 Julia Limitone, Fmr. McDonald’s USA CEO: $35K Robots Cheaper Than Hiring at $15 Per Hour, FOX BUS. (May 24, 2016), http://www.foxbusiness.com/features/2016/05/24/fmr- mcdonalds-usa-ceo-35k-robots-cheaper-than-hiring-at-15-per-hour.html [https://perma.cc/ W65G-697K] (claiming that a $15 minimum wage results in $30,000 a year for a full-time employee).
54 See Ed Rensi, The Ugly Truth About a $15 Minimum Wage, FORBES (Apr. 25, 2016, 6:30 AM), https://www.forbes.com/sites/realspin/2016/04/25/mcdonalds-minimum-wage-real- ity/#1f50a0d93edd [https://perma.cc/TT3E-G5SR]. Automated cashiers are already the “norm” in European countries with high labor costs, and McDonald’s is now experimenting with self-serve McCafe kiosks. See id.
55 See CARL BENEDIKT FREY & MICHAEL OSBORNE, TECHNOLOGY AT WORK: THE FUTURE OF INNOVATION AND EMPLOYMENT 14 (2015) http://www.oxfordmartin.ox.ac.uk/downloads/ reports/Citi_GPS_Technology_Work.pdf [https://perma.cc/YE22-D6AE].
56 See Angela Monaghan, US Wealth Inequality - Top 0.1% Worth as Much as the Bottom 90%, GUARDIAN (Nov. 13, 2014, 7:00 AM), https://www.theguardian.com/business/2014/nov/ 13/us-wealth-inequality-top-01-worth-as-much-as-the-bottom-90 [https://perma.cc/62U8- 6ADA].
57 Elliot Blair Smith & Phil Kuntz, CEO Pay 1,795-to-1 Multiple of Wages Skirts U.S. Law, BLOOMBERG MARKETS (Apr. 30, 2013, 12:01 AM), https://www.bloomberg.com/news/ articles/2013-04-30/ceo-pay-1-795-to-1-multiple-of-workers-skirts-law-as-sec-delays [https:// perma.cc/NNF6-P2X6].
58 See ELISE GOULD, ECONOMIC POLICY INSTITUTE, 2014 CONTINUES A 35-YEAR TREND OF BROAD-BASED WAGE STAGNATION (2015), http://www.epi.org/files/pdf/stagnant-wages-in- 2014.pdf [https://perma.cc/YN3U-9LMJ].
59 COMM. ON TECH., supra note 8, at 2.
60 See Katie Allen, ILO Warns of Rise in Social Unrest and Migration as Inequality Widens, GUARDIAN (Jan. 12, 2017, 4:00 PM), https://www.theguardian.com/business/2017/jan/ 12/ilo-warns-of-rise-in-social-unrest-and-migration-as-inequality-widens [https://perma.cc/ 3DHG-T2WH].
 
156 476 Harvard Law & Policy Review [Vol. 12
“[e]veryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine- owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increas- ing inequality.”61
3. The Ugly: Reduced Tax Remittances
One of automation’s most pronounced and unappreciated effects relates to taxes. Automation substantially reduces tax revenue. Most of the U.S. government’s tax revenue comes from taxes on workers.62 By stating that most tax revenue comes from workers, we refer to the aggregate amount of wage tax, income tax, and indirect taxes levied on income or wages derived from work at all levels of government. Much of the prior tax policy debate focused solely on income taxation by the federal government.63 Of course, a substantial portion of income subject to federal income tax arises from work and falls within our definition of worker taxation. However, the tax policy debate has been misleading since wage taxes are also levied on labor income and comprise more than one-third of federal remittances. Likewise, indirect state taxes are levied on workers. Consequently, by replacing employees with machines, the government loses out on employee and employer wage taxes levied by federal, state, and local taxing authorities. In addition, tax revenue may be further reduced from businesses claiming accelerated tax depreciation on capital outlays for machines and from other tax incentives related to indirect taxation, such as sales tax or value-added tax (VAT) exemptions.64
B. History of the Automation Scare
Fears of the consequences of automation have been expressed since the industrial revolution.65 In 1801, the writer Thomas Mortimer objected to ma- chines, “which are intended almost totally to exclude the labor of the human
61 Akshat Rathi, Stephen Hawking: Robots aren’t just taking our jobs, they’re making soci- ety more unequal, QUARTZ (Oct. 9, 2015), http://qz.com/520907/stephen-hawking-robots- arent-just-taking-our-jobs-theyre-making-society-more-unequal [https://perma.cc/A2BN- VYY5].
62 See OFFICE OF MGMT. & BUDGET, supra note 25, at 32–33 tbl.2.1.
63 See, e.g., CURTIS S. Dubay, THE HERITAGE FOUNDATION, THE RICH PAY MORE TAXES: TOP 20 PERCENT PAY RECORD SHARE OF INCOME TAXES (2009), http://www.heritage.org/pov- erty-and-inequality/report/the-rich-pay-more-taxes-top-20-percent-pay-record-share-income- taxes [https://perma.cc/N97H-VETS].
64 See infra Part III.
65 For that matter, broader social issues related to automation have been discussed since Aristotle’s time. See, e.g., JOHANNES HANEL, ASSESSING INDUCED TECHNOLOGY: SOMBART’S UNDERSTANDING OF TECHNICAL CHANGE IN THE HISTORY OF ECONOMICS 91 (2008) (noting Aristotle’s hope that machines could occupy the place of slaves in a utopian society).
 
477
2018] Should Robots Pay Taxes? 157
race.”66 In 1821, the economist David Ricardo argued that automation would result in inequality, and that “substitution of machinery for human labour, is often very injurious to the interests of the class of labourers. . . . [It] may render the population redundant, and deteriorate the condition of the la- bourer.”67 In 1839, the philosopher Thomas Carlyle more poetically wrote:
[T]he huge demon of Mechanism smokes and thunders, panting at his great task, in all sections of English land; changing his shape like a very Proteus; and infallibly, at every change of shape, over- setting whole multitudes of workmen, as if with the waving of his shadow from afar, hurling them asunder, this way and that, in their crowded march and course of work or traffic; so that the wisest no longer knows his whereabout[s].68
The Industrial Revolution even gave birth to a social movement and group protesting the use of new technologies: the Luddites.69 Luddites were primarily English textile workers who objected to working conditions in the nineteenth century. They believed that automation threatened their liveli- hoods, and they were opposed to the introduction of industrial machinery.70 Some Luddites engaged in violent episodes of machine-breaking, in re- sponse to which the English government made machine-breaking a capital offense.71
The Luddite movement died out, but automation concerns persisted throughout the twentieth century, often flaring during times of rapid techno- logical progress.72 For instance, the debate was revitalized in the 1950s and 1960s with the widespread introduction of office computers and factory ro-
66 THOMAS MORTIMER, LECTURES ON THE ELEMENTS OF COMMERCE, POLITICS, AND FI- NANCES 72 (London, A. Strahan, for T. N. Longman and O. Rees 1801).
67 DAVID RICARDO, ON THE PRINCIPLES OF POLITICAL ECONOMY AND TAXATION 283–84 (Batoche Books 2001) (3d ed. 1821).
68 2 THOMAS CARLYLE, THE WORKS OF THOMAS CARLYLE: CRITICAL AND MISCELLANE- OUS ESSAYS 141–42 (Henry Duff Traill ed., Cambridge Univ. Press 2010) (1899). Thomas Carlyle called the Industrial Revolution “the Mechanical Age.” Id at 59. Carlyle wrote that technology was causing a “mighty change” in their “modes of thought and feeling. Men are grown mechanical in head and in heart, as well as in hand.” Id. at 63.
69 See Richard Conniff, What the Luddites Really Fought Against, SMITHSONIAN MAG. (Mar. 2011), https://www.smithsonianmag.com/history/what-the-luddites-really-fought- against-264412/ [https://perma.cc/98RV-LNJ2].
70 See Ian Coulson, Power, Politics & Protest: The Growth of Political Rights in Britain in the 19th Century: Luddites, NAT’L ARCHIVES (U.K.), https://www.nationalarchives.gov.uk/edu- cation/politics/g3/ [https://perma.cc/96H4-4NAR].
71 See id.; see also Conniff, supra note 69. The “Luddite fallacy” now describes the fear that innovation will have long-term harmful labor effects. See Vivek Wadhwa, Sorry, but the jobless future isn’t a luddite fallacy, WASH. POST (July 7, 2015), https://www.washingtonpost .com/news/innovations/wp/2015/07/07/sorry-but-the-jobless-future-isnt-a-luddite-fallacy/?utm _term=.f52e3687022c [https://perma.cc/5JUA-YPUE].
72 In 1924, Mohandas Karamchand Gandhi wrote, “What I object to, is the craze for ma- chinery, not machinery as such. The craze is for what they call labour-saving machinery. Men go on ‘saving labour’, till thousands are without work and thrown on the open streets to die of starvation.” MOHANDAS K. GANDHI, YOUNG INDIA (1924), reprinted in ALL MEN ARE BROTH- ERS: LIFE AND THOUGHTS OF MAHATMA GANDHI AS TOLD IN HIS OWN WORDS 126 (Krishna Krapilani ed., 1958).
 
158 478 Harvard Law & Policy Review [Vol. 12
bots.73 In his 1960 election campaign, John F. Kennedy suggested that auto- mation offered “hope of a new prosperity for labor and a new abundance for America,” but that it also “carries the dark menace of industrial dislocation, increasing unemployment, and deepening poverty.”74
Despite these concerns, technological advances have historically re- sulted in overall job creation. The computer eliminated jobs, but created jobs for working with information created by computers. The automobile elimi- nated jobs, but created jobs in the motel and fast-food industries. The tractor and other agricultural advances eliminated jobs, but drove job growth in other areas of the economy. In 1900, forty-one percent of the workforce was employed in agriculture.75 In 2000, less than two percent of the employed labor force worked in agriculture.76 Yet this has not translated to a thirty-nine percent increase in unemployment. Even as agriculture-based employment and agriculture’s relative contribution to the GDP decreased, the productivity of farmworkers skyrocketed and agriculture’s absolute contribution to the GDP increased.77 Indeed, in each era when concerns have been expressed about automation causing mass unemployment, technology has created more jobs than it has destroyed.
C. Is This Time Different?
The automation debate is resurfacing with a vengeance due to recent advances in AI and other automation technologies. Once more, prognosti- cators are divided into two camps: the optimists who claim there will be a net creation of jobs, and the pessimists who predict mass unemployment and growing inequality.78
History favors the optimists.79 They argue that technological advances will generate widespread benefits together with overall job creation. They
73 See Kremen, supra note 14 (“The dawn of the Atomic Age had witnessed the imple- mentation of a new technology that threatened to replace men with machines.”); see also Douglas A. Irwin, Comments, in JAGDISH BHAGWATI & ALAN S. BLINDER, OFFSHORING OF AMERICAN JOBS: WHAT RESPONSE FROM U.S. ECONOMIC POLICY? 79 (Benjamin M. Friedman ed., 2009).
74 Irwin, supra note 73, at 80.
75 See CAROLYN DIMITRI ET AL., U.S. DEP’T OF AGRIC., THE 20TH CENTURY TRANSFORMA- TION OF U.S. AGRICULTURE AND FARM POLICY 2 (June 2005), https://www.ers.usda.gov/ webdocs/publications/44197/13566_eib3_1_.pdf?v=41055 [https://perma.cc/FRJ7-V3QA].
76 See id.
77 Id.; see also JULIAN M. ALSTON ET AL., PERSISTENCE PAYS: U.S. AGRICULTURAL PRO- DUCTIVITY GROWTH AND THE BENEFITS FROM PUBLIC R&D SPENDING 43, 105 (2010).
78 See Schwab & Samans, supra note 9, at v–vi; see also Dorini, supra note 9, at 233 (“The ranks of the unemployed are swelling with former service sector workers, such as secre- taries, receptionists, clerks, and cashiers. These workers are being replaced by what Rifkin calls the silicon-collar workforce: answering machines, scanners, voice and handwriting recog- nition devices, electronic mail, and inventory control and monitoring devices.”) (citation omitted).
79 See John Maynard Keynes, Economic Possibilities for our Grandchildren, in ESSAYS IN PERSUASION 321–32 (Palgrave Macmillan 2010) (1930) (predicting that the combination of technological innovation and capital accumulation will eventually solve the problem of mate- rial needs).
 
479
2018] Should Robots Pay Taxes? 159
also argue that current unemployment may relate more to globalization and offshoring than to technology, and that any future technological unemploy- ment would be “only a temporary phase of maladjustment.”80
But there is reason to think that this time may be different.81 Computers are improving exponentially, and there are fewer limits to what they can do than ever before. Computers can replace low-skilled workers and manual laborers as well as white-collar workers and professionals in a variety of fields. Computers are already working as doctors, lawyers, artists, and in- ventors.82 All of this is occurring at a time when labor costs are rising and computer costs are declining. In 2012, Vinod Khosla, the co-founder of Sun Microsystems, predicted that diagnostic software would take the jobs of eighty percent of physicians in the next twenty years.83
While the optimists and pessimists disagree about automation’s effects on long-term unemployment, both agree it causes short-term job losses and industry-specific disruption. During past episodes of widespread automation and technological change, it took decades to develop new worker skill sets on a significant scale and to build new job markets.84 Although the Industrial Revolution ultimately resulted in net job creation, it also resulted in periods of mass unemployment and human suffering. In the coming “Automation Revolution,” whether there are detrimental long-term effects, there will al- most certainly be significant short-term disruptions.85
80 Id. at 325; see also 1 JOHN STUART MILL, PRINCIPLES OF POLITICAL ECONOMY 97 (Cosimo Classics 2006) (1848).
81 See, e.g., Stevens, supra note 17, at 368–69 (“This time there may be some distinctions requiring widespread and perhaps novel solutions, unlike other periods in history.”).
82 See Parloff, supra note 3; see also Yonghui Wu et al., Google’s Neural Machine Trans- lation System: Bridging the Gap between Human and Machine Translation, CORNELL U. LIBR.: ARXIV 20 (Oct. 8, 2016), arxiv.org/abs/1609.08144 [https://perma.cc/KGU8-9RRB] (claiming that the Google Neural Machine Translation system is approaching human-level accuracy); see also Croft, supra note 3 (discussing various software programs that can outperform attorneys and paralegals in document review); but see generally Remus & Levy, supra note 3 (arguing that AI will refocus rather than replace attorneys).
83 See Liat Clark, Vinod Khosla: Machines Will Replace 80 Percent of Doctors, WIRED UK (Sept. 4, 2012), http://www.wired.co.uk/article/doctors-replaced-with-machines [https:// perma.cc/QNL8-WP4M].
84 See Schwab & Samans, supra note 9, at 20.
85 For example, a substantial number of transportation workers are likely to be displaced by self-driving vehicles, and about three percent of the population is employed in the transpor- tation industry. See Richard Henderson, Industry Employment and Output Projections to 2024, BUREAU OF LAB. STAT.: MONTHLY LAB. REV. tbl. 1 (Dec. 2015), https://www.bls.gov/opub/ mlr/2015/article/industry-employment-and-output-projections-to-2024.htm [https://perma.cc/ 54FB-LDMM]. Tesla, for example, plans to make all its vehicles self-driving. See Tesla to Make All Its New Cars Self-Driving, BBC NEWS: TECH. (Oct. 20, 2016), http://www.bbc.co.uk/ news/technology-37711489 [https://perma.cc/DS4X-YYM2]. Tesla is only one of many com- panies developing such technologies. See 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehi- cles-corporations-list/ [https://perma.cc/4YNE-KDTZ]; see also Investment Into Auto Tech On Pace To Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/ blog/auto-tech-funding-h1-2016/ [https://perma.cc/HY5A-XGFH]. Elon Musk, the CEO of Tesla, has even claimed that self-driving cars will be so much safer than human drivers that there will need to be a future ban on human driving. See Stuart Dredge, Elon Musk: Self- driving Cars Could Lead to Ban on Human Drivers, GUARDIAN (Mar. 18, 2015, 3:22 AM),
 
160 480 Harvard Law & Policy Review [Vol. 12 D. Automation Social Policy
It is important that policy makers act to ensure that automation benefits everyone. Our policy goal should be to accommodate and even encourage advances that promote economic value, while redistributing benefits to those negatively affected. In the midst of the Industrial Revolution, the philoso- pher John Stuart Mill wrote that while automation would ultimately benefit laborers:
this does not discharge governments from the obligation of allevi- ating, and if possible preventing, the evils of which this source of ultimate benefit is or may be productive to an existing genera- tion. . . . [T]here cannot be a more legitimate object of the legisla- tor’s care than the interests of those who are thus sacrificed to the gains of their fellow-citizens and of posterity.86
Or, as the U.S. National Science and Technology Council Committee on Technology argued in 2016:
Public policy can address these risks, ensuring that workers are retrained and able to succeed in occupations that are complemen- tary to, rather than competing with, automation. Public policy can also ensure that the economic benefits created by AI are shared broadly, and assure that AI responsibly ushers in a new age in the global economy.87
Efforts to alleviate the harms and share the benefits of automation have focused on education and social benefits. As mentioned earlier, in December 2016, the Executive Office of the President, then under Barack Obama, is- sued a report which outlined policy responses to AI and automation, namely: to invest in AI, educate and train Americans for future jobs, and transition workers to ensure widespread benefits.88 In terms of education, it is thought that technologically unemployed workers need retraining to transition to new job types. Historically, numerous government and industry programs have combated technological unemployment with education.89 The nation’s first and most sweeping federal training program, the Manpower Development and Training Act of 1962, was signed into law by President Kennedy to train workers unemployed due to technological advances and automation.90 More
https://www.theguardian.com/technology/2015/mar/18/elon-musk-self-driving-cars-ban- human-drivers [https://perma.cc/5CPB-PVHS].
86 MILL, supra note 80, at 98.
87 COMM. ON TECH., supra note 8, at 2.
88 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3. 89 A particularly interesting example is the Armour Meat Packing Company, which cre-
ated “a special ‘automation fund’ for retraining purposes. The company paid a 14-cent levy into the fund, established in 1959, for every 100 tons of meat shipped, up to $500,000, to pay for retraining operations.” Kremen, supra note 14.
90 Id. Also of note, a year earlier, the Office of Automation and Manpower was created at the Department of Labor to anticipate technological change and create occupational guidance. Id. For extensive reviews of automation issues in the 1960s, see generally OFFICE OF MAN-
 
481
2018] Should Robots Pay Taxes? 161
recently, President Obama provided billions of dollars to fund worker train- ing in part to address technological unemployment.91 More ambitiously, he proposed a plan to make two years of community college free for “responsi- ble students” in his 2015 State of the Union Address, although this proposal was never adopted.92
As the third prong of President Obama’s 2016 strategy report notes, social benefit investments are also critical.93 The report advocates strength- ening the social safety net through greater investments in programs such as unemployment insurance and Medicaid.94 It also proposes the creation of new programs for wage insurance and emergency aid.95 In addition, it argues for building a twenty-first century retirement system, expanding health care access, and increasing worker bargaining power.96 President Trump’s admin- istration does not appear to have announced a policy response to AI and automation.97
POWER, AUTOMATION, & TRAINING, U.S. DEP’T OF LABOR, UNEMPLOYMENT AND RETRAINING: AN ANNOTATED BIBLIOGRAPHY OF RESEARCH (1965), https://babel.hathitrust.org/cgi/pt?id= umn.31951p010922940;view=1up;seq=1 [https://perma.cc/BZK8-3UYR]; see also SUND- QUIST, supra note 14, at 77.
91 See Press Release, The White House Office of the Press Sec’y, Fact Sheet: President Obama Proposes New ‘First Job’ Funding to Connect Young Americans with Jobs and Skills Training to Start Their Careers (Feb. 4, 2016), www.whitehouse.gov/the-press-office/2016/02/ 04/fact-sheet-president-obama-proposes-new-first-job-funding-connect-young [https://perma .cc/CV2T-SGB3].
92 John Morgan, Barack Obama Free Community College Plan Backed by $100M Fund- ing, TIMES HIGHER EDUC. (Apr. 27, 2016), www.timeshighereducation.com/news/barack- obama-free-community-college-plan-backed-by-one-hundred-million-dollar-funding [https:// perma.cc/5NTN-P3ZP].
93 See ARTIFICIAL INTELLIGENCE, AUTOMATION, AND THE ECONOMY, supra note 15, at 3–4. 94 See id.
95 See id. at 4.
96 See id.
97 Treasury Secretary Steve Mnuchin stated in March 2017 when asked about technologi- cal unemployment that, “In terms of artificial intelligence taking over American jobs, I think we’re . . . so far away from that that [it’s] not even on my radar screen . . . . I think it’s 50 or 100 more years.” Interview on Health Care and Tax Reform with Steven Mnuchin, Treasury Secretary, at 33:30–47, C-SPAN (Mar. 24, 2017), https://www.c-span.org/video/?425894-1/ treasury-secretary-steven-mnuchin-talks-axios-founder-mike-allen.&start=1992 [https://per ma.cc/6KYR-A82G]. By contrast, Larry Summers, the Obama administration’s first director of the National Economic Council, predicted that AI could result in about “a third of men be- tween the ages of 25 and 54 not working by the end of this half century.” Christopher Mat- thews, Summers: Automation is The Middle Class’ Worst Enemy, AXIOS, https://www.axios .com/summers-automation-is-the-middle-class-worst-enemy-1513302420-754facf2-aaca-478 8-9a41-38f87fb0dd99.html (last visited Jan. 7, 2018) [https://perma.cc/2UEA-PVU4]. Of note, China appears be adopting the findings of the White House strategy. On July 20, 2017, China’s State Council released its Next Generation Artificial Development Plan which adopts many of the policies proposed in the White House strategy. CHINA STATE COUNCIL, STATE COUNCIL NOTICE ON THE ISSUANCE OF THE NEXT GENERATION ARTIFICIAL INTELLIGENCE DE- VELOPMENT PLAN (Rogier Creemers et al. trans., 2017), https://www.newamerica.org/cyber- security-initiative/blog/chinas-plan-lead-ai-purpose-prospects-and-problems/ [https://perma .cc/5F3L-YE9K]. The plan argues that AI will be foundational to future economic growth and military dominance, and calls for China to surpass other nations in AI technology by 2030. See generally id.
 
162 482 Harvard Law & Policy Review [Vol. 12
Revitalized concerns about technological unemployment have breathed new life into an old social benefit proposal—guaranteed minimum income.98 The basic idea is that the government would provide a fixed amount of money to its citizens regardless of their situation. This has been implemented numerous times on a relatively small scale, most recently in Finland.99 In 2017, Finland began a pilot program to give about $600 per month to 2,000 unemployed citizens, with no other requirements.100 Proponents argue this will reduce unemployment, poverty, and disincentives for the unemployed to work (as under conventional unemployment schemes recipients generally lose their unemployment benefits after returning to work).101 It might also encourage education by providing support for a period of training. Critics have argued that a guaranteed minimum income will encourage recipients to remain unemployed and discourage additional education.102 In any case, Fin- land plans to eventually replace earnings-based insurance benefits with a basic income.103 Y Combinator, the Silicon Valley start-up incubator, has plans to launch a similar private program in Oakland, California.104
Improving education and social benefit systems will not be easy. Liber- als and conservatives alike can agree on the desirability of improving worker training as it will enlarge the productive labor force, but “[d]elivering this education and training will require significant investments.”105 Enhancing the social benefit system will also require significant investment, but such a goal is even more challenging because liberals and conservatives generally disagree that enhanced benefits are a desirable aim.106
That automation creates a need for greater government investment is well known, but what has so far been largely ignored in the automation debate is that automation will make it far more difficult for the government to make investments once tax revenues are reduced.
98 See supra note 16 and accompanying text.
99 See Mallett, supra note 17.
100 See Kevin Lui, Finland is Giving Nearly $600 a Month to 2,000 Jobless Citizens, No
Questions Asked, FORTUNE (Jan. 3, 2017, 1:26 AM), amp.timeinc.net/fortune/2017/01/03/fin- land-universal-basic-income-experiment/?source=dam [https://perma.cc/BFY3-JX2L]. It is also worth noting that the U.S. has operated a guaranteed basic income since 1999. The Alaska Permanent Fund pays each person who has lived the past year in Alaska $1,680. See Van Parijs, supra note 16.
101 See supra note 16 and accompanying text.
102 See id.
103 See Lui, supra note 100.
104 See Chris Weller, The Inside Story of One Man’s Mission to Give Americans Uncondi-
tional Free Money, BUS. INSIDER UK (June 27, 2016, 1:07 PM), uk.businessinsider.com/in- side-y-combinators-basic-income-project-2016-6?r=US&IR=T [https://perma.cc/48QT- H66H].
105 COMM. ON TECH., supra note 8, at 3.
106 See supra note 17 and accompanying text.
 
483
2018] Should Robots Pay Taxes? 163
II. CURRENT TAX POLICIES FAVOR AUTOMATION AND REDUCE TAX REVENUE
A. Introduction
Worker automation is often thought of as a matter of efficiency, where efficiency refers to the ratio of useful output to total input.107 For example, if a machine and a person create the same output, but the machine is less ex- pensive, then automation generates cost savings and improves efficiency.108 If a robot costs a firm $40,000 a year and a human worker costs $45,000 a year, with both workers producing the same output, the firm would yield a $5,000 annual cost savings by automating.
However, it may also be the case that the robot costs more than a human worker before taxes, and only becomes cheaper on a post-tax basis. For instance, the capital outlay for the robot, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as robots, together with the costs for operating the robot (electricity, etc.), might be estimated at $50,000 over some period, whereas the wages and other costs associated with an employee (healthcare, retirement funding, etc.) might be $45,000 over the same period. The robot may be associated with tax benefits that do not apply to human workers and which reduce its cost to $40,000. A firm using a rational cost-based decision model would choose to automate and realize the machine’s tax benefit. In this example, tax policy has ren- dered the robot a more efficient worker. In simple terms, the heavy relative taxation of the living worker drives the firm toward automation to generate tax savings.
The tax system is not neutral as between work performed by robots versus people.109 Automation provides several major tax advantages. Firms that automate avoid employee and employer wage taxes levied by federal, state, and local taxing authorities and claim accelerated tax depreciation on capital costs for automated workers. The tax system also provides indirect incentives for automated workers. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital.110 In
107 Expressed mathematically, efficiency “r” is equal to the amount of useful output (“P”) divided by the amount (“C”) of resources consumed: r=P/C.
108 See Stevens, supra note 17, at 373 (“Technology is very attractive to owners of capital. Machines require no pay, benefits, sick leave, vacation, lunch breaks, or weekends off. They are less prone to err and are more productive than human beings. In a race for the same job, it is therefore difficult for humans to compete with machines.”).
109 The analysis of the “neutrality” of taxation is a common practice in the field of taxa- tion. See generally Peggy Richman (Musgrave), Taxation of Foreign-Source Business Income and the Incentive to Foreign Investment, in PEGGY RICHMAN, TAXATION OF FOREIGN INVEST- MENT INCOME: AN ECONOMIC ANALYSIS (1963), reprinted in PEGGY R. MUSGRAVE, TAX POL- ICY IN THE GLOBAL ECONOMY: SELECTED ESSAYS 3–57 (E. Elgar Publishing 2002) (introducing the term “capital export neutrality”).
110 See MEISEL, supra note 32, at 220 (“An automation tax described as a payroll tax on computers conveys the basic concept. It helps level the playing field. The automation tax serves two purposes: (1) it provides an incentive for a company to create jobs by means such
 
164 484 Harvard Law & Policy Review [Vol. 12 fact, as described below, automated workers are taxed less than human
workers at both the employer and employee level.
B. Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to indi- viduals to fund social benefit programs including Social Security, Medicare, and Medicaid. Presently, in the United States, the employer and employee pay matching amounts totaling 12.4% of an employee’s salary, plus match- ing Medicare payments totaling 2.9% (applied on the first $127,200 of earn- ings), plus an additional 0.9% Medicare surcharge (applied on earnings over $200,000).111 Many states and localities also levy wage taxes that apply in addition to the federal levies.112
C. Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers
“Tax depreciation” refers here to the deduction (a reduction in the tax base) claimed by the firm in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow the firm to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction works differently for robot and human workers.
The timing of claiming a deduction may have a significant effect on a firm’s tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time).113 For example, assume a robot has a total
as investing in human-computer synergy; and (2) it proves governmental revenues that, prop- erly used, can create more consumption and thus boost the economy.”).
111 See I.R.C. § 3101(a) (2012 & Supp. II 2014), § 3111(a) (Supp. III 2016), § 3102(a) (2012), § 3121(a)(1) (2012 & Supp. II 2014); I.R.S., SOCIAL SECURITY AND MEDICARE WITH- HOLDING RATES (2017), www.irs.gov/taxtopics/tc751.html [https://perma.cc/3F5R-UEES]; see also Richard Winchester, The Gap in the Employment Tax Gap, 20 STAN. L. & POL’Y REV. 127, 132 (2009) (“The tax imposed by FICA has two components. The first is the old-age, survivors, and disability insurance component, often referred to as OASDI. It is [levied on] . . . ‘wages’ from employment. One half of the tax is deducted from the employee’s compensa- tion. The employer pays the other half. This component of the FICA tax is earmarked to cover social security benefits. There is a limit on the amount of wages that can be taxed. . . . The contribution and benefit base is adjusted each year to reflect increases in average wages of the U.S. economy.”) (citations omitted).
112 For an explanation of the U.S. states that levy sales taxes, see generally SCOTT DRENKARD & NICOLE KAEDING, TAX FOUND., STATE AND LOCAL SALES TAX RATES IN 2016 (2016), https://files.taxfoundation.org/legacy/docs/TaxFoundation_FF504.pdf [https://perma .cc/VLE2-KCHV]. For an explanation of EU tax policy including the VAT, see generally CE ́- CILE REMEUR, EUR. PARLIAMENTARY RESEARCH SERV., TAX POLICY IN THE EU: ISSUES AND CHALLENGES (2015), http://www.europarl.europa.eu/RegData/etudes/IDAN/2015/549001/ EPRS_IDA(2015)549001_EN.pdf. [https://perma.cc/FUE9-ZV58].
113 See Yoram Margalioth, Not a Panacea for Economic Growth: The Case of Accelerated Depreciation, 26 VA. TAX REV. 493, 494–95, 499 (2007) (“Accelerated depreciation policy can be traced back to an influential 1953 paper by Evsey Domar. . . . [Elaborating on the]
 
485
2018] Should Robots Pay Taxes? 165
capital cost of $100,000 and seven years of useful life, while an employee has a total wage cost of $100,000 over seven years. If accelerated deprecia- tion for capital is available,114 the firm may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year one rather than pro- rata over seven years.115 For instance, the firm might claim tax depreciation for an automated worker of $50,000 in year one, $30,000 in year two, $10,000 in year three, and in diminishing amounts to year seven. By con- trast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a present value benefit will accrue from claiming accelerated tax de- ductions for automated workers relative to the pro-rata tax deductions for employee wages, even where the $100,000 capital outlay is paid up-front.116 This is possible because the present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return the firm could make by investing the free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the eco- nomic depreciation.117 “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to buy the same robot next year than it costs today. The issue becomes sig- nificant where, as in the prior example, it was presumed for tax purposes that
Harrod-Domar model, [Domar predicted] that Gross Domestic Product (GDP) was propor- tional to the number of machines; namely, that investment is the key to growth. . . . A later model, developed by Nobel Laureate Robert Solow between 1956–57, points out that the Har- rod-Domar model cannot explain sustained growth. Solow showed that as capital per worker increases, the marginal productivity of capital declines until the capital-labor ratio approaches a steady-state level. At that point, savings . . . are just sufficient to replace worn out machines and equip new workers (assuming population growth), so productivity growth is zero.”) (citing Evsey D. Domar, Capital Expansion, Rate of Growth, and Employment, 14 ECONOMETRICA 137 (1946); Roy F. Harrod, An Essay in Dynamic Theory, 49 ECON. J. 14 (1939); Robert M. Solow, A Contribution to the Theory of Economic Growth, 70 Q. J. ECON. 65 (1956); Robert M. Solow, Technical Change and the Aggregate Production Function, 39 REV. ECON. & STAT. 312 (1957)).
114 See Margalioth, supra note 113, at 505 (“For tax reporting purposes, the Code allows the use of much more accelerated depreciation methods than the straight-line method.”).
115 See id. at 505–506 (“The vast majority of U.S. corporations use a depreciation method called ‘straight-line’ for financial reporting purposes. According to the straight-line deprecia- tion method, annual depreciation is calculated by subtracting the salvage value of the asset from the purchase price and dividing this number by the estimated useful life of the asset. The outcome is equal periodical deductions throughout the asset’s useful life. If the asset in the above example is depreciated under the straight-line method, its $1000 cost is allocated uni- formly over its useful life period of five years, resulting in $200 of depreciation deduction each year.”) (citations omitted).
116 Most large corporations have significant cash accumulations and do not need to borrow funds (and pay interest) to make capital expenditure on automation. Notably, if corporate bor- rowing is required to fund capital expenditure, then present value will depend on the adjusted cost of capital, taking into account the value of tax deductions for interest paid. In summary, accelerated tax depreciation yields an economic benefit where the firm has balance sheet cash earning a low rate of return that it can instead deploy to yield tax deductions on an accelerated basis.
117 See Margalioth, supra note 113, at 508 (“In times of inflation, recovery of the nominal cost of investment is not sufficient to match income and expenses. Because of inflation, the income generated by the asset is expressed in a larger number of dollars though it has the same purchasing power.”).
 
166 486 Harvard Law & Policy Review [Vol. 12
the robot wears out after seven years, but it turns out the robot actually increases in nominal value. An incremental tax benefit thus accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary) difference is never recaptured in the tax system. In the corporate setting, this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not ad- justed for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential inves- tors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit.118 Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate re- ported earnings could use automation to achieve such a timing benefit. This increase to reported earnings may be an even more significant motivation for large firms to automate than a cash tax savings.
D. Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the firm level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary examples are the Retail Sales Tax (RST) levied by states and municipalities in the United States and the VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportion- ately to offset the indirect tax.119 In the case of automated workers, however, the burden of indirect taxes is entirely avoided by the firm because it does not need to provide for a machine’s consumption.120 In general, business ex-
118 See id. at 505 (“Accounting for depreciation is also required for financial reporting purposes. Generally accepted accounting principles (GAAP) require the depreciation of the (depreciable) cost of income generating assets, usually, tangible assets. The cost has to be allocated among accounting periods on a systematic and rational basis that reflects the use of the asset in the revenue generating process over the asset’s operational life.”) (citations omitted).
119 See CTR. FOR RESEARCH ON THE PUB. SECTOR AT UNIV. BOCCONI, THE ROLE AND IMPACT OF LABOR TAXATION 14 (2011) [hereinafter BOCCONI].
120 The capital assets comprising automated workers might be subject to property taxation by some local jurisdictions as business personal property. However, such personal property taxation is often successfully mitigated by tax planning or with tax waivers by local jurisdic- tions and municipalities negotiated by municipalities. Further, human employees also engender some degree of attached personal property (e.g., office fixtures, personal computers), which are also subject to personal property taxation.
 
487
2018] Should Robots Pay Taxes? 167
penditures for capital assets such as machinery are exempt from indirect taxation or yield a deduction for RST or VAT.121
E. Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing.122 For 2015, the Internal Revenue Service (IRS) reported that out of the nearly $3 trillion in net collections, individual income taxes accounted for 49.8%, employment taxes 35.2%, business income taxes 11.7%, excise taxes 2.6%, and estate and gift taxes 0.7%.123 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now com- prises roughly one-half of its respective share compared to prior decades.124 In fact, the Trump administration’s recently enacted Tax Cuts and Jobs Act reduces the corporate tax rate from a maximum of 35 percent to a flat 21 percent beginning in 2018.125 In Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.126
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For in- stance, an employee cannot use transfer pricing techniques to shift earned income into a 0%-taxed entity in the Cayman Islands.127 Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human
121 See John Mikesell, Sales Tax Incentives for Economic Development: Why Shouldn’t Production Exemptions Be General?, 54 NAT’L TAX J. 557, 562 (2001).
122 See SOI Tax Stats – Collections and Refunds, by Type of Tax, IRS Data Book Table 1, I.R.S. (Aug. 28, 2017), https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by- type-of-tax-irs-data-book-table-1 [https://perma.cc/R282-7P76] (containing reported aggre- gate collections and refunds from 2015 to 1995). For example, from 1995 to 2015, business income taxes decreased from 12.3% of total collections to 11.7%, while individual taxes in- creased from 46.5% to 49.8%. Id.
123 See id. Individual income taxes here include estate and trust incomes taxes, which represent 1.1% of overall collections. Employment taxes consist of primarily old-age, survi- vors, disability, and hospital insurance, which is almost entirely Federal Insurance Contribu- tions payments and a small amount of Self-Employment Insurance Contributions. It also includes a small amount of Unemployment Insurance and Railroad retirement. Id.
124 See JOEL FRIEDMAN, CTR. ON BUDGET AND POL’Y PRIORITIES, THE DECLINE OF CORPO- RATE INCOME TAX REVENUES 3 (2003), https://www.cbpp.org/sites/default/files/atoms/files/10- 16-03tax.pdf [https://perma.cc/66JG-QSU3].
125 See Act of Dec. 22, 2017 (Tax Cuts & Jobs Act) Pub. L. No. 115-97, §13001, 131 Stat. 2054 (codified as amended at 26 U.S.C. § 11).
126 See MEISEL, supra note 32, at 223 (“What numbers are used in the ratio of revenues to employees? I recommend using revenues generated within the taxing country and employees within the country in the ratio.”).
127 The Cayman Islands has no corporate tax. See DELOITTE, INTERNATIONAL TAX: CAY- MAN ISLANDS HIGHLIGHTS 2017 1 (2017), https://www2.deloitte.com/content/dam/Deloitte/ global/Documents/Tax/dttl-tax-caymanislandshighlights-2017.pdf [https://perma.cc/FZL8- 33YR].
 
168 488 Harvard Law & Policy Review [Vol. 12
capital is also not depreciable, so a person does not typically get a tax deduc- tion for education or medical costs, at least not up to the full amount of the investment.128 By contrast, machinery or other equipment yields an immedi- ate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.129
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, this would be expected to have negative economic effects over the long term.
III. TAX POLICY OPTIONS FOR AN AUTOMATION TAX
The current tax system is designed to principally tax human workers and not robot workers. All else being equal, this creates a situation in which firms prefer robots since substantially less tax per output is accrued or remit- ted in respect of an automated worker. At the same time, the automation of large segments of the labor force threatens long-term fiscal solvency because of the potential reduction in tax collections.
A major automation policy issue is therefore how to adjust the tax sys- tem to be neutral as between robot and human workers, or even to create incentives for human rather than robot workers to incentivize employment. In doing so, it is important to consider that capital investment of any kind (including for robots) is thought to be beneficial to economic growth.130 Na- tions engage in tax competition to draw capital into their jurisdictions. Any disallowance of capital deduction would serve as a disincentive to invest- ment and would, theoretically, be economically undesirable. For example, if only one taxing jurisdiction disallowed tax deductions for automated work- ers, multinational firms might shift their capital investments to other juris-
 128 For the U.S. incentives with an election for deduction or credit on higher education costs, see 26 U.S.C. § 25A (2012 & Supp. III 2016).
129 See, e.g., BOCCONI, supra note 119, at 14.
130 See, e.g., Eduardo Borensztein et al., How Does Foreign Direct Investment Affect Eco- nomic Growth?, 45 J. INT’L ECON. 115, 116 (1998); see also Gert Wehinger, Fostering Long- Term Investment and Economic Growth: Summary of a High-Level OECD Roundtable, 2011 OECD J. FIN. MKT. TRENDS 1, 2 (2011).

489
2018] Should Robots Pay Taxes? 169
dictions.131 It is therefore important to consider international tax competition in evaluating various options to create an automation-neutral tax system.132
A. Disallowance of Corporate Tax Deductions for Automated Workers
A first option is to attempt to disallow the respective corporate income tax deductions for capital investments that give rise to the automation tax benefit. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The recent South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines.133
To begin with federal income taxation, the disallowance of tax prefer- ences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phase out.”134 Phase outs reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower-income taxpayers.135 For instance, student loan interest is deductible, but not for individuals with more than $80,000 in modified adjusted gross income (MAGI) ($160,000 for joint filers).136 Some phase outs reduce credits, others reduce deductions.137
A new code provision could be designed with a similar phase out, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker automation could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance.
In respect of indirect taxation, a simpler solution may be possible. Indi- rect tax preferences for capital outlay in respect to automated workers could
131 However, the shift would be from one high-tax jurisdiction to another high-tax juris- diction to claim the deduction’s full value, rather than a shift into tax havens with a zero percent corporate tax rate, where the capital tax deductions for automated workers would not have any value (i.e., the value of a tax deduction in a zero percent tax jurisdiction is zero). Thus, multinational firms should not be expected to make capital investment in robots in tax havens where the value of deductions is zero, especially where transfer pricing strategies are available to shift income arising from the automated workers.
132 See MEISEL, supra note 32.
133 See McGoogan, supra note 22.
134 See, e.g., Emmanuel Saez, Do Taxpayers Bunch at Kink Points?, 2 AM. ECON. J. ECON
POL’Y 180, 180 (2010).
135 See, e.g., I.R.S., TEN FACTS ABOUT THE CHILD TAX CREDIT (2011), https://www.irs
.gov/newsroom/ten-facts-about-the-child-tax-credit [https://perma.cc/3YEV-V8XD].
136 See I.R.S., TAX BENEFITS FOR EDUCATION 2 (2016), https://www.irs.gov/pub/irs-pdf/
p970.pdf [https://perma.cc/X5ZV-MXMX].
137 See generally I.R.S., TAX GUIDE 2016: FOR INDIVIDUALS 25, (2016), https://www.irs
.gov/pub/irs-pdf/p17.pdf [https://perma.cc/FS97-G5LE] (discussing the various types of cred- its and deductions available and the income levels at which they phase out).
 
170 490 Harvard Law & Policy Review [Vol. 12
be disallowed outright at the state level. Thus, for example, where the firm attempts to claim an RST/VAT exemption or refund for tax payments made to purchase and maintain automated workers, this would not be permitted.
These measures could achieve greater balance between taxing human workers and robots, but the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base used to fund social insurance benefits.
B. Levy of an Automation Tax
A second option is to levy an incremental federal “automation tax” to the extent workers are laid off or replaced by machines.138 A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings.139 Employers must pay into an unemployment insurance scheme based on their ratings, so a business which has more layoffs pays more in taxes for unemployment insurance.140 A federal automation tax could be de- signed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determined the layoffs were due to automation.
A potential drawback to the levy of an additional automation tax is that it would essentially increase the corporate effective tax rate for many firms, and also increase the relative complexity of the tax system. Economic theory suggests that higher rates and added complexity are negatives in terms of international tax competition.141 Another drawback is that firms might accel- erate layoffs upon passage (or debate) of the bill prior to implementation to
138 Cf. Michael Kraich, The Chilling Realities of the Telecommuting Tax: Adapting Twenti- eth Century Policies for Twenty-First Century Technologies, 15 U. PITT. J. TECH. L. & POL’Y 224 (2015) (providing a comprehensive discussion of a “telecommuting tax”).
139 See DAVID RATNER, FED. RESERVE BD., UNEMPLOYMENT INSURANCE EXPERIENCE RAT- ING AND LABOR MARKET DYNAMICS 1 (2013), https://www.federalreserve.gov/pubs/feds/2013/ 201386/201386pap.pdf [https://perma.cc/T5W3-5YJD] (“The United States is the only OECD country to finance unemployment insurance (UI) through a tax system which penalizes layoffs. The original intent of this institution, known as ‘experience rating,’ was to apportion the costs of UI to the highest turnover firms and thereby stabilize employment. Experience rating can stabilize employment through a layoff cost. The layoff cost is levied when a firm lays off a worker and is assessed a higher tax rate in the future.”).
140 See id.
141 See generally Michael Keen & Kai A. Konrad, The Theory of International Tax Com- petition and Coordination, in 5 HANDBOOK OF PUBLIC ECONOMICS 257 (Alan Auerbach et al. eds., 2013), http://gabriel-zucman.eu/files/teaching/KeenKonrad13.pdf [https://perma.cc/ 3VC5-HLN3] (exploring models that suggest a country might prefer to raise its tax rate in response to lower tax rates in other countries); but see Bret N. Bogenschneider, Causation, Science & Taxation, 10 ELON L. REV. (forthcoming, Spring 2018) (“The hypothesis that tax cuts cause economic growth is a central tenet of neoclassical economic theory. Yet, it is not clear why economists hold this belief, as empirical evidence of any posited causal relation is conspicuously absent. . . . The available evidence indicates to the contrary of the hypotheses that tax cuts cause economic growth is that higher ratios of taxation to gross domestic product are associated with higher rates of national economic growth in most countries.”).
 
491
2018] Should Robots Pay Taxes? 171
avoid the tax by reducing the number of employees upon the effective date of the law. Accordingly, a retroactive effective date for measurement of em- ployment levels for the automation tax would be a practical necessity.
C. Grant Offsetting Tax Preferences for Human Workers
A third option is to attempt to grant offsetting tax preferences for firms that employ human workers for each category of tax benefit. To begin with wage taxation, the tax preference could entail a repeal of the employer con- tributions to the Social Security and Medicare systems. The result would be that both human and automated workers would be exempt for the employer in terms of wage taxes—not just automated workers. However, this would accelerate the insolvency of the Social Security system unless the resultant decrease in tax collections were otherwise offset.142
In terms of income taxation, an offsetting preference for human work- ers could be designed as an accelerated deduction for future wage compensa- tion expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for automated workers. In terms of indirect tax- ation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income. This would constitute an incentive for firms to employ human workers.
D. Levy of a Corporate Self-Employment Tax
A fourth option is to increase corporate level taxation for firms that produce outputs without using human labor. The additional taxes would be a substitute amount for Social Security and Medicare wage taxes avoided by the firm with automated labor.143 In part, this is the corollary to the individ- ual self-employment tax where a small-business owner is required to pay monies into the Social Security system approximating the Social Security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work.144 The corporate self- employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount deter- mined by the Treasury (in reference to industry standards), then backup
142 This would require a very significant offset. Federal Insurance Contributions and Self- Employment Insurance Contributions currently make up about 34.6% of net federal tax collec- tions. Federal Insurance Contributions include both employee and employer payments to fund Social Security and Medicare. See I.R.S., supra note 122.
143 MEISEL, supra note 32, at 222–23 (“Returning to the payroll tax analogy, companies that hire fewer people pay fewer payroll taxes. The payroll tax in the US helps fund social security, Medicare, and unemployment insurance. In Europe, payroll taxes are even higher than in the US.”).
144 Id. at 227 (“The automation tax might encourage companies to prefer productivity improvements achieved by using a combination of human and computer capabilities.”).
 
172 492 Harvard Law & Policy Review [Vol. 12
withholding could apply on corporate profits. The gross amount of the auto- mation tax could be designed to match the wage taxes avoided by the firm with automated workers.
William Meisel has similarly proposed an “automation tax” which he referred to in lay terms as a “payroll tax on computers.”145 This would be like the corporate self-employment tax described here. Meisel wrote:
I propose that a national automation tax be based on the ratio of a company’s revenues (total sales) to their number of employ- ees. . . .[T]he automation tax should increase as a percentage as the revenue-per-employee [ratio] grows, making it more attractive to create jobs than to replace them with automation. . . . I prefer applying the percentage to revenues. . . . Profits can be manipu- lated with deductions and other accounting complexities much more than revenues.146
Meisel’s “automation tax” differs from our proposed corporate self-em- ployment tax in that the former uses a sales ratio as opposed to a profit ratio. A sales ratio may be unworkable in practice since the tax would prohibi- tively fall on firms with high sales but low profit margins, such as dis- counted retailers. Since automation often occurs in the high-tech industry among companies with high profit margins, it seems preferable that a viable “automation tax” using a ratio to employee expense should be premised on profits, not sales.
E. Increase the Corporate Tax Rate
A fifth option would be to significantly increase the corporate tax rate, with the intent of increasing the relative portion of the tax base borne by capital and decreasing that borne by labor. The counter-intuitive advantage of this approach is that higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment re- fers to incremental investment made only because of the tax system.147 As one of us has explained, “[t]he experienced tax attorney always counsels the client that marginal capital investment is tax deductible.”148 Thus, mul- tinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions to claim tax deductions of relatively higher value. In part for this reason, for smaller and growing firms that are reinvest- ing profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
145 Id. at 220 (“If software is to take over many jobs, why not have an income tax on software? We could perhaps think of it as a payroll tax on computers.”).
146 Id. at 221–23.
147 See Bret N. Bogenschneider, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74 (2015).
148 Id. at 61 (second emphasis added).
 
493
2018] Should Robots Pay Taxes? 173
The drawbacks to increasing the corporate tax rate are well-known and may be summarized as follows: First, the corporate tax rate might be a signal to firms about the tax climate of a jurisdiction, so higher tax rates could have a negative psychological effect on capital investment decision making.149 Second, accelerated tax deductions would be a stronger automation incen- tive with a higher corporate tax rate as the deduction would have greater value. This means that an increase in the corporate tax rate should be taken in combination with our other proposals. Third, the increase in corporate tax rates would affect all firms, even those not engaged in worker automation. Hence, the increase in corporate tax rate option might be viewed as one version of zero-sum analysis, in which tax policy is designed not to allow a shift of the tax burden from capital to other taxpayers. Further, any increase in corporate tax rates may prompt firms to attempt to shift the tax incidence to workers or consumers.150 Finally, increasing corporate tax rates may be politically unfeasible. As Meisel notes in an understated fashion, “[c]orporations might instinctively fight a corporate tax.”151
F. Issues in Economic Efficiency Relevant to Automation Tax Policy Proposals
The tax policy analysis developed here comes from the perspective of average effective tax rates as opposed to solely marginal rates.152 Any margi- nal tax rate methodology excludes an analysis of taxation relative to the overall share of the tax base. For example, technology and pharmaceutical companies often pay a very low average effective tax rate (e.g., less than 10%) but could also be correctly found to simultaneously have a high margi- nal effective tax rate (e.g., about 35%). A corporate taxpayer which pays very little tax relative to its level of taxable income could correctly describe its marginal tax rate as “high.” Accordingly, the last dollar of income may nearly always be found to be taxed at a “high” marginal tax rate, even where the average effective tax rate is relatively low.153
Economic models of taxation are typically designed by modeling the hypothetical effects of changes in marginal tax rates.154 Marginal tax rates
149 Id. at 60–61 (“Any income tax system is designed initially to favor active investors. This is because no matter how high the actual tax rate, it is levied only on what is referred to as ‘taxable income.’ Of course, ‘taxable income’ means the amount of profits less deductions. Every tax professional is aware of this feature of an income tax system and counsels the client accordingly.”).
150 See Kimberly Clausing, In Search of Corporate Tax Incidence, 65 TAX L. REV. 433, 468 (2012). Firms, however, behave as if they bear the incidence of corporate taxation.
151 MEISEL, supra note 32, at 225.
152 The calculation of a marginal tax rate is essentially the theoretical opposite of the calculation of taxation as a percentage of the share of the overall tax base.
153 For example, a firm may have an overall tax rate of 20% on all of its earnings; how- ever, with respect to a hypothetical decision of whether to earn incremental income, the margi- nal tax rate might be 35%.
154 For a discussion of marginal tax rates in economic analysis, see David Madden, The Poverty Effects of a ‘Fat Tax’ in Ireland, 24 HEALTH ECON. 104, 106 (2015) (“The difficulties
 
174 494 Harvard Law & Policy Review [Vol. 12
again represent incremental changes to the statutory tax rate on the last dol- lar of income.155 For example, a change in the statutory corporate tax rate from 35% to 30% would be reflected in economic models premised on mar- ginal rate analysis. The trouble with this form of economic modeling is that its validity relies on the presumption that firm decisions are made based on tax effects on the marginal investment and not based on an average. This approach has major implications for tax policy design as tax cuts to the stat- utory rate are nearly certain to have a marginal effect even where the firm does not pay a high tax rate overall. Thus, business and investment decisions are presumed not to proceed at the average tax rate for all earned income, but only with respect to incremental tax changes relevant to marginal income.
Other economic modeling proceeds on a marginal effective tax rate ba- sis (i.e., reflecting that corporate taxpayers do not pay the statutory rate). For example, the granting of an additional deduction for manufacturing activity to corporations could reduce the marginal effective tax rate on the last dollar of income from 30% to 27% where the statutory rate is 35%. By this method, the firm would be presumed to make an investment decision based on the average tax rate at the margin. Both approaches are distinguishable from analysis using simply an average effective tax rate, which for large corporations is now calculated at approximately 20% (including permanent deferrals) and trending downward.156 However, for many tech companies, the effective tax rate is below 10%. At such very low average effective tax rates, it is not clear that economic analysis of marginal effects of tax cuts is a realistic method of tax policy analysis. By such methods, significant macroeconomic benefits can be posited where corporate effective tax rates are reduced from very low levels to even lower levels (e.g., from 2% to 1%), but where it is likely that factors other than marginal taxation are likely to drive firm investment decisions. Also, the positing of economic growth from marginal tax cuts does not consider the effect changes in the composition of the overall tax base, where the taxation of one factor is substantially re- duced, namely capital, and the taxation of another factor is increased (or overall borrowing is increased). Further, multinational firms do not engage in tax avoidance planning to reduce income which they do not intend to earn.
associated with non-marginal tax reforms have led a number of analysts to concentrate on marginal tax reforms. This approach has the advantage of not requiring estimates of individual demand and utility functions.”) (internal citation omitted).
155 The U.S. federal statutory corporate tax rate is thirty-five percent for corporate income in excess of ten million. See I.R.C. § 11 (2012). Various individual U.S. states also levy an incremental state-level corporate tax. See generally NICOLE KAEDING, TAX FOUND., STATE INCOME CORPORATE TAX RATES AND BRACKETS FOR 2016 (2016), https://files.taxfoundation .org/legacy/docs/TaxFoundation-FF497.pdf [https://perma.cc/J37Z-Y8X6].
156 For effective tax rates on multinational firms including the delay in taxation of foreign earnings for U.S. multinationals, see generally Bret N. Bogenschneider, The Effective Tax Rates of U.S. Firms with Permanent Deferral, 145 TAX NOTES 1391 (2015).
 
495
2018] Should Robots Pay Taxes? 175
In summary, notwithstanding that the statutory corporate tax rate, or marginal corporate effective tax rates, might be correctly described as “high” in the economic theory of taxation, such analysis is also subject to a relative or zero-sum form of analysis, where tax cuts for one party are trans- ferred as tax increases to another party. The average effective tax rate on workers is relatively “high” where all types of taxation are taken into ac- count.157 The taxation of workers comprises the bulk of the tax base in the United States and that of most developed countries. As workers are substi- tuted or replaced by automation, follow-on effects are possible not only from the direct reduction in the tax base, but also indirectly where the relative taxes are transferred to other workers in the economy.
CONCLUSION
Automation promises to be one of the great social challenges of our generation. It can benefit everyone, or it can benefit the select few at the expense of the many. Tax is a critical component of any automation policy. Existing tax policies both encourage automation and dramatically reduce the government’s tax revenue. This means that attempts to craft policy solutions to deal with automation will be inadequate if they fail to take taxation into account. In this article, we have proposed a series of tax policy changes that could level the playing field for human workers. Whether these proposals are adopted may depend on whether policy makers are prepared to make politically challenging decisions about how to deal with automation.
 157 See Bogenschneider, supra note 27.

496
The Reasonable Computer: Disrupting the Paradigm of Tort Liability
Ryan Abbott* ABSTRACT
Artificial intelligence is part of our daily lives. Whether working as chauf- feurs, accountants, or police, computers are taking over a growing number of tasks once performed by people. As this occurs, computers will also cause the injuries inevitably associated with these activities. Accidents happen, and now computer-generated accidents happen. The recent fatality involving Tesla’s au- tonomous driving software is just one example in a long series of “computer- generated torts.”
Yet hysteria over such injuries is misplaced. In fact, machines are, or at least have the potential to be, substantially safer than people. Self-driving cars will cause accidents, but they will cause fewer accidents than human drivers. Because automation will result in substantial safety benefits, tort law should encourage its adoption as a means of accident prevention.
Under current legal frameworks, suppliers of computer tortfeasors are likely strictly responsible for their harms. This Article argues that where a sup- plier can show that an autonomous computer, robot, or machine is safer than a reasonable person, the supplier should be liable in negligence rather than strict liability. The negligence test would focus on the computer’s act instead of its design, and in a sense, it would treat a computer tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, and it would continue to reward sup- pliers for improving safety.
More importantly, principles of harm avoidance suggest that once com- puters become safer than people, human tortfeasors should no longer be mea- sured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against computers. To appropriate the immortal words of Justice Holmes, we are all “hasty and awkward” compared to the reasonable computer.
TABLE OF CONTENTS
INTRODUCTION ................................................. 2 I. LIABILITY FOR MACHINE INJURIES ..................... 8 A. A Brief History ..................................... 8
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at UCLA. Thanks to Hrafn Asgeirsson, Bret Bogenschneider, Richard Epstein, Marie Newhouse, Alexander Sarch, and Christopher Taggart for their insightful comments.
 January 2018 Vol. 86 No. 1
1

2
497 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
B. Tort Law as a Mechanism for Accident Prevention . 11 C. Negligence .......................................... 12 D. Strict and Product Liability ......................... 13
II. COMPUTER-GENERATED TORTS......................... 16
A. Automation Will Prevent Accidents ................. 16
B. Tort Liability Discourages Automation.............. 19
C. Computer-Generated Torts Should Be Negligence
Based ............................................... 22
D. Computer-Generated Torts as a Type of Machine
Injury............................................... 24
E. Implementation ..................................... 26
F. Financial Liability................................... 30
G. Alternatives to Negligence ........................... 32
III. THE REASONABLE ROBOT .............................. 35
A. When Negligence Is Strict ........................... 35
B. The New Hasty and Awkward ...................... 36
C. Reasonable People Use Autonomous Computers . . . . 39
D. The Reasonable Computer Standard for Computer
Tortfeasors.......................................... 41
E. The Automation Problem ........................... 42
CONCLUSION ................................................... 44
INTRODUCTION
An automation revolution is coming, and it is going to be hugely disruptive.1 Ever cheaper, faster, and more sophisticated computers are able to do the work of people in a wide variety of fields and on an unprecedented scale. They may do this at a fraction of the cost of existing workers, and in some instances, they already outperform their human competition.2 Today’s automation is not limited to manual la- bor; modern machines are already diagnosing disease,3 conducting le-
1 See generally JAMES MANYIKA ET AL., MCKINSEY & CO., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013).
2 See, e.g., Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible Are Jobs to Computerisation?, 114 TECHNOLOGICAL FORECASTING & SOC. CHANGE 254, 265–66 (2017) (reporting in a seminal paper that “47 percent of total US employment is [at] high risk” of automation, and stating that “recent developments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future”).
3 See Roger Parloff, Why Deep Learning Is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning [https://perma.cc/E3UA-N2TZ]. Several artificial intelligence systems are already capable of au- tomating medical diagnoses. See id. For instance, Freenome has a system for diagnosing cancer from blood samples that is competitive with pathologists. See id.; see also FREENOME, http:// www.freenome.com (last visited Jan. 4, 2018).
 
2018] 498 THE REASONABLE COMPUTER 3
gal due diligence,4 and providing translation services.5 For better or worse, automation is the way of the future—the economics are simply too compelling for any other outcome.6 But what of the injuries these automatons will inevitably cause? What happens when a machine fails to diagnose a cancer, ignores an incriminating email, or inadvertently starts a war?7 How should the law respond to computer-generated torts?
Tort law has answers to these questions based on a system of common law that has evolved over centuries to deal with unintended harms.8 The goals of this body of law are many: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.9 Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law.10 By creating a framework for loss shifting from injured victims to tortfeasors, tort law deters unsafe conduct.11 A purely financially motivated rational
4 See Jane Croft, Legal Firms Unleash Office Automatons, FIN. TIMES (May 16, 2016), https://www.ft.com/content/19807d3e-1765-11e6-9d98-00386a18e39d (discussing various software programs that can outperform attorneys and paralegals in document review); cf. Dana Remus & Frank S. Levy, Can Robots Be Lawyers? Computers, Lawyers, and the Practice of Law (Nov. 27, 2016), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2701092 (unpublished manuscript) (arguing that artificial intelligence will refocus rather than replace attorneys).
5 See Yonghui Wu et al., Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation (Sept. 26, 2016), https://arxiv.org/pdf/1609.08144.pdf (unpublished manuscript). Google now claims its Google Neural Machine Translation system is approaching human-level translation accuracy. Id. at 2.
6 See, e.g., DELOITTE, FROM BRAWN TO BRAINS: THE IMPACT OF TECHNOLOGY ON JOBS IN THE UK 4 (2015), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Growth/ deloitte-uk-insights-from-brawns-to-brain.pdf (suggesting that every nation and region of the U.K. has benefitted from automation and that automation has resulted in £140 billion to the U.K.’s economy in new wages).
7 See, e.g., Fiona Macdonald, The Greatest Mistranslations Ever, BBC (Feb. 2, 2015), http://www.bbc.co.uk/culture/story/20150202-the-greatest-mistranslations-ever (describing some of the unfortunate outcomes associated with mistranslation).
8 See generally MORTON J. HORWITZ, THE TRANSFORMATION OF AMERICAN LAW, 1780–1860 (1977) [hereinafter HORWITZ, 1780–1860]; MORTON J. HORWITZ, THE TRANSFORMA- TION OF AMERICAN LAW 1870–1960 (1992).
9 See George L. Priest, Satisfying the Multiple Goals of Tort Law, 22 VAL. U. L. REV. 643, 648 (1988).
10 See, e.g., George L. Priest, The Invention of Enterprise Liability: A Critical History of the Intellectual Foundations of Modern Tort Law, 14 J. LEGAL STUD. 461 (1985); see also Robert F. Blomquist, Goals, Means, and Problems for Modern Tort Law: A Reply to Professor Priest, 22 VAL. U. L. REV. 621 (1988) (arguing that economic theory and moral philosophy both require accident reduction to be the primary aim of tort law).
 11 See George L. Priest, Modern Tort Law and Its Reform, 22 VAL. U. L. REV. 1, 7 (1987).

4 499 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.12 This liability framework has far-reaching and sometimes complex impacts on be- havior. It can either accelerate or impede the introduction of new technologies.13
Most injuries people cause are evaluated under a negligence stan- dard where unreasonable conduct establishes liability.14 When com- puters cause the same injuries, however, a strict liability standard applies.15 This distinction has financial consequences and a corre- sponding impact on the rate of technology adoption.16 It discourages automation, because machines incur greater liability than people. It also means that in cases where automation will improve safety, the current framework to prevent accidents now has the opposite effect.
This Article argues that the acts of autonomous computer tortfeasors should be evaluated under a negligence standard, rather than a strict liability standard, in cases where an autonomous com- puter is occupying the position of a reasonable person in the tradi- tional negligence paradigm and where automation is likely to improve safety. For the purposes of ultimate financial liability, the computer’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments under standard principles of product liability law.
This Article employs a functional approach to distinguish an au- tonomous computer, robot, or machine from an ordinary product.17
12 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (applying rule that balances the burden of additional protections on the actor with the probability and gravity of an injury).
13 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Gideon Parchomovsky & Alex Stein, Torts and Innovation, 107 MICH. L. REV. 285, 286 (2008) (discussing how the role of custom in tort law impedes innovation). Nor is the idea that tort liability is a barrier to developments in machine intelligence new. See Steven J. Frank, Tort Adjudication and the Emergence of Artificial Intelligence Software, 21 SUFFOLK U. L. REV. 623, 639 (1987).
14 See infra text accompanying notes 62–71.
15 See infra text accompanying notes 93–100.
16 See, e.g., Amy Finkelstein, Static and Dynamic Effects of Health Policy: Evidence from
the Vaccine Industry, 119 Q.J. ECON. 527, 535 (2004) (explaining that establishment of the Vac- cine Injury Compensation Fund encouraged vaccine development by indemnifying manufactur- ers from liability).
17 Terms such as “robot,” “machine,” “artificial intelligence,” “machine intelligence,” and even “computer” are not used consistently even in the scientific literature. See, e.g., NEIL JOHN- SON ET AL., ABRUPT RISE OF NEW MACHINE ECOLOGY BEYOND HUMAN RESPONSE TIME 2 (2013), https://www.nature.com/articles/srep02627.pdf (discussing autonomy in the context of ar- tificial intelligence); Matthew U. Scherer, Regulating Artificial Intelligence Systems: Risks, Chal-
 
2018] 500 THE REASONABLE COMPUTER 5
Society’s relationship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are given tasks to complete and determine for themselves how to complete those tasks. For instance, a person could instruct a self-driving car to take them from point A to point B, but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. This distinction is analogous to the distinction be- tween employees and independent contractors, which centers on the degree of control and independence.18 As this Article uses such terms, autonomous machines or computer tortfeasors control the means of completing tasks, regardless of their programming.19
The most important implication of this line of reasoning is that just as computer tortfeasors should be compared to human tortfeasors, so too should humans be compared to computers. Once computers become safer than people and practical to substitute, com- puters should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical, reasonable person would have done in their situation, but what a computer would have done. In time, as computers come to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy,20 but people would engage in certain activi- ties at their own peril. Such a rule is entirely consistent with the ratio- nale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving
lenges, Competencies, and Strategies, 29 HARV. J.L. & TECH. 353, 359–61 (2016) (discussing difficulties with defining artificial intelligence); John McCarthy, What Is Artificial Intelligence? 2–3 (Nov. 12, 2007), http://jmc.stanford.edu/articles/whatisai/whatisai.pdf (discussing the lack of a standardized definition of artificial intelligence by the scientist who coined the term).
18 See Yewens v. Noakes [1880] 6 QB 530 at 532–33 (Eng.) (“A servant is a person subject to the command of his master as to the manner in which he shall do his work.”). Also see O’Connor v. Uber Technologies, Inc., No. 14-16078 (9th Cir. argued Sept. 20, 2017), for one of the many ongoing lawsuits against Uber highlighting modern challenges distinguishing between employees and independent contractors.
19 See, e.g., Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) (discussing types of machine architectures, including conventional knowledge-based systems with expert rules as well as types of machine intelligence algorithms that result in unexpected machine behavior).
20 See generally Richard M. Ryan & Edward L. Deci, Overview of Self-Determination The- ory: An Organismic Dialectical Perspective, in HANDBOOK OF SELF-DETERMINATION RESEARCH 3, 6 (Edward L. Deci & Richard M. Ryan eds., 2002) (arguing that people have three basic psychological needs: connectedness, autonomy, and feeling competent).
 
6 501 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
“reasonable computer” standard should even apply to computer tortfeasors, such that computers will be held to the standard of other computers. By this time, computers will cause so little harm that the primary effect of the standard would be to make human tortfeasors essentially strictly liable for their harms.
This Article uses self-driving cars as a case study to demonstrate the need for a new torts paradigm.21 There is public concern over the safety of self-driving cars, but a staggering ninety-four percent of crashes involve human error.22 These contribute to over 37,000 fatali- ties a year in the United States at a cost of about $242 billion.23 Auto- mated vehicles may already be safer than human drivers, but if not, they will be soon.24 Shifting to negligence would accelerate the adop- tion of driverless technologies, which, according to a report by the consulting firm McKinsey & Company, may otherwise not be wide- spread until the middle of the century.25
Automated vehicles may be the most prominent and disruptive upcoming example of robots changing society, but this analysis applies to any context with computer tortfeasors. For instance, IBM’s flagship artificial intelligence system, Watson, is working with clinicians at Me- morial Sloan Kettering to analyze patient medical records and provide
21 Others have written about tort liability and self-driving vehicles, although primarily dealing with how existing law deals with accidents involving autonomous vehicles. See, e.g., Jef- frey K. Gurney, Sue My Car Not Me: Products Liability and Accidents Involving Autonomous Vehicles, 2013 U. ILL. J.L. TECH. & POL’Y 247; F. Patrick Hubbard, “Sophisticated Robots”: Balancing Liability, Regulation, and Innovation, 66 FLA. L. REV. 1803, 1803 (2014) (arguing, using the example of self-driving vehicles, that the current framework “provides an appropriate balance of innovation and liability for personal injury”); Gary E. Marchant & Rachel A. Lindor, The Coming Collision Between Autonomous Vehicles and the Liability System, 52 SANTA CLARA L. REV. 1321 (2012).
22 See NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 115, CRITICAL REASONS FOR CRASHES INVESTIGATED IN THE NATIONAL MOTOR VEHICLE CRASH CAUSATION SURVEY 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublica- tion/812115.
23 General Statistics, INS. INST. FOR HIGHWAY SAFETY (Dec. 2017), http://www.iihs.org/ iihs/topics/t/general-statistics/fatalityfacts/overview-of-fatality-facts [https://perma.cc/2J5P- Y27C]; see NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 013, THE ECONOMIC AND SOCIETAL IMPACT OF MOTOR VEHICLE CRASHES, 2010 (REVISED) 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812013.
24 See Cadie Thompson, Why Driverless Cars Will Be Safer Than Human Drivers, BUS. INSIDER (Nov. 16, 2016, 9:24 PM), http://www.businessinsider.com/why-driverless-cars-will-be- safer-than-human-drivers-2016-11.
25 Michele Bertoncello & Dominik Wee, Ten Ways Autonomous Driving Could Redefine the Automotive World, MCKINSEY & CO. (June 2015), http://www.mckinsey.com/industries/auto- motive-and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
 
2018] 502 THE REASONABLE COMPUTER 7
evidence-based cancer treatment options.26 It even provides support- ing literature to human physicians to support its recommendations.27 Like self-driving cars, Watson does not need to be perfect to improve safety—it just needs to be better than people. In that respect, the bar is unfortunately low. Medical error is one of the leading causes of death.28 A 2016 study in the British Medical Journal reported that it is the third leading cause of death in the United States, ranking just be- hind cardiovascular disease and cancer.29 Some companies already claim their artificial intelligence systems outperform doctors, and that claim is not hard to swallow.30 Why should a computer not be able to outperform doctors when the computer can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?31
This Article is divided into three Parts. Part I provides back- ground on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negli- gence and strict product liability. Part II argues that while some forms of automation should prevent accidents, tort law may act as a deter- rent to adopting safer technologies. To encourage automation and im- prove safety, this Part proposes a new categorization of “computer- generated torts” for a subset of machine injuries. This would apply to cases in which an autonomous computer, robot, or machine is occupy- ing the position of a reasonable person in the traditional negligence paradigm and where automation is likely to improve safety. This Part contends that the acts of computer tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and it goes on to propose rules for implementing the system.
26 Oncology and Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-ge- nomics [https://perma.cc/Z6H7-S5W4].
27 Id.
28 See INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER HEALTH SYSTEM (Linda T. Kohn et al. eds., 2000); Martin A. Makary & Michael Daniel, Medical Error—The Third Leading Cause of Death in the US, 353 BMJ 2139, 2139 (2016). The landmark report published by the Institute of Medicine in 2000 was a wake-up call to the medical profession about the harmful effects of medical error. See INST. OF MED., supra. Yet the report was based on studies conducted in 1984 and 1992. See id.
29 Makary & Daniel, supra note 28, at 2143 fig.1.
30 Parloff, supra note 3. For example, Enlitic has a program for detecting and classifying
lung cancers which the company claims has already outperformed human radiologists. Id.
31 See, e.g., Saul N. Weingart et al., Epidemiology of Medical Error, 320 BMJ 774, 775 (2010) (discussing some of the causes of human medical error).
 
8 503 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
Finally, Part III argues that once computer operators become safer than people and automation is practical, the “reasonable computer” should become the new standard of care. It explains how this standard would work, argues the reasonable computer standard works better than a reasonable person using an autonomous machine, and consid- ers when the standard should apply to computer tortfeasors. At some point, computers will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
This Article is focused on the effects of automation on accidents, but automation implicates a host of social concerns. It is important that policymakers act to ensure that automation benefits everyone. Automation may increase productivity and wealth, but it may also contribute to unemployment, financial disparities, and decreased so- cial mobility. These and other concerns are certainly important to con- sider in the automation discussion, but tort liability may not be the best mechanism to address every issue related to automation.32
I. LIABILITY FOR MACHINE INJURIES
A. A Brief History
Injuries caused by machines are nothing new. For as long as peo- ple have used machines, injuries have resulted—and machines have been with us for quite some time. The earliest evidence of simple ma- chines—tools that redirect force to make work easier, like axes— dates back millions of years to the beginning of the Stone Age.33 In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as hand axes.34 The pri- mary function of these tools was to hunt and cut meat,35 but they were also used to facilitate violence against people.36 Machines used in the furtherance of intentional torts were likely used negligently as well.
32 See, e.g., Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL’Y REV. 145 (2018) (arguing that the tax system incentivizes automation even in cases where it is not otherwise efficient and that automation decreases government tax revenue, and proposing changes to existing tax policies as a solution).
33 Kate Wong, Ancient Cut Marks Reveal Far Earlier Origin of Butchery, SCI. AM. (Aug. 11, 2010), https://www.scientificamerican.com/article/ancient-cutmarks-reveal-butchery/.
34 See Stone Age, MERRIAM-WEBSTER, https://www.merriam-webster.com/dictionary/ Stone%20Age [https://perma.cc/U6W4-M4M8]. See generally Sonia Harmand et al., 3.3-Million- Year-Old Stone Tools from Lomekwi 3, West Turkana, Kenya, 521 NATURE 310 (2015).
35 Wong, supra note 33.
36 M. Mirazo ́n Lahr et al., Inter-group Violence Among Early Holocene Hunter-Gatherers
of West Turkana, Kenya, 529 NATURE 394, 396 (2016).
 
2018] 504 THE REASONABLE COMPUTER 9
Given that home knife accidents led to about a third of a million emergency room visits in the United States in 2011 alone, it is not difficult to imagine that during the Stone Age these simple machines caused accidents.37
As history progressed, and the use and complexity of simple ma- chines grew, so too did the resultant injuries38: Mesopotamian sur- geons botched procedures,39 Greek construction zones were so dangerous they required physicians on site,40 and Egyptian embalmers accidently left instruments in their subjects.41 Such injuries continued unabated from the time complex machines were invented by the an- cient Chinese and Greeks to the time of the first modern industrial machines.42
The Industrial Revolution marked a turning point in the role of machines in society.43 Major technological advances occurred during
37 See Joe Yonan, Knife Injuries and Other Kitchen Mishaps Afflict Both Top Chefs and Everyday Cooks, WASH. POST (Jan. 7, 2013), https://www.washingtonpost.com/national/health- science/knife-injuries-and-other-kitchen-mishaps-afflict-both-top-chefs-and-everyday-cooks/ 2013/01/07/92e191f8-4af0-11e2-b709-667035ff9029_story.html.
38 See generally Y.C. CHIU, AN INTRODUCTION TO THE HISTORY OF PROJECT MANAGE- MENT 19–115 (2010) (discussing the use of technology in industrial activities). For example, al- most half a million people died building the Great Wall of China, although the number of these deaths due to machine injuries is unknown. Great Wall of China, HISTORY.COM, http:// www.history.com/topics/great-wall-of-china [https://perma.cc/7MP5-FJX5]. So common were machine and industrial injuries in the ancient world that ancient Greek, Roman, Arab, and Chi- nese laws provided for compensation schedules for accidents. See Gregory P. Guyton, A Brief History of Workers’ Compensation, 19 IOWA ORTHOPAEDIC J. 106, 106 (1999). Under ancient Arab law, “loss of a joint of the thumb was worth one-half the value of a finger. The loss of a penis was compensated by the amount of length lost, and the value an ear was based on its surface area.” Id.
39 See Emily K. Teall, Medicine and Doctoring in Ancient Mesopotamia, 3 GRAND VALLEY J. HIST. 1, 5 (2014). Unfortunately for these doctors, medical malpractice in Babylon was corpo- rally punishable. Allen D. Spiegel & Christopher R. Springer, Babylonian Medicine, Managed Care and Codex Hammurabi, Circa 1700 B.C., 22 J. COMMUNITY HEALTH 69, 81 (1997); see also GUIDO MAJNO, THE HEALING HAND: MAN AND WOUND IN THE ANCIENT WORLD 53 (1975).
40 DAVID MATZ, VOICES OF ANCIENT GREECE AND ROME: CONTEMPORARY ACCOUNTS OF DAILY LIFE 58 (2012).
41 Granted, this example involves cadavers rather than living patients, or so one hopes. Owen Jarus, Oops! Brain-Removal Tool Left in Mummy’s Skull, LIVE SCI. (Dec. 14, 2012, 8:03 AM), http://www.livescience.com/25536-mummy-brain-removal-tool.html/. It certainly portends modern medical malpractice cases involving retained surgical instruments. See, e.g., Atul A. Ga- wande et al., Risk Factors for Retained Instruments and Sponges After Surgery, 348 NEW ENG. J. MED. 229, 230 (2003).
42 Peter J. Lu, Early Precision Compound Machine from Ancient China, 304 SCIENCE 1638 (2004); cf. Russell Fowler, The Deep Roots of Workers’ Comp, 49 TENN. B.J. 10, 10–12 (2013) (discussing historical development of workers’ compensation schemes from the medieval through the modern era).
 43 Economists have argued the Industrial Revolution was “certainly the most important

10 505 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
this period in textiles, transportation, and iron making, which resulted in the development of machines for shaping materials and the rise of the factory system.44 It also resulted in a dramatic increase in the num- ber and severity of machine injuries.45 Working in industrial settings was a dangerous business, in part because employers often had mini- mal liability for employee harms.46 These dangerous working condi- tions persisted well into the twentieth century before the U.S. government began collecting data on work-related injuries in a sys- tematic way.47 In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of 61 deaths per 100,000 workers.48
In the modern era, the rate of work-related injuries has declined significantly. In 2016, for example, the U.S. Bureau of Labor reported 5190 fatal work injuries, a rate of 3.6 per 100,000 workers.49 The rea- son for this decline is multifactorial: changes to tort liability, evolved societal and ethics norms that place a greater priority on human wel- fare, a modern system of regulations and criminal liability that pro- tects worker wellbeing, as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Workplace accidents were responsible for
event in the history of humanity since the domestication of animals and plants, perhaps the most important since the invention of language. It bids fair to free us all, eventually.” Deirdre Mc- Closkey, Review of The Cambridge Economic History of Modern Britain, PRUDENTIA (Jan. 15, 2004), http://www.deirdremccloskey.com/articles/floud.php [https://perma.cc/UAP4-6ZZ3].
44 See generally History of Technology: The Industrial Revolution (1750–1900), EN- CYCLOPæDIA BRITANNICA, https://www.britannica.com/technology/history-of-technology/The-In- dustrial-Revolution-1750-1900 [https://perma.cc/QV7K-LKLK].
45 See generally HENRY ROGERS SEAGER, SOCIAL INSURANCE: A PROGRAM OF SOCIAL REFORM 24–52 (1910) (including a chapter on industrial accidents in a classic exposition of the philosophical movement for social insurance).
46 John S. Haller, Jr., Industrial Accidents—Worker Compensation Laws and the Medical Response, 148 WEST J. MED. 341–48 (1988); see also HORWITZ, 1780–1860, supra note 8, at 90. 47 See Progressive Era Investigations, U.S. DEP’T LAB., https://www.dol.gov/dol/aboutdol/ history/mono-regsafepart05.htm [https://perma.cc/HUT4-5WQE]. The first systematic U.S. sur- vey of workplace fatalities found that 526 workers died in “work accidents” in Allegheny County from July 1906 to June 1907. Improvements in Workplace Safety—United States, 1900–1999, 48 CDC MORBIDITY & MORTALITY WKLY. REP. 461, 461 (1999). Of those fatalities, 195 were steel-
workers. Id. Contrast that with 17 national steelworker fatalities in 1997. Id.
48 Improvements in Workplace Safety—United States, 1900–1999, supra note 47, at 461. The National Safety Council estimated that 18,000–21,000 workers died from work-related inju-
ries in 1912. Id.
49 BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR, NATIONAL CENSUS OF FATAL
OCCUPATIONAL INJURIES IN 2016, at 1 (2017), http://www.bls.gov/news.release/pdf/cfoi.pdf [https://perma.cc/2YMS-RA8F].
 
2018] 506 THE REASONABLE COMPUTER 11
approximately 4000 deaths in the United States in 2014 and a total cost of about $140 billion.50 More broadly, there were a total of almost 200,000 injury-related deaths in 2014 in the United States, with all un- intentional injuries costing some $850 billion.51 Unintentional injuries are the fourth leading cause of death.52
B. Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law now provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health.53
A tort is a harmful civil act, other than under contract, where one person is damaged by another, and it gives way to a right to sue.54 A variety of goals have been proposed for tort law: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.55 Whether tort law is the best means for achieving all of these goals is a matter of endless dispute.56 Jurists are united, however, in considering accident reduction as one of the central goals of tort law, if not the primary goal.57 By creating a framework for loss shifting from injured
50 NAT’L SAFETY COUNCIL, INJURY FACTS: 2016 EDITION 3, 8 (2016).
51 Id. Lost quality of life from those injuries is valued at an additional $3345.5 billion. Id. at
8.
52 Id. at 2.
53 Tort law primarily grew out of a focus on bodily injury and physical property damage,
but protection in modern times has been extended beyond the physical to include harm to emo- tional well-being, and economic loss.
The range of torts is as broad as human experience and includes such wrongful conduct as negligence (personal injury law for unintentional harm), intentional torts (e.g., assault, battery, trespass to land), products liability (defective products), abnormally dangerous activities liability (e.g., blasting, aerial pesticide spraying), nuisance (e.g., air, water, and noise pollution), defamation (libel and slander), pri- vacy invasion (private area intrusion and personal autonomy interference), and fraud (misrepresentation). Tort law study also includes consideration of legislative measures related to torts and alternatives to tort liability, for example, automobile no-fault compensation systems.
DOMINICK VETRI ET AL., TORT LAW AND PRACTICE 3 (5th ed. 2016).
54 See id. at 2. A tort governs loss shifting from injured victims to tortfeasors, and it dic-
tates who can sue and what they can sue for. See id. It is “the set of legal rules establishing liability and compensation for personal injury and death caused by the intentional or careless conduct of a third party.” Id.
55 See, e.g., Priest, supra note 9, at 645 n.23, 648.
56 See, e.g., Priest, supra note 10.
57 See Blomquist, supra note 10, at 628–29 (arguing that economic theory and moral phi-
 losophy both require accident reduction to be the primary aim of tort law).

12 507 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
victims to tortfeasors, tort law deters unsafe conduct.58 A purely finan- cially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.59
On a broader level, the law of torts is one of the primary ways in which society choses to allocate liability. And allocating liability has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing and pulse oximeters, or it can discourage the use of new technologies, as is usually the case where the standard of care is based on custom.60
Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts which do not require fault.61 Covering the “great mass of cases” in the middle are harms involving negligence.62
C. Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms.63 In practice, to pre- vail in most personal injury cases, a plaintiff must prove by a prepon- derance of the evidence that the defendant owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages.64 This generally requires proof that the defendant acted neg- ligently, which is to say, acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circum-
58 See Priest, supra note 9, at 648.
59 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (stating that
liability calculations should consider whether the probability of injury times potential damages is lower than the burden imposed).
60 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Parchomovsky & Stein, supra note 13, at 306 (discussing how the role of custom in
 tort law 61
62 63 64
impedes innovation).
Oliver Wendell Holmes, Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873). Id.
See Thomas C. Grey, Accidental Torts, 54 VAND. L. REV. 1225, 1283–84 (2001). See RESTATEMENT (SECOND) OF TORTS § 281 (AM. LAW INST. 1965).

2018] 508 THE REASONABLE COMPUTER 13
stances.65 Thus, if the courts determined that a reasonable person would not have headed out to sea without a radio to warn of storm conditions,66 manufactured a ginger beer with a snail inside,67 or dropped heavy objects off the side of a building,68 then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compen- sating victims as well as encouraging economic growth and progress.69 One way that tort law attempts to achieve this balance is by permit- ting recovery in negligence only where there has been socially blame- worthy conduct.70 Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will gen- erally be no liability. Juries play a key role in determining the reasona- ble person standard as applied to the facts of a case.71
D. Strict and Product Liability
While negligence governs virtually all accidents, there are excep- tions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities such as hazardous waste disposal or blasting.72 Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy.73 Thus, a defendant cor-
65 The idea that negligence involves conduct that falls below an objective standard was first articulated by Baron Alderson in the case of Blyth v. Birmingham Waterworks Co.:
Negligence is the omission to do something which a reasonable man, guided upon those considerations which ordinarily regulate the conduct of human affairs, would do, or doing something which a prudent and reasonable man would not do. The defendants might have been liable for negligence, if, unintentionally, they omitted to do that which a reasonable person would have done, or did that which a person taking reasonable precautions would not have done.
(1856) 156 Eng. Rep. 1047, 1049; 11 Ex. 781, 784.
66 See The T.J. Hooper, 53 F.2d 107 (S.D.N.Y. 1931).
67 See Donoghue v. Stevenson [1932] AC 562 (HL) (appeal taken from Scot.).
68 See Byrne v. Boadle (1863) 159 Eng. Rep. 299.
69 VETRI ET AL., supra note 53, at 12.
70 See James Barr Ames, Law and Morals, 22 HARV. L. REV. 97, 99 (1908).
71 VETRI ET AL., supra note 53, at 10.
72 Id. at 11.
73 See Frederick Pollock, Duties of Insuring Safety: The Rule in Rylands v. Fletcher, 2 L.Q.
REV. 52, 53 (1886). While early English common law imposed strict liability for certain wrongs such as trespass, Rylands v. Fletcher (1868) 3 LRE & I App. 330 (HL), was the progenitor of the doctrine of strict liability for abnormally dangerous activities, and its ruling had a major impact on the development of tort law. Pollock, supra, at 52, 59. In the case, Fletcher’s reservoir burst and flooded a neighboring mine run by Rylands through no fault of Fletcher. Id. at 53. This court held that “the person who for his own purposes brings on his lands and collects and keeps there,
 
14 509 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
poration that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defec- tive or because its properties are falsely represented.74 Product inju- ries cause upwards of 200 million injuries a year in the United States.75 In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products.76 The bulk of product liability cases involve claims for damages against a manufac- turer or retailer by a person injured while using a product.77 Typically, a plaintiff will try to prove that an injury was the result of some inher- ent defect of a product or its marketing and that the product was flawed or falsely advertised.78 Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and ac- curately marketed.79 Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product or that something other than the product caused the harm.80
Product liability was not always governed by strict liability. Origi- nally, American courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise.81 Toward the end of the nineteenth century, however, states began increasingly em- ploying the doctrine of caveat venditor and an implied warranty of merchantable quality.82 Under this doctrine, “[s]elling for a sound price raises an implied warranty that the thing sold is free from de-
anything likely to do mischief if it escapes, must keep it in at his peril, and, if he does not do so, is prima facie answerable for all the damage which is the natural consequence of its escape.” Id. at 54. Critics of the case objected to its potential impact on economic activity. See, e.g., THOMAS C. GREY, FORMALISM AND PRAGMATISM IN AMERICAN LAW 248 (2014) (noting that many “prestig- ious judges and commentators” repudiated Rylands on the basis that “liberal principles of formal equality and economic freedom, or a devotion to economic development, required rejection of tort liability without fault”).
74 DAVID G. OWEN, PRODUCTS LIABILITY LAW 1 (3d ed. 2014).
75 Id. at 1.
76 Id. at 3.
77 Id.
78 Id.
79 Id.
80 Id.
81 Id. at 17–18.
82 Id. at 18.
 
2018] 510 THE REASONABLE COMPUTER 15
fects, known and unknown (to the seller).”83 Ultimately, the doctrine of implied warranty of merchantable quality was reduced to statutory form in the Uniform Sales Act of 1906.84 Yet even so, manufacturers were in large part able to avoid liability for defective products by ar- guing they lacked privity of contract with consumers.85 This was possi- ble because in most cases consumers purchased products from third- party retailers rather than directly from manufacturers.86
That changed in 1916 with the New York Court of Appeals deci- sion in MacPherson v. Buick Motor Co.87 The case involved a motorist who was injured when one of the wooden wheels of his Buick col- lapsed.88 He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if the manufacturer of such a foreseeably dangerous product knows that it “will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”89 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted MacPherson’s holding.90 This shift was ac- companied by growing public support for consumer protection to- gether with the understanding that liability would not unduly burden economic activity.91 Businesses are often in the best position to pre- vent product injuries and can distribute liability through insurance.92
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products, Inc.,93 which held that manufacturers of defec- tive products are strictly liable for injuries caused by such products.94
83 Id. (quoting S. Iron & Equip. Co. v. Bamberg, E. & W. Ry. Co., 149 S.E. 271, 278 (S.C. 1929)).
84 Id.; see U.C.C. § 2-314 (AM. LAW INST. & UNIF. LAW COMM’N 2014). See generally Friedrich Kessler, The Protection of the Consumer Under Modern Sales Law, Part 1, 74 YALE L.J. 262 (1964).
85 OWEN, supra note 74, at 18.
86 Id.
87 111 N.E. 1050 (N.Y. 1916).
88 Id. at 1051.
89 Id. at 1053.
90 OWEN, supra note 74, at 22. Maine was the last state to abolish the privity requirement
in negligence actions in 1982. Id.
91 See id. at 22–23.
92 See id.
93 377 P.2d 897 (Cal. 1963) (in bank).
94 Id. at 900. Of note, Justice Roger Traynor, who wrote the majority opinion in the case,
had suggested this strict liability rule nineteen years earlier in a concurring opinion in Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436 (Cal. 1944). He argued responsibility should “be
 
16 511 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
This case represents the birth of modern products liability law in America.95 After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s, with the American Law Institute memorializing the rule in Section 402A of the Restatement (Second) of Torts.96
Of course, today’s products liability law is not as simple as this brief narrative suggests.97 It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts.98 Since the 1960s, a variety of state statutes have attempted to reform prod- ucts liability law, often to limit the rights of consumers in order to protect manufacturers.99 For our purposes, however, it suffices to say that as a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.100
II. COMPUTER-GENERATED TORTS
A. Automation Will Prevent Accidents
On May 7, 2016, a Tesla driver was killed in the first known fatal crash of a self-driving car.101 Tesla reported that the autopilot system
fixed wherever it will most effectively reduce the hazards to life and health inherent in defective products that reach the market.” Id. at 440 (Traynor, J., concurring). A few years before this case, the Supreme Court of New Jersey found manufacturers strictly liable in warrantee to re- mote consumers in Henningsen v. Bloomfield Motors, Inc., 161 A.2d 69, 77, 84 (N.J. 1960).
95 OWEN, supra note 74, at 23.
96 RESTATEMENT (SECOND) OF TORTS § 402A (AM. LAW INST. 1965); see OWEN, supra note 74, at 23.
97 For a more comprehensive view on products liability, the American Law Institute pub- lished a Restatement specifically on products liability in 1998. RESTATEMENT (THIRD) OF TORTS: PRODUCTS LIABILITY (AM. LAW INST. 1998).
98 OWEN, supra note 74, at 4.
99 Id. at 23.
100 See Vandermark v. Ford Motor Co., 391 P.2d 168, 171–72 (Cal. 1964) (in bank) (“Retail-
ers like manufacturers are engaged in the business of distributing goods to the public. They are an integral part of the overall producing and marketing enterprise that should bear the cost of injuries resulting from defective products. In some cases the retailer may be the only member of that enterprise reasonably available to the injured plaintiff. In other cases the retailer himself may play a substantial part in insuring that the product is safe or may be in a position to exert pressure on the manufacturer to that end; the retailer’s strict liability thus serves as an added incentive to safety. Strict liability on the manufacturer and retailer alike affords maximum pro- tection to the injured plaintiff and works no injustice to the defendants, for they can adjust the costs of such protection between them in the course of their continuing business relationship.” (citation omitted)).
101 Sam Levin & Nicky Woolf, Tesla Driver Killed While Using Autopilot Was Watching Harry Potter, Witness Says, GUARDIAN (July 1, 2016, 1:43 PM), https://www.theguardian.com/
 
2018] 512 THE REASONABLE COMPUTER 17
did not apply the brakes after the car’s sensor system failed to detect an eighteen-wheel truck and trailer.102 The car attempted to drive full speed under the trailer and the bottom of the trailer impacted the car’s windshield.103 The driver, whom Tesla claims should have re- mained alert and who also failed to apply the brakes, may have been watching a Harry Potter movie at the time.104
Surveys of attitudes toward self-driving cars have produced mixed results, but they have often uncovered negative opinions.105 A survey by the American Automobile Association in March 2016 re- ported that three out of four U.S. drivers surveyed said they would feel “afraid” to ride in a self-driving car.106 Only one in five said they would trust a driverless car to drive itself while they were inside.107 Another recent survey found that most U.K. citizens would feel un- comfortable with self-driving vehicles on the road, and more than
technology/2016/jul/01/tesla-driver-killed-autopilot-self-driving-car-harry-potter; see Anjali Singhvi & Karl Russell, Inside the Self-Driving Tesla Fatal Accident, N.Y. TIMES (July 12, 2016), http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html?_r=0. This has been the first reported fatality, but not the only reported crash for which a self-driving vehicle has been at fault. See, e.g., Tan Weizhen, Self-Driving Car in Accident with Lorry at One-North, TODAY (Oct. 18, 2016), http://www.todayonline.com/singapore/self-driving-car-involved-acci- dent-one-north. Other, nonfatal accidents have been attributed to self-driving vehicles. See Dave Lee, Google Self-Driving Car Hits a Bus, BBC NEWS (Feb. 29, 2016), http://www.bbc.co.uk/news/ technology-35692845. The National Highway Traffic Safety Administration (“NHTSA”) investi- gated this accident and issued a report in January 2017 stating that “[a] safety-related defect trend has not been identified at this time and further examination of this issue does not appear to be warranted.” NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., INVESTI- GATION PE 16-007 (2017), https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF. The NHTSA found the accident was beyond the capabilities of the vehicle’s Autopilot and Auto- matic Emergency Breaking systems. Id. The report went on to state that overall crash rates decreased by nearly forty percent after installation of Tesla’s Autosteer technology. Id. at 10.
102 Levin & Woolf, supra note 101.
103 Id.
104 Id.
105 Similarly, a poll of 1869 registered voters in January 2016 by Morning Consult found
that forty-three percent of registered voters said self-driving cars were unsafe, while only thirty- two percent said they were safe. Amir Nasr & Fawn Johnson, Voters Aren’t Ready for Driverless Cars, Poll Shows, MORNING CONSULT (Feb. 8, 2016), https://morningconsult.com/2016/02/08/vot- ers-arent-ready-for-driverless-cars-poll-shows/. Fifty-one percent of respondents said they would not ride in a driverless car, while twenty-five percent said they would. Id.; see Paul Lienert, Tesla Crash Does Little to Sway Public Opinion on Self-Driving Cars, AUTOMOTIVE NEWS (July 29, 2016, 2:21 PM), http://www.autonews.com/article/20160729/OEM06/160729812/tesla-crash-does- little-to-sway-public-opinion-on-self-driving-cars (discussing the results of other surveys).
106 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM (Mar. 1, 2016), http://newsroom.aaa.com/2016/03/three-quarters-of-ameri- cans-afraid-to-ride-in-a-self-driving-vehicle/.
 107 Id.

18 513 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
three-quarters would want to retain a steering wheel.108 Regulators are more optimistic than the public, but they are still cautious.109 Until very recently, California required human drivers to be present in all self-driving cars being tested on public roads.110 Two laws passed in 2016, however, now permit unmanned vehicles to operate on public roads under certain circumstances.111
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether computers are perfect (they are not), but whether they are safer than people (they are). Nearly all crashes involve human error.112 A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs.113 The U.S. Department of Transportation reports that more than 35,000 people died from motor vehicle accidents in the United States in 2015.114 It estimates the economic costs of those accidents at over $240 billion.115
By contrast, the Tesla fatality was the first known autopilot death in about 130 million miles driven by the system.116 It is also important to note that driverless technologies are in their infancy. Imagine how improved such technologies will be in ten years. One academic expert predicted in September 2016 that self-driving cars will be ten times safer than human drivers in three years, and one hundred times safer in ten years.117 At the point where automated cars are ten times safer
108 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, INQUIRER (Oct. 17, 2016), http://www.theinquirer.net/inquirer/news/2474351/over-half-of-brits- wont-feel-safe-using-the-streets-with-driverless-cars.
109 This caution is reflected, for example, in guidelines released in September 2016 by the Department of Transportation for safe design, development, and testing of self-driving cars. NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., FEDERAL AUTOMATED VE- HICLES POLICY: ACCELERATING THE NEXT REVOLUTION IN ROADWAY SAFETY 5–7 (2016), https://www.transportation.gov/sites/dot.gov/files/docs/AV%20policy%20guidance%20PDF.pdf.
110 Susmita Baral, Driverless Car Laws in California Get Major Changes in September, INT’L BUS. TIMES (Oct. 3, 2016, 5:40 PM), http://www.ibtimes.com/driverless-car-laws-california- get-major-changes-september-2425689.
111 Id.
112 NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., supra note 109, at 5.
113 ALEXANDER HARS, TOP MISCONCEPTIONS OF AUTONOMOUS CARS AND SELF-DRIVING
VEHICLES 1, 6 (2016), http://www.inventivio.com/innovationbriefs/2016-09/Top-misconceptions- of-self-driving-cars.pdf.
114 General Statistics, supra note 23.
115 Id.
116 A Tragic Loss, TESLA (June 30, 2016), https://www.tesla.com/en_GB/blog/tragic-loss
[https://perma.cc/LZ8X-UW2F].
117 Michael Belfiore, Self-Driving Cars Will Be 10x Safer Than Human Drivers in 3 Years,
MICHAEL BELFIORE BLOG (Sept. 20, 2016), http://michaelbelfiore.com/2016/09/20/self-driving- cars-will-be-10x-safer-than-human-drivers-in-3-years/ [https://perma.cc/4T78-CEWD]. Similarly,
 
2018] 514 THE REASONABLE COMPUTER 19
than human drivers, that could reduce the annual number of motor vehicle fatalities to about 3500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicted au- tonomous vehicles would reduce the number of auto deaths by about 30,000 a year.118 However, the report estimated that self-driving tech- nologies would not be adopted widely enough to permit this outcome until the middle of the century.119
B. Tort Liability Discourages Automation
To see why tort law discourages automation, it is important to look at the question of when it makes economic sense for a business to replace a human operator with a machine operator. In practice, it might be complex to calculate the cost of each operator. Human em- ployees have costs in excess of their salaries and wages, such as tax liability for employer portions of Social Security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; em- ployer portions of health insurance; paid holidays, vacations, and sick days; contributions toward retirement, pension, savings, and profit- sharing plans, etc.120 Computer costs may be simpler to estimate, but they may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision.121 For example, a person may require vocational training or be unable to work due to sickness; a computer may require software updates or be unable to work due to malfunction. Human operators may result in greater expenses for le- gal fees, administrative and overhead costs, as well as compliance with regulatory and employment requirements.122 Automation may provide
Bob Lutz, former General Motors (“GM”) vice chairman, predicted that GM’s first autonomous cars would have an accident rate about ten percent of those of human drivers. Michelle Fox, Self- Driving Cars Safer than Those Driven by Humans: Bob Lutz, CNBC (Sept. 8, 2014, 3:30 PM), http://www.cnbc.com/2014/09/08/self-driving-cars-safer-than-those-driven-by-humans-bob-lutz. html.
118 Bertoncello & Wee, supra note 25.
119 Id.
120 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level,
145 TAX NOTES 117, 118 (2014); see also WAYNE F. CASCIO, COSTING HUMAN RESOURCES (4th ed. 2000).
121 See ALFRED MARSHALL, PRINCIPLES OF ECONOMICS 368, 376 (8th ed. 1920).
122 See Cost of Small Business Employment, CTR. FOR ECON. & BUS. RES., www.cebr.com/
reports/cost-of-small-business-employment/ [https://perma.cc/V3F6-USE6].
 
20 515 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
tax benefits,123 but may infringe patents or result in negative public- ity.124 Whether to staff with a person or a machine may also take into account broader social policies. For instance, automation may pro- mote income inequality and unemployment. But businesses are re- quired to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to pro- mote social responsibility.125
The decision of whether to employ a computer or human opera- tor, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making—estimating uncertain future costs relatively accurately and making decisions as rational economic actors.126 Tort liability will only be one factor to consider when deciding whether to employ a computer or human op- erator. But, in the aggregate, tort liability will influence automation.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, businesses may not be directly liable for harms caused by autonomous computers.127 The computer’s manufacturer and other members of the supply chain will generally be liable. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability, for instance, by relying on independent contrac-
 123 See Abbott & Bogenschneider, supra note 32.
124 See, e.g., Kate Taylor, McDonald’s Ex-CEO Just Revealed a Terrifying Reality for Fast-
Food Workers, BUS. INSIDER (May 25, 2016, 10:05 AM), http://www.businessinsider.com/ mcdonalds-ex-ceo-takes-on-minimum-wage-2016-5 (discussing criticism of McDonald’s for re- placing workers with machines).
125 See generally Dodge v. Ford Motor Co., 170 N.W. 668, 682–84 (Mich. 1919). Of course, many companies argue they promote corporate social responsibility, and in some circumstances, there may be a business case for doing so. See, e.g., Archie B. Carroll & Kareem M. Shabana, The Business Case for Corporate Social Responsibility: A Review of Concepts, Research and Practice, 12 INT’L J. MGMT. REVS. 85 (2010).
126 See, e.g., Hugh Courtney, Jane Kirkland & Patrick Viguerie, Strategy Under Uncertainty, HARV. BUS. REV., Nov.–Dec. 1997.
127 See Mark A. Chinen, The Co-Evolution of Autonomous Machines and Legal Responsi- bility, 20 VA. J.L. & TECH. 338, 347–48 (2016).

2018] 516 THE REASONABLE COMPUTER 21
tors.128 Businesses are not usually liable for negligent harms caused by their independent contractors.129
Yet even in cases where liability rests with a business’s supplier or an independent contractor, such liability may indirectly impact a busi- ness. A manufacturer or retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent con- tractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product.130 Yet the fact that tort liability may be indirect and complex or that firms may purchase insurance to manage risk does not change the fact that tort liability has a financial cost which influences behavior.
Leaving aside tort liability, if both operators cost a business the same amount to employ, the decision of whether to utilize a person or computer should be neutral. If a business introduces the variable of tort liability into the decision, a human operator would be preferred. Harms caused by a person will be evaluated in negligence, but the same harms caused by a computer will be evaluated in strict liability. It is easier to establish strict liability than negligence.131 Strict liability does not require careless manufacturer behavior, only that a defect be present in a product.132 At least with regard to tort liability, the law
128 See, e.g., Kleeman v. Rheingold, 614 N.E.2d 712, 715 (N.Y. 1993). There are, however, limits on the extent to which businesses can rely on independent contractors or attempt to clas- sify employees as independent contractors. See, e.g., In re Morton, 30 N.E.2d 369, 371 (N.Y. 1940). As another example of how business can avoid tort liability for the actions of human operators, employers are not generally liable for intentional torts committed by employees. See, e.g., Ocana v. Am. Furniture Co., 91 P.3d 58, 70–71 (N.M. 2004).
129 See Kleeman, 614 N.E.2d at 715.
130 See generally RBB ECONOMICS, COST PASS-THROUGH: THEORY, MEASUREMENT, AND
POTENTIAL POLICY IMPLICATIONS (2014).
131 See Cronin v. J.B.E. Olson Corp., 501 P.2d 1153, 1162 (Cal. 1972) (in bank) (“[T]he very purpose of our pioneering efforts in [strict product liability] was to relieve the plaintiff from problems of proof inherent in pursuing negligence and warranty remedies, and thereby ‘to insure that the costs of injuries resulting from defective products are borne by the manufacturers . . . .’” (ellipsis in original) (citations omitted) (quoting Greenman v. Yuba Power Prods., Inc., 377 P.2d 897, 901 (Cal. 1963))); see also Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436, 441 (Cal. 1944) (Traynor, J., concurring) (“It is to the public interest to discourage the marketing of products having defects that are a menace to the public. If such products nevertheless find their way into the market it is to the public interest to place the responsibility for whatever injury they may cause upon the manufacturer, who, even if he is not negligent in the manufacture of the product, is responsible for its reaching the market. However intermittently such injuries may occur and however haphazardly they may strike, the risk of their occurrence is a constant risk and a general one. Against such a risk there should be general and constant protection and the manufacturer is best situated to afford such protection.”).
 132 See Cronin, 501 P.2d at 1162.

22 517 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
thus favors people over machines. This will hold true as long as com- puters are treated as “ordinary products” as to which strict liability is the default rule.
C. Computer-Generated Torts Should Be Negligence Based
Holding computer-generated torts to a negligence standard will result in an improved outcome: it will accelerate the adoption of auto- mation where doing so would reduce accidents. Of course, moving from a strict liability to a negligence standard would have some draw- backs. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to insure against loss. Indeed, this is why courts initially adopted strict product liability.133 Computer-gen- erated torts, however, differ from other product harms in that—once machines become safer than people—automation will result in net safety gains.
To illustrate this, imagine that with current technology a com- puter driver would be ten times safer than a human driver. In this case, it would be better that one human driver is replaced by a ma- chine than that the same machine becomes 100 times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for a computer driver (model C-A) is one fatality per 1 billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now imagine that we are able to improve C-A an additional ten- fold such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of 10 billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, how- ever, instead of focusing our efforts on improving C-A we simply re- place the human driver with another C-A, then over the course of 10 billion miles driven by C-A & C-A there will be a total of 20 fatalities. Once computers become safer than people, and particularly once computers become substantially safer than people, very significant re- ductions in accident rates will be gained by automation. Therefore—at some point—it is preferable to weaken the incentive to gain incremen-
 133 See, e.g., Greenman, 377 P.2d at 901.

2018] 518 THE REASONABLE COMPUTER 23
tal improvements in product safety to increase the adoption of safer technologies.
Also, even under a negligence standard, manufacturers will be incentivized to improve the safety of their computer systems because they may still be liable for accidents. Manufacturers will likely have the best information available to determine whether it would be bet- ter to pay to further reduce accident risks, e.g., whether an additional $10,000 per vehicle is worth a one percent reduction in accident risk, or whether to pay claims for additional accidents. Higher safety levels are not always better; inefficiently high safety levels may result in pro- hibitively high prices for consumers.134 To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, non-tort mechanisms could be brought to bear, such as regula- tory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.135
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and “computer tortfeasors” like Tesla’s autonomous driving software. Society’s rela- tionship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, computers are stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from a computer tortfeasor in this system are the concepts of independence and con- trol. Autonomous computers, robots, or machines are given tasks to complete, but they determine for themselves the means of completing those tasks.136 In some instances, machine learning can generate un- predictable behavior such that the means are not predictable either by those giving tasks to computers or even by the computer’s original programmers.137 But the difference between ordinary products and
134 David G. Owen, Rethinking the Policies of Strict Products Liability, 33 VAND. L. REV. 681, 710 (1980).
135 Id. at 694.
136 Curtis E.A. Karnow, The Application of Traditional Tort Theory to Embodied Machine
Intelligence, in ROBOT LAW 51, 52 (Ryan Calo et al. eds., 2016).
137 Id. Unlike Karnow, the author does not agree that the relevant distinction between autonomous and nonautonomous machines should be the degree to which they are unpredict- able. See id. at 55. Tort law should pursue functional solutions, and for the purposes of accident reduction, it should not matter whether a self-driving car operates per expert rules or per unpre-
 
24 519 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
autonomous computers should not be based on predictability, only on social and practical outcomes.138 It makes no difference to a person run over by a self-driving car what type of computer was operating the vehicle. Whether a computer acts according to fixed or expert rules created by a programmer or more complex machine-learning algo- rithms such as neural networks that generate new and sometimes un- foreseen behaviors, the physical outcome is the same.139 Leaving aside difficulties with courts attempting to distinguish between different types of computer architecture, ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.
D. Computer-Generated Torts as a Type of Machine Injury
Not all machine injuries would be computer-generated torts. To illustrate, consider two hypothetical accidents:
1) A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop off.
2) A crane operator is manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and one may assume) that the operator is at fault (al- though a creative plaintiff’s attorney might argue the crane was negli- gently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a fac- tual cause of the injury in torts vernacular, the machine did not inter- rupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an exten- sion of the operator, in the same way that the operator could commit a battery by throwing a rock at another person.140 In the second hypo- thetical, allocating fault is once again intuitively obvious. The machine
dictable machine-learning algorithms. See Abbott, supra note 19, at 1109 (arguing in the patent context that it would be impossible or impractical to distinguish between different computer architectures for determining whether a computer qualifies as an inventor and that the distinc- tion is irrelevant to promoting innovation).
138 Cf., e.g., David C. Vladeck, Machines Without Principals: Liability Rules and Artificial Intelligence, 89 WASH. L. REV. 117, 127 (2014) (arguing different liability rules may need to apply to injuries caused by computers that cannot be traced to a “design, manufacturing, or programming defect”).
139 See, e.g., Jack M. Balkin, The Path of Robotics Law, 6 CALIF. L. REV. CIR. 45, 45–46 (2015) (arguing against a focus on formalism and essentialism in the law).
140 See, e.g., R v. Day (1845) 1 Cox 207, 208 (holding that slashing a victim’s clothing with a knife constitutes battery).
 
2018] 520 THE REASONABLE COMPUTER 25
is at fault rather than the operator. The operator acted with reasona- ble care, and the injury was due to (one may assume) a flawed crane. These two scenarios would result in very different liability out- comes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby even if the manufacturer had exercised the utmost care in the design and con-
struction of the crane.
In both scenarios, an operator is using a crane in much the same
way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dy- namic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza Pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.141
Now imagine a third scenario:
3) A computer-operated, unmanned crane drops a steel frame on a passerby after incorrectly identifying the location for drop off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a ma- chine is being used as a tool. In Example 3, a computer has stepped into the shoes of the worker; it has replaced a person, and it is per- forming in essentially the same manner as a person. If the computer were a person, the computer would be liable in negligence and held to the standard of a reasonable person.142
Holding computer tortfeasors to a negligence standard requires rules for distinguishing between computer-generated torts and other
141 See J.J. Coulton, Lifting in Early Greek Architecture, 94 J. HELLENIC STUD. 1, 1, 12, 15–17 (1974).
142 The author has previously argued for a similar rule in the intellectual property context, where he proposed that computers should be recognized as authors and inventors if they inde- pendently perform creative acts. See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 187, 187 (Cassidy R. Sugimoto et al. eds., 2016); Abbott, supra note 19, at 1081. This rule would generate innovation by creating financial incentives for developing creative computers. See Abbott, supra; Abbott, supra note 19, at 1081.
 
26 521 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is a “computer tortfeasor” (Example 3).
Computer-generated torts could be those cases in which an au- tonomous computer occupies the position of a reasonable person in the negligence calculus and where automating promotes safety. It is only beneficial to encourage automation when doing so would reduce accidents. It would be harmful to encourage automation while human drivers outperform self-driving cars (though, it might still be beneficial to encourage automation for a subset of cases, for instance, the class of bad drivers). To shift from strict liability to negligence, manufactur- ers should have the burden to show by a preponderance of the evi- dence that a computer tortfeasor is safer on average than a person.
E. Implementation
Automation may occur on a more or less permanent basis, or it may be situational. For example, an autonomous vehicle may only permit machine control, or it may allow a person to switch between human and machine control. Where automation is all-or-nothing, the relevant inquiry should be whether a specific instance of automation would be expected to result in a net reduction in accidents, rather than to reduce the risk of the specific harm that occurred. For instance, if self-driving cars were better than people at avoiding collisions with other vehicles, but worse at avoiding collisions specifically with white cars, a computer driver might decrease the overall risk of accidents but increase the risk of colliding with white cars. In a case involving a collision with a white car, a negligence standard should still apply. Better that there should be more collisions with white cars so long as there are fewer collisions in total (assuming collisions with white cars do not result in disproportionate harm).
Even where automation is situational, it makes sense to apply a negligence standard. Hypothetically, if a self-driving car is on average ten times safer than a person, but only half as safe as a person in rainy conditions, a person should rely on autonomous driving software most of the time but operate the vehicle conventionally in the rain. If some- one instead uses self-driving software in the rain, the computer should still be evaluated under a negligence standard. It may be difficult for a user to know in advance what circumstances an autonomous computer is likely to encounter as well as when an autonomous computer will outperform a person. In addition, the manufacturer—as the liable

2018] 522 THE REASONABLE COMPUTER 27
party—may not have input into how its computers are used situation- ally. Manufacturers could utilize non-tort mechanisms to prevent un- safe uses, such as by warning users that self-driving cars may not be operated in the rain or by building in technological safeguards to pre- vent self-driving cars from operating in the rain. If self-driving cars prove to be less safe than human drivers in the rain, it is likely manu- facturers would still be liable for accidents in negligence.
Similarly, software used to diagnose disease based on medical imaging may outperform physicians generally, but underperform at detecting certain diseases. Ideally, this might result in human-machine collaborative review of imaging. If a machine were to underperform detecting lung cancer, for example, it should still be evaluated in negli- gence for its failures. The computer will likely be liable if a physician should have detected the lung cancer. In instances where a computer is generally safer than a person but underperforms in a certain area, it is likely to be liable in negligence when underperforming. This retains the ex ante incentive to improve an autonomous computer to reduce accidents and still allows victims to be compensated.
The basic inquiry about automation safety should focus on whether automation reduces, or is expected to reduce, overall acci- dents, not whether it did in fact reduce accidents in a specific instance. If Tesla can prove its self-driving cars are more likely safer overall than human drivers, this should be sufficient to shift to negligence even in a case where a particular substitution of a human driver with a self-driving car results in more accidents. Better that there should be fewer accidents in total even if one normal self-driving car gets in more accidents than the class average.
This new standard might sometimes involve complex problems of proof. A manufacturer would have the initial burden to prove its com- puters are safer than people, which creates an incentive to misrepre- sent a computer’s safety.143 Even when manufacturers are acting in good faith, it may be difficult to determine whether a computer is safer than a person. Research conducted to the highest scientific stan- dards sometimes fails to accurately predict real-world outcomes.144 It may be that Tesla has reason to believe its self-driving cars are signifi- cantly safer than human drivers, but once its cars enter the market-
143 Ryan Abbott, Big Data and Pharmacovigilance: Using Health Information Exchanges to Revolutionize Drug Safety, 99 IOWA L. REV. 225, 232–37 (2013) (discussing differences between premarket and postmarket data for evaluating safety in the pharmaceutical context and the in- centive for manufacturers to misrepresent safety profiles).
 144 Id.

28 523 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
place, they fail to meet expectations. For instance, Tesla’s research might fail to consider the reactions of drivers to self-driving vehicles in states other than California.145 In practice, automation may turn out to be safer or more dangerous than initially predicted. Decisions often must be made based on incomplete information, and waiting for per- fect knowledge risks sacrificing probable benefits at the altar of precaution.146
Adversarial legal proceedings are well suited for resolving such factual issues, and plaintiffs could use those proceedings to challenge manufacturer claims of safety.147 Thus, if Tesla presents evidence that its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs show that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles, that should shift the standard back to strict liability. It is worth noting that postmarket data is not always superior to premarket data; sometimes premarket data may be more predictive of future outcomes, particularly where postmarket data is limited or skewed.148
145 For example, although Google’s self-driving vehicles have been involved in accidents, nearly all accidents involving these vehicles have been the fault of human drivers. Chris Ziegler, A Google Self-Driving Car Caused a Crash for the First Time, VERGE (Feb. 29, 2016, 1:50 PM), http://www.theverge.com/2016/2/29/11134344/google-self-driving-car-crash-report. Pre-2017 monthly reports of accidents involving Google’s self-driving cars were originally available on Google’s website. See Steve Kovach, Google Quietly Stopped Publishing Monthly Accident Re- ports for Its Self Driving Cars, BUS. INSIDER (Jan. 18, 2017, 6:32 PM), http:// www.businessinsider.com/waymo-ends-publishing-self-driving-car-accident-reports-website- 2017-1. However, in 2017 the Google Self-Driving Car Project rebranded as Waymo, and Waymo no longer publishes monthly accident reports. See id.
146 Ryan Abbott & Ian Ayres, Evidence and Extrapolation: Mechanisms for Regulating Off- Label Uses of Drugs and Devices, 64 DUKE. L.J. 377, 380 (2014).
147 See Abbott, supra note 143, at 266 (discussing benefits of adversarial dispute resolu- tion). Alternately, manufacturers could have a duty to evaluate the safety of automation technol- ogies before sale and an ongoing duty to monitor their postmarket performance. This could mean that instead of plaintiffs and defendants engaging in a “battle of the experts” focused on objective safety outcomes, a manufacturer’s good faith belief that its computers were safe would be sufficient to give rise to a negligence standard. Plaintiffs could only rebut the presumption that a manufacturer acted in good faith. Thus, Tesla would remain liable in negligence if it could prove its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs could prove that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles. Unless plaintiffs could prove Tesla knew, or should have known, that its initial predictions were not accurate or prove that Tesla failed to monitor the performance of its cars, Tesla would not be liable. But this would create a greater risk that manufacturers would fail to aggressively monitor, or that manufacturers would fail to monitor appropriately despite their best efforts. Better to base the standard on objective evidence of safety than a manufacturer’s subjective knowledge. Better also to empower plaintiffs’ attorneys to hold manufacturers to account than to put foxes in charge of guarding henhouses.
148 See generally Ryan Abbott, The Sentinel Initiative as a Cultural Commons, in GOV- ERNING MEDICAL KNOWLEDGE COMMONS (Katherine J. Strandburg et al. eds., 2017), https://
 
2018] 524 THE REASONABLE COMPUTER 29
It should not be necessary for a computer tortfeasor to physically replace a human operator for negligence to apply. It should be suffi- cient that a computer is performing a task which a person could rea- sonably do. For example, if a new taxi company goes into business using a fleet of only self-driving vehicles, computers would not have replaced human operators, but they would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software, e.g., the engine, could not be reasona- bly substituted. A person could drive a taxi instead of a computer, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as a computer tortfeasor, but the other parts of the vehicle would not.
Once a manufacturer establishes that a computer tortfeasor is safer than a person, the negligence test should focus on whether the computer’s act was negligent, rather than whether the computer was negligently designed or marketed. Again, the computer is taking the place of a person in the traditional negligence paradigm, and this par- adigm would treat the computer more like a person than a product. It makes no difference to an accident victim what a computer was “thinking”; only how the computer acted.149 Accident victims have a right to demand careful conduct regardless of how well a computer tortfeasor may have been designed.150
Applying the above rules to the crane examples, Example 1 would result in human liability because the human operator acted carelessly and the crane did not interrupt a foreseeable chain of events. It would retain strict manufacturer liability for Example 2 be- cause a person could not reasonably be substituted for a crane. It would permit negligent manufacturer liability for Example 3 (because the computer was automating a task which a person could have per- formed), but only if the computer tortfeasor is on average safer than a human operator.
www.cambridge.org/core/books/governing-medical-knowledge-commons/sentinel-initiative-as-a- knowledge-commons/FE736CE30779C4FFE5BA740F2A0FBBFE/core-reader (discussing diffi- culties with using real-world data to predict safety outcomes in an example using the medication Dabigatran).
149 To appropriate criminal law terminology, we are interested in the actus reus rather than the mens rea. See generally DENNIS J. BAKER, TEXTBOOK OF CRIMINAL LAW 167 (3d ed. 2012) (explaining the concept of actus reus). There is no benefit to punishing computer tortfeasors for wrongful actions, even under civil law.
150 See Oliver Wendell Holmes, Lecture III: Torts—Trespass and Negligence, in 3 THE COL- LECTED WORKS OF JUSTICE HOLMES 154, 157–58 (Sheldon M. Novick ed., 1995).
 
30 525 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
In the context of automated driving, human drivers would be lia- ble for harms they cause due to their own driving decisions, while a manufacturer would be strictly liable for harms caused by defective machines that are not automating human functions (as would be the case for MacPherson’s Buick151), but manufacturers would be liable in negligence rather than strict liability for errors made by autonomous driving software if the software were proven safer on average than a person.
F. Financial Liability
Autonomy exists on a continuum. In practice, the divide between an ordinary product and an autonomous computer may not be clear cut. In the self-driving car context, for example, under one widely adopted framework, vehicles are categorized on a zero to five scale based on who does what, when.152 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction be- tween person and machine. When computers and people share deci- sionmaking, traditional principles of joint and several liability should apply.153 For instance, where a human driver and a computer driver are both at fault, as may be the case where a Tesla system fails to detect a truck while a human driver is watching a movie, both drivers could be liable for either the entire injury or in proportion to their wrongdoing.154
Whether in strict liability or negligence, computers could not be financially liable for their harms. Computers do not have property rights, are owned as chattel, and would not be influenced by the spec- ter of liability in the way a person might be influenced. For the pur- poses of financial liability, the computer’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product or where several parties are involved in the distribu-
151 See supra notes 87–89 and accompanying text.
152 See SAE INT’L, AUTOMATED DRIVING (2014) (on file with the Law Review) (describing
the SAE taxonomy).
153 See generally Richard W. Wright, The Logic and Fairness of Joint and Several Liability, 23 MEM. ST. U. L. REV. 45 (1992) (reviewing and analyzing the public policy debate over joint and several liability).
 154 Id. at 46.

2018] 526 THE REASONABLE COMPUTER 31
tion chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which Gen- eral Motors licenses and incorporates in its vehicles, and the vehicles are then leased by an independent retailer to Lyft. Common law liabil- ity rules could be altered by firms in the supply chain. That would be particularly likely to occur where manufacturers and retailers are large, sophisticated entities. For example, General Motors might in- demnify Apple, Delphi, and Lyft in return for more favorable licens- ing and leasing terms.
Alternately, the computer’s owner could be liable for its harms. That would be somewhat akin to treating computer tortfeasors as em- ployees and making owners liable under theories of vicarious liabil- ity.155 It is particularly easy to imagine owners purchasing insurance for harms caused by autonomous computers in the self-driving car context, where insurance policies may soon come with a rider (or dis- count) for autonomous software. Owner liability might further incen- tivize the production of autonomous computers given that manufacturers would have less liability, but it might reduce adoption because owners would be taking on that liability. These two effects might offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of computer tortfeasors, and because manufacturers are in the best position to im- prove product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self-driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars be- come significantly safer than human drivers, insurance rates will de- crease for self-driving cars and perhaps increase for human drivers.156 This should have a nudging effect on self-driving car adoption as fi- nancially sensitive individuals take auto premiums into account in de- ciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower pre- miums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsi- ble for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
155 See generally Fleming James, Jr., Vicarious Liability, 28 TUL. L. REV. 161 (1954).
156 See supra text accompanying notes 112–19.
 
32 527 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 G. Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the cre- ation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation.157 For exam- ple, incentives could take the form of grants for research and develop- ment,158 loans to build production facilities,159 enhanced intellectual property rights,160 prizes,161 preferential tax treatments,162 or govern- ment guarantees.163
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allow- ance Rebate System (“CARS”), better known as “cash for clunkers.”164 CARS provided consumers trading in old vehicles with
157 See generally Nancy Gallini & Suzanne Scotchmer, Intellectual Property: When Is It the Best Incentive System?, in 2 INNOVATION POLICY AND THE ECONOMY 51 (Adam B. Jaffe et al. eds., 2002).
158 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes De- bate, 92 TEX. L. REV. 303, 321 (2013) (discussing the role of government grants in innovation policy).
Today, direct federal R&D spending (which includes the very small amount cur- rently spent on prizes) is about $130–$140 billion per year—slightly more than half of which is defense-related. Many states also provide direct R&D support: in fiscal year 2009, states spent $3.6 billion on support for R&D at state universities and another $1.3 billion on other grants and facilities for in-state research.
Id. (footnote omitted).
159 See, e.g., Joe Stephens & Carol D. Leonnig, Solyndra: Politics Infused Obama Energy
Programs, WASH. POST (Dec. 25, 2011), https://www.washingtonpost.com/solyndra-politics-in fused-obama-energy-programs/2011/12/14/gIQA4HllHP_story.html?utm_term=.Bb171adb15da (providing background information on the billions in unexpected costs to taxpayers from contro- versial loans defaulted on by green technology programs).
160 See, e.g., Ryan Abbott, Treating the Health Care Crisis: Complementary and Alternative Medicine for PPACA, 14 DEPAUL J. HEALTH CARE L. 35, 62–98 (2011) (noting that pharmaceu- tical manufacturers can receive market exclusivity, extended patent terms, or even sui generis forms of intellectual property protection for preferred technologies).
161 See, e.g., Richard A. Posner, Intellectual Property: The Law and Economics Approach, 19 J. ECON. PERSP. 57, 58–59 (2005).
162 See, e.g., Nick Bloom et al., Do R&D Tax Credits Work? Evidence from a Panel of Countries 1979–1997, 85 J. PUB. ECON. 1, 2 (2002); Bronwyn Hall & John Van Reenen, How Effective Are Fiscal Incentives for R&D? A Review of the Evidence, 29 RES. POL’Y 449, 449 (2000).
163 See, e.g., Gunhild Berg & Michael Fuchs, Bank Financing of SMEs in Five Sub-Saharan African Countries: The Role of Competition, Innovation, and the Government (World Bank, Pol- icy Research Working Paper No. 6563, 2013).
164 TED GAYER & EMILY PARKER, CASH FOR CLUNKERS: AN EVALUATION OF THE CAR ALLOWANCE REBATE SYSTEM 1 (2013), https://www.brookings.edu/wp-content/uploads/2016/06/ cash_for_clunkers_evaluation_paper_gayer.pdf.
 
2018] 528 THE REASONABLE COMPUTER 33
vouchers of between $3500 and $4500 to purchase new cars.165 It was a nearly $3 billion U.S. federal program designed as a short-term eco- nomic stimulus and to benefit U.S. auto manufacturers.166 It was also intended to promote safer, cleaner, more fuel-efficient vehicles.167 Ul- timately, while critics dispute the effectiveness of the program at stim- ulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel efficiency and safety, and it was popular with consumers.168 In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alter- natives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that place caps on damages, limit contingency fees, eliminate joint and several liability, mandate periodic payments, or reduce the statute of limitations.169
Finally, the government could promote safety by means of regula- tion. This could involve requirements for industries to achieve mini- mum safety targets or direct requirements to adopt certain technologies.170 At the point where self-driving cars become ten or a
165 Id.
166 See id. at 1–2; $2 Billion More for Clunker Car Trade-Ins Passes Senate, N.Y. TIMES: CAUCUS (Aug. 6, 2009, 9:05 PM), https://thecaucus.blogs.nytimes.com/2009/08/06/2-billion-more- for-clunker-car-trade-ins-passes-senate/.
167 See GAYER & PARKER, supra note 164, at 1–2.
168 The Department of Transportation reported the program succeeded at boosting eco-
nomic growth and creating jobs. Press Release, Nat’l Highway Traffic Safety Admin., Secretary LaHood Touts Success of Cash for Clunkers; Responds to Reports by DOT Inspector General, GAO (Apr. 29, 2010), https://www.nhtsa.gov/press-releases/secretary-lahood-touts-success-cash- clunkers-responds-reports-dot-inspector-general. Others were less bullish. One study found that the total costs of the program outweighed the benefits by $1.4 billion. See Burton A. Abrams & George R. Parsons, Is CARS a Clunker?, ECONOMISTS’ VOICE, Aug. 2009, at 4. Another study argued that the program increased short-term spending, but decreased overall spending on new cars. Mark Hoekstra et al., Cash for Corollas: When Stimulus Reduces Spending 23 (Nat’l Bureau of Econ. Research, NBER Working Paper Series No. 20349, 2014), http://www.nber.org/papers/ w20349.pdf. With regard to fuel efficiency, one study found that the program improved the aver- age fuel economy of all vehicles purchased by 0.6 mpg in July 2009, and by 0.7 mpg in August 2009. MICHAEL SIVAK & BRANDON SCHOETTLE, U. MICH. TRANSP. RESEARCH INST., THE EF- FECT OF THE “CASH FOR CLUNKERS” PROGRAM ON THE OVERALL FUEL ECONOMY OF PUR- CHASED NEW VEHICLES 4 (2009), http://deepblue.lib.umich.edu/bitstream/2027.42/64025/1/ 102323.pdf.
169 These are some of the reforms created by the Medical Injury Compensation Reform Act of 1975 (“MICRA”) enacted by the California legislature to lower medical malpractice lia- bility insurance premiums. Cal. Civ. Code §§ 3333–3333.2 (West 2016).
170 See generally HEALTH & SAFETY EXEC., A GUIDE TO HEALTH AND SAFETY REGULA- TION IN GREAT BRITAIN 11 (2013), http://www.hse.gov.uk/pubns/hse49.pdf (outlining the occu-
 
34 529 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
hundred times safer than human drivers, nonautonomous driving could be prohibited.171 Regulatory solutions may be most appropriate where the benefits of automation are overwhelming and where it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer- and business-friendly so- lution. While consumers would have more difficulty seeking to re- cover for accidents, they would also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. For busi- nesses, it would lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not re- quire government funding, additional regulatory burdens on industry, or new administrative responsibilities. Additionally, it is an incremen- tal solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than more radical solutions.172 For all the above reasons, shifting to negli- gence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer cred- its, combined with direct regulations in certain instances.
Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable be- cause it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy.173 Indeed, automation
pational health and safety system in Great Britain and the various types of safety standards imposed on businesses).
171 See Stuart Dredge, Elon Musk: Self-Driving Cars Could Lead to Ban on Human Driv- ers, GUARDIAN (Mar. 18, 2015, 3:22 AM), https://www.theguardian.com/technology/2015/mar/18/ elon-musk-self-driving-cars-ban-human-drivers.
172 Indeed, some critics argued that CARS primarily subsidized Japanese auto manufactur- ers, while a similar Japanese stimulus program excluded American auto manufacturers. John Crawley, Japanese, Koreans Gain Most from Cash for Clunkers, REUTERS (Aug. 26, 2009, 5:34 PM), http://www.reuters.com/article/retire-us-usa-clunkers-sales-idUSTRE57P5C220090826; Douglas Stanglin, U.S. Cars Excluded from Japan’s Cash-for-Clunkers Program, USA TODAY (Dec. 11, 2009, 2:09 PM), http://content.usatoday.com/communities/ondeadline/post/2009/12/us- cars-excluded-from-japans-cash-for-clunkers-program-/1#.WDwOQXfc-t8.
173 See, e.g., Scherer, supra note 17, at 389–90 (discussing the reactionary nature of court proceedings); see also Bibb v. Navajo Freight Lines, Inc., 359 U.S. 520, 524 (1959) (“Policy deci- sions are for the . . . legislature . . . .”).
 
2018] 530 THE REASONABLE COMPUTER 35
to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could independently adopt these rules. Lawmakers would then have the option of modifying the common law.
III. THE REASONABLE ROBOT
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
—Oliver Wendell Holmes, Jr.174
A. When Negligence Is Strict
Negligence may function almost like strict liability for people with below average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.”175 This issue was at the heart of Vaughan v. Menlove176 in 1837, which con- cerned a defendant who lacked normal intelligence.177 The defense ar- gued that it would thus be unfair to hold him to the standard of an ordinary person and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, hold- ing that ordinary prudence should apply in every case of negligence.178 As Oliver Wendell Holmes, Jr., articulated in 1881, “The law consid- ers . . . what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”179 That re- mains the case today; a modern defendant cannot generally escape
174 O.W. HOLMES, JR., THE COMMON LAW 108 (1881).
175 Id. Holmes did distinguish between a lack of “intelligence and prudence” and “distinct
defect[s]” which he believed did not generally lead to strict liability. Id. at 108–10.
176 (1837) 132 Eng. Rep. 490, 492; 3 Bing. (N.C.) 468, 471.
177 Id. at 492.
178 Id. at 490, 492.
Instead, therefore, of saying that the liability for negligence should be co-extensive with the judgment of each individual, which would be as variable as the length of the foot of each individual, we ought rather to adhere to the rule which requires in all cases a regard to caution such as a man of ordinary prudence would observe. That was in substance the criterion presented to the jury in this case, and therefore the present rule must be discharged.
Id. at 493.
179 HOLMES, supra note 174, at 108.
 
36 531 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
liability for causing a motor vehicle accident because she has slow re- flexes, poor vision, or anxiety while driving.180
There are benefits to such a rule. Logistically, as Justice Tindal noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state.181 Better for administrative purposes to work with an external, objective stan- dard than to prove individual capacities and state of mind. Substan- tively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal con- duct of others.182 As Holmes articulated, damages caused by individu- als with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare, but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else perhaps “should refrain from operating an automobile” at all.183
B. The New Hasty and Awkward
Collectively, people are not the best drivers, even when they re- frain from drinking behind the wheel,184 falling asleep on the high- way,185 or colliding into police cars while playing Poke ́mon Go.186 But compared to computers? It will not be long until computers are safer than the average person and then safer than any human driver. Princi- ples of harm avoidance suggest that once it becomes practical to auto- mate, and once doing so is safer, a computer should become the new “reasonable person” or standard of care.
180 See, e.g., Roberts v. Ring, 173 N.W. 437, 437–38 (Minn. 1919).
181 Vaughan, 132 Eng. Rep. at 493.
182 See Holmes, supra note 150, at 154–55.
183 Roberts, 173 N.W. at 438. In this case, a seventy-seven-year-old defendant with defec-
tive sight and hearing was held liable for running over a seven-year-old boy when it was estab- lished that a reasonable driver could have stopped the car. Id.
184 See J. Michael Kennedy, Allowed in 26 States: Drinking and Driving: A Legal Mix, L.A. TIMES (Jan. 26, 1985), http://articles.latimes.com/1985-01-26/news/mn-13688_1_container-law (noting that until recently, it was even legal in many states to “sip[] on a Scotch and soda while cruising down the interstate”).
185 See David Boroff, Two Women Dead as Greyhound Bus Driver Falls Asleep at Wheel During California Crash; Driver was ‘Fatigued,’ Police Say, N.Y. DAILY NEWS (Jan. 19, 2016, 9:01 PM), http://www.nydailynews.com/news/national/greyhound-bus-crash-kills-2-injures-18-ar- ticle-1.2501658.
186 See Sarah Begley, Driver Hits Cop Car While Playing Poke ́mon Go. The Whole Thing Was Caught on Video, TIME (July 20, 2016), http://time.com/4414998/pokemon-go-hits-cop-body- cam/ (discussing a driver playing Poke ́mon Go who collided with a police car and had the inci- dent captured on video, and quoting the driver as saying, “That’s what I get for playing this dumb – game”).
 
2018] 532 THE REASONABLE COMPUTER 37
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defen- dant would be judged against what a computer would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an automated car would have prevented the injury. In fact, it may be that the automated vehicle is only able to prevent such an accident because it has superhuman abili- ties. It may have software capable of ultrafast decisionmaking, monitors that surpass human senses, and external cameras that ex- pand peripheral view beyond that of a person.187
With the reasonable person test, jurors are asked to put them- selves in the shoes of a reasonable person and decide what that person would have done.188 It may be a challenge for a juror to follow that reasoning in the case of a reasonable computer (or reasonable robot or machine). The reasonable computer, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasona- ble” in the context of a computer is an anthropomorphism to assist people conceptually. In fact, computers largely function according to fixed rules which—when all goes well—result in foreseeable behav- ior.189 Even those computers which can generate unpredictable behav- ior are still likely to be more predictable than people, particularly where such machines have been found to improve safety.190 It should be more or less possible to determine what a computer would have done in a particular situation.
To take a simple case, imagine an individual driving on dry pave- ment at forty miles per hour colliding with a child running into the road 150 feet ahead of the driver’s vehicle.191 To determine whether the driver is liable under the reasonable computer standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable computer would not have collided with
187 See supra notes 116–19 and accompanying text.
188 See supra notes 65, 71 and accompanying text.
189 THOMAS A. PETERS, COMPUTERIZED MONITORING AND ONLINE PRIVACY 97 (1999).
Malfunctioning computers would not be “reasonable” computers.
190 See id.
191 See Why Your Reaction Time Matters at Speed, NAT’L HIGHWAY TRAFFIC SAFETY AD-
MIN. (Aug. 2015), www.nhtsa.gov/nhtsa/Safety1nNum3ers/august2015/S1N_Aug15_Speeding_ 1.html.
 
38 533 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of self-driving software would be known, and the breaking distance could be standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software. Even in more com- plex cases, it should be easier to predict how a computer would have behaved than a person because computers are more predictable. Thus, it is possible to have a more objective test for the reasonable com- puter than for the reasonable person.
A defendant might argue that it is unfair for his best efforts to result in liability. A computer standard of care essentially makes peo- ple strictly liable for their accidental harms. That is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below- average driver due to computers. It may appear unfair to impose lia- bility on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or a computer.
Tort liability would not prohibit people from driving even at the point where computers become substantially safer than people. If that were a desired outcome it could be accomplished through command- and-control legislation.192 Instead, a computer standard of care would mean that people drive at their own risk. If a driver causes an acci- dent, he or she will be liable for the resultant damages. A tort-based incentive may be superior to an inflexible statutory mandate because there may be benefits to human driving unrelated to accidents, for instance, promoting freedom and autonomy.193 Individuals who partic- ularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, a computer standard of care is likely to have a significant impact on behavior. Making individ- uals and businesses essentially strictly liable for their harms will strongly discourage certain undertakings. In the self-driving car con- text, it would likely result in far fewer human drivers as insurance
192 See Orly Lobel, The Renew Deal: The Fall of Regulation and the Rise of Governance in Contemporary Legal Thought, 89 MINN. L. REV. 342, 371–404 (2004) (discussing the trend from regulations to incentive-based regimes).
193 See generally Ryan & Deci, supra note 20, at 6–7 (arguing that people have three basic psychological needs: (1) connectedness, (2) autonomy, and (3) feeling competent).
 
2018] 534 THE REASONABLE COMPUTER 39
rates for traditional vehicles become prohibitively expensive relative to rates for self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. Automatons may be prohibitively expensive or only available in limited quantities. That is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. There- fore, to introduce a computer standard of care, a plaintiff should have to show by a preponderance of the evidence that a person was per- forming a task that could be performed by a computer and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of a computer operator where 1) no such operator existed at the time of the accident, 2) no computer operator was available to the defendant, 3) a computer operator was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory require- ments for a human operator). If Tesla could manufacturer a com- pletely safe autonomous vehicle but at a cost of $1 million dollars, it would not be reasonable to require consumers to automate.
C. Reasonable People Use Autonomous Computers
As an alternative to the reasonable computer standard, the rea- sonable person could be a person using an autonomous computer. For example, once self-driving cars become safer than traditional vehicles, a jury might find that it is unreasonable to drive yourself rather than to use a self-driving car. Applying the “reasonable person using an autonomous computer” standard to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on her driving in the first place. A reasonable person would not have driven.
Under either the reasonable person or reasonable computer stan- dard, a human driver would be compared with a self-driving car, but in different ways. With the reasonable computer standard, courts would evaluate the human driver’s proximally harmful act, whereas with the reasonable person standard, courts would evaluate the human driver’s a priori decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable per- son standard would be more in line with the existing negligence re- gime, and it would be a less radical way to accomplish the goal of incentivizing automation to improve safety.

40 535 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
While keeping the reasonable person standard would be concep- tually easier, in practice it would be less desirable. The goal is to com- pare the harmful act of the person and computer, not to target the initial decision to automate. It is problematic to base liability on the decision to automate because it either must focus on the question of whether automation is generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform a machine. A situational focus must still compare the harmful act of a person versus a computer.
It is likely that as autonomous computers are introduced they will be safer at automating certain activities than others. For instance, au- tomated computers working to diagnose disease may be superior to physicians at detecting certain conditions, but not others. Self-driving cars may be safer than human drivers on average, but not safer than professional or above-average drivers. Autonomous vehicles may also be safer under most conditions, but might be relatively poor at, for example, driving off road. So, while automation may generally im- prove safety, optimal accident reduction may require a mix of com- puter and human activity.
Suppose a self-driving car is ten times safer than a human driver generally, but only half as safe as a human driver in icy conditions. Now suppose a human driver encounters a patch of black ice and causes an accident under circumstances in which she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable computer, she would es- cape liability if the computer would have been unable to avoid the accident (which is likely if the computer is half as safe in icy condi- tions). If the reasonable person using an autonomous computer test focuses on whether an autonomous computer is generally safer, how- ever, she would be liable. That test would conclude that it would have been unreasonable not to use a self-driving car because self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an autonomous com- puter evaluation could be situational. For instance, it could be reason- able not to use an autonomous computer, but only in icy conditions. However, this is just a more convoluted version of the reasonable computer test because it requires evaluating whether a computer would be safer than a person in a particular instance. That essentially asks how the computer would have acted in a situation—which is the

2018] 536 THE REASONABLE COMPUTER 41
reasonable computer standard.194 It would then require asking, based on that knowledge, which might be impractical for a person to have, whether an earlier decision to automate was reasonable. On top of that, it presupposes the ability to activate and deactivate automation as needed. In the black ice hypothetical, it could require the driver to know in advance of activating self-driving software whether there were icy conditions and how the computer would perform in icy con- ditions. It might require the driver to activate or deactivate automa- tion only during icy conditions or to understand whether the risk of using the computer in icy conditions outweighed the benefits of using the computer for other parts of the trip.
D. The Reasonable Computer Standard for Computer Tortfeasors
This Article proposes holding computer tortfeasors to a negli- gence standard and comparing their acts to the acts of a reasonable person after technology has advanced to the point that computers have been proven safer than people.195 It also proposes replacing the reasonable person standard with the reasonable computer standard, again, once this point has been reached.196 This means that computer tortfeasors would be held to the reasonable computer standard.
There will be instances in which it still makes sense to apply the reasonable person standard to computer tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of a computer, for instance, where auto- mation is prohibitively expensive or where computer operators are not widely available. We would not want to hold a computer tortfeasor to a higher standard than a human defendant. In some in- dustries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become customary or to meet the criteria proposed earlier for adopting the reasonable com- puter standard.
Eventually, once a reasonable computer becomes the standard of care, it would also be the standard for computer tortfeasors. For in- stance, if a self-driving Audi collided with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Volvo cars. There are a variety of ways to deter- mine the reasonable computer standard, for example, considering the industry customary, average, or safest technology. Under any stan-
194 See supra text accompanying notes 184–93.
195 See supra text accompanying notes 184–93.
196 See supra text accompanying notes 184–93.
 
42 537 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
dard, this is a different test than the current strict liability standard, in which the inquiry focuses on whether a product was defectively de- signed or its properties falsely represented.
As computers improve, the reasonable computer standard would grow stricter. That is alright, because once the reasonable computer is exponentially safer than a person, it is likely that computer tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable computer standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until computers are exponentially safer than people.
E. The Automation Problem
The impact of automation goes far beyond accident reduction. Just focusing on autonomous vehicles, the widespread adoption of this technology could have revolutionary benefits. It will allow people to be more productive and mobile, and it will reduce emissions and con- gestion.197 One autonomous vehicle could replace up to twelve normal cars.198 Given that the average automobile spends about ninety-five percent of its time sitting in place, self-driving cars may also eliminate the need for most parking.199 Getting rid of parking just in the United States would free up space the size of Connecticut and could allow redesigned, pedestrian-friendly urban areas.200 Automation will in- crease freedom for the disabled, blind, and unlicensed. It might elimi- nate traffic lights and the need for private car ownership.201 The net result of self-driving cars could be substantial environmental, eco- nomic, and social benefits.202
Driverless technologies may also result in the displacement of human workers, increased unemployment, greater wealth disparities, and a reduction of the tax base. Automation threatens the jobs of truck, bus, and taxi drivers who collectively make up about three per- cent of the working population.203 In other industries, automation has
197 DEP’T FOR TRANSPORT, THE PATHWAY TO DRIVERLESS CARS: SUMMARY REPORT AND ACTION PLAN 6 (2015).
198 Clive Thompson, No Parking Here, MOTHER JONES (Jan.–Feb. 2016), http:// www.motherjones.com/environment/2016/01/future-parking-self-driving-cars.
199 Id.
200 Id.
201 DEP’T FOR TRANSPORT, supra note 197, at 6.
202 Id.
203 RICHARD HENDERSON, INDUSTRY EMPLOYMENT AND OUTPUT PROJECTIONS TO 2024,
 
2018] 538 THE REASONABLE COMPUTER 43
resulted in reduced workforces.204 For instance, employment at com- puter and electronic companies decreased forty-five percent from 2001 to 2016.205 Employment at semiconductor makers decreased by half during the same period.206
These are all important issues to consider in formulating automa- tion policies, but tort law may not be the best mechanism to address these broader concerns.207 Ultimately, tort liability alone will not de- termine whether automation occurs. Consumer demand and the eco- nomics of automation will bring about increasing automation in the absence of laws prohibiting it.208 Tesla, for example, is planning to make all its cars self-driving, and Tesla is far from alone in automating vehicles.209 Billions of dollars have been invested in self-driving tech- nologies by at least forty-four corporations including Apple, Google, and General Motors.210
at 2 (2015); see AUSTL. BUREAU OF STATISTICS, 2011 CENSUS COMMUNITY PROFILES, http:// www.censusdata.abs.gov.au/census_services/getproduct/census/2011/communityprofile/0?open document&navpos=220 (last updated Jan. 12, 2017) (select “Working Population Profile”).
204 For example, WhatsApp had fifty-five employees when Facebook acquired it for $21.8 billion in 2014. Jon Swartz, Tech’s Gilded Glory Didn’t Mean Much to Trump’s Supporters, USA TODAY (Nov. 14, 2016), http://www.usatoday.com/story/tech/2016/11/14/techs-gilded-glory-didnt- mean-much-trumps-supporters/93598484/. Amazon, Tesla, and other companies have developed production lines that minimize the use of people. Id.
205 Id. 206 Id.
207 See, e.g., Priest, supra note 11, at 5–6.
208 See Brad Templeton, Robotaxi Economics, BRAD IDEAS (Sept. 8, 2016, 2:07 PM), http://
ideas.4brad.com/robotaxi-economics [https://perma.cc/T4JU-D866]; see also Who’s Self-Driving Your Car?, ECONOMIST (Sept. 22, 2016), http://www.economist.com/news/business/21707600-bat tle-driverless-cars-revs-up-whos-self-driving-your-car (noting a tight race between major tech- nology companies competing to make autonomous driving software due to financial expectations).
209 Tesla to Make All Its New Cars Self-Driving, BBC NEWS (Oct. 20, 2016), http:// www.bbc.co.uk/news/technology-37711489. Not all autonomous vehicles are created equal. A va- riety of technologies are in development to automate cars to a greater or lesser degree—ranging from driverless cars to self-parking vehicles. See generally SCIENCEWISE EXPERT RES. CTR., AU- TOMATED VEHICLES: WHAT THE PUBLIC THINKS (2014), http://www.sciencewise-erc.org.uk/cms/ assets/Uploads/Automated-Vehicles-Update-Jan-2015.pdf.
210 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehicles-corporations-list/ [https:// perma.cc/JM38-TR7D]; see Investment into Auto Tech on Pace to Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/blog/auto-tech-funding-h1-2016/ [https:// perma.cc/ZTE9-MH7E].
 
44 539 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 CONCLUSION
In the coming decades, as people and machines compete in an expanding array of activities, it is vital that appropriate legal and pol- icy frameworks be put in place to guide the development of technol- ogy and to ensure its widespread benefits.211 It is particularly important that tort liability be structured to optimize accident deterrence.
Technological advances present new challenges to existing frameworks. At some point in the future, there are likely to be few or no activities for which computers cannot outperform people.212 Self- driving cars may eventually be a thousand times safer than the best human driver.213 At some point, computers will cause so little harm that the economics of negligence versus strict liability will be irrele- vant. Autonomous computers will have become so ubiquitous that the constantly improving reasonable computer should be the benchmark for most or all areas of accident law. In fact, autonomous computers are likely to become so safe that regulatory mandates for automation will be desirable.
In the meantime, creating incentives for developing and adopting safer technologies could prevent countless accidents. It has become acceptable for more than a million people a year to die in traffic acci- dents worldwide, but only because there has not been a reasonable alternative until now.214 We could soon be living in a world where no one dies from unintended injury, or from medical error for that mat- ter. Once the third and fourth leading causes of death are eliminated, that would just leave us to deal with the leading two causes of death:
211 See, e.g., Press Release, European Parliament, Robots: Legal Affairs Committee Calls for EU-Wide Rules (Jan. 12, 2017), http://www.europarl.europa.eu/sides/getDoc.do?type=IM- PRESS&reference=20170110IPR57613&language=EN&format=XML (“EU rules for the fast- evolving field of robotics, to settle issues such as compliance with ethical standards and liability for accidents involving driverless cars, should be put forward by the EU Commission, urged the Legal Affairs Committee . . . .”).
212 See generally RAY KURZWEIL, THE SINGULARITY IS NEAR 7 (2005) (predicting that machines will be able to automate all human work in “a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed”).
213 See Dredge, supra note 171.
214 See Press Release, United Nations Secretary-General, Traffic Accidents Kill 1.3 Million
People Each Year, but with Commitment Roads Can Be Made Safer for All, Secretary General Says in Video Message (May 6, 2013), https://www.un.org/press/en/2013/sgsm15005.doc.htm [https://perma.cc/B2QQ-UN59].
 
2018] 540 THE REASONABLE COMPUTER 45 cardiovascular disease and cancer. Automation may eliminate those as
well.215
 215 See Abbott, supra note 19, at 1118 (hypothesizing about how artificial intelligence could cure cancer in an article about creative computers that are already independently generating patentable subject matter).

541
  Punishing Artificial Intelligence: Legal Fiction or Science Fiction
Ryan Abbott†* and Alex Sarch**
Whether causing flash crashes in financial markets, purchasing illegal drugs, or running over pedestrians, AI is increasingly engaging in activity that would be criminal for a natural person, or even an artificial person like a corporation. We argue that criminal law falls short in cases where an AI causes certain types of harm and there are no practically or legally identifiable upstream criminal actors. This Article explores potential solutions to this problem, focusing on holding AI directly criminally liable where it is acting autonomously and irreducibly. Conventional wisdom holds that punishing AI is incongruous with basic criminal law principles such as the capacity for culpability and the requirement of a guilty mind.
Drawing on analogies to corporate and strict criminal liability, as well as familiar imputation principles, we show how a coherent theoretical case can be constructed for AI punishment. AI punishment could result in general deterrence and expressive benefits, and it need not run afoul of negative limitations such as punishing in excess of culpability. Ultimately, however, punishing AI is not justified, because it might entail significant costs and it would certainly require radical legal changes. Modest changes to existing criminal laws that target persons, together with potentially expanded civil liability, are a better solution to AI crime.
TABLE OF CONTENTS
INTRODUCTION ................................................................................... 325
I.
†
*
** Alex Sarch, Reader (Associate Professor) in Legal Philosophy, University of Surrey School of Law. Thanks to Antony Duff, Sandra Marshall, Mark D’Souza, and Steve Bero for their insightful comments.
323
ARTIFICIAL INTELLIGENCE AND PUNISHMENT........................... 329 A. Introduction to Artificial Intelligence ................................. 329
Copyright © 2019 Ryan Abbott and Alex Sarch.
 Ryan Abbott, Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at University of California, Los Angeles.

542
  324
University of California, Davis [Vol. 53:323
A Framework for Understanding AI Crime ........................ 332 A Mainstream Theory of Punishment ................................. 337 1. Affirmative Reasons to Punish ................................... 338 2. Negative (Retributive) Limitations ............................ 341 3. Alternatives to Punishment........................................ 343 4. Putting the Pieces Together ....................................... 344
B. C.
II. THE AFFIRMATIVE CASE ........................................................... 344
A. Consequentialist Benefits ................................................... 344
B. Expressive Considerations ................................................. 346
III. RETRIBUTIVE AND CONCEPTUAL LIMITATIONS.......................... 349
A. The Eligibility Challenge ................................................... 349
1. Answer 1: Respondeat Superior ................................. 350
2. Answer 2: Strict Liability............................................ 352
3. Answer3:AFrameworkforDirectMensRea
Analysis for AI ............................................................ 354
B. Further Retributivist Challenges: Reducibility and
Spillover ............................................................................ 360 1. Reducibility................................................................361
2. Spillover ...................................................................... 362
C. Not Really Punishment?..................................................... 364 IV. FEASIBLE ALTERNATIVES ........................................................... 368
A. First Alternative: The Status Quo....................................... 369
1. WhattheAIcriminalgapisnot:reducibleharmful conduct by AI ............................................................. 369
2. WhattheAIcriminalgapis:irreduciblecriminal conduct by AI ............................................................. 373
B. The Costs of Punishing AI.................................................. 374
C. Second Alternative: Minimally Extending Criminal Law.... 378
D. Third Alternative: Moderate Changes to Civil Liability...... 381
E. Concluding Thoughts ......................................................... 383

543
  2019] Punishing Artificial Intelligence 325 INTRODUCTION
In 2015, an artist going by the moniker “Random Darknet Shopper” (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit.1 This was part of a performance project where RDS was given $100 in the cryptocurrency bitcoin each week to make a purchase from an online marketplace. The items were then shipped to a Swiss art gallery and put on exhibit. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.2
What makes this story interesting for our purposes is that RDS was an artificial intelligence (“AI”), and hardly the first to have a run-in with law enforcement.3 If RDS had been a natural person located in the United States, it could be criminally prosecuted under U.S. law.4 For that matter, entities involved in this activity other than RDS could also be criminally prosecuted, such as those supplying the bitcoin and hosting the exhibition.5 Luckily for RDS and crew, the Swiss authorities were art fans.6
Cases like this will pose new challenges, including for criminal law doctrine.7 The RDS case may be relatively straightforward, but programs exist that are autonomous, decentralized, and “unstoppable.”8 What if
1 Arjun Kharpal, Robot with $100 Bitcoin Buys Drugs, Gets Arrested, CNBC (Apr. 22, 2015, 5:09 AM), https://www.cnbc.com/2015/04/21/robot-with-100-bitcoin-buys- drugs-gets-arrested.html.
2 See id.
3 See Matt Novak, Was This the First Robot Ever Arrested?, GIZMODO (Feb. 18, 2014,
12:00 PM), https://paleofuture.gizmodo.com/was-this-the-first-robot-ever-arrested- 1524686968 (describing police confiscation in 1982 of a robot: “The police considered citing [its owner] for failing to obtain a permit for advertising . . . but no charges were filed and the robot was ultimately returned.”). Robot encounters with law enforcement are becoming more common. See, e.g., Peter Dockrill, A Robot Was Just ‘Arrested’ by Russian Police, SCI. ALERT (Sept. 20, 2016), https://www.sciencealert.com/a-robot-was- just-arrested-by-russian-police.
4 See 21 U.S.C. § 841(a)(1) (2019) (criminalizing distribution and possession with intent to distribute a controlled substance).
5 See 18 U.S.C. § 2(a) (2019) (criminalizing aiding and abetting offenses).
6 Random Darknet Shopper was eventually returned to its creators together with
all of the purchases except the Ecstasy. See Kharpal, supra note 1 (noting the prosecutor’s comment that “the possession of Ecstasy was indeed a reasonable means for the purpose of sparking public debate about questions related to the exhibition”). Apparently, the Hungarian passport was also returned. See id.
7 See Christopher Markou, We Could Soon Face a Robot Crimewave. . .The Law Needs to Be Ready, CONVERSATION (Apr. 11, 2017, 9:36 AM), https://theconversation. com/we-could-soon-face-a-robot-crimewave-the-law-needs-to-be-ready-75276.
8 See infra Part I.A (discussing The Decentralized Autonomous Organization (“The DAO”)).
 
544
  326 University of California, Davis [Vol. 53:323
RDS had been open source software that individuals from around the world independently helped program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike.9 Perhaps the best-known defender of punishing AI is Gabriel Hallevy. He contends that “[w]hen an AI entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”10 In his view, “[i]f all of its specific requirements are met, criminal liability may be imposed upon any entity — human, corporate or AI entity.”11 Drawing on the analogy to corporations,12 Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations and on AI entities.”13 “Modern times,” he contends, “warrant modern legal
9 See, e.g., Gabriel Hallevy, The Punishibility of Artificial Intelligence Technology, in LIABILITY FOR CRIMES INVOLVING ARTIFICIAL INTELLIGENCE SYSTEMS 185-229 (2014); J.K.C. Kingston, Artificial Intelligence and Legal Liability, in RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XXXIII: INCORPORATING APPLICATIONS AND INNOVATIONS IN INTELLIGENT SYSTEMS XXIV 269 (Max Bramer & Miltos Petridis eds., 2016), https://arxiv.org/pdf/1802.07782.pdf; Christina Mulligan, Revenge Against Robots, 69 S.C. L. REV. 579, 580 (2018); Jeffrey Wale & David Yuratich, Robot Law: What Happens If Intelligent Machines Commit Crimes?, CONVERSATION (July 1, 2015, 8:06 AM), http://theconversation.com/robot-law-what-happens-if-intelligent-machines-commit- crimes-44058; infra Part I.A (discussing The DAO).
10 Gabriel Hallevy, The Criminal Liability of Artificial Intelligence Entities — From Science Fiction to Legal Social Control, 4 AKRON INTELL. PROP. J. 171, 191 (2010).
11 Id. at 199.
12 See id. at 200 (asking why AI entities should be treated “different from corporations”).
13 Id. at 200-01.
 
545
  2019] Punishing Artificial Intelligence 327
measures.”14 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.15 Her arguments likewise draw heavily on the analogy to corporate criminal liability.16
In contrast to AI punishment expansionists like Hallevy and Hu, skeptics might be inclined to write off the idea of punishing AI from the start as conceptual confusion — akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental concepts of the criminal law like culpability — a “guilty mind” that is characterized by insufficient regard for legally protected values17 — would be misplaced. One might think the whole idea of punishing AI can be easily dispensed with as inconsistent with basic criminal law principles.
The idea of punishing AI is due for fresh consideration. This Article takes a measured look at the proposal, informed by theory and practice alike. We argue punishment of AI cannot be categorically ruled out. Harm caused by a sophisticated AI may be more than a mere accident where no wrongdoing is implicated. Some AI-generated harms may stem from difficult-to-reduce behaviors of an autonomous system,
14 Id. at 199.
15 Ying Hu, Robot Criminals, 52 MICH. J.L. REFORM 487, 531 (2019); see also id. at
490 (“[A]n argument can be made for robot criminal liability, provided that the robot satisfies three threshold conditions . . . . [T]he robot must be (1) equipped with algorithms that can make nontrivial morally relevant decisions; (2) capable of communicating its moral decisions to humans; and (3) permitted to act on its environment without immediate human supervision.”).
16 See Ying Hu, Robot Criminal Liability Revisited, in DANGEROUS IDEAS IN LAW 494, 497-98 (Jin Soo Yoon, Sang Hoon Han & Seong Jo Ahn eds., 2018) (arguing that corporations are “structurally similar” to “robots that are equipped with machine learning algorithms to determine the appropriate course of actions in specific circumstances,” and concluding that “if there is reason to treat corporations as moral agents, there is reason to treat sophisticated robots as moral agents as well”); Hu, supra note 15, at 520-21 (“One may argue that a smart robot can act intentionally in the same way that a corporation can. A robot’s moral algorithms are functionally similar to a corporation’s internal decision structure . . . . By analogy, . . . any act made pursuant to a smart robot’s moral algorithms is an act done for the robot’s own reasons and would therefore amount to an intentional action.”). Unlike Hu, we do not argue that AIs have genuine moral responsibility. We focus on the legal notion of culpability, which involves institutional design constraints that allow it to diverge from moral responsibility or blameworthiness.
17 Alexander Sarch, Who Cares What You Think? Criminal Culpability and the Irrelevance of Unmanifested Mental States, 36 L. & PHIL. 707, 709 (2017) [hereinafter Who Cares].
 
546
  328 University of California, Davis [Vol. 53:323
whose actions resemble those of other subjects of the criminal law, especially corporations. These harms may be irreducible where, for a variety of reasons, they are not directly attributable to the activity of a particular person or persons.18 Corporations similarly can directly face criminal charges when their defective procedures generate condemnable harms19 — particularly in scenarios where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals.20
It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented into criminal law doctrine, and to consider the alternatives. Our primary focus is not what form AI punishment would take, which could directly target AIs through censure, deactivation, or reprogramming, or could involve negative outcomes directed at natural persons or companies involved in the use or creation of AI.21 Rather, our focus is the prior question of whether the doctrinal and theoretical commitments of the criminal law can be reconciled with criminal liability for AI.
Our inquiry focuses on the strongest case for punishing AI: scenarios where crimes are functionally committed by machines and there is no identifiable person who has acted with criminal culpability. We call these Hard AI Crimes. This can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important negative, retributivist-style limitations that apply to persons need not prohibit AI punishment. On the other hand, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep.22 In
18 See infra Part II.B.
19 See MODEL PENAL CODE § 2.07 (AM. LAW INST. 1962) (outlining conditions under
which a corporation could be convicted of an offense).
20 See William S. Laufer, Corporate Bodies and Guilty Minds, 43 EMORY L.J. 647, 664- 68 (1994) (outlining prevalent models of “genuine corporate culpability” including proactive fault, reactive fault, corporate ethos, and corporate policy); infra notes 166– 168 and accompanying text (discussing ways to defend the irreducibility of corporate culpability).
21 See Hu, supra note 15, at 529-30 (discussing the question of how a robot should be punished, and proposing “a range of measures [that] might be taken to ensure that the robot commits fewer offenses in the future”);
  “robot death penalty”). 22 See infra Part III.
Mark A. Lemley & Bryan Casey,
 Remedies for Robots 86 U. CHI. L. REV. 1311, 1316, 1389-93 (2019)
(discussing the

547
  2019] Punishing Artificial Intelligence 329
the end, our conclusion is this: While a coherent theoretical case can be made for punishing AI, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This Article proceeds as follows. Part I provides a brief background of AI and “AI crime.” It then provides a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Part II considers potential benefits to AI punishment, and argues it could provide general deterrence and expressive benefits. Part III examines whether punishment of AI would violate any of the negative limitations on punishment that relate to desert, fairness, and the capacity for culpability. It finds that the most important constraints on punishment, such as requiring a capacity for culpability for it to be appropriately imposed, would not be violated by AI punishment.
Finally, Part IV considers feasible alternatives to AI punishment. It argues the status quo is or will be inadequate for properly addressing AI crime. While direct AI punishment is a solution, this would require problematic changes to criminal law. Alternately, AI crime could be addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. We argue that civil liability is generally preferable to criminal liability for AI activity as it is proportionate to the scope of the current problem and a less significant departure from existing practice with fewer costs. In this way, the Article aims to map out the possible responses to the problem of harmful AI activity and makes the case for approaching AI punishment with extreme caution.
I. ARTIFICIAL INTELLIGENCE AND PUNISHMENT A. Introduction to Artificial Intelligence
We use the term “AI” to refer to a machine that is capable of completing tasks otherwise typically requiring human cognition.23 AI only sometimes has the ability to directly act physically, as in the case of a “robot,” but it is not necessary for an AI to directly affect physical activity to cause harm (as the RDS case demonstrates).
23 AI lacks a standard definition, but its very first definition in 1955 holds up reasonably well: “[T]he artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” J. MCCARTHY ET AL., A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE (1955), http://www-formal.stanford.edu/jmc/history/dartmouth/ dartmouth.html.
 
548
  330 University of California, Davis [Vol. 53:323
AI is rapidly improving, driven by advances in software, computing power, and big data.24 Hardly a day goes by without a new report of some impressive feat achieved by AI. In 2017, Alphabet’s flagship DeepMind AI beat the world champion of the board game Go.25 This was considered an important feat in the AI community, because of the sheer complexity of the game.26 There are more possible Go board configurations than there are atoms in the universe.27 Thus, a machine designed to play Go cannot simply be preprogrammed with optimal predetermined moves, or solely rely on a brute force approach to considering a large number of future moves.28 Go was the last traditional board game in which people had been able to outperform machines.29
In some areas, AI already makes significant practical contributions. For instance, Google Translate supports more than 100 languages, including 37 by photo input, 32 by voice input, and 27 in “augmented reality mode.”30 The increasing prevalence and capability of AI will lead to widespread social benefit, but will also cause harm. Virtually all activity involves a risk of harm, and as AI comes to do more it will inevitably cause more harm.31
A few features of AI are important to highlight. First, AI has the potential to act unpredictably.32 Some leading AIs rely on machine learning or similar technologies which involve a computer program, initially created by individuals, further developing in response to data without explicit programming.33 This is one means by which AI can
24 See Ryan Abbott, Everything Is Obvious, 66 UCLA L. REV. 2, 23-28 (2019).
25 See id. at 24.
26 See id.
27 See id.
28 See id.
29 See id.
30 GOOGLE TRANSLATE, https://translate.google.com/intl/en/about/languages/ (last
visited Oct. 9, 2019).
31 See, e.g., Daisuke Wakabayashi, Self-Driving Uber Car Kills Pedestrian in Arizona,
Where Robots Roam, N.Y. TIMES (Mar. 19, 2018), https://www.nytimes.com/2018/03/19/ technology/uber-driverless-fatality.html.
32 See, e.g., Taha Yasseri, Never Mind Killer Robots — Even the Good Ones Are Scarily Unpredictable, PHYS.ORG (Aug. 25, 2017), https://phys.org/news/2017-08-mind-killer- robots-good-scarily.html; Why Did the Neural Network Cross the Road?, AI WEIRDNESS (2018), http://aiweirdness.com/post/174691534037/why-did-the-neural-network-cross- the-road (describing a programmer who made her machine learning algorithm attempt to tell jokes).
33 See, e.g., Davide Castelvecchi, Can We Open the Black Box of AI?, NATURE (Oct. 5, 2016), https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731.
 
549
  2019] Punishing Artificial Intelligence 331
engage in activities its original programmers may not have intended or foreseen.34
Second, AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.35 This has led to some AIs being described as “black box” systems.36 For instance, an algorithm may refuse a credit application but not be able to articulate why the application was rejected.37 That is particularly likely in the case of AIs that learn from data, and which may have been exposed to millions or billions of data points.38 Even if it is theoretically possible to explain an AI outcome, it may be impracticable given the potentially resource intensive nature of such inquiries, and the need to maintain earlier iterative versions of AI and specific data.
Third, AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual. Suppose an individual creates an AI to steal financial information by mimicking a bank’s website, stealing user information, and posting that information online. While the theft may be entirely reducible to an individual who is using the AI as a tool, the AI may continue to act in harmful ways without further human involvement. It may even be the case that the individual who sets an AI in motion is not able to regain control of the AI, which could be by design.39
Fourth, while AI can already outperform people in spectacular fashion in some domains, like playing board games, in other domains AI is not even competitive with toddlers.40 That is because all AI is
34 There has been a recent focus on biased decisions by machine learning algorithms — sometimes due to a programmer’s implicit bias, sometimes due to biased training data. See, e.g., Chris DeBrusk, The Risk of Machine-Learning Bias (and How to Prevent It), MIT SLOAN MGMT. REV. (Mar. 26, 2018), https://sloanreview.mit.edu/article/the-risk-of- machine-learning-bias-and-how-to-prevent-it/.
35 See, e.g., Castelvecchi, supra note 33.
36 Id.
37 See id.
38 See id.
39 “The DAO” was the most famous attempt to create a decentralized autonomous
organization. See Samuel Falkon, The Story of the DAO