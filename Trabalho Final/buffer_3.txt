force, but “[d]elivering this education and training will require significant investments.”105 Enhancing the social benefit system will also require significant investment, but such a goal is even more challenging because liberals and conservatives generally disagree that enhanced benefits are a desirable aim.106
That automation creates a need for greater government investment is well known, but what has so far been largely ignored in the automation debate is that automation will make it far more difficult for the government to make investments once tax revenues are reduced.
98 See supra note 16 and accompanying text.
99 See Mallett, supra note 17.
100 See Kevin Lui, Finland is Giving Nearly $600 a Month to 2,000 Jobless Citizens, No
Questions Asked, FORTUNE (Jan. 3, 2017, 1:26 AM), amp.timeinc.net/fortune/2017/01/03/fin- land-universal-basic-income-experiment/?source=dam [https://perma.cc/BFY3-JX2L]. It is also worth noting that the U.S. has operated a guaranteed basic income since 1999. The Alaska Permanent Fund pays each person who has lived the past year in Alaska $1,680. See Van Parijs, supra note 16.
101 See supra note 16 and accompanying text.
102 See id.
103 See Lui, supra note 100.
104 See Chris Weller, The Inside Story of One Man’s Mission to Give Americans Uncondi-
tional Free Money, BUS. INSIDER UK (June 27, 2016, 1:07 PM), uk.businessinsider.com/in- side-y-combinators-basic-income-project-2016-6?r=US&IR=T [https://perma.cc/48QT- H66H].
105 COMM. ON TECH., supra note 8, at 3.
106 See supra note 17 and accompanying text.
 
483
2018] Should Robots Pay Taxes? 163
II. CURRENT TAX POLICIES FAVOR AUTOMATION AND REDUCE TAX REVENUE
A. Introduction
Worker automation is often thought of as a matter of efficiency, where efficiency refers to the ratio of useful output to total input.107 For example, if a machine and a person create the same output, but the machine is less ex- pensive, then automation generates cost savings and improves efficiency.108 If a robot costs a firm $40,000 a year and a human worker costs $45,000 a year, with both workers producing the same output, the firm would yield a $5,000 annual cost savings by automating.
However, it may also be the case that the robot costs more than a human worker before taxes, and only becomes cheaper on a post-tax basis. For instance, the capital outlay for the robot, which includes money spent to acquire, maintain, repair, or upgrade fixed or capital assets such as robots, together with the costs for operating the robot (electricity, etc.), might be estimated at $50,000 over some period, whereas the wages and other costs associated with an employee (healthcare, retirement funding, etc.) might be $45,000 over the same period. The robot may be associated with tax benefits that do not apply to human workers and which reduce its cost to $40,000. A firm using a rational cost-based decision model would choose to automate and realize the machine’s tax benefit. In this example, tax policy has ren- dered the robot a more efficient worker. In simple terms, the heavy relative taxation of the living worker drives the firm toward automation to generate tax savings.
The tax system is not neutral as between work performed by robots versus people.109 Automation provides several major tax advantages. Firms that automate avoid employee and employer wage taxes levied by federal, state, and local taxing authorities and claim accelerated tax depreciation on capital costs for automated workers. The tax system also provides indirect incentives for automated workers. Any outputs produced by human labor are thus effectively penalized compared to outputs produced by capital.110 In
107 Expressed mathematically, efficiency “r” is equal to the amount of useful output (“P”) divided by the amount (“C”) of resources consumed: r=P/C.
108 See Stevens, supra note 17, at 373 (“Technology is very attractive to owners of capital. Machines require no pay, benefits, sick leave, vacation, lunch breaks, or weekends off. They are less prone to err and are more productive than human beings. In a race for the same job, it is therefore difficult for humans to compete with machines.”).
109 The analysis of the “neutrality” of taxation is a common practice in the field of taxa- tion. See generally Peggy Richman (Musgrave), Taxation of Foreign-Source Business Income and the Incentive to Foreign Investment, in PEGGY RICHMAN, TAXATION OF FOREIGN INVEST- MENT INCOME: AN ECONOMIC ANALYSIS (1963), reprinted in PEGGY R. MUSGRAVE, TAX POL- ICY IN THE GLOBAL ECONOMY: SELECTED ESSAYS 3–57 (E. Elgar Publishing 2002) (introducing the term “capital export neutrality”).
110 See MEISEL, supra note 32, at 220 (“An automation tax described as a payroll tax on computers conveys the basic concept. It helps level the playing field. The automation tax serves two purposes: (1) it provides an incentive for a company to create jobs by means such
 
164 484 Harvard Law & Policy Review [Vol. 12 fact, as described below, automated workers are taxed less than human
workers at both the employer and employee level.
B. Avoiding Employee and Employer Wage Taxes via Automation
Wage taxes as discussed here are levied solely on wages paid to indi- viduals to fund social benefit programs including Social Security, Medicare, and Medicaid. Presently, in the United States, the employer and employee pay matching amounts totaling 12.4% of an employee’s salary, plus match- ing Medicare payments totaling 2.9% (applied on the first $127,200 of earn- ings), plus an additional 0.9% Medicare surcharge (applied on earnings over $200,000).111 Many states and localities also levy wage taxes that apply in addition to the federal levies.112
C. Tax Benefit from Accelerated Tax Depreciation on Capital Outlays for Automated Workers
“Tax depreciation” refers here to the deduction (a reduction in the tax base) claimed by the firm in respect to capital outlay for automated workers. Deductions for capital outlays for automation equipment will allow the firm to reduce its tax base over time, which reduces the amount of tax that is payable. Of course, wages paid to individuals are also tax deductible, but the timing of the deduction works differently for robot and human workers.
The timing of claiming a deduction may have a significant effect on a firm’s tax burden. An accelerated tax deduction means that the deduction may be claimed earlier than its actual economic depreciation (the reduction in the value of an asset over time).113 For example, assume a robot has a total
as investing in human-computer synergy; and (2) it proves governmental revenues that, prop- erly used, can create more consumption and thus boost the economy.”).
111 See I.R.C. § 3101(a) (2012 & Supp. II 2014), § 3111(a) (Supp. III 2016), § 3102(a) (2012), § 3121(a)(1) (2012 & Supp. II 2014); I.R.S., SOCIAL SECURITY AND MEDICARE WITH- HOLDING RATES (2017), www.irs.gov/taxtopics/tc751.html [https://perma.cc/3F5R-UEES]; see also Richard Winchester, The Gap in the Employment Tax Gap, 20 STAN. L. & POL’Y REV. 127, 132 (2009) (“The tax imposed by FICA has two components. The first is the old-age, survivors, and disability insurance component, often referred to as OASDI. It is [levied on] . . . ‘wages’ from employment. One half of the tax is deducted from the employee’s compensa- tion. The employer pays the other half. This component of the FICA tax is earmarked to cover social security benefits. There is a limit on the amount of wages that can be taxed. . . . The contribution and benefit base is adjusted each year to reflect increases in average wages of the U.S. economy.”) (citations omitted).
112 For an explanation of the U.S. states that levy sales taxes, see generally SCOTT DRENKARD & NICOLE KAEDING, TAX FOUND., STATE AND LOCAL SALES TAX RATES IN 2016 (2016), https://files.taxfoundation.org/legacy/docs/TaxFoundation_FF504.pdf [https://perma .cc/VLE2-KCHV]. For an explanation of EU tax policy including the VAT, see generally CE ́- CILE REMEUR, EUR. PARLIAMENTARY RESEARCH SERV., TAX POLICY IN THE EU: ISSUES AND CHALLENGES (2015), http://www.europarl.europa.eu/RegData/etudes/IDAN/2015/549001/ EPRS_IDA(2015)549001_EN.pdf. [https://perma.cc/FUE9-ZV58].
113 See Yoram Margalioth, Not a Panacea for Economic Growth: The Case of Accelerated Depreciation, 26 VA. TAX REV. 493, 494–95, 499 (2007) (“Accelerated depreciation policy can be traced back to an influential 1953 paper by Evsey Domar. . . . [Elaborating on the]
 
485
2018] Should Robots Pay Taxes? 165
capital cost of $100,000 and seven years of useful life, while an employee has a total wage cost of $100,000 over seven years. If accelerated deprecia- tion for capital is available,114 the firm may be able to claim a large portion of the $100,000 depreciation as a tax deduction in year one rather than pro- rata over seven years.115 For instance, the firm might claim tax depreciation for an automated worker of $50,000 in year one, $30,000 in year two, $10,000 in year three, and in diminishing amounts to year seven. By con- trast, wage taxes must be deducted as paid (i.e., 1/7th in each year). In this case, a present value benefit will accrue from claiming accelerated tax de- ductions for automated workers relative to the pro-rata tax deductions for employee wages, even where the $100,000 capital outlay is paid up-front.116 This is possible because the present value of the accelerated tax deduction on capital investment is greater than the discounted value of the return the firm could make by investing the free cash held on its balance sheet.
Tax depreciation (whether accelerated or not) is also generally available even where the actual rate of inflation is equal to or greater than the eco- nomic depreciation.117 “Inflation” here refers to the rate at which the general level of prices for goods and services is rising such that it would cost more to buy the same robot next year than it costs today. The issue becomes sig- nificant where, as in the prior example, it was presumed for tax purposes that
Harrod-Domar model, [Domar predicted] that Gross Domestic Product (GDP) was propor- tional to the number of machines; namely, that investment is the key to growth. . . . A later model, developed by Nobel Laureate Robert Solow between 1956–57, points out that the Har- rod-Domar model cannot explain sustained growth. Solow showed that as capital per worker increases, the marginal productivity of capital declines until the capital-labor ratio approaches a steady-state level. At that point, savings . . . are just sufficient to replace worn out machines and equip new workers (assuming population growth), so productivity growth is zero.”) (citing Evsey D. Domar, Capital Expansion, Rate of Growth, and Employment, 14 ECONOMETRICA 137 (1946); Roy F. Harrod, An Essay in Dynamic Theory, 49 ECON. J. 14 (1939); Robert M. Solow, A Contribution to the Theory of Economic Growth, 70 Q. J. ECON. 65 (1956); Robert M. Solow, Technical Change and the Aggregate Production Function, 39 REV. ECON. & STAT. 312 (1957)).
114 See Margalioth, supra note 113, at 505 (“For tax reporting purposes, the Code allows the use of much more accelerated depreciation methods than the straight-line method.”).
115 See id. at 505–506 (“The vast majority of U.S. corporations use a depreciation method called ‘straight-line’ for financial reporting purposes. According to the straight-line deprecia- tion method, annual depreciation is calculated by subtracting the salvage value of the asset from the purchase price and dividing this number by the estimated useful life of the asset. The outcome is equal periodical deductions throughout the asset’s useful life. If the asset in the above example is depreciated under the straight-line method, its $1000 cost is allocated uni- formly over its useful life period of five years, resulting in $200 of depreciation deduction each year.”) (citations omitted).
116 Most large corporations have significant cash accumulations and do not need to borrow funds (and pay interest) to make capital expenditure on automation. Notably, if corporate bor- rowing is required to fund capital expenditure, then present value will depend on the adjusted cost of capital, taking into account the value of tax deductions for interest paid. In summary, accelerated tax depreciation yields an economic benefit where the firm has balance sheet cash earning a low rate of return that it can instead deploy to yield tax deductions on an accelerated basis.
117 See Margalioth, supra note 113, at 508 (“In times of inflation, recovery of the nominal cost of investment is not sufficient to match income and expenses. Because of inflation, the income generated by the asset is expressed in a larger number of dollars though it has the same purchasing power.”).
 
166 486 Harvard Law & Policy Review [Vol. 12
the robot wears out after seven years, but it turns out the robot actually increases in nominal value. An incremental tax benefit thus accrues where the rate of inflation is higher than the rate of the actual diminishment in economic value, and where the nominal (or inflationary) difference is never recaptured in the tax system. In the corporate setting, this recapture of tax book to inflation difference would only accrue on the disposal of the asset, which rarely occurs. The same principle applies to commercial real estate, where tax depreciation is allowable on an asset that is actually increasing (not decreasing) in nominal value over time, and the difference is not ad- justed for tax purposes.
Finally, firms can use accounting “tricks” to report a tax benefit to earnings due to automation, which they may want to do for a variety of reasons, such as making the company look more attractive to potential inves- tors. Where tax depreciation is accelerated relative to book depreciation (the amount reported on financial statements), a firm may generally claim a profit (or earnings benefit) to reported earnings from the tax benefit.118 Thus, a large corporation enjoys a book benefit to reported financial earnings from the differential in depreciation periods. Any firm seeking to accelerate re- ported earnings could use automation to achieve such a timing benefit. This increase to reported earnings may be an even more significant motivation for large firms to automate than a cash tax savings.
D. Indirect Tax Incentives for Automated Workers
The indirect tax system also benefits automated workers at the firm level. Indirect taxation refers to taxes levied on goods and services rather than on profits; the primary examples are the Retail Sales Tax (RST) levied by states and municipalities in the United States and the VAT in most other countries. Employers are thought to bear some of the incidence of indirect tax, as worker salaries and retirement benefits must be increased proportion- ately to offset the indirect tax.119 In the case of automated workers, however, the burden of indirect taxes is entirely avoided by the firm because it does not need to provide for a machine’s consumption.120 In general, business ex-
118 See id. at 505 (“Accounting for depreciation is also required for financial reporting purposes. Generally accepted accounting principles (GAAP) require the depreciation of the (depreciable) cost of income generating assets, usually, tangible assets. The cost has to be allocated among accounting periods on a systematic and rational basis that reflects the use of the asset in the revenue generating process over the asset’s operational life.”) (citations omitted).
119 See CTR. FOR RESEARCH ON THE PUB. SECTOR AT UNIV. BOCCONI, THE ROLE AND IMPACT OF LABOR TAXATION 14 (2011) [hereinafter BOCCONI].
120 The capital assets comprising automated workers might be subject to property taxation by some local jurisdictions as business personal property. However, such personal property taxation is often successfully mitigated by tax planning or with tax waivers by local jurisdic- tions and municipalities negotiated by municipalities. Further, human employees also engender some degree of attached personal property (e.g., office fixtures, personal computers), which are also subject to personal property taxation.
 
487
2018] Should Robots Pay Taxes? 167
penditures for capital assets such as machinery are exempt from indirect taxation or yield a deduction for RST or VAT.121
E. Automation Reduces Tax Revenue
The share of the tax base borne by labor is increasing.122 For 2015, the Internal Revenue Service (IRS) reported that out of the nearly $3 trillion in net collections, individual income taxes accounted for 49.8%, employment taxes 35.2%, business income taxes 11.7%, excise taxes 2.6%, and estate and gift taxes 0.7%.123 In the European Union, high rates of wage taxation are levied in addition to VAT, which is also thought to burden workers, this time in their role as consumers. Moreover, capital taxation is trending sharply downwards in nearly all jurisdictions. Corporate taxation now com- prises roughly one-half of its respective share compared to prior decades.124 In fact, the Trump administration’s recently enacted Tax Cuts and Jobs Act reduces the corporate tax rate from a maximum of 35 percent to a flat 21 percent beginning in 2018.125 In Europe, lower taxation of capital relative to other types of taxes is welcomed as a means of international tax competition.126
Worker taxation is different from corporate taxation in several respects. Tax avoidance planning is not generally available to wage earners. For in- stance, an employee cannot use transfer pricing techniques to shift earned income into a 0%-taxed entity in the Cayman Islands.127 Also, wage earnings are not subject to potential deferral, meaning labor income is taxed currently whereas capital may be taxed upon future disposition of an asset. Human
121 See John Mikesell, Sales Tax Incentives for Economic Development: Why Shouldn’t Production Exemptions Be General?, 54 NAT’L TAX J. 557, 562 (2001).
122 See SOI Tax Stats – Collections and Refunds, by Type of Tax, IRS Data Book Table 1, I.R.S. (Aug. 28, 2017), https://www.irs.gov/statistics/soi-tax-stats-collections-and-refunds-by- type-of-tax-irs-data-book-table-1 [https://perma.cc/R282-7P76] (containing reported aggre- gate collections and refunds from 2015 to 1995). For example, from 1995 to 2015, business income taxes decreased from 12.3% of total collections to 11.7%, while individual taxes in- creased from 46.5% to 49.8%. Id.
123 See id. Individual income taxes here include estate and trust incomes taxes, which represent 1.1% of overall collections. Employment taxes consist of primarily old-age, survi- vors, disability, and hospital insurance, which is almost entirely Federal Insurance Contribu- tions payments and a small amount of Self-Employment Insurance Contributions. It also includes a small amount of Unemployment Insurance and Railroad retirement. Id.
124 See JOEL FRIEDMAN, CTR. ON BUDGET AND POL’Y PRIORITIES, THE DECLINE OF CORPO- RATE INCOME TAX REVENUES 3 (2003), https://www.cbpp.org/sites/default/files/atoms/files/10- 16-03tax.pdf [https://perma.cc/66JG-QSU3].
125 See Act of Dec. 22, 2017 (Tax Cuts & Jobs Act) Pub. L. No. 115-97, §13001, 131 Stat. 2054 (codified as amended at 26 U.S.C. § 11).
126 See MEISEL, supra note 32, at 223 (“What numbers are used in the ratio of revenues to employees? I recommend using revenues generated within the taxing country and employees within the country in the ratio.”).
127 The Cayman Islands has no corporate tax. See DELOITTE, INTERNATIONAL TAX: CAY- MAN ISLANDS HIGHLIGHTS 2017 1 (2017), https://www2.deloitte.com/content/dam/Deloitte/ global/Documents/Tax/dttl-tax-caymanislandshighlights-2017.pdf [https://perma.cc/FZL8- 33YR].
 
168 488 Harvard Law & Policy Review [Vol. 12
capital is also not depreciable, so a person does not typically get a tax deduc- tion for education or medical costs, at least not up to the full amount of the investment.128 By contrast, machinery or other equipment yields an immedi- ate and ongoing tax deduction to a firm until the equipment’s tax basis is reduced to zero. Workers are additionally subject to various forms of indirect taxation, particularly in Europe and in states or local jurisdictions, whereas business machinery is often exempted from RST and VAT.129
If corporate taxes decline as a share of the tax base while the overall level of taxation holds constant, other types of taxation may increase to cover the difference. While a government may choose to increase borrowing or decrease spending, this would be expected to have negative economic effects over the long term.
III. TAX POLICY OPTIONS FOR AN AUTOMATION TAX
The current tax system is designed to principally tax human workers and not robot workers. All else being equal, this creates a situation in which firms prefer robots since substantially less tax per output is accrued or remit- ted in respect of an automated worker. At the same time, the automation of large segments of the labor force threatens long-term fiscal solvency because of the potential reduction in tax collections.
A major automation policy issue is therefore how to adjust the tax sys- tem to be neutral as between robot and human workers, or even to create incentives for human rather than robot workers to incentivize employment. In doing so, it is important to consider that capital investment of any kind (including for robots) is thought to be beneficial to economic growth.130 Na- tions engage in tax competition to draw capital into their jurisdictions. Any disallowance of capital deduction would serve as a disincentive to invest- ment and would, theoretically, be economically undesirable. For example, if only one taxing jurisdiction disallowed tax deductions for automated work- ers, multinational firms might shift their capital investments to other juris-
 128 For the U.S. incentives with an election for deduction or credit on higher education costs, see 26 U.S.C. § 25A (2012 & Supp. III 2016).
129 See, e.g., BOCCONI, supra note 119, at 14.
130 See, e.g., Eduardo Borensztein et al., How Does Foreign Direct Investment Affect Eco- nomic Growth?, 45 J. INT’L ECON. 115, 116 (1998); see also Gert Wehinger, Fostering Long- Term Investment and Economic Growth: Summary of a High-Level OECD Roundtable, 2011 OECD J. FIN. MKT. TRENDS 1, 2 (2011).

489
2018] Should Robots Pay Taxes? 169
dictions.131 It is therefore important to consider international tax competition in evaluating various options to create an automation-neutral tax system.132
A. Disallowance of Corporate Tax Deductions for Automated Workers
A first option is to attempt to disallow the respective corporate income tax deductions for capital investments that give rise to the automation tax benefit. The basic idea is to reverse each of the tax benefits accruing in the case of worker automation in relation to avoidance of levy of wage taxes, accelerated or timing difference of deductions, and indirect tax benefits. The recent South Korean “robot tax” adopted this strategy in part by reducing deductions for investment in automated machines.133
To begin with federal income taxation, the disallowance of tax prefer- ences upon some threshold of income level is a common practice in the Internal Revenue Code and is often referred to as a “phase out.”134 Phase outs reduce tax benefits for higher-income taxpayers, such as the child tax credit and certain contributions to retirement accounts, and they target tax benefits to middle- and lower-income taxpayers.135 For instance, student loan interest is deductible, but not for individuals with more than $80,000 in modified adjusted gross income (MAGI) ($160,000 for joint filers).136 Some phase outs reduce credits, others reduce deductions.137
A new code provision could be designed with a similar phase out, where depreciation or other expenses related to automated workers would be disallowed based on a reported level of automation, rather than income. For example, firms with high levels of worker automation could have their tax depreciation automatically reduced beyond a certain threshold. The Treasury Department would need to craft detailed regulations and criteria to identify the threshold and to measure the level of automation required to trigger the disallowance.
In respect of indirect taxation, a simpler solution may be possible. Indi- rect tax preferences for capital outlay in respect to automated workers could
131 However, the shift would be from one high-tax jurisdiction to another high-tax juris- diction to claim the deduction’s full value, rather than a shift into tax havens with a zero percent corporate tax rate, where the capital tax deductions for automated workers would not have any value (i.e., the value of a tax deduction in a zero percent tax jurisdiction is zero). Thus, multinational firms should not be expected to make capital investment in robots in tax havens where the value of deductions is zero, especially where transfer pricing strategies are available to shift income arising from the automated workers.
132 See MEISEL, supra note 32.
133 See McGoogan, supra note 22.
134 See, e.g., Emmanuel Saez, Do Taxpayers Bunch at Kink Points?, 2 AM. ECON. J. ECON
POL’Y 180, 180 (2010).
135 See, e.g., I.R.S., TEN FACTS ABOUT THE CHILD TAX CREDIT (2011), https://www.irs
.gov/newsroom/ten-facts-about-the-child-tax-credit [https://perma.cc/3YEV-V8XD].
136 See I.R.S., TAX BENEFITS FOR EDUCATION 2 (2016), https://www.irs.gov/pub/irs-pdf/
p970.pdf [https://perma.cc/X5ZV-MXMX].
137 See generally I.R.S., TAX GUIDE 2016: FOR INDIVIDUALS 25, (2016), https://www.irs
.gov/pub/irs-pdf/p17.pdf [https://perma.cc/FS97-G5LE] (discussing the various types of cred- its and deductions available and the income levels at which they phase out).
 
170 490 Harvard Law & Policy Review [Vol. 12
be disallowed outright at the state level. Thus, for example, where the firm attempts to claim an RST/VAT exemption or refund for tax payments made to purchase and maintain automated workers, this would not be permitted.
These measures could achieve greater balance between taxing human workers and robots, but the disallowance of corporate income tax deductions will not adequately address the decline in the wage tax base used to fund social insurance benefits.
B. Levy of an Automation Tax
A second option is to levy an incremental federal “automation tax” to the extent workers are laid off or replaced by machines.138 A similar system is in place with respect to unemployment compensation in many states where worker layoffs are tracked and employers are given corresponding ratings.139 Employers must pay into an unemployment insurance scheme based on their ratings, so a business which has more layoffs pays more in taxes for unemployment insurance.140 A federal automation tax could be de- signed to do essentially the same thing where worker layoff data could be obtained from the states and then used to levy an additional federal tax to the extent the Treasury Department determined the layoffs were due to automation.
A potential drawback to the levy of an additional automation tax is that it would essentially increase the corporate effective tax rate for many firms, and also increase the relative complexity of the tax system. Economic theory suggests that higher rates and added complexity are negatives in terms of international tax competition.141 Another drawback is that firms might accel- erate layoffs upon passage (or debate) of the bill prior to implementation to
138 Cf. Michael Kraich, The Chilling Realities of the Telecommuting Tax: Adapting Twenti- eth Century Policies for Twenty-First Century Technologies, 15 U. PITT. J. TECH. L. & POL’Y 224 (2015) (providing a comprehensive discussion of a “telecommuting tax”).
139 See DAVID RATNER, FED. RESERVE BD., UNEMPLOYMENT INSURANCE EXPERIENCE RAT- ING AND LABOR MARKET DYNAMICS 1 (2013), https://www.federalreserve.gov/pubs/feds/2013/ 201386/201386pap.pdf [https://perma.cc/T5W3-5YJD] (“The United States is the only OECD country to finance unemployment insurance (UI) through a tax system which penalizes layoffs. The original intent of this institution, known as ‘experience rating,’ was to apportion the costs of UI to the highest turnover firms and thereby stabilize employment. Experience rating can stabilize employment through a layoff cost. The layoff cost is levied when a firm lays off a worker and is assessed a higher tax rate in the future.”).
140 See id.
141 See generally Michael Keen & Kai A. Konrad, The Theory of International Tax Com- petition and Coordination, in 5 HANDBOOK OF PUBLIC ECONOMICS 257 (Alan Auerbach et al. eds., 2013), http://gabriel-zucman.eu/files/teaching/KeenKonrad13.pdf [https://perma.cc/ 3VC5-HLN3] (exploring models that suggest a country might prefer to raise its tax rate in response to lower tax rates in other countries); but see Bret N. Bogenschneider, Causation, Science & Taxation, 10 ELON L. REV. (forthcoming, Spring 2018) (“The hypothesis that tax cuts cause economic growth is a central tenet of neoclassical economic theory. Yet, it is not clear why economists hold this belief, as empirical evidence of any posited causal relation is conspicuously absent. . . . The available evidence indicates to the contrary of the hypotheses that tax cuts cause economic growth is that higher ratios of taxation to gross domestic product are associated with higher rates of national economic growth in most countries.”).
 
491
2018] Should Robots Pay Taxes? 171
avoid the tax by reducing the number of employees upon the effective date of the law. Accordingly, a retroactive effective date for measurement of em- ployment levels for the automation tax would be a practical necessity.
C. Grant Offsetting Tax Preferences for Human Workers
A third option is to attempt to grant offsetting tax preferences for firms that employ human workers for each category of tax benefit. To begin with wage taxation, the tax preference could entail a repeal of the employer con- tributions to the Social Security and Medicare systems. The result would be that both human and automated workers would be exempt for the employer in terms of wage taxes—not just automated workers. However, this would accelerate the insolvency of the Social Security system unless the resultant decrease in tax collections were otherwise offset.142
In terms of income taxation, an offsetting preference for human work- ers could be designed as an accelerated deduction for future wage compensa- tion expense (i.e., the firm would get an accelerated tax deduction) to match the accelerated depreciation for automated workers. In terms of indirect tax- ation typically levied by the states, the contemplated offset would be for indirect taxes not typically levied on wage income. This would constitute an incentive for firms to employ human workers.
D. Levy of a Corporate Self-Employment Tax
A fourth option is to increase corporate level taxation for firms that produce outputs without using human labor. The additional taxes would be a substitute amount for Social Security and Medicare wage taxes avoided by the firm with automated labor.143 In part, this is the corollary to the individ- ual self-employment tax where a small-business owner is required to pay monies into the Social Security system approximating the Social Security taxes that would be paid on his or her own wages deemed to be paid to self. The corporate self-employment tax would be calculated as a substitute for what employment taxes would have been on the worker and employer if a human worker had continued to perform the work.144 The corporate self- employment tax could be calculated based on a ratio of corporate profits to gross employee compensation expense. If the ratio exceeds an amount deter- mined by the Treasury (in reference to industry standards), then backup
142 This would require a very significant offset. Federal Insurance Contributions and Self- Employment Insurance Contributions currently make up about 34.6% of net federal tax collec- tions. Federal Insurance Contributions include both employee and employer payments to fund Social Security and Medicare. See I.R.S., supra note 122.
143 MEISEL, supra note 32, at 222–23 (“Returning to the payroll tax analogy, companies that hire fewer people pay fewer payroll taxes. The payroll tax in the US helps fund social security, Medicare, and unemployment insurance. In Europe, payroll taxes are even higher than in the US.”).
144 Id. at 227 (“The automation tax might encourage companies to prefer productivity improvements achieved by using a combination of human and computer capabilities.”).
 
172 492 Harvard Law & Policy Review [Vol. 12
withholding could apply on corporate profits. The gross amount of the auto- mation tax could be designed to match the wage taxes avoided by the firm with automated workers.
William Meisel has similarly proposed an “automation tax” which he referred to in lay terms as a “payroll tax on computers.”145 This would be like the corporate self-employment tax described here. Meisel wrote:
I propose that a national automation tax be based on the ratio of a company’s revenues (total sales) to their number of employ- ees. . . .[T]he automation tax should increase as a percentage as the revenue-per-employee [ratio] grows, making it more attractive to create jobs than to replace them with automation. . . . I prefer applying the percentage to revenues. . . . Profits can be manipu- lated with deductions and other accounting complexities much more than revenues.146
Meisel’s “automation tax” differs from our proposed corporate self-em- ployment tax in that the former uses a sales ratio as opposed to a profit ratio. A sales ratio may be unworkable in practice since the tax would prohibi- tively fall on firms with high sales but low profit margins, such as dis- counted retailers. Since automation often occurs in the high-tech industry among companies with high profit margins, it seems preferable that a viable “automation tax” using a ratio to employee expense should be premised on profits, not sales.
E. Increase the Corporate Tax Rate
A fifth option would be to significantly increase the corporate tax rate, with the intent of increasing the relative portion of the tax base borne by capital and decreasing that borne by labor. The counter-intuitive advantage of this approach is that higher corporate tax rates increase the relative value of tax deductions for marginal investment, where “marginal” investment re- fers to incremental investment made only because of the tax system.147 As one of us has explained, “[t]he experienced tax attorney always counsels the client that marginal capital investment is tax deductible.”148 Thus, mul- tinational firms may make capital investment into higher tax jurisdictions in lieu of tax haven jurisdictions to claim tax deductions of relatively higher value. In part for this reason, for smaller and growing firms that are reinvest- ing profits back into their businesses, the higher rate of corporate tax is not a major disincentive because ongoing tax deductions will substantially reduce the tax base regardless of the ultimate tax rate to be applied.
145 Id. at 220 (“If software is to take over many jobs, why not have an income tax on software? We could perhaps think of it as a payroll tax on computers.”).
146 Id. at 221–23.
147 See Bret N. Bogenschneider, The Tax Paradox of Capital Investment, 33 J. TAX’N INV. 59, 74 (2015).
148 Id. at 61 (second emphasis added).
 
493
2018] Should Robots Pay Taxes? 173
The drawbacks to increasing the corporate tax rate are well-known and may be summarized as follows: First, the corporate tax rate might be a signal to firms about the tax climate of a jurisdiction, so higher tax rates could have a negative psychological effect on capital investment decision making.149 Second, accelerated tax deductions would be a stronger automation incen- tive with a higher corporate tax rate as the deduction would have greater value. This means that an increase in the corporate tax rate should be taken in combination with our other proposals. Third, the increase in corporate tax rates would affect all firms, even those not engaged in worker automation. Hence, the increase in corporate tax rate option might be viewed as one version of zero-sum analysis, in which tax policy is designed not to allow a shift of the tax burden from capital to other taxpayers. Further, any increase in corporate tax rates may prompt firms to attempt to shift the tax incidence to workers or consumers.150 Finally, increasing corporate tax rates may be politically unfeasible. As Meisel notes in an understated fashion, “[c]orporations might instinctively fight a corporate tax.”151
F. Issues in Economic Efficiency Relevant to Automation Tax Policy Proposals
The tax policy analysis developed here comes from the perspective of average effective tax rates as opposed to solely marginal rates.152 Any margi- nal tax rate methodology excludes an analysis of taxation relative to the overall share of the tax base. For example, technology and pharmaceutical companies often pay a very low average effective tax rate (e.g., less than 10%) but could also be correctly found to simultaneously have a high margi- nal effective tax rate (e.g., about 35%). A corporate taxpayer which pays very little tax relative to its level of taxable income could correctly describe its marginal tax rate as “high.” Accordingly, the last dollar of income may nearly always be found to be taxed at a “high” marginal tax rate, even where the average effective tax rate is relatively low.153
Economic models of taxation are typically designed by modeling the hypothetical effects of changes in marginal tax rates.154 Marginal tax rates
149 Id. at 60–61 (“Any income tax system is designed initially to favor active investors. This is because no matter how high the actual tax rate, it is levied only on what is referred to as ‘taxable income.’ Of course, ‘taxable income’ means the amount of profits less deductions. Every tax professional is aware of this feature of an income tax system and counsels the client accordingly.”).
150 See Kimberly Clausing, In Search of Corporate Tax Incidence, 65 TAX L. REV. 433, 468 (2012). Firms, however, behave as if they bear the incidence of corporate taxation.
151 MEISEL, supra note 32, at 225.
152 The calculation of a marginal tax rate is essentially the theoretical opposite of the calculation of taxation as a percentage of the share of the overall tax base.
153 For example, a firm may have an overall tax rate of 20% on all of its earnings; how- ever, with respect to a hypothetical decision of whether to earn incremental income, the margi- nal tax rate might be 35%.
154 For a discussion of marginal tax rates in economic analysis, see David Madden, The Poverty Effects of a ‘Fat Tax’ in Ireland, 24 HEALTH ECON. 104, 106 (2015) (“The difficulties
 
174 494 Harvard Law & Policy Review [Vol. 12
again represent incremental changes to the statutory tax rate on the last dol- lar of income.155 For example, a change in the statutory corporate tax rate from 35% to 30% would be reflected in economic models premised on mar- ginal rate analysis. The trouble with this form of economic modeling is that its validity relies on the presumption that firm decisions are made based on tax effects on the marginal investment and not based on an average. This approach has major implications for tax policy design as tax cuts to the stat- utory rate are nearly certain to have a marginal effect even where the firm does not pay a high tax rate overall. Thus, business and investment decisions are presumed not to proceed at the average tax rate for all earned income, but only with respect to incremental tax changes relevant to marginal income.
Other economic modeling proceeds on a marginal effective tax rate ba- sis (i.e., reflecting that corporate taxpayers do not pay the statutory rate). For example, the granting of an additional deduction for manufacturing activity to corporations could reduce the marginal effective tax rate on the last dollar of income from 30% to 27% where the statutory rate is 35%. By this method, the firm would be presumed to make an investment decision based on the average tax rate at the margin. Both approaches are distinguishable from analysis using simply an average effective tax rate, which for large corporations is now calculated at approximately 20% (including permanent deferrals) and trending downward.156 However, for many tech companies, the effective tax rate is below 10%. At such very low average effective tax rates, it is not clear that economic analysis of marginal effects of tax cuts is a realistic method of tax policy analysis. By such methods, significant macroeconomic benefits can be posited where corporate effective tax rates are reduced from very low levels to even lower levels (e.g., from 2% to 1%), but where it is likely that factors other than marginal taxation are likely to drive firm investment decisions. Also, the positing of economic growth from marginal tax cuts does not consider the effect changes in the composition of the overall tax base, where the taxation of one factor is substantially re- duced, namely capital, and the taxation of another factor is increased (or overall borrowing is increased). Further, multinational firms do not engage in tax avoidance planning to reduce income which they do not intend to earn.
associated with non-marginal tax reforms have led a number of analysts to concentrate on marginal tax reforms. This approach has the advantage of not requiring estimates of individual demand and utility functions.”) (internal citation omitted).
155 The U.S. federal statutory corporate tax rate is thirty-five percent for corporate income in excess of ten million. See I.R.C. § 11 (2012). Various individual U.S. states also levy an incremental state-level corporate tax. See generally NICOLE KAEDING, TAX FOUND., STATE INCOME CORPORATE TAX RATES AND BRACKETS FOR 2016 (2016), https://files.taxfoundation .org/legacy/docs/TaxFoundation-FF497.pdf [https://perma.cc/J37Z-Y8X6].
156 For effective tax rates on multinational firms including the delay in taxation of foreign earnings for U.S. multinationals, see generally Bret N. Bogenschneider, The Effective Tax Rates of U.S. Firms with Permanent Deferral, 145 TAX NOTES 1391 (2015).
 
495
2018] Should Robots Pay Taxes? 175
In summary, notwithstanding that the statutory corporate tax rate, or marginal corporate effective tax rates, might be correctly described as “high” in the economic theory of taxation, such analysis is also subject to a relative or zero-sum form of analysis, where tax cuts for one party are trans- ferred as tax increases to another party. The average effective tax rate on workers is relatively “high” where all types of taxation are taken into ac- count.157 The taxation of workers comprises the bulk of the tax base in the United States and that of most developed countries. As workers are substi- tuted or replaced by automation, follow-on effects are possible not only from the direct reduction in the tax base, but also indirectly where the relative taxes are transferred to other workers in the economy.
CONCLUSION
Automation promises to be one of the great social challenges of our generation. It can benefit everyone, or it can benefit the select few at the expense of the many. Tax is a critical component of any automation policy. Existing tax policies both encourage automation and dramatically reduce the government’s tax revenue. This means that attempts to craft policy solutions to deal with automation will be inadequate if they fail to take taxation into account. In this article, we have proposed a series of tax policy changes that could level the playing field for human workers. Whether these proposals are adopted may depend on whether policy makers are prepared to make politically challenging decisions about how to deal with automation.
 157 See Bogenschneider, supra note 27.

496
The Reasonable Computer: Disrupting the Paradigm of Tort Liability
Ryan Abbott* ABSTRACT
Artificial intelligence is part of our daily lives. Whether working as chauf- feurs, accountants, or police, computers are taking over a growing number of tasks once performed by people. As this occurs, computers will also cause the injuries inevitably associated with these activities. Accidents happen, and now computer-generated accidents happen. The recent fatality involving Tesla’s au- tonomous driving software is just one example in a long series of “computer- generated torts.”
Yet hysteria over such injuries is misplaced. In fact, machines are, or at least have the potential to be, substantially safer than people. Self-driving cars will cause accidents, but they will cause fewer accidents than human drivers. Because automation will result in substantial safety benefits, tort law should encourage its adoption as a means of accident prevention.
Under current legal frameworks, suppliers of computer tortfeasors are likely strictly responsible for their harms. This Article argues that where a sup- plier can show that an autonomous computer, robot, or machine is safer than a reasonable person, the supplier should be liable in negligence rather than strict liability. The negligence test would focus on the computer’s act instead of its design, and in a sense, it would treat a computer tortfeasor as a person rather than a product. Negligence-based liability would incentivize automation when doing so would reduce accidents, and it would continue to reward sup- pliers for improving safety.
More importantly, principles of harm avoidance suggest that once com- puters become safer than people, human tortfeasors should no longer be mea- sured against the standard of the hypothetical reasonable person that has been employed for hundreds of years. Rather, individuals should be judged against computers. To appropriate the immortal words of Justice Holmes, we are all “hasty and awkward” compared to the reasonable computer.
TABLE OF CONTENTS
INTRODUCTION ................................................. 2 I. LIABILITY FOR MACHINE INJURIES ..................... 8 A. A Brief History ..................................... 8
* Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at UCLA. Thanks to Hrafn Asgeirsson, Bret Bogenschneider, Richard Epstein, Marie Newhouse, Alexander Sarch, and Christopher Taggart for their insightful comments.
 January 2018 Vol. 86 No. 1
1

2
497 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
B. Tort Law as a Mechanism for Accident Prevention . 11 C. Negligence .......................................... 12 D. Strict and Product Liability ......................... 13
II. COMPUTER-GENERATED TORTS......................... 16
A. Automation Will Prevent Accidents ................. 16
B. Tort Liability Discourages Automation.............. 19
C. Computer-Generated Torts Should Be Negligence
Based ............................................... 22
D. Computer-Generated Torts as a Type of Machine
Injury............................................... 24
E. Implementation ..................................... 26
F. Financial Liability................................... 30
G. Alternatives to Negligence ........................... 32
III. THE REASONABLE ROBOT .............................. 35
A. When Negligence Is Strict ........................... 35
B. The New Hasty and Awkward ...................... 36
C. Reasonable People Use Autonomous Computers . . . . 39
D. The Reasonable Computer Standard for Computer
Tortfeasors.......................................... 41
E. The Automation Problem ........................... 42
CONCLUSION ................................................... 44
INTRODUCTION
An automation revolution is coming, and it is going to be hugely disruptive.1 Ever cheaper, faster, and more sophisticated computers are able to do the work of people in a wide variety of fields and on an unprecedented scale. They may do this at a fraction of the cost of existing workers, and in some instances, they already outperform their human competition.2 Today’s automation is not limited to manual la- bor; modern machines are already diagnosing disease,3 conducting le-
1 See generally JAMES MANYIKA ET AL., MCKINSEY & CO., DISRUPTIVE TECHNOLOGIES: ADVANCES THAT WILL TRANSFORM LIFE, BUSINESS, AND THE GLOBAL ECONOMY (2013).
2 See, e.g., Carl Benedikt Frey & Michael A. Osborne, The Future of Employment: How Susceptible Are Jobs to Computerisation?, 114 TECHNOLOGICAL FORECASTING & SOC. CHANGE 254, 265–66 (2017) (reporting in a seminal paper that “47 percent of total US employment is [at] high risk” of automation, and stating that “recent developments in [machine learning] will put a substantial share of employment, across a wide range of occupations, at risk in the near future”).
3 See Roger Parloff, Why Deep Learning Is Suddenly Changing Your Life, FORTUNE (Sept. 28, 2016, 5:00 PM), http://fortune.com/ai-artificial-intelligence-deep-machine-learning [https://perma.cc/E3UA-N2TZ]. Several artificial intelligence systems are already capable of au- tomating medical diagnoses. See id. For instance, Freenome has a system for diagnosing cancer from blood samples that is competitive with pathologists. See id.; see also FREENOME, http:// www.freenome.com (last visited Jan. 4, 2018).
 
2018] 498 THE REASONABLE COMPUTER 3
gal due diligence,4 and providing translation services.5 For better or worse, automation is the way of the future—the economics are simply too compelling for any other outcome.6 But what of the injuries these automatons will inevitably cause? What happens when a machine fails to diagnose a cancer, ignores an incriminating email, or inadvertently starts a war?7 How should the law respond to computer-generated torts?
Tort law has answers to these questions based on a system of common law that has evolved over centuries to deal with unintended harms.8 The goals of this body of law are many: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.9 Whether tort law is the best means for achieving all of these goals is debatable, but jurists are united in considering accident reduction as one of the central, if not the primary, aims of tort law.10 By creating a framework for loss shifting from injured victims to tortfeasors, tort law deters unsafe conduct.11 A purely financially motivated rational
4 See Jane Croft, Legal Firms Unleash Office Automatons, FIN. TIMES (May 16, 2016), https://www.ft.com/content/19807d3e-1765-11e6-9d98-00386a18e39d (discussing various software programs that can outperform attorneys and paralegals in document review); cf. Dana Remus & Frank S. Levy, Can Robots Be Lawyers? Computers, Lawyers, and the Practice of Law (Nov. 27, 2016), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2701092 (unpublished manuscript) (arguing that artificial intelligence will refocus rather than replace attorneys).
5 See Yonghui Wu et al., Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation (Sept. 26, 2016), https://arxiv.org/pdf/1609.08144.pdf (unpublished manuscript). Google now claims its Google Neural Machine Translation system is approaching human-level translation accuracy. Id. at 2.
6 See, e.g., DELOITTE, FROM BRAWN TO BRAINS: THE IMPACT OF TECHNOLOGY ON JOBS IN THE UK 4 (2015), https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/Growth/ deloitte-uk-insights-from-brawns-to-brain.pdf (suggesting that every nation and region of the U.K. has benefitted from automation and that automation has resulted in £140 billion to the U.K.’s economy in new wages).
7 See, e.g., Fiona Macdonald, The Greatest Mistranslations Ever, BBC (Feb. 2, 2015), http://www.bbc.co.uk/culture/story/20150202-the-greatest-mistranslations-ever (describing some of the unfortunate outcomes associated with mistranslation).
8 See generally MORTON J. HORWITZ, THE TRANSFORMATION OF AMERICAN LAW, 1780–1860 (1977) [hereinafter HORWITZ, 1780–1860]; MORTON J. HORWITZ, THE TRANSFORMA- TION OF AMERICAN LAW 1870–1960 (1992).
9 See George L. Priest, Satisfying the Multiple Goals of Tort Law, 22 VAL. U. L. REV. 643, 648 (1988).
10 See, e.g., George L. Priest, The Invention of Enterprise Liability: A Critical History of the Intellectual Foundations of Modern Tort Law, 14 J. LEGAL STUD. 461 (1985); see also Robert F. Blomquist, Goals, Means, and Problems for Modern Tort Law: A Reply to Professor Priest, 22 VAL. U. L. REV. 621 (1988) (arguing that economic theory and moral philosophy both require accident reduction to be the primary aim of tort law).
 11 See George L. Priest, Modern Tort Law and Its Reform, 22 VAL. U. L. REV. 1, 7 (1987).

4 499 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.12 This liability framework has far-reaching and sometimes complex impacts on be- havior. It can either accelerate or impede the introduction of new technologies.13
Most injuries people cause are evaluated under a negligence stan- dard where unreasonable conduct establishes liability.14 When com- puters cause the same injuries, however, a strict liability standard applies.15 This distinction has financial consequences and a corre- sponding impact on the rate of technology adoption.16 It discourages automation, because machines incur greater liability than people. It also means that in cases where automation will improve safety, the current framework to prevent accidents now has the opposite effect.
This Article argues that the acts of autonomous computer tortfeasors should be evaluated under a negligence standard, rather than a strict liability standard, in cases where an autonomous com- puter is occupying the position of a reasonable person in the tradi- tional negligence paradigm and where automation is likely to improve safety. For the purposes of ultimate financial liability, the computer’s supplier (e.g., manufacturers and retailers) should still be responsible for satisfying judgments under standard principles of product liability law.
This Article employs a functional approach to distinguish an au- tonomous computer, robot, or machine from an ordinary product.17
12 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (applying rule that balances the burden of additional protections on the actor with the probability and gravity of an injury).
13 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Gideon Parchomovsky & Alex Stein, Torts and Innovation, 107 MICH. L. REV. 285, 286 (2008) (discussing how the role of custom in tort law impedes innovation). Nor is the idea that tort liability is a barrier to developments in machine intelligence new. See Steven J. Frank, Tort Adjudication and the Emergence of Artificial Intelligence Software, 21 SUFFOLK U. L. REV. 623, 639 (1987).
14 See infra text accompanying notes 62–71.
15 See infra text accompanying notes 93–100.
16 See, e.g., Amy Finkelstein, Static and Dynamic Effects of Health Policy: Evidence from
the Vaccine Industry, 119 Q.J. ECON. 527, 535 (2004) (explaining that establishment of the Vac- cine Injury Compensation Fund encouraged vaccine development by indemnifying manufactur- ers from liability).
17 Terms such as “robot,” “machine,” “artificial intelligence,” “machine intelligence,” and even “computer” are not used consistently even in the scientific literature. See, e.g., NEIL JOHN- SON ET AL., ABRUPT RISE OF NEW MACHINE ECOLOGY BEYOND HUMAN RESPONSE TIME 2 (2013), https://www.nature.com/articles/srep02627.pdf (discussing autonomy in the context of ar- tificial intelligence); Matthew U. Scherer, Regulating Artificial Intelligence Systems: Risks, Chal-
 
2018] 500 THE REASONABLE COMPUTER 5
Society’s relationship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are given tasks to complete and determine for themselves how to complete those tasks. For instance, a person could instruct a self-driving car to take them from point A to point B, but would not control how the machine does so. By contrast, a person driving a conventional vehicle from point A to point B controls how the machine travels. This distinction is analogous to the distinction be- tween employees and independent contractors, which centers on the degree of control and independence.18 As this Article uses such terms, autonomous machines or computer tortfeasors control the means of completing tasks, regardless of their programming.19
The most important implication of this line of reasoning is that just as computer tortfeasors should be compared to human tortfeasors, so too should humans be compared to computers. Once computers become safer than people and practical to substitute, com- puters should set the baseline for the new standard of care. This means that human defendants would no longer have their liability based on what a hypothetical, reasonable person would have done in their situation, but what a computer would have done. In time, as computers come to increasingly outperform people, this rule would mean that someone’s best efforts would no longer be sufficient to avoid liability. It would not mandate automation in the interests of freedom and autonomy,20 but people would engage in certain activi- ties at their own peril. Such a rule is entirely consistent with the ratio- nale for the objective standard of the reasonable person, and it would benefit the general welfare. Eventually, the continually improving
lenges, Competencies, and Strategies, 29 HARV. J.L. & TECH. 353, 359–61 (2016) (discussing difficulties with defining artificial intelligence); John McCarthy, What Is Artificial Intelligence? 2–3 (Nov. 12, 2007), http://jmc.stanford.edu/articles/whatisai/whatisai.pdf (discussing the lack of a standardized definition of artificial intelligence by the scientist who coined the term).
18 See Yewens v. Noakes [1880] 6 QB 530 at 532–33 (Eng.) (“A servant is a person subject to the command of his master as to the manner in which he shall do his work.”). Also see O’Connor v. Uber Technologies, Inc., No. 14-16078 (9th Cir. argued Sept. 20, 2017), for one of the many ongoing lawsuits against Uber highlighting modern challenges distinguishing between employees and independent contractors.
19 See, e.g., Ryan Abbott, I Think, Therefore I Invent: Creative Computers and the Future of Patent Law, 57 B.C. L. REV. 1079, 1083–91 (2016) (discussing types of machine architectures, including conventional knowledge-based systems with expert rules as well as types of machine intelligence algorithms that result in unexpected machine behavior).
20 See generally Richard M. Ryan & Edward L. Deci, Overview of Self-Determination The- ory: An Organismic Dialectical Perspective, in HANDBOOK OF SELF-DETERMINATION RESEARCH 3, 6 (Edward L. Deci & Richard M. Ryan eds., 2002) (arguing that people have three basic psychological needs: connectedness, autonomy, and feeling competent).
 
6 501 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
“reasonable computer” standard should even apply to computer tortfeasors, such that computers will be held to the standard of other computers. By this time, computers will cause so little harm that the primary effect of the standard would be to make human tortfeasors essentially strictly liable for their harms.
This Article uses self-driving cars as a case study to demonstrate the need for a new torts paradigm.21 There is public concern over the safety of self-driving cars, but a staggering ninety-four percent of crashes involve human error.22 These contribute to over 37,000 fatali- ties a year in the United States at a cost of about $242 billion.23 Auto- mated vehicles may already be safer than human drivers, but if not, they will be soon.24 Shifting to negligence would accelerate the adop- tion of driverless technologies, which, according to a report by the consulting firm McKinsey & Company, may otherwise not be wide- spread until the middle of the century.25
Automated vehicles may be the most prominent and disruptive upcoming example of robots changing society, but this analysis applies to any context with computer tortfeasors. For instance, IBM’s flagship artificial intelligence system, Watson, is working with clinicians at Me- morial Sloan Kettering to analyze patient medical records and provide
21 Others have written about tort liability and self-driving vehicles, although primarily dealing with how existing law deals with accidents involving autonomous vehicles. See, e.g., Jef- frey K. Gurney, Sue My Car Not Me: Products Liability and Accidents Involving Autonomous Vehicles, 2013 U. ILL. J.L. TECH. & POL’Y 247; F. Patrick Hubbard, “Sophisticated Robots”: Balancing Liability, Regulation, and Innovation, 66 FLA. L. REV. 1803, 1803 (2014) (arguing, using the example of self-driving vehicles, that the current framework “provides an appropriate balance of innovation and liability for personal injury”); Gary E. Marchant & Rachel A. Lindor, The Coming Collision Between Autonomous Vehicles and the Liability System, 52 SANTA CLARA L. REV. 1321 (2012).
22 See NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 115, CRITICAL REASONS FOR CRASHES INVESTIGATED IN THE NATIONAL MOTOR VEHICLE CRASH CAUSATION SURVEY 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublica- tion/812115.
23 General Statistics, INS. INST. FOR HIGHWAY SAFETY (Dec. 2017), http://www.iihs.org/ iihs/topics/t/general-statistics/fatalityfacts/overview-of-fatality-facts [https://perma.cc/2J5P- Y27C]; see NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., DOT HS 812 013, THE ECONOMIC AND SOCIETAL IMPACT OF MOTOR VEHICLE CRASHES, 2010 (REVISED) 1 (2015), https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812013.
24 See Cadie Thompson, Why Driverless Cars Will Be Safer Than Human Drivers, BUS. INSIDER (Nov. 16, 2016, 9:24 PM), http://www.businessinsider.com/why-driverless-cars-will-be- safer-than-human-drivers-2016-11.
25 Michele Bertoncello & Dominik Wee, Ten Ways Autonomous Driving Could Redefine the Automotive World, MCKINSEY & CO. (June 2015), http://www.mckinsey.com/industries/auto- motive-and-assembly/our-insights/ten-ways-autonomous-driving-could-redefine-the-automotive- world.
 
2018] 502 THE REASONABLE COMPUTER 7
evidence-based cancer treatment options.26 It even provides support- ing literature to human physicians to support its recommendations.27 Like self-driving cars, Watson does not need to be perfect to improve safety—it just needs to be better than people. In that respect, the bar is unfortunately low. Medical error is one of the leading causes of death.28 A 2016 study in the British Medical Journal reported that it is the third leading cause of death in the United States, ranking just be- hind cardiovascular disease and cancer.29 Some companies already claim their artificial intelligence systems outperform doctors, and that claim is not hard to swallow.30 Why should a computer not be able to outperform doctors when the computer can access the entire wealth of medical literature with perfect recall, benefit from the experience of directly having treated millions of patients, and be immune to fatigue?31
This Article is divided into three Parts. Part I provides back- ground on the historical development of injuries caused by machines and how the law has evolved to address these harms. It discusses the role of tort law in injury prevention and the development of negli- gence and strict product liability. Part II argues that while some forms of automation should prevent accidents, tort law may act as a deter- rent to adopting safer technologies. To encourage automation and im- prove safety, this Part proposes a new categorization of “computer- generated torts” for a subset of machine injuries. This would apply to cases in which an autonomous computer, robot, or machine is occupy- ing the position of a reasonable person in the traditional negligence paradigm and where automation is likely to improve safety. This Part contends that the acts of computer tortfeasors should be evaluated under a negligence standard rather than under principles of product liability, and it goes on to propose rules for implementing the system.
26 Oncology and Genomics, IBM, https://www.ibm.com/watson/health/oncology-and-ge- nomics [https://perma.cc/Z6H7-S5W4].
27 Id.
28 See INST. OF MED., TO ERR IS HUMAN: BUILDING A SAFER HEALTH SYSTEM (Linda T. Kohn et al. eds., 2000); Martin A. Makary & Michael Daniel, Medical Error—The Third Leading Cause of Death in the US, 353 BMJ 2139, 2139 (2016). The landmark report published by the Institute of Medicine in 2000 was a wake-up call to the medical profession about the harmful effects of medical error. See INST. OF MED., supra. Yet the report was based on studies conducted in 1984 and 1992. See id.
29 Makary & Daniel, supra note 28, at 2143 fig.1.
30 Parloff, supra note 3. For example, Enlitic has a program for detecting and classifying
lung cancers which the company claims has already outperformed human radiologists. Id.
31 See, e.g., Saul N. Weingart et al., Epidemiology of Medical Error, 320 BMJ 774, 775 (2010) (discussing some of the causes of human medical error).
 
8 503 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
Finally, Part III argues that once computer operators become safer than people and automation is practical, the “reasonable computer” should become the new standard of care. It explains how this standard would work, argues the reasonable computer standard works better than a reasonable person using an autonomous machine, and consid- ers when the standard should apply to computer tortfeasors. At some point, computers will be so safe that the standard’s most significant effect would be to internalize the cost of accidents on human tortfeasors.
This Article is focused on the effects of automation on accidents, but automation implicates a host of social concerns. It is important that policymakers act to ensure that automation benefits everyone. Automation may increase productivity and wealth, but it may also contribute to unemployment, financial disparities, and decreased so- cial mobility. These and other concerns are certainly important to con- sider in the automation discussion, but tort liability may not be the best mechanism to address every issue related to automation.32
I. LIABILITY FOR MACHINE INJURIES
A. A Brief History
Injuries caused by machines are nothing new. For as long as peo- ple have used machines, injuries have resulted—and machines have been with us for quite some time. The earliest evidence of simple ma- chines—tools that redirect force to make work easier, like axes— dates back millions of years to the beginning of the Stone Age.33 In fact, the Stone Age is so named because it was characterized by the use of stone to make simple machines such as hand axes.34 The pri- mary function of these tools was to hunt and cut meat,35 but they were also used to facilitate violence against people.36 Machines used in the furtherance of intentional torts were likely used negligently as well.
32 See, e.g., Ryan Abbott & Bret Bogenschneider, Should Robots Pay Taxes? Tax Policy in the Age of Automation, 12 HARV. L. & POL’Y REV. 145 (2018) (arguing that the tax system incentivizes automation even in cases where it is not otherwise efficient and that automation decreases government tax revenue, and proposing changes to existing tax policies as a solution).
33 Kate Wong, Ancient Cut Marks Reveal Far Earlier Origin of Butchery, SCI. AM. (Aug. 11, 2010), https://www.scientificamerican.com/article/ancient-cutmarks-reveal-butchery/.
34 See Stone Age, MERRIAM-WEBSTER, https://www.merriam-webster.com/dictionary/ Stone%20Age [https://perma.cc/U6W4-M4M8]. See generally Sonia Harmand et al., 3.3-Million- Year-Old Stone Tools from Lomekwi 3, West Turkana, Kenya, 521 NATURE 310 (2015).
35 Wong, supra note 33.
36 M. Mirazo ́n Lahr et al., Inter-group Violence Among Early Holocene Hunter-Gatherers
of West Turkana, Kenya, 529 NATURE 394, 396 (2016).
 
2018] 504 THE REASONABLE COMPUTER 9
Given that home knife accidents led to about a third of a million emergency room visits in the United States in 2011 alone, it is not difficult to imagine that during the Stone Age these simple machines caused accidents.37
As history progressed, and the use and complexity of simple ma- chines grew, so too did the resultant injuries38: Mesopotamian sur- geons botched procedures,39 Greek construction zones were so dangerous they required physicians on site,40 and Egyptian embalmers accidently left instruments in their subjects.41 Such injuries continued unabated from the time complex machines were invented by the an- cient Chinese and Greeks to the time of the first modern industrial machines.42
The Industrial Revolution marked a turning point in the role of machines in society.43 Major technological advances occurred during
37 See Joe Yonan, Knife Injuries and Other Kitchen Mishaps Afflict Both Top Chefs and Everyday Cooks, WASH. POST (Jan. 7, 2013), https://www.washingtonpost.com/national/health- science/knife-injuries-and-other-kitchen-mishaps-afflict-both-top-chefs-and-everyday-cooks/ 2013/01/07/92e191f8-4af0-11e2-b709-667035ff9029_story.html.
38 See generally Y.C. CHIU, AN INTRODUCTION TO THE HISTORY OF PROJECT MANAGE- MENT 19–115 (2010) (discussing the use of technology in industrial activities). For example, al- most half a million people died building the Great Wall of China, although the number of these deaths due to machine injuries is unknown. Great Wall of China, HISTORY.COM, http:// www.history.com/topics/great-wall-of-china [https://perma.cc/7MP5-FJX5]. So common were machine and industrial injuries in the ancient world that ancient Greek, Roman, Arab, and Chi- nese laws provided for compensation schedules for accidents. See Gregory P. Guyton, A Brief History of Workers’ Compensation, 19 IOWA ORTHOPAEDIC J. 106, 106 (1999). Under ancient Arab law, “loss of a joint of the thumb was worth one-half the value of a finger. The loss of a penis was compensated by the amount of length lost, and the value an ear was based on its surface area.” Id.
39 See Emily K. Teall, Medicine and Doctoring in Ancient Mesopotamia, 3 GRAND VALLEY J. HIST. 1, 5 (2014). Unfortunately for these doctors, medical malpractice in Babylon was corpo- rally punishable. Allen D. Spiegel & Christopher R. Springer, Babylonian Medicine, Managed Care and Codex Hammurabi, Circa 1700 B.C., 22 J. COMMUNITY HEALTH 69, 81 (1997); see also GUIDO MAJNO, THE HEALING HAND: MAN AND WOUND IN THE ANCIENT WORLD 53 (1975).
40 DAVID MATZ, VOICES OF ANCIENT GREECE AND ROME: CONTEMPORARY ACCOUNTS OF DAILY LIFE 58 (2012).
41 Granted, this example involves cadavers rather than living patients, or so one hopes. Owen Jarus, Oops! Brain-Removal Tool Left in Mummy’s Skull, LIVE SCI. (Dec. 14, 2012, 8:03 AM), http://www.livescience.com/25536-mummy-brain-removal-tool.html/. It certainly portends modern medical malpractice cases involving retained surgical instruments. See, e.g., Atul A. Ga- wande et al., Risk Factors for Retained Instruments and Sponges After Surgery, 348 NEW ENG. J. MED. 229, 230 (2003).
42 Peter J. Lu, Early Precision Compound Machine from Ancient China, 304 SCIENCE 1638 (2004); cf. Russell Fowler, The Deep Roots of Workers’ Comp, 49 TENN. B.J. 10, 10–12 (2013) (discussing historical development of workers’ compensation schemes from the medieval through the modern era).
 43 Economists have argued the Industrial Revolution was “certainly the most important

10 505 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
this period in textiles, transportation, and iron making, which resulted in the development of machines for shaping materials and the rise of the factory system.44 It also resulted in a dramatic increase in the num- ber and severity of machine injuries.45 Working in industrial settings was a dangerous business, in part because employers often had mini- mal liability for employee harms.46 These dangerous working condi- tions persisted well into the twentieth century before the U.S. government began collecting data on work-related injuries in a sys- tematic way.47 In 1913, the Bureau of Labor estimated that 23,000 workers died from work-related injuries (albeit an imperfect proxy for machine injuries) out of a workforce of 38 million, which works out to a rate of 61 deaths per 100,000 workers.48
In the modern era, the rate of work-related injuries has declined significantly. In 2016, for example, the U.S. Bureau of Labor reported 5190 fatal work injuries, a rate of 3.6 per 100,000 workers.49 The rea- son for this decline is multifactorial: changes to tort liability, evolved societal and ethics norms that place a greater priority on human wel- fare, a modern system of regulations and criminal liability that pro- tects worker wellbeing, as well as improvements in safety technology. Yet despite significant progress in workplace safety, accidents are still a serious societal concern. Workplace accidents were responsible for
event in the history of humanity since the domestication of animals and plants, perhaps the most important since the invention of language. It bids fair to free us all, eventually.” Deirdre Mc- Closkey, Review of The Cambridge Economic History of Modern Britain, PRUDENTIA (Jan. 15, 2004), http://www.deirdremccloskey.com/articles/floud.php [https://perma.cc/UAP4-6ZZ3].
44 See generally History of Technology: The Industrial Revolution (1750–1900), EN- CYCLOPæDIA BRITANNICA, https://www.britannica.com/technology/history-of-technology/The-In- dustrial-Revolution-1750-1900 [https://perma.cc/QV7K-LKLK].
45 See generally HENRY ROGERS SEAGER, SOCIAL INSURANCE: A PROGRAM OF SOCIAL REFORM 24–52 (1910) (including a chapter on industrial accidents in a classic exposition of the philosophical movement for social insurance).
46 John S. Haller, Jr., Industrial Accidents—Worker Compensation Laws and the Medical Response, 148 WEST J. MED. 341–48 (1988); see also HORWITZ, 1780–1860, supra note 8, at 90. 47 See Progressive Era Investigations, U.S. DEP’T LAB., https://www.dol.gov/dol/aboutdol/ history/mono-regsafepart05.htm [https://perma.cc/HUT4-5WQE]. The first systematic U.S. sur- vey of workplace fatalities found that 526 workers died in “work accidents” in Allegheny County from July 1906 to June 1907. Improvements in Workplace Safety—United States, 1900–1999, 48 CDC MORBIDITY & MORTALITY WKLY. REP. 461, 461 (1999). Of those fatalities, 195 were steel-
workers. Id. Contrast that with 17 national steelworker fatalities in 1997. Id.
48 Improvements in Workplace Safety—United States, 1900–1999, supra note 47, at 461. The National Safety Council estimated that 18,000–21,000 workers died from work-related inju-
ries in 1912. Id.
49 BUREAU OF LABOR STATISTICS, U.S. DEP’T OF LABOR, NATIONAL CENSUS OF FATAL
OCCUPATIONAL INJURIES IN 2016, at 1 (2017), http://www.bls.gov/news.release/pdf/cfoi.pdf [https://perma.cc/2YMS-RA8F].
 
2018] 506 THE REASONABLE COMPUTER 11
approximately 4000 deaths in the United States in 2014 and a total cost of about $140 billion.50 More broadly, there were a total of almost 200,000 injury-related deaths in 2014 in the United States, with all un- intentional injuries costing some $850 billion.51 Unintentional injuries are the fourth leading cause of death.52
B. Tort Law as a Mechanism for Accident Prevention
Part of the reason for the decline in workplace injuries is that tort law now provides a stronger financial incentive for safer conduct. The law has evolved from a system designed to insulate employers and manufacturers from liability to one with greater regard for worker and consumer health.53
A tort is a harmful civil act, other than under contract, where one person is damaged by another, and it gives way to a right to sue.54 A variety of goals have been proposed for tort law: to reduce accidents, promote fairness, provide a peaceful means of dispute resolution, real- locate and spread losses, promote positive social values, and so forth.55 Whether tort law is the best means for achieving all of these goals is a matter of endless dispute.56 Jurists are united, however, in considering accident reduction as one of the central goals of tort law, if not the primary goal.57 By creating a framework for loss shifting from injured
50 NAT’L SAFETY COUNCIL, INJURY FACTS: 2016 EDITION 3, 8 (2016).
51 Id. Lost quality of life from those injuries is valued at an additional $3345.5 billion. Id. at
8.
52 Id. at 2.
53 Tort law primarily grew out of a focus on bodily injury and physical property damage,
but protection in modern times has been extended beyond the physical to include harm to emo- tional well-being, and economic loss.
The range of torts is as broad as human experience and includes such wrongful conduct as negligence (personal injury law for unintentional harm), intentional torts (e.g., assault, battery, trespass to land), products liability (defective products), abnormally dangerous activities liability (e.g., blasting, aerial pesticide spraying), nuisance (e.g., air, water, and noise pollution), defamation (libel and slander), pri- vacy invasion (private area intrusion and personal autonomy interference), and fraud (misrepresentation). Tort law study also includes consideration of legislative measures related to torts and alternatives to tort liability, for example, automobile no-fault compensation systems.
DOMINICK VETRI ET AL., TORT LAW AND PRACTICE 3 (5th ed. 2016).
54 See id. at 2. A tort governs loss shifting from injured victims to tortfeasors, and it dic-
tates who can sue and what they can sue for. See id. It is “the set of legal rules establishing liability and compensation for personal injury and death caused by the intentional or careless conduct of a third party.” Id.
55 See, e.g., Priest, supra note 9, at 645 n.23, 648.
56 See, e.g., Priest, supra note 10.
57 See Blomquist, supra note 10, at 628–29 (arguing that economic theory and moral phi-
 losophy both require accident reduction to be the primary aim of tort law).

12 507 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
victims to tortfeasors, tort law deters unsafe conduct.58 A purely finan- cially motivated rational actor will reduce potentially harmful activity to the extent that the cost of accidents exceeds the benefits of the activity.59
On a broader level, the law of torts is one of the primary ways in which society choses to allocate liability. And allocating liability has far-reaching and sometimes complex impacts on behavior. In its quest to reduce accidents, tort law can either accelerate the introduction of new technologies, as was the case with the use of glaucoma testing and pulse oximeters, or it can discourage the use of new technologies, as is usually the case where the standard of care is based on custom.60
Torts are typically categorized based on the level of fault they require (or based on the interests they protect). On one end of the spectrum are intentional torts involving intent to harm or malice; on the other are strict liability torts which do not require fault.61 Covering the “great mass of cases” in the middle are harms involving negligence.62
C. Negligence
The concept of negligence is the primary theory through which courts deal with accidents and unintended harms.63 In practice, to pre- vail in most personal injury cases, a plaintiff must prove by a prepon- derance of the evidence that the defendant owed the plaintiff a duty of reasonable care, the defendant breached that duty, the breach caused the plaintiff’s damages, and the plaintiff suffered compensable damages.64 This generally requires proof that the defendant acted neg- ligently, which is to say, acted unreasonably considering foreseeable risks. This standard is premised on what an objective and hypothetical “reasonable” person would have done under the same circum-
58 See Priest, supra note 9, at 648.
59 See United States v. Carroll Towing Co., 159 F.2d 169, 173 (2d Cir. 1947) (stating that
liability calculations should consider whether the probability of injury times potential damages is lower than the burden imposed).
60 See Helling v. Carey, 519 P.2d 981, 983 (Wash. 1974) (holding that the standard of care in the profession of ophthalmology should not insulate providers from failure to test for glaucoma); Parchomovsky & Stein, supra note 13, at 306 (discussing how the role of custom in
 tort law 61
62 63 64
impedes innovation).
Oliver Wendell Holmes, Jr., The Theory of Torts, 7 AM. L. REV 652, 653 (1873). Id.
See Thomas C. Grey, Accidental Torts, 54 VAND. L. REV. 1225, 1283–84 (2001). See RESTATEMENT (SECOND) OF TORTS § 281 (AM. LAW INST. 1965).

2018] 508 THE REASONABLE COMPUTER 13
stances.65 Thus, if the courts determined that a reasonable person would not have headed out to sea without a radio to warn of storm conditions,66 manufactured a ginger beer with a snail inside,67 or dropped heavy objects off the side of a building,68 then these activities could expose a defendant to liability.
Negligence strikes a balance between the interests of plaintiffs and defendants. Society has interests in reducing injuries and compen- sating victims as well as encouraging economic growth and progress.69 One way that tort law attempts to achieve this balance is by permit- ting recovery in negligence only where there has been socially blame- worthy conduct.70 Thus, where a defendant has acted reasonably, even if the defendant has caused serious injury to a plaintiff, there will gen- erally be no liability. Juries play a key role in determining the reasona- ble person standard as applied to the facts of a case.71
D. Strict and Product Liability
While negligence governs virtually all accidents, there are excep- tions. For instance, defendants may be strictly liable for harms they cause as a result of certain types of activities such as hazardous waste disposal or blasting.72 Strict liability is a theory of liability without fault; it is essentially based on causation without regard to whether a defendant’s conduct is socially blameworthy.73 Thus, a defendant cor-
65 The idea that negligence involves conduct that falls below an objective standard was first articulated by Baron Alderson in the case of Blyth v. Birmingham Waterworks Co.:
Negligence is the omission to do something which a reasonable man, guided upon those considerations which ordinarily regulate the conduct of human affairs, would do, or doing something which a prudent and reasonable man would not do. The defendants might have been liable for negligence, if, unintentionally, they omitted to do that which a reasonable person would have done, or did that which a person taking reasonable precautions would not have done.
(1856) 156 Eng. Rep. 1047, 1049; 11 Ex. 781, 784.
66 See The T.J. Hooper, 53 F.2d 107 (S.D.N.Y. 1931).
67 See Donoghue v. Stevenson [1932] AC 562 (HL) (appeal taken from Scot.).
68 See Byrne v. Boadle (1863) 159 Eng. Rep. 299.
69 VETRI ET AL., supra note 53, at 12.
70 See James Barr Ames, Law and Morals, 22 HARV. L. REV. 97, 99 (1908).
71 VETRI ET AL., supra note 53, at 10.
72 Id. at 11.
73 See Frederick Pollock, Duties of Insuring Safety: The Rule in Rylands v. Fletcher, 2 L.Q.
REV. 52, 53 (1886). While early English common law imposed strict liability for certain wrongs such as trespass, Rylands v. Fletcher (1868) 3 LRE & I App. 330 (HL), was the progenitor of the doctrine of strict liability for abnormally dangerous activities, and its ruling had a major impact on the development of tort law. Pollock, supra, at 52, 59. In the case, Fletcher’s reservoir burst and flooded a neighboring mine run by Rylands through no fault of Fletcher. Id. at 53. This court held that “the person who for his own purposes brings on his lands and collects and keeps there,
 
14 509 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
poration that takes every reasonable care to prevent injury before dusting crops may nevertheless find itself liable for injuries it causes to a bystander.
One of the most important modern applications of strict liability is to product liability. Product liability refers to responsibility for the commercial transfer of a product that causes harm because it is defec- tive or because its properties are falsely represented.74 Product inju- ries cause upwards of 200 million injuries a year in the United States.75 In most instances, members of the supply chain (e.g., manufacturers and retailers) are strictly liable for defective products.76 The bulk of product liability cases involve claims for damages against a manufac- turer or retailer by a person injured while using a product.77 Typically, a plaintiff will try to prove that an injury was the result of some inher- ent defect of a product or its marketing and that the product was flawed or falsely advertised.78 Defendants, in turn, attempt to prove that their products were reasonably designed, properly made, and ac- curately marketed.79 Defendants may argue that plaintiff injuries were the result of improper and unforeseeable use of the product or that something other than the product caused the harm.80
Product liability was not always governed by strict liability. Origi- nally, American courts followed the English doctrine of caveat emptor (let the buyer beware) for product liability claims, reflecting a national philosophy embracing individualism and free enterprise.81 Toward the end of the nineteenth century, however, states began increasingly em- ploying the doctrine of caveat venditor and an implied warranty of merchantable quality.82 Under this doctrine, “[s]elling for a sound price raises an implied warranty that the thing sold is free from de-
anything likely to do mischief if it escapes, must keep it in at his peril, and, if he does not do so, is prima facie answerable for all the damage which is the natural consequence of its escape.” Id. at 54. Critics of the case objected to its potential impact on economic activity. See, e.g., THOMAS C. GREY, FORMALISM AND PRAGMATISM IN AMERICAN LAW 248 (2014) (noting that many “prestig- ious judges and commentators” repudiated Rylands on the basis that “liberal principles of formal equality and economic freedom, or a devotion to economic development, required rejection of tort liability without fault”).
74 DAVID G. OWEN, PRODUCTS LIABILITY LAW 1 (3d ed. 2014).
75 Id. at 1.
76 Id. at 3.
77 Id.
78 Id.
79 Id.
80 Id.
81 Id. at 17–18.
82 Id. at 18.
 
2018] 510 THE REASONABLE COMPUTER 15
fects, known and unknown (to the seller).”83 Ultimately, the doctrine of implied warranty of merchantable quality was reduced to statutory form in the Uniform Sales Act of 1906.84 Yet even so, manufacturers were in large part able to avoid liability for defective products by ar- guing they lacked privity of contract with consumers.85 This was possi- ble because in most cases consumers purchased products from third- party retailers rather than directly from manufacturers.86
That changed in 1916 with the New York Court of Appeals deci- sion in MacPherson v. Buick Motor Co.87 The case involved a motorist who was injured when one of the wooden wheels of his Buick col- lapsed.88 He subsequently attempted to sue the manufacturer (Buick) rather than the dealership from which he purchased the vehicle. In rejecting a defense based on privity of contact, the court held that if the manufacturer of such a foreseeably dangerous product knows that it “will be used by persons other than the purchaser, and used without new tests, then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully.”89 MacPherson spurred negligence claims against manufacturers across the country as state courts one-by-one adopted MacPherson’s holding.90 This shift was ac- companied by growing public support for consumer protection to- gether with the understanding that liability would not unduly burden economic activity.91 Businesses are often in the best position to pre- vent product injuries and can distribute liability through insurance.92
In 1963, the Supreme Court of California decided Greenman v. Yuba Power Products, Inc.,93 which held that manufacturers of defec- tive products are strictly liable for injuries caused by such products.94
83 Id. (quoting S. Iron & Equip. Co. v. Bamberg, E. & W. Ry. Co., 149 S.E. 271, 278 (S.C. 1929)).
84 Id.; see U.C.C. § 2-314 (AM. LAW INST. & UNIF. LAW COMM’N 2014). See generally Friedrich Kessler, The Protection of the Consumer Under Modern Sales Law, Part 1, 74 YALE L.J. 262 (1964).
85 OWEN, supra note 74, at 18.
86 Id.
87 111 N.E. 1050 (N.Y. 1916).
88 Id. at 1051.
89 Id. at 1053.
90 OWEN, supra note 74, at 22. Maine was the last state to abolish the privity requirement
in negligence actions in 1982. Id.
91 See id. at 22–23.
92 See id.
93 377 P.2d 897 (Cal. 1963) (in bank).
94 Id. at 900. Of note, Justice Roger Traynor, who wrote the majority opinion in the case,
had suggested this strict liability rule nineteen years earlier in a concurring opinion in Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436 (Cal. 1944). He argued responsibility should “be
 
16 511 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
This case represents the birth of modern products liability law in America.95 After this decision, the doctrine of strict product liability spread rapidly across the nation in the 1960s, with the American Law Institute memorializing the rule in Section 402A of the Restatement (Second) of Torts.96
Of course, today’s products liability law is not as simple as this brief narrative suggests.97 It combines tort law (e.g., negligence, strict liability, and deceit), contract law (e.g., warranty), both common and statutory law (e.g., statutory sales law under Article 2 of the Uniform Commercial Code), and a hodgepodge of state “reform” acts.98 Since the 1960s, a variety of state statutes have attempted to reform prod- ucts liability law, often to limit the rights of consumers in order to protect manufacturers.99 For our purposes, however, it suffices to say that as a general matter, manufacturers and retailers are strictly liable for injuries caused by defective products.100
II. COMPUTER-GENERATED TORTS
A. Automation Will Prevent Accidents
On May 7, 2016, a Tesla driver was killed in the first known fatal crash of a self-driving car.101 Tesla reported that the autopilot system
fixed wherever it will most effectively reduce the hazards to life and health inherent in defective products that reach the market.” Id. at 440 (Traynor, J., concurring). A few years before this case, the Supreme Court of New Jersey found manufacturers strictly liable in warrantee to re- mote consumers in Henningsen v. Bloomfield Motors, Inc., 161 A.2d 69, 77, 84 (N.J. 1960).
95 OWEN, supra note 74, at 23.
96 RESTATEMENT (SECOND) OF TORTS § 402A (AM. LAW INST. 1965); see OWEN, supra note 74, at 23.
97 For a more comprehensive view on products liability, the American Law Institute pub- lished a Restatement specifically on products liability in 1998. RESTATEMENT (THIRD) OF TORTS: PRODUCTS LIABILITY (AM. LAW INST. 1998).
98 OWEN, supra note 74, at 4.
99 Id. at 23.
100 See Vandermark v. Ford Motor Co., 391 P.2d 168, 171–72 (Cal. 1964) (in bank) (“Retail-
ers like manufacturers are engaged in the business of distributing goods to the public. They are an integral part of the overall producing and marketing enterprise that should bear the cost of injuries resulting from defective products. In some cases the retailer may be the only member of that enterprise reasonably available to the injured plaintiff. In other cases the retailer himself may play a substantial part in insuring that the product is safe or may be in a position to exert pressure on the manufacturer to that end; the retailer’s strict liability thus serves as an added incentive to safety. Strict liability on the manufacturer and retailer alike affords maximum pro- tection to the injured plaintiff and works no injustice to the defendants, for they can adjust the costs of such protection between them in the course of their continuing business relationship.” (citation omitted)).
101 Sam Levin & Nicky Woolf, Tesla Driver Killed While Using Autopilot Was Watching Harry Potter, Witness Says, GUARDIAN (July 1, 2016, 1:43 PM), https://www.theguardian.com/
 
2018] 512 THE REASONABLE COMPUTER 17
did not apply the brakes after the car’s sensor system failed to detect an eighteen-wheel truck and trailer.102 The car attempted to drive full speed under the trailer and the bottom of the trailer impacted the car’s windshield.103 The driver, whom Tesla claims should have re- mained alert and who also failed to apply the brakes, may have been watching a Harry Potter movie at the time.104
Surveys of attitudes toward self-driving cars have produced mixed results, but they have often uncovered negative opinions.105 A survey by the American Automobile Association in March 2016 re- ported that three out of four U.S. drivers surveyed said they would feel “afraid” to ride in a self-driving car.106 Only one in five said they would trust a driverless car to drive itself while they were inside.107 Another recent survey found that most U.K. citizens would feel un- comfortable with self-driving vehicles on the road, and more than
technology/2016/jul/01/tesla-driver-killed-autopilot-self-driving-car-harry-potter; see Anjali Singhvi & Karl Russell, Inside the Self-Driving Tesla Fatal Accident, N.Y. TIMES (July 12, 2016), http://www.nytimes.com/interactive/2016/07/01/business/inside-tesla-accident.html?_r=0. This has been the first reported fatality, but not the only reported crash for which a self-driving vehicle has been at fault. See, e.g., Tan Weizhen, Self-Driving Car in Accident with Lorry at One-North, TODAY (Oct. 18, 2016), http://www.todayonline.com/singapore/self-driving-car-involved-acci- dent-one-north. Other, nonfatal accidents have been attributed to self-driving vehicles. See Dave Lee, Google Self-Driving Car Hits a Bus, BBC NEWS (Feb. 29, 2016), http://www.bbc.co.uk/news/ technology-35692845. The National Highway Traffic Safety Administration (“NHTSA”) investi- gated this accident and issued a report in January 2017 stating that “[a] safety-related defect trend has not been identified at this time and further examination of this issue does not appear to be warranted.” NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., INVESTI- GATION PE 16-007 (2017), https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF. The NHTSA found the accident was beyond the capabilities of the vehicle’s Autopilot and Auto- matic Emergency Breaking systems. Id. The report went on to state that overall crash rates decreased by nearly forty percent after installation of Tesla’s Autosteer technology. Id. at 10.
102 Levin & Woolf, supra note 101.
103 Id.
104 Id.
105 Similarly, a poll of 1869 registered voters in January 2016 by Morning Consult found
that forty-three percent of registered voters said self-driving cars were unsafe, while only thirty- two percent said they were safe. Amir Nasr & Fawn Johnson, Voters Aren’t Ready for Driverless Cars, Poll Shows, MORNING CONSULT (Feb. 8, 2016), https://morningconsult.com/2016/02/08/vot- ers-arent-ready-for-driverless-cars-poll-shows/. Fifty-one percent of respondents said they would not ride in a driverless car, while twenty-five percent said they would. Id.; see Paul Lienert, Tesla Crash Does Little to Sway Public Opinion on Self-Driving Cars, AUTOMOTIVE NEWS (July 29, 2016, 2:21 PM), http://www.autonews.com/article/20160729/OEM06/160729812/tesla-crash-does- little-to-sway-public-opinion-on-self-driving-cars (discussing the results of other surveys).
106 Erin Stepp, Three-Quarters of Americans “Afraid” to Ride in Self-Driving Vehicle, AAA NEWSROOM (Mar. 1, 2016), http://newsroom.aaa.com/2016/03/three-quarters-of-ameri- cans-afraid-to-ride-in-a-self-driving-vehicle/.
 107 Id.

18 513 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
three-quarters would want to retain a steering wheel.108 Regulators are more optimistic than the public, but they are still cautious.109 Until very recently, California required human drivers to be present in all self-driving cars being tested on public roads.110 Two laws passed in 2016, however, now permit unmanned vehicles to operate on public roads under certain circumstances.111
Yet much of the public discourse on self-driving cars is misguided. The critical issue is not whether computers are perfect (they are not), but whether they are safer than people (they are). Nearly all crashes involve human error.112 A human driver causes a fatality about every 100 million miles, resulting in tremendous human and financial costs.113 The U.S. Department of Transportation reports that more than 35,000 people died from motor vehicle accidents in the United States in 2015.114 It estimates the economic costs of those accidents at over $240 billion.115
By contrast, the Tesla fatality was the first known autopilot death in about 130 million miles driven by the system.116 It is also important to note that driverless technologies are in their infancy. Imagine how improved such technologies will be in ten years. One academic expert predicted in September 2016 that self-driving cars will be ten times safer than human drivers in three years, and one hundred times safer in ten years.117 At the point where automated cars are ten times safer
108 David Neal, Over Half of Brits Won’t Feel Safe Using the Streets with Driverless Cars, INQUIRER (Oct. 17, 2016), http://www.theinquirer.net/inquirer/news/2474351/over-half-of-brits- wont-feel-safe-using-the-streets-with-driverless-cars.
109 This caution is reflected, for example, in guidelines released in September 2016 by the Department of Transportation for safe design, development, and testing of self-driving cars. NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., U.S. DEP’T OF TRANSP., FEDERAL AUTOMATED VE- HICLES POLICY: ACCELERATING THE NEXT REVOLUTION IN ROADWAY SAFETY 5–7 (2016), https://www.transportation.gov/sites/dot.gov/files/docs/AV%20policy%20guidance%20PDF.pdf.
110 Susmita Baral, Driverless Car Laws in California Get Major Changes in September, INT’L BUS. TIMES (Oct. 3, 2016, 5:40 PM), http://www.ibtimes.com/driverless-car-laws-california- get-major-changes-september-2425689.
111 Id.
112 NAT’L HIGHWAY TRAFFIC SAFETY ADMIN., supra note 109, at 5.
113 ALEXANDER HARS, TOP MISCONCEPTIONS OF AUTONOMOUS CARS AND SELF-DRIVING
VEHICLES 1, 6 (2016), http://www.inventivio.com/innovationbriefs/2016-09/Top-misconceptions- of-self-driving-cars.pdf.
114 General Statistics, supra note 23.
115 Id.
116 A Tragic Loss, TESLA (June 30, 2016), https://www.tesla.com/en_GB/blog/tragic-loss
[https://perma.cc/LZ8X-UW2F].
117 Michael Belfiore, Self-Driving Cars Will Be 10x Safer Than Human Drivers in 3 Years,
MICHAEL BELFIORE BLOG (Sept. 20, 2016), http://michaelbelfiore.com/2016/09/20/self-driving- cars-will-be-10x-safer-than-human-drivers-in-3-years/ [https://perma.cc/4T78-CEWD]. Similarly,
 
2018] 514 THE REASONABLE COMPUTER 19
than human drivers, that could reduce the annual number of motor vehicle fatalities to about 3500. That was the conclusion of a report from the consulting firm McKinsey & Company, which predicted au- tonomous vehicles would reduce the number of auto deaths by about 30,000 a year.118 However, the report estimated that self-driving tech- nologies would not be adopted widely enough to permit this outcome until the middle of the century.119
B. Tort Liability Discourages Automation
To see why tort law discourages automation, it is important to look at the question of when it makes economic sense for a business to replace a human operator with a machine operator. In practice, it might be complex to calculate the cost of each operator. Human em- ployees have costs in excess of their salaries and wages, such as tax liability for employer portions of Social Security tax, Medicare tax, state and federal unemployment tax, and workers’ compensation; em- ployer portions of health insurance; paid holidays, vacations, and sick days; contributions toward retirement, pension, savings, and profit- sharing plans, etc.120 Computer costs may be simpler to estimate, but they may also be uncertain. In addition to purchase or license costs and taxes, there may be costs associated with repair, maintenance, and operation.
Added to the direct financial costs associated with employing an operator, there may be indirect financial and nonfinancial costs, known and unknown, that guide a decision.121 For example, a person may require vocational training or be unable to work due to sickness; a computer may require software updates or be unable to work due to malfunction. Human operators may result in greater expenses for le- gal fees, administrative and overhead costs, as well as compliance with regulatory and employment requirements.122 Automation may provide
Bob Lutz, former General Motors (“GM”) vice chairman, predicted that GM’s first autonomous cars would have an accident rate about ten percent of those of human drivers. Michelle Fox, Self- Driving Cars Safer than Those Driven by Humans: Bob Lutz, CNBC (Sept. 8, 2014, 3:30 PM), http://www.cnbc.com/2014/09/08/self-driving-cars-safer-than-those-driven-by-humans-bob-lutz. html.
118 Bertoncello & Wee, supra note 25.
119 Id.
120 See Bret N. Bogenschneider, The Effective Tax Rate of U.S. Persons by Income Level,
145 TAX NOTES 117, 118 (2014); see also WAYNE F. CASCIO, COSTING HUMAN RESOURCES (4th ed. 2000).
121 See ALFRED MARSHALL, PRINCIPLES OF ECONOMICS 368, 376 (8th ed. 1920).
122 See Cost of Small Business Employment, CTR. FOR ECON. & BUS. RES., www.cebr.com/
reports/cost-of-small-business-employment/ [https://perma.cc/V3F6-USE6].
 
20 515 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
tax benefits,123 but may infringe patents or result in negative public- ity.124 Whether to staff with a person or a machine may also take into account broader social policies. For instance, automation may pro- mote income inequality and unemployment. But businesses are re- quired to act in the best interests of shareholders, and most businesses interpret this duty as a mandate to maximize profit rather than to pro- mote social responsibility.125
The decision of whether to employ a computer or human opera- tor, even where the two are capable of functioning interchangeably, may therefore be a complex one. Nevertheless, these are precisely the sorts of decisions that businesses are skilled at making—estimating uncertain future costs relatively accurately and making decisions as rational economic actors.126 Tort liability will only be one factor to consider when deciding whether to employ a computer or human op- erator. But, in the aggregate, tort liability will influence automation.
As with some of these other factors, the costs of tort liability may not be straightforward. For instance, businesses may not be directly liable for harms caused by autonomous computers.127 The computer’s manufacturer and other members of the supply chain will generally be liable. By contrast, businesses will generally be liable for negligent harms caused by their employees, although businesses can attempt to limit this liability, for instance, by relying on independent contrac-
 123 See Abbott & Bogenschneider, supra note 32.
124 See, e.g., Kate Taylor, McDonald’s Ex-CEO Just Revealed a Terrifying Reality for Fast-
Food Workers, BUS. INSIDER (May 25, 2016, 10:05 AM), http://www.businessinsider.com/ mcdonalds-ex-ceo-takes-on-minimum-wage-2016-5 (discussing criticism of McDonald’s for re- placing workers with machines).
125 See generally Dodge v. Ford Motor Co., 170 N.W. 668, 682–84 (Mich. 1919). Of course, many companies argue they promote corporate social responsibility, and in some circumstances, there may be a business case for doing so. See, e.g., Archie B. Carroll & Kareem M. Shabana, The Business Case for Corporate Social Responsibility: A Review of Concepts, Research and Practice, 12 INT’L J. MGMT. REVS. 85 (2010).
126 See, e.g., Hugh Courtney, Jane Kirkland & Patrick Viguerie, Strategy Under Uncertainty, HARV. BUS. REV., Nov.–Dec. 1997.
127 See Mark A. Chinen, The Co-Evolution of Autonomous Machines and Legal Responsi- bility, 20 VA. J.L. & TECH. 338, 347–48 (2016).

2018] 516 THE REASONABLE COMPUTER 21
tors.128 Businesses are not usually liable for negligent harms caused by their independent contractors.129
Yet even in cases where liability rests with a business’s supplier or an independent contractor, such liability may indirectly impact a busi- ness. A manufacturer or retailer may pass along its costs in the form of higher prices, or a business may need to pay an independent con- tractor more than an employee to have the contractor assume risk. The percentage of cost passed on to the business or consumer will depend on the market and price elasticity for that product.130 Yet the fact that tort liability may be indirect and complex or that firms may purchase insurance to manage risk does not change the fact that tort liability has a financial cost which influences behavior.
Leaving aside tort liability, if both operators cost a business the same amount to employ, the decision of whether to utilize a person or computer should be neutral. If a business introduces the variable of tort liability into the decision, a human operator would be preferred. Harms caused by a person will be evaluated in negligence, but the same harms caused by a computer will be evaluated in strict liability. It is easier to establish strict liability than negligence.131 Strict liability does not require careless manufacturer behavior, only that a defect be present in a product.132 At least with regard to tort liability, the law
128 See, e.g., Kleeman v. Rheingold, 614 N.E.2d 712, 715 (N.Y. 1993). There are, however, limits on the extent to which businesses can rely on independent contractors or attempt to clas- sify employees as independent contractors. See, e.g., In re Morton, 30 N.E.2d 369, 371 (N.Y. 1940). As another example of how business can avoid tort liability for the actions of human operators, employers are not generally liable for intentional torts committed by employees. See, e.g., Ocana v. Am. Furniture Co., 91 P.3d 58, 70–71 (N.M. 2004).
129 See Kleeman, 614 N.E.2d at 715.
130 See generally RBB ECONOMICS, COST PASS-THROUGH: THEORY, MEASUREMENT, AND
POTENTIAL POLICY IMPLICATIONS (2014).
131 See Cronin v. J.B.E. Olson Corp., 501 P.2d 1153, 1162 (Cal. 1972) (in bank) (“[T]he very purpose of our pioneering efforts in [strict product liability] was to relieve the plaintiff from problems of proof inherent in pursuing negligence and warranty remedies, and thereby ‘to insure that the costs of injuries resulting from defective products are borne by the manufacturers . . . .’” (ellipsis in original) (citations omitted) (quoting Greenman v. Yuba Power Prods., Inc., 377 P.2d 897, 901 (Cal. 1963))); see also Escola v. Coca Cola Bottling Co. of Fresno, 150 P.2d 436, 441 (Cal. 1944) (Traynor, J., concurring) (“It is to the public interest to discourage the marketing of products having defects that are a menace to the public. If such products nevertheless find their way into the market it is to the public interest to place the responsibility for whatever injury they may cause upon the manufacturer, who, even if he is not negligent in the manufacture of the product, is responsible for its reaching the market. However intermittently such injuries may occur and however haphazardly they may strike, the risk of their occurrence is a constant risk and a general one. Against such a risk there should be general and constant protection and the manufacturer is best situated to afford such protection.”).
 132 See Cronin, 501 P.2d at 1162.

22 517 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
thus favors people over machines. This will hold true as long as com- puters are treated as “ordinary products” as to which strict liability is the default rule.
C. Computer-Generated Torts Should Be Negligence Based
Holding computer-generated torts to a negligence standard will result in an improved outcome: it will accelerate the adoption of auto- mation where doing so would reduce accidents. Of course, moving from a strict liability to a negligence standard would have some draw- backs. As mentioned earlier, strict liability creates a stronger incentive for manufacturers to make safer products, and manufacturers may be better positioned than consumers to insure against loss. Indeed, this is why courts initially adopted strict product liability.133 Computer-gen- erated torts, however, differ from other product harms in that—once machines become safer than people—automation will result in net safety gains.
To illustrate this, imagine that with current technology a com- puter driver would be ten times safer than a human driver. In this case, it would be better that one human driver is replaced by a ma- chine than that the same machine becomes 100 times safer than a human driver. To see why that is so, assume a closed system with only two vehicles, where the risk of injury for a human driver is one fatality per 100 million miles driven and the risk of injury for a computer driver (model C-A) is one fatality per 1 billion miles driven. C-A is ten times safer than a person. Over the course of ten billion miles driven by the person and C-A, there will be an average of 110 fatalities.
Now imagine that we are able to improve C-A an additional ten- fold such that its risk of causing injury is reduced to one fatality per 10 billion miles (C-A+). Then, over the course of 10 billion miles driven by the person and C-A+, there will be a total of 101 fatalities. If, how- ever, instead of focusing our efforts on improving C-A we simply re- place the human driver with another C-A, then over the course of 10 billion miles driven by C-A & C-A there will be a total of 20 fatalities. Once computers become safer than people, and particularly once computers become substantially safer than people, very significant re- ductions in accident rates will be gained by automation. Therefore—at some point—it is preferable to weaken the incentive to gain incremen-
 133 See, e.g., Greenman, 377 P.2d at 901.

2018] 518 THE REASONABLE COMPUTER 23
tal improvements in product safety to increase the adoption of safer technologies.
Also, even under a negligence standard, manufacturers will be incentivized to improve the safety of their computer systems because they may still be liable for accidents. Manufacturers will likely have the best information available to determine whether it would be bet- ter to pay to further reduce accident risks, e.g., whether an additional $10,000 per vehicle is worth a one percent reduction in accident risk, or whether to pay claims for additional accidents. Higher safety levels are not always better; inefficiently high safety levels may result in pro- hibitively high prices for consumers.134 To the extent that society is not satisfied with a manufacturer’s risk-benefit analysis on optimum safety levels, non-tort mechanisms could be brought to bear, such as regula- tory mandates for minimum safety standards. Finally, to the extent that risk spreading is a concern, even though businesses may be better positioned to acquire insurance, consumers also have options to purchase insurance, particularly in the automobile context.135
There is further justification for separating out harms caused by ordinary products like MacPherson’s Buick and “computer tortfeasors” like Tesla’s autonomous driving software. Society’s rela- tionship with technology has changed. Computers are no longer just inert tools directed by individuals. Rather, in at least some instances, computers are taking over activities once performed by people and causing the same sorts of harm these activities generate. In other words, computers are stepping into the shoes of a reasonable person.
What distinguishes an ordinary product from a computer tortfeasor in this system are the concepts of independence and con- trol. Autonomous computers, robots, or machines are given tasks to complete, but they determine for themselves the means of completing those tasks.136 In some instances, machine learning can generate un- predictable behavior such that the means are not predictable either by those giving tasks to computers or even by the computer’s original programmers.137 But the difference between ordinary products and
134 David G. Owen, Rethinking the Policies of Strict Products Liability, 33 VAND. L. REV. 681, 710 (1980).
135 Id. at 694.
136 Curtis E.A. Karnow, The Application of Traditional Tort Theory to Embodied Machine
Intelligence, in ROBOT LAW 51, 52 (Ryan Calo et al. eds., 2016).
137 Id. Unlike Karnow, the author does not agree that the relevant distinction between autonomous and nonautonomous machines should be the degree to which they are unpredict- able. See id. at 55. Tort law should pursue functional solutions, and for the purposes of accident reduction, it should not matter whether a self-driving car operates per expert rules or per unpre-
 
24 519 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
autonomous computers should not be based on predictability, only on social and practical outcomes.138 It makes no difference to a person run over by a self-driving car what type of computer was operating the vehicle. Whether a computer acts according to fixed or expert rules created by a programmer or more complex machine-learning algo- rithms such as neural networks that generate new and sometimes un- foreseen behaviors, the physical outcome is the same.139 Leaving aside difficulties with courts attempting to distinguish between different types of computer architecture, ultimately, the goals of tort law should be functional. Tort law should aspire to lower accident rates, not to create a formalistically pure theory of autonomy.
D. Computer-Generated Torts as a Type of Machine Injury
Not all machine injuries would be computer-generated torts. To illustrate, consider two hypothetical accidents:
1) A crane operator drops a steel frame on a passerby after incorrectly identifying the location for drop off.
2) A crane operator is manipulating a crane under normal conditions when it tips over and lands on a passerby.
In the first example, as between the machine and the operator, it seems obvious (and one may assume) that the operator is at fault (al- though a creative plaintiff’s attorney might argue the crane was negli- gently designed to allow such an outcome). While the accident could not have occurred without the machine’s involvement, making it a fac- tual cause of the injury in torts vernacular, the machine did not inter- rupt a direct and foreseeable chain of events set in motion by the operator’s action. The machine is essentially functioning as an exten- sion of the operator, in the same way that the operator could commit a battery by throwing a rock at another person.140 In the second hypo- thetical, allocating fault is once again intuitively obvious. The machine
dictable machine-learning algorithms. See Abbott, supra note 19, at 1109 (arguing in the patent context that it would be impossible or impractical to distinguish between different computer architectures for determining whether a computer qualifies as an inventor and that the distinc- tion is irrelevant to promoting innovation).
138 Cf., e.g., David C. Vladeck, Machines Without Principals: Liability Rules and Artificial Intelligence, 89 WASH. L. REV. 117, 127 (2014) (arguing different liability rules may need to apply to injuries caused by computers that cannot be traced to a “design, manufacturing, or programming defect”).
139 See, e.g., Jack M. Balkin, The Path of Robotics Law, 6 CALIF. L. REV. CIR. 45, 45–46 (2015) (arguing against a focus on formalism and essentialism in the law).
140 See, e.g., R v. Day (1845) 1 Cox 207, 208 (holding that slashing a victim’s clothing with a knife constitutes battery).
 
2018] 520 THE REASONABLE COMPUTER 25
is at fault rather than the operator. The operator acted with reasona- ble care, and the injury was due to (one may assume) a flawed crane. These two scenarios would result in very different liability out- comes. In the first, the operator, and possibly the operator’s employer, would be liable to the passerby in negligence because the operator failed to exercise reasonable care. In the second, the manufacturer and retailer of the crane would be strictly liable to the passerby even if the manufacturer had exercised the utmost care in the design and con-
struction of the crane.
In both scenarios, an operator is using a crane in much the same
way cranes have been used in construction for thousands of years. Granted, today’s cranes utilize more sophisticated designs, are built from sturdier materials, and have electric power, but the basic dy- namic between person and machine has not changed much. The cranes used to build skyscrapers, the pulleys used to build the Giza Pyramids, and the cranes used to build the Parthenon all involved human operators controlling the movements of a simple or complex machine to redirect and amplify force.141
Now imagine a third scenario:
3) A computer-operated, unmanned crane drops a steel frame on a passerby after incorrectly identifying the location for drop off.
The law now treats Examples 2 and 3 the same way because they both involve defective products. Yet in important respects, Examples 1 and 3 are more closely related. Both Examples 1 and 3 involve the same sort of action and the same physical result. In Example 2, a ma- chine is being used as a tool. In Example 3, a computer has stepped into the shoes of the worker; it has replaced a person, and it is per- forming in essentially the same manner as a person. If the computer were a person, the computer would be liable in negligence and held to the standard of a reasonable person.142
Holding computer tortfeasors to a negligence standard requires rules for distinguishing between computer-generated torts and other
141 See J.J. Coulton, Lifting in Early Greek Architecture, 94 J. HELLENIC STUD. 1, 1, 12, 15–17 (1974).
142 The author has previously argued for a similar rule in the intellectual property context, where he proposed that computers should be recognized as authors and inventors if they inde- pendently perform creative acts. See Ryan Abbott, Hal the Inventor: Big Data and Its Use by Artificial Intelligence, in BIG DATA IS NOT A MONOLITH 187, 187 (Cassidy R. Sugimoto et al. eds., 2016); Abbott, supra note 19, at 1081. This rule would generate innovation by creating financial incentives for developing creative computers. See Abbott, supra; Abbott, supra note 19, at 1081.
 
26 521 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
harms. The goal is to distinguish between cases in which a machine is used as a mere instrument and a person is at fault (Example 1), cases in which an ordinary product is at fault (Example 2), and cases in which there is a “computer tortfeasor” (Example 3).
Computer-generated torts could be those cases in which an au- tonomous computer occupies the position of a reasonable person in the negligence calculus and where automating promotes safety. It is only beneficial to encourage automation when doing so would reduce accidents. It would be harmful to encourage automation while human drivers outperform self-driving cars (though, it might still be beneficial to encourage automation for a subset of cases, for instance, the class of bad drivers). To shift from strict liability to negligence, manufactur- ers should have the burden to show by a preponderance of the evi- dence that a computer tortfeasor is safer on average than a person.
E. Implementation
Automation may occur on a more or less permanent basis, or it may be situational. For example, an autonomous vehicle may only permit machine control, or it may allow a person to switch between human and machine control. Where automation is all-or-nothing, the relevant inquiry should be whether a specific instance of automation would be expected to result in a net reduction in accidents, rather than to reduce the risk of the specific harm that occurred. For instance, if self-driving cars were better than people at avoiding collisions with other vehicles, but worse at avoiding collisions specifically with white cars, a computer driver might decrease the overall risk of accidents but increase the risk of colliding with white cars. In a case involving a collision with a white car, a negligence standard should still apply. Better that there should be more collisions with white cars so long as there are fewer collisions in total (assuming collisions with white cars do not result in disproportionate harm).
Even where automation is situational, it makes sense to apply a negligence standard. Hypothetically, if a self-driving car is on average ten times safer than a person, but only half as safe as a person in rainy conditions, a person should rely on autonomous driving software most of the time but operate the vehicle conventionally in the rain. If some- one instead uses self-driving software in the rain, the computer should still be evaluated under a negligence standard. It may be difficult for a user to know in advance what circumstances an autonomous computer is likely to encounter as well as when an autonomous computer will outperform a person. In addition, the manufacturer—as the liable

2018] 522 THE REASONABLE COMPUTER 27
party—may not have input into how its computers are used situation- ally. Manufacturers could utilize non-tort mechanisms to prevent un- safe uses, such as by warning users that self-driving cars may not be operated in the rain or by building in technological safeguards to pre- vent self-driving cars from operating in the rain. If self-driving cars prove to be less safe than human drivers in the rain, it is likely manu- facturers would still be liable for accidents in negligence.
Similarly, software used to diagnose disease based on medical imaging may outperform physicians generally, but underperform at detecting certain diseases. Ideally, this might result in human-machine collaborative review of imaging. If a machine were to underperform detecting lung cancer, for example, it should still be evaluated in negli- gence for its failures. The computer will likely be liable if a physician should have detected the lung cancer. In instances where a computer is generally safer than a person but underperforms in a certain area, it is likely to be liable in negligence when underperforming. This retains the ex ante incentive to improve an autonomous computer to reduce accidents and still allows victims to be compensated.
The basic inquiry about automation safety should focus on whether automation reduces, or is expected to reduce, overall acci- dents, not whether it did in fact reduce accidents in a specific instance. If Tesla can prove its self-driving cars are more likely safer overall than human drivers, this should be sufficient to shift to negligence even in a case where a particular substitution of a human driver with a self-driving car results in more accidents. Better that there should be fewer accidents in total even if one normal self-driving car gets in more accidents than the class average.
This new standard might sometimes involve complex problems of proof. A manufacturer would have the initial burden to prove its com- puters are safer than people, which creates an incentive to misrepre- sent a computer’s safety.143 Even when manufacturers are acting in good faith, it may be difficult to determine whether a computer is safer than a person. Research conducted to the highest scientific stan- dards sometimes fails to accurately predict real-world outcomes.144 It may be that Tesla has reason to believe its self-driving cars are signifi- cantly safer than human drivers, but once its cars enter the market-
143 Ryan Abbott, Big Data and Pharmacovigilance: Using Health Information Exchanges to Revolutionize Drug Safety, 99 IOWA L. REV. 225, 232–37 (2013) (discussing differences between premarket and postmarket data for evaluating safety in the pharmaceutical context and the in- centive for manufacturers to misrepresent safety profiles).
 144 Id.

28 523 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
place, they fail to meet expectations. For instance, Tesla’s research might fail to consider the reactions of drivers to self-driving vehicles in states other than California.145 In practice, automation may turn out to be safer or more dangerous than initially predicted. Decisions often must be made based on incomplete information, and waiting for per- fect knowledge risks sacrificing probable benefits at the altar of precaution.146
Adversarial legal proceedings are well suited for resolving such factual issues, and plaintiffs could use those proceedings to challenge manufacturer claims of safety.147 Thus, if Tesla presents evidence that its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs show that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles, that should shift the standard back to strict liability. It is worth noting that postmarket data is not always superior to premarket data; sometimes premarket data may be more predictive of future outcomes, particularly where postmarket data is limited or skewed.148
145 For example, although Google’s self-driving vehicles have been involved in accidents, nearly all accidents involving these vehicles have been the fault of human drivers. Chris Ziegler, A Google Self-Driving Car Caused a Crash for the First Time, VERGE (Feb. 29, 2016, 1:50 PM), http://www.theverge.com/2016/2/29/11134344/google-self-driving-car-crash-report. Pre-2017 monthly reports of accidents involving Google’s self-driving cars were originally available on Google’s website. See Steve Kovach, Google Quietly Stopped Publishing Monthly Accident Re- ports for Its Self Driving Cars, BUS. INSIDER (Jan. 18, 2017, 6:32 PM), http:// www.businessinsider.com/waymo-ends-publishing-self-driving-car-accident-reports-website- 2017-1. However, in 2017 the Google Self-Driving Car Project rebranded as Waymo, and Waymo no longer publishes monthly accident reports. See id.
146 Ryan Abbott & Ian Ayres, Evidence and Extrapolation: Mechanisms for Regulating Off- Label Uses of Drugs and Devices, 64 DUKE. L.J. 377, 380 (2014).
147 See Abbott, supra note 143, at 266 (discussing benefits of adversarial dispute resolu- tion). Alternately, manufacturers could have a duty to evaluate the safety of automation technol- ogies before sale and an ongoing duty to monitor their postmarket performance. This could mean that instead of plaintiffs and defendants engaging in a “battle of the experts” focused on objective safety outcomes, a manufacturer’s good faith belief that its computers were safe would be sufficient to give rise to a negligence standard. Plaintiffs could only rebut the presumption that a manufacturer acted in good faith. Thus, Tesla would remain liable in negligence if it could prove its vehicles were predicted to cause a fatality every 200 million miles, but plaintiffs could prove that Tesla’s self-driving vehicles actually caused a fatality every 50 million miles. Unless plaintiffs could prove Tesla knew, or should have known, that its initial predictions were not accurate or prove that Tesla failed to monitor the performance of its cars, Tesla would not be liable. But this would create a greater risk that manufacturers would fail to aggressively monitor, or that manufacturers would fail to monitor appropriately despite their best efforts. Better to base the standard on objective evidence of safety than a manufacturer’s subjective knowledge. Better also to empower plaintiffs’ attorneys to hold manufacturers to account than to put foxes in charge of guarding henhouses.
148 See generally Ryan Abbott, The Sentinel Initiative as a Cultural Commons, in GOV- ERNING MEDICAL KNOWLEDGE COMMONS (Katherine J. Strandburg et al. eds., 2017), https://
 
2018] 524 THE REASONABLE COMPUTER 29
It should not be necessary for a computer tortfeasor to physically replace a human operator for negligence to apply. It should be suffi- cient that a computer is performing a task which a person could rea- sonably do. For example, if a new taxi company goes into business using a fleet of only self-driving vehicles, computers would not have replaced human operators, but they would be doing work that human drivers could have done. By contrast, the portions of the taxis other than the self-driving software, e.g., the engine, could not be reasona- bly substituted. A person could drive a taxi instead of a computer, but a person could not reasonably replace the entire vehicle. So, the software operating the self-driving taxi could qualify as a computer tortfeasor, but the other parts of the vehicle would not.
Once a manufacturer establishes that a computer tortfeasor is safer than a person, the negligence test should focus on whether the computer’s act was negligent, rather than whether the computer was negligently designed or marketed. Again, the computer is taking the place of a person in the traditional negligence paradigm, and this par- adigm would treat the computer more like a person than a product. It makes no difference to an accident victim what a computer was “thinking”; only how the computer acted.149 Accident victims have a right to demand careful conduct regardless of how well a computer tortfeasor may have been designed.150
Applying the above rules to the crane examples, Example 1 would result in human liability because the human operator acted carelessly and the crane did not interrupt a foreseeable chain of events. It would retain strict manufacturer liability for Example 2 be- cause a person could not reasonably be substituted for a crane. It would permit negligent manufacturer liability for Example 3 (because the computer was automating a task which a person could have per- formed), but only if the computer tortfeasor is on average safer than a human operator.
www.cambridge.org/core/books/governing-medical-knowledge-commons/sentinel-initiative-as-a- knowledge-commons/FE736CE30779C4FFE5BA740F2A0FBBFE/core-reader (discussing diffi- culties with using real-world data to predict safety outcomes in an example using the medication Dabigatran).
149 To appropriate criminal law terminology, we are interested in the actus reus rather than the mens rea. See generally DENNIS J. BAKER, TEXTBOOK OF CRIMINAL LAW 167 (3d ed. 2012) (explaining the concept of actus reus). There is no benefit to punishing computer tortfeasors for wrongful actions, even under civil law.
150 See Oliver Wendell Holmes, Lecture III: Torts—Trespass and Negligence, in 3 THE COL- LECTED WORKS OF JUSTICE HOLMES 154, 157–58 (Sheldon M. Novick ed., 1995).
 
30 525 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
In the context of automated driving, human drivers would be lia- ble for harms they cause due to their own driving decisions, while a manufacturer would be strictly liable for harms caused by defective machines that are not automating human functions (as would be the case for MacPherson’s Buick151), but manufacturers would be liable in negligence rather than strict liability for errors made by autonomous driving software if the software were proven safer on average than a person.
F. Financial Liability
Autonomy exists on a continuum. In practice, the divide between an ordinary product and an autonomous computer may not be clear cut. In the self-driving car context, for example, under one widely adopted framework, vehicles are categorized on a zero to five scale based on who does what, when.152 At level zero, the human driver does everything; at level five, the vehicle can perform all driving tasks under all conditions that a human driver could perform. In between, there are various degrees of assistance, control, and interaction be- tween person and machine. When computers and people share deci- sionmaking, traditional principles of joint and several liability should apply.153 For instance, where a human driver and a computer driver are both at fault, as may be the case where a Tesla system fails to detect a truck while a human driver is watching a movie, both drivers could be liable for either the entire injury or in proportion to their wrongdoing.154
Whether in strict liability or negligence, computers could not be financially liable for their harms. Computers do not have property rights, are owned as chattel, and would not be influenced by the spec- ter of liability in the way a person might be influenced. For the pur- poses of financial liability, the computer’s manufacturer and other members of the supply chain should still be responsible for satisfying judgments under standard principles of product liability law. Product liability law already has rules for allocating liability in complex cases where several parties contribute to the design and production of an ordinary product or where several parties are involved in the distribu-
151 See supra notes 87–89 and accompanying text.
152 See SAE INT’L, AUTOMATED DRIVING (2014) (on file with the Law Review) (describing
the SAE taxonomy).
153 See generally Richard W. Wright, The Logic and Fairness of Joint and Several Liability, 23 MEM. ST. U. L. REV. 45 (1992) (reviewing and analyzing the public policy debate over joint and several liability).
 154 Id. at 46.

2018] 526 THE REASONABLE COMPUTER 31
tion chain. For example, those rules could apply in a case in which Apple and Delphi jointly design self-driving car software, which Gen- eral Motors licenses and incorporates in its vehicles, and the vehicles are then leased by an independent retailer to Lyft. Common law liabil- ity rules could be altered by firms in the supply chain. That would be particularly likely to occur where manufacturers and retailers are large, sophisticated entities. For example, General Motors might in- demnify Apple, Delphi, and Lyft in return for more favorable licens- ing and leasing terms.
Alternately, the computer’s owner could be liable for its harms. That would be somewhat akin to treating computer tortfeasors as em- ployees and making owners liable under theories of vicarious liabil- ity.155 It is particularly easy to imagine owners purchasing insurance for harms caused by autonomous computers in the self-driving car context, where insurance policies may soon come with a rider (or dis- count) for autonomous software. Owner liability might further incen- tivize the production of autonomous computers given that manufacturers would have less liability, but it might reduce adoption because owners would be taking on that liability. These two effects might offset each other if reduced manufacturer liability were to result in lower purchase prices. Ultimately, owner liability is not an ideal solution because owners may be the most likely victims of computer tortfeasors, and because manufacturers are in the best position to im- prove product safety and to weigh the risks and benefits of new technologies.
In practice, the economic impact of different liability standards for accidents by self-driving cars will be seen in the cost of insurance. Insurers base their premiums on risk, and once self-driving cars be- come significantly safer than human drivers, insurance rates will de- crease for self-driving cars and perhaps increase for human drivers.156 This should have a nudging effect on self-driving car adoption as fi- nancially sensitive individuals take auto premiums into account in de- ciding whether to drive. To the extent self-driving cars are judged under a more lenient negligence standard, we would expect lower pre- miums for self-driving cars, further incentivizing their adoption. If manufacturers and retailers rather than car owners are held responsi- ble for accidents, the burden of insurance would shift from owners to manufacturers, although this cost may then be reflected in higher car purchase prices.
155 See generally Fleming James, Jr., Vicarious Liability, 28 TUL. L. REV. 161 (1954).
156 See supra text accompanying notes 112–19.
 
32 527 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 G. Alternatives to Negligence
Shifting from strict liability to negligence is not the only means of encouraging automation. The government could provide a variety of financial incentives to manufacturers and retailers to promote the cre- ation and sale of safer technologies. In other contexts, government incentives have been effective at promoting innovation.157 For exam- ple, incentives could take the form of grants for research and develop- ment,158 loans to build production facilities,159 enhanced intellectual property rights,160 prizes,161 preferential tax treatments,162 or govern- ment guarantees.163
The government could even provide credits to consumers to purchase self-driving cars. This could be modeled after the Car Allow- ance Rebate System (“CARS”), better known as “cash for clunkers.”164 CARS provided consumers trading in old vehicles with
157 See generally Nancy Gallini & Suzanne Scotchmer, Intellectual Property: When Is It the Best Incentive System?, in 2 INNOVATION POLICY AND THE ECONOMY 51 (Adam B. Jaffe et al. eds., 2002).
158 See, e.g., Daniel J. Hemel & Lisa Larrimore Ouellette, Beyond the Patents-Prizes De- bate, 92 TEX. L. REV. 303, 321 (2013) (discussing the role of government grants in innovation policy).
Today, direct federal R&D spending (which includes the very small amount cur- rently spent on prizes) is about $130–$140 billion per year—slightly more than half of which is defense-related. Many states also provide direct R&D support: in fiscal year 2009, states spent $3.6 billion on support for R&D at state universities and another $1.3 billion on other grants and facilities for in-state research.
Id. (footnote omitted).
159 See, e.g., Joe Stephens & Carol D. Leonnig, Solyndra: Politics Infused Obama Energy
Programs, WASH. POST (Dec. 25, 2011), https://www.washingtonpost.com/solyndra-politics-in fused-obama-energy-programs/2011/12/14/gIQA4HllHP_story.html?utm_term=.Bb171adb15da (providing background information on the billions in unexpected costs to taxpayers from contro- versial loans defaulted on by green technology programs).
160 See, e.g., Ryan Abbott, Treating the Health Care Crisis: Complementary and Alternative Medicine for PPACA, 14 DEPAUL J. HEALTH CARE L. 35, 62–98 (2011) (noting that pharmaceu- tical manufacturers can receive market exclusivity, extended patent terms, or even sui generis forms of intellectual property protection for preferred technologies).
161 See, e.g., Richard A. Posner, Intellectual Property: The Law and Economics Approach, 19 J. ECON. PERSP. 57, 58–59 (2005).
162 See, e.g., Nick Bloom et al., Do R&D Tax Credits Work? Evidence from a Panel of Countries 1979–1997, 85 J. PUB. ECON. 1, 2 (2002); Bronwyn Hall & John Van Reenen, How Effective Are Fiscal Incentives for R&D? A Review of the Evidence, 29 RES. POL’Y 449, 449 (2000).
163 See, e.g., Gunhild Berg & Michael Fuchs, Bank Financing of SMEs in Five Sub-Saharan African Countries: The Role of Competition, Innovation, and the Government (World Bank, Pol- icy Research Working Paper No. 6563, 2013).
164 TED GAYER & EMILY PARKER, CASH FOR CLUNKERS: AN EVALUATION OF THE CAR ALLOWANCE REBATE SYSTEM 1 (2013), https://www.brookings.edu/wp-content/uploads/2016/06/ cash_for_clunkers_evaluation_paper_gayer.pdf.
 
2018] 528 THE REASONABLE COMPUTER 33
vouchers of between $3500 and $4500 to purchase new cars.165 It was a nearly $3 billion U.S. federal program designed as a short-term eco- nomic stimulus and to benefit U.S. auto manufacturers.166 It was also intended to promote safer, cleaner, more fuel-efficient vehicles.167 Ul- timately, while critics dispute the effectiveness of the program at stim- ulating the economy and promoting domestically produced automobiles, it did succeed at improving fuel efficiency and safety, and it was popular with consumers.168 In a similar manner, consumers trading in conventional vehicles could be provided with a voucher to purchase self-driving cars.
Even if incentives are limited to tort liability, there are still alter- natives to shifting to negligence. For example, manufactures could have their liability limited through state or federal tort reform acts that place caps on damages, limit contingency fees, eliminate joint and several liability, mandate periodic payments, or reduce the statute of limitations.169
Finally, the government could promote safety by means of regula- tion. This could involve requirements for industries to achieve mini- mum safety targets or direct requirements to adopt certain technologies.170 At the point where self-driving cars become ten or a
165 Id.
166 See id. at 1–2; $2 Billion More for Clunker Car Trade-Ins Passes Senate, N.Y. TIMES: CAUCUS (Aug. 6, 2009, 9:05 PM), https://thecaucus.blogs.nytimes.com/2009/08/06/2-billion-more- for-clunker-car-trade-ins-passes-senate/.
167 See GAYER & PARKER, supra note 164, at 1–2.
168 The Department of Transportation reported the program succeeded at boosting eco-
nomic growth and creating jobs. Press Release, Nat’l Highway Traffic Safety Admin., Secretary LaHood Touts Success of Cash for Clunkers; Responds to Reports by DOT Inspector General, GAO (Apr. 29, 2010), https://www.nhtsa.gov/press-releases/secretary-lahood-touts-success-cash- clunkers-responds-reports-dot-inspector-general. Others were less bullish. One study found that the total costs of the program outweighed the benefits by $1.4 billion. See Burton A. Abrams & George R. Parsons, Is CARS a Clunker?, ECONOMISTS’ VOICE, Aug. 2009, at 4. Another study argued that the program increased short-term spending, but decreased overall spending on new cars. Mark Hoekstra et al., Cash for Corollas: When Stimulus Reduces Spending 23 (Nat’l Bureau of Econ. Research, NBER Working Paper Series No. 20349, 2014), http://www.nber.org/papers/ w20349.pdf. With regard to fuel efficiency, one study found that the program improved the aver- age fuel economy of all vehicles purchased by 0.6 mpg in July 2009, and by 0.7 mpg in August 2009. MICHAEL SIVAK & BRANDON SCHOETTLE, U. MICH. TRANSP. RESEARCH INST., THE EF- FECT OF THE “CASH FOR CLUNKERS” PROGRAM ON THE OVERALL FUEL ECONOMY OF PUR- CHASED NEW VEHICLES 4 (2009), http://deepblue.lib.umich.edu/bitstream/2027.42/64025/1/ 102323.pdf.
169 These are some of the reforms created by the Medical Injury Compensation Reform Act of 1975 (“MICRA”) enacted by the California legislature to lower medical malpractice lia- bility insurance premiums. Cal. Civ. Code §§ 3333–3333.2 (West 2016).
170 See generally HEALTH & SAFETY EXEC., A GUIDE TO HEALTH AND SAFETY REGULA- TION IN GREAT BRITAIN 11 (2013), http://www.hse.gov.uk/pubns/hse49.pdf (outlining the occu-
 
34 529 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
hundred times safer than human drivers, nonautonomous driving could be prohibited.171 Regulatory solutions may be most appropriate where the benefits of automation are overwhelming and where it is undisputed that automation would result in massive safety gains.
Yet there is reason to think that shifting to negligence may be a preferred mechanism. It is both a consumer- and business-friendly so- lution. While consumers would have more difficulty seeking to re- cover for accidents, they would also benefit from a reduced risk of accidents. Most consumers would probably prefer to avoid harm rather than to improve their odds of receiving compensation. For busi- nesses, it would lower costs associated with liability (which may also result in lower consumer prices). Shifting to negligence would not re- quire government funding, additional regulatory burdens on industry, or new administrative responsibilities. Additionally, it is an incremen- tal solution that relies on existing mechanisms for distributing liability and builds upon the established common law. There may be less risk that shifting to negligence would produce unexpected outcomes than more radical solutions.172 For all the above reasons, shifting to negli- gence should be a politically feasible solution.
Ultimately, to the extent that policymakers agree that automation should be promoted when it improves safety, there is no need to rely on a single mechanism. Negligence shifting could operate alongside government grants for research and development and consumer cred- its, combined with direct regulations in certain instances.
Shifting to negligence could be accomplished through legislation or judicial activism. Legislative implementation may be preferable be- cause it would be faster than waiting on courts, and legislatures may be better suited for establishing public policy.173 Indeed, automation
pational health and safety system in Great Britain and the various types of safety standards imposed on businesses).
171 See Stuart Dredge, Elon Musk: Self-Driving Cars Could Lead to Ban on Human Driv- ers, GUARDIAN (Mar. 18, 2015, 3:22 AM), https://www.theguardian.com/technology/2015/mar/18/ elon-musk-self-driving-cars-ban-human-drivers.
172 Indeed, some critics argued that CARS primarily subsidized Japanese auto manufactur- ers, while a similar Japanese stimulus program excluded American auto manufacturers. John Crawley, Japanese, Koreans Gain Most from Cash for Clunkers, REUTERS (Aug. 26, 2009, 5:34 PM), http://www.reuters.com/article/retire-us-usa-clunkers-sales-idUSTRE57P5C220090826; Douglas Stanglin, U.S. Cars Excluded from Japan’s Cash-for-Clunkers Program, USA TODAY (Dec. 11, 2009, 2:09 PM), http://content.usatoday.com/communities/ondeadline/post/2009/12/us- cars-excluded-from-japans-cash-for-clunkers-program-/1#.WDwOQXfc-t8.
173 See, e.g., Scherer, supra note 17, at 389–90 (discussing the reactionary nature of court proceedings); see also Bibb v. Navajo Freight Lines, Inc., 359 U.S. 520, 524 (1959) (“Policy deci- sions are for the . . . legislature . . . .”).
 
2018] 530 THE REASONABLE COMPUTER 35
to improve public safety is precisely the sort of activity that lawmakers should facilitate because it benefits the general welfare. If legislatures fail to act, courts could independently adopt these rules. Lawmakers would then have the option of modifying the common law.
III. THE REASONABLE ROBOT
If, for instance, a man is born hasty and awkward, is always having accidents and hurting himself or his neighbors, no doubt his congenital defects will be allowed for in the courts of Heaven, but his slips are no less troublesome to his neighbors than if they sprang from guilty neglect.
—Oliver Wendell Holmes, Jr.174
A. When Negligence Is Strict
Negligence may function almost like strict liability for people with below average abilities. Individuals with special challenges and disabilities may not be capable of always exercising ordinary prudence and may be unable to maintain “a certain average of conduct.”175 This issue was at the heart of Vaughan v. Menlove176 in 1837, which con- cerned a defendant who lacked normal intelligence.177 The defense ar- gued that it would thus be unfair to hold him to the standard of an ordinary person and that he should instead be held to the standard of a person with below-average intelligence. The court disagreed, hold- ing that ordinary prudence should apply in every case of negligence.178 As Oliver Wendell Holmes, Jr., articulated in 1881, “The law consid- ers . . . what would be blameworthy in the average man, the man of ordinary intelligence and prudence, and determines liability by that. If we fall below the level in those gifts, it is our misfortune.”179 That re- mains the case today; a modern defendant cannot generally escape
174 O.W. HOLMES, JR., THE COMMON LAW 108 (1881).
175 Id. Holmes did distinguish between a lack of “intelligence and prudence” and “distinct
defect[s]” which he believed did not generally lead to strict liability. Id. at 108–10.
176 (1837) 132 Eng. Rep. 490, 492; 3 Bing. (N.C.) 468, 471.
177 Id. at 492.
178 Id. at 490, 492.
Instead, therefore, of saying that the liability for negligence should be co-extensive with the judgment of each individual, which would be as variable as the length of the foot of each individual, we ought rather to adhere to the rule which requires in all cases a regard to caution such as a man of ordinary prudence would observe. That was in substance the criterion presented to the jury in this case, and therefore the present rule must be discharged.
Id. at 493.
179 HOLMES, supra note 174, at 108.
 
36 531 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
liability for causing a motor vehicle accident because she has slow re- flexes, poor vision, or anxiety while driving.180
There are benefits to such a rule. Logistically, as Justice Tindal noted in Vaughan, it is difficult to take individual peculiarities into account and to determine a defendant’s actual mental state.181 Better for administrative purposes to work with an external, objective stan- dard than to prove individual capacities and state of mind. Substan- tively, the rule reinforces social norms, creates greater deterrent pressure, and strengthens each person’s right to demand normal con- duct of others.182 As Holmes articulated, damages caused by individu- als with reduced capabilities are no less burdensome than those caused by ordinary people. This rule thus benefits the general welfare, but at the cost of telling some individuals that their best is not good enough. Those with diminished capabilities drive at their own peril, or else perhaps “should refrain from operating an automobile” at all.183
B. The New Hasty and Awkward
Collectively, people are not the best drivers, even when they re- frain from drinking behind the wheel,184 falling asleep on the high- way,185 or colliding into police cars while playing Poke ́mon Go.186 But compared to computers? It will not be long until computers are safer than the average person and then safer than any human driver. Princi- ples of harm avoidance suggest that once it becomes practical to auto- mate, and once doing so is safer, a computer should become the new “reasonable person” or standard of care.
180 See, e.g., Roberts v. Ring, 173 N.W. 437, 437–38 (Minn. 1919).
181 Vaughan, 132 Eng. Rep. at 493.
182 See Holmes, supra note 150, at 154–55.
183 Roberts, 173 N.W. at 438. In this case, a seventy-seven-year-old defendant with defec-
tive sight and hearing was held liable for running over a seven-year-old boy when it was estab- lished that a reasonable driver could have stopped the car. Id.
184 See J. Michael Kennedy, Allowed in 26 States: Drinking and Driving: A Legal Mix, L.A. TIMES (Jan. 26, 1985), http://articles.latimes.com/1985-01-26/news/mn-13688_1_container-law (noting that until recently, it was even legal in many states to “sip[] on a Scotch and soda while cruising down the interstate”).
185 See David Boroff, Two Women Dead as Greyhound Bus Driver Falls Asleep at Wheel During California Crash; Driver was ‘Fatigued,’ Police Say, N.Y. DAILY NEWS (Jan. 19, 2016, 9:01 PM), http://www.nydailynews.com/news/national/greyhound-bus-crash-kills-2-injures-18-ar- ticle-1.2501658.
186 See Sarah Begley, Driver Hits Cop Car While Playing Poke ́mon Go. The Whole Thing Was Caught on Video, TIME (July 20, 2016), http://time.com/4414998/pokemon-go-hits-cop-body- cam/ (discussing a driver playing Poke ́mon Go who collided with a police car and had the inci- dent captured on video, and quoting the driver as saying, “That’s what I get for playing this dumb – game”).
 
2018] 532 THE REASONABLE COMPUTER 37
In practice, this would mean that instead of judging a defendant’s action against what a reasonable person would have done, the defen- dant would be judged against what a computer would have done. For instance, today a defendant might not be liable for striking a child running in front of their car if a reasonable driver would not have been able to stop immediately. But that person would soon be liable under the exact same circumstances if an automated car would have prevented the injury. In fact, it may be that the automated vehicle is only able to prevent such an accident because it has superhuman abili- ties. It may have software capable of ultrafast decisionmaking, monitors that surpass human senses, and external cameras that ex- pand peripheral view beyond that of a person.187
With the reasonable person test, jurors are asked to put them- selves in the shoes of a reasonable person and decide what that person would have done.188 It may be a challenge for a juror to follow that reasoning in the case of a reasonable computer (or reasonable robot or machine). The reasonable computer, however, is a far less nebulous and fictional concept than the reasonable person. The term “reasona- ble” in the context of a computer is an anthropomorphism to assist people conceptually. In fact, computers largely function according to fixed rules which—when all goes well—result in foreseeable behav- ior.189 Even those computers which can generate unpredictable behav- ior are still likely to be more predictable than people, particularly where such machines have been found to improve safety.190 It should be more or less possible to determine what a computer would have done in a particular situation.
To take a simple case, imagine an individual driving on dry pave- ment at forty miles per hour colliding with a child running into the road 150 feet ahead of the driver’s vehicle.191 To determine whether the driver is liable under the reasonable computer standard, a plaintiff could present a jury with evidence that when a child runs in front of the same make and model of car being operated by automated software under the same conditions, the vehicle stops in about 100 feet. Because the reasonable computer would not have collided with
187 See supra notes 116–19 and accompanying text.
188 See supra notes 65, 71 and accompanying text.
189 THOMAS A. PETERS, COMPUTERIZED MONITORING AND ONLINE PRIVACY 97 (1999).
Malfunctioning computers would not be “reasonable” computers.
190 See id.
191 See Why Your Reaction Time Matters at Speed, NAT’L HIGHWAY TRAFFIC SAFETY AD-
MIN. (Aug. 2015), www.nhtsa.gov/nhtsa/Safety1nNum3ers/august2015/S1N_Aug15_Speeding_ 1.html.
 
38 533 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
the child, the human driver would be liable. Juries would not need to take distraction into account, the reaction time of self-driving software would be known, and the breaking distance could be standardized if the driver’s vehicle could not directly be compared because it was not a vehicle type operated by self-driving software. Even in more com- plex cases, it should be easier to predict how a computer would have behaved than a person because computers are more predictable. Thus, it is possible to have a more objective test for the reasonable com- puter than for the reasonable person.
A defendant might argue that it is unfair for his best efforts to result in liability. A computer standard of care essentially makes peo- ple strictly liable for their accidental harms. That is the case now for below-average drivers, and the underlying rationale for the rule will not change when an above-average human driver becomes a below- average driver due to computers. It may appear unfair to impose lia- bility on human drivers for doing their best, but it would be more unfair to prevent accident victims from recovering for harms that would have been avoided had a robot been driving. It does not matter to an accident victim whether he was run over by a person or a computer.
Tort liability would not prohibit people from driving even at the point where computers become substantially safer than people. If that were a desired outcome it could be accomplished through command- and-control legislation.192 Instead, a computer standard of care would mean that people drive at their own risk. If a driver causes an acci- dent, he or she will be liable for the resultant damages. A tort-based incentive may be superior to an inflexible statutory mandate because there may be benefits to human driving unrelated to accidents, for instance, promoting freedom and autonomy.193 Individuals who partic- ularly value their freedom may still choose to drive and accept the consequences of their accidents.
While not outright prohibiting activities, a computer standard of care is likely to have a significant impact on behavior. Making individ- uals and businesses essentially strictly liable for their harms will strongly discourage certain undertakings. In the self-driving car con- text, it would likely result in far fewer human drivers as insurance
192 See Orly Lobel, The Renew Deal: The Fall of Regulation and the Rise of Governance in Contemporary Legal Thought, 89 MINN. L. REV. 342, 371–404 (2004) (discussing the trend from regulations to incentive-based regimes).
193 See generally Ryan & Deci, supra note 20, at 6–7 (arguing that people have three basic psychological needs: (1) connectedness, (2) autonomy, and (3) feeling competent).
 
2018] 534 THE REASONABLE COMPUTER 39
rates for traditional vehicles become prohibitively expensive relative to rates for self-driving cars.
A rule requiring automation at the time it first becomes available would be too harsh. Automatons may be prohibitively expensive or only available in limited quantities. That is particularly likely early in a technology’s lifecycle. It would be unfair to penalize people for not automating when doing so would be impossible or impractical. There- fore, to introduce a computer standard of care, a plaintiff should have to show by a preponderance of the evidence that a person was per- forming a task that could be performed by a computer and that it would have been practicable for the defendant to automate. This means that a defendant would not be judged against the standard of a computer operator where 1) no such operator existed at the time of the accident, 2) no computer operator was available to the defendant, 3) a computer operator was prohibitively expensive, or 4) there were other overriding interests for not automating (e.g., regulatory require- ments for a human operator). If Tesla could manufacturer a com- pletely safe autonomous vehicle but at a cost of $1 million dollars, it would not be reasonable to require consumers to automate.
C. Reasonable People Use Autonomous Computers
As an alternative to the reasonable computer standard, the rea- sonable person could be a person using an autonomous computer. For example, once self-driving cars become safer than traditional vehicles, a jury might find that it is unreasonable to drive yourself rather than to use a self-driving car. Applying the “reasonable person using an autonomous computer” standard to the earlier hypothetical involving a child running into the street, the human driver’s negligence would not be based on failing to stop in 100 feet as a self-driving car would have; rather, liability would be based on her driving in the first place. A reasonable person would not have driven.
Under either the reasonable person or reasonable computer stan- dard, a human driver would be compared with a self-driving car, but in different ways. With the reasonable computer standard, courts would evaluate the human driver’s proximally harmful act, whereas with the reasonable person standard, courts would evaluate the human driver’s a priori decision to automate (a bad decision would then be considered the harmful act). Maintaining the reasonable per- son standard would be more in line with the existing negligence re- gime, and it would be a less radical way to accomplish the goal of incentivizing automation to improve safety.

40 535 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
While keeping the reasonable person standard would be concep- tually easier, in practice it would be less desirable. The goal is to com- pare the harmful act of the person and computer, not to target the initial decision to automate. It is problematic to base liability on the decision to automate because it either must focus on the question of whether automation is generally or situationally beneficial. A general focus fails to consider instances in which a person will outperform a machine. A situational focus must still compare the harmful act of a person versus a computer.
It is likely that as autonomous computers are introduced they will be safer at automating certain activities than others. For instance, au- tomated computers working to diagnose disease may be superior to physicians at detecting certain conditions, but not others. Self-driving cars may be safer than human drivers on average, but not safer than professional or above-average drivers. Autonomous vehicles may also be safer under most conditions, but might be relatively poor at, for example, driving off road. So, while automation may generally im- prove safety, optimal accident reduction may require a mix of com- puter and human activity.
Suppose a self-driving car is ten times safer than a human driver generally, but only half as safe as a human driver in icy conditions. Now suppose a human driver encounters a patch of black ice and causes an accident under circumstances in which she would not be negligent by comparison to a reasonable human driver. If courts were to hold her to the standard of a reasonable computer, she would es- cape liability if the computer would have been unable to avoid the accident (which is likely if the computer is half as safe in icy condi- tions). If the reasonable person using an autonomous computer test focuses on whether an autonomous computer is generally safer, how- ever, she would be liable. That test would conclude that it would have been unreasonable not to use a self-driving car because self-driving cars are generally safer. This would penalize human action even when it would be preferred.
Alternately, the reasonable person using an autonomous com- puter evaluation could be situational. For instance, it could be reason- able not to use an autonomous computer, but only in icy conditions. However, this is just a more convoluted version of the reasonable computer test because it requires evaluating whether a computer would be safer than a person in a particular instance. That essentially asks how the computer would have acted in a situation—which is the

2018] 536 THE REASONABLE COMPUTER 41
reasonable computer standard.194 It would then require asking, based on that knowledge, which might be impractical for a person to have, whether an earlier decision to automate was reasonable. On top of that, it presupposes the ability to activate and deactivate automation as needed. In the black ice hypothetical, it could require the driver to know in advance of activating self-driving software whether there were icy conditions and how the computer would perform in icy con- ditions. It might require the driver to activate or deactivate automa- tion only during icy conditions or to understand whether the risk of using the computer in icy conditions outweighed the benefits of using the computer for other parts of the trip.
D. The Reasonable Computer Standard for Computer Tortfeasors
This Article proposes holding computer tortfeasors to a negli- gence standard and comparing their acts to the acts of a reasonable person after technology has advanced to the point that computers have been proven safer than people.195 It also proposes replacing the reasonable person standard with the reasonable computer standard, again, once this point has been reached.196 This means that computer tortfeasors would be held to the reasonable computer standard.
There will be instances in which it still makes sense to apply the reasonable person standard to computer tortfeasors. As described above, there will be cases in which a human defendant would not be judged against the standard of a computer, for instance, where auto- mation is prohibitively expensive or where computer operators are not widely available. We would not want to hold a computer tortfeasor to a higher standard than a human defendant. In some in- dustries, it may take decades after the introduction of autonomous technologies for the use of such technologies to become customary or to meet the criteria proposed earlier for adopting the reasonable com- puter standard.
Eventually, once a reasonable computer becomes the standard of care, it would also be the standard for computer tortfeasors. For in- stance, if a self-driving Audi collided with a child running in front of the vehicle, the negligence test could take into account the stopping times of self-driving Volvo cars. There are a variety of ways to deter- mine the reasonable computer standard, for example, considering the industry customary, average, or safest technology. Under any stan-
194 See supra text accompanying notes 184–93.
195 See supra text accompanying notes 184–93.
196 See supra text accompanying notes 184–93.
 
42 537 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1
dard, this is a different test than the current strict liability standard, in which the inquiry focuses on whether a product was defectively de- signed or its properties falsely represented.
As computers improve, the reasonable computer standard would grow stricter. That is alright, because once the reasonable computer is exponentially safer than a person, it is likely that computer tortfeasors will rarely cause accidents. At that point, the economic impact of tort liability on automation adoption may be slight, and the primary effect of the reasonable computer standard would be to internalize the cost of accidents on human tortfeasors. For certain types of automation, it may take a lifetime until computers are exponentially safer than people.
E. The Automation Problem
The impact of automation goes far beyond accident reduction. Just focusing on autonomous vehicles, the widespread adoption of this technology could have revolutionary benefits. It will allow people to be more productive and mobile, and it will reduce emissions and con- gestion.197 One autonomous vehicle could replace up to twelve normal cars.198 Given that the average automobile spends about ninety-five percent of its time sitting in place, self-driving cars may also eliminate the need for most parking.199 Getting rid of parking just in the United States would free up space the size of Connecticut and could allow redesigned, pedestrian-friendly urban areas.200 Automation will in- crease freedom for the disabled, blind, and unlicensed. It might elimi- nate traffic lights and the need for private car ownership.201 The net result of self-driving cars could be substantial environmental, eco- nomic, and social benefits.202
Driverless technologies may also result in the displacement of human workers, increased unemployment, greater wealth disparities, and a reduction of the tax base. Automation threatens the jobs of truck, bus, and taxi drivers who collectively make up about three per- cent of the working population.203 In other industries, automation has
197 DEP’T FOR TRANSPORT, THE PATHWAY TO DRIVERLESS CARS: SUMMARY REPORT AND ACTION PLAN 6 (2015).
198 Clive Thompson, No Parking Here, MOTHER JONES (Jan.–Feb. 2016), http:// www.motherjones.com/environment/2016/01/future-parking-self-driving-cars.
199 Id.
200 Id.
201 DEP’T FOR TRANSPORT, supra note 197, at 6.
202 Id.
203 RICHARD HENDERSON, INDUSTRY EMPLOYMENT AND OUTPUT PROJECTIONS TO 2024,
 
2018] 538 THE REASONABLE COMPUTER 43
resulted in reduced workforces.204 For instance, employment at com- puter and electronic companies decreased forty-five percent from 2001 to 2016.205 Employment at semiconductor makers decreased by half during the same period.206
These are all important issues to consider in formulating automa- tion policies, but tort law may not be the best mechanism to address these broader concerns.207 Ultimately, tort liability alone will not de- termine whether automation occurs. Consumer demand and the eco- nomics of automation will bring about increasing automation in the absence of laws prohibiting it.208 Tesla, for example, is planning to make all its cars self-driving, and Tesla is far from alone in automating vehicles.209 Billions of dollars have been invested in self-driving tech- nologies by at least forty-four corporations including Apple, Google, and General Motors.210
at 2 (2015); see AUSTL. BUREAU OF STATISTICS, 2011 CENSUS COMMUNITY PROFILES, http:// www.censusdata.abs.gov.au/census_services/getproduct/census/2011/communityprofile/0?open document&navpos=220 (last updated Jan. 12, 2017) (select “Working Population Profile”).
204 For example, WhatsApp had fifty-five employees when Facebook acquired it for $21.8 billion in 2014. Jon Swartz, Tech’s Gilded Glory Didn’t Mean Much to Trump’s Supporters, USA TODAY (Nov. 14, 2016), http://www.usatoday.com/story/tech/2016/11/14/techs-gilded-glory-didnt- mean-much-trumps-supporters/93598484/. Amazon, Tesla, and other companies have developed production lines that minimize the use of people. Id.
205 Id. 206 Id.
207 See, e.g., Priest, supra note 11, at 5–6.
208 See Brad Templeton, Robotaxi Economics, BRAD IDEAS (Sept. 8, 2016, 2:07 PM), http://
ideas.4brad.com/robotaxi-economics [https://perma.cc/T4JU-D866]; see also Who’s Self-Driving Your Car?, ECONOMIST (Sept. 22, 2016), http://www.economist.com/news/business/21707600-bat tle-driverless-cars-revs-up-whos-self-driving-your-car (noting a tight race between major tech- nology companies competing to make autonomous driving software due to financial expectations).
209 Tesla to Make All Its New Cars Self-Driving, BBC NEWS (Oct. 20, 2016), http:// www.bbc.co.uk/news/technology-37711489. Not all autonomous vehicles are created equal. A va- riety of technologies are in development to automate cars to a greater or lesser degree—ranging from driverless cars to self-parking vehicles. See generally SCIENCEWISE EXPERT RES. CTR., AU- TOMATED VEHICLES: WHAT THE PUBLIC THINKS (2014), http://www.sciencewise-erc.org.uk/cms/ assets/Uploads/Automated-Vehicles-Update-Jan-2015.pdf.
210 44 Corporations Working on Autonomous Vehicles, CB INSIGHTS (May 18, 2017), https://www.cbinsights.com/blog/autonomous-driverless-vehicles-corporations-list/ [https:// perma.cc/JM38-TR7D]; see Investment into Auto Tech on Pace to Break Annual Records, CB INSIGHTS (July 14, 2016), https://www.cbinsights.com/blog/auto-tech-funding-h1-2016/ [https:// perma.cc/ZTE9-MH7E].
 
44 539 THE GEORGE WASHINGTON LAW REVIEW [Vol. 86:1 CONCLUSION
In the coming decades, as people and machines compete in an expanding array of activities, it is vital that appropriate legal and pol- icy frameworks be put in place to guide the development of technol- ogy and to ensure its widespread benefits.211 It is particularly important that tort liability be structured to optimize accident deterrence.
Technological advances present new challenges to existing frameworks. At some point in the future, there are likely to be few or no activities for which computers cannot outperform people.212 Self- driving cars may eventually be a thousand times safer than the best human driver.213 At some point, computers will cause so little harm that the economics of negligence versus strict liability will be irrele- vant. Autonomous computers will have become so ubiquitous that the constantly improving reasonable computer should be the benchmark for most or all areas of accident law. In fact, autonomous computers are likely to become so safe that regulatory mandates for automation will be desirable.
In the meantime, creating incentives for developing and adopting safer technologies could prevent countless accidents. It has become acceptable for more than a million people a year to die in traffic acci- dents worldwide, but only because there has not been a reasonable alternative until now.214 We could soon be living in a world where no one dies from unintended injury, or from medical error for that mat- ter. Once the third and fourth leading causes of death are eliminated, that would just leave us to deal with the leading two causes of death:
211 See, e.g., Press Release, European Parliament, Robots: Legal Affairs Committee Calls for EU-Wide Rules (Jan. 12, 2017), http://www.europarl.europa.eu/sides/getDoc.do?type=IM- PRESS&reference=20170110IPR57613&language=EN&format=XML (“EU rules for the fast- evolving field of robotics, to settle issues such as compliance with ethical standards and liability for accidents involving driverless cars, should be put forward by the EU Commission, urged the Legal Affairs Committee . . . .”).
212 See generally RAY KURZWEIL, THE SINGULARITY IS NEAR 7 (2005) (predicting that machines will be able to automate all human work in “a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed”).
213 See Dredge, supra note 171.
214 See Press Release, United Nations Secretary-General, Traffic Accidents Kill 1.3 Million
People Each Year, but with Commitment Roads Can Be Made Safer for All, Secretary General Says in Video Message (May 6, 2013), https://www.un.org/press/en/2013/sgsm15005.doc.htm [https://perma.cc/B2QQ-UN59].
 
2018] 540 THE REASONABLE COMPUTER 45 cardiovascular disease and cancer. Automation may eliminate those as
well.215
 215 See Abbott, supra note 19, at 1118 (hypothesizing about how artificial intelligence could cure cancer in an article about creative computers that are already independently generating patentable subject matter).

541
  Punishing Artificial Intelligence: Legal Fiction or Science Fiction
Ryan Abbott†* and Alex Sarch**
Whether causing flash crashes in financial markets, purchasing illegal drugs, or running over pedestrians, AI is increasingly engaging in activity that would be criminal for a natural person, or even an artificial person like a corporation. We argue that criminal law falls short in cases where an AI causes certain types of harm and there are no practically or legally identifiable upstream criminal actors. This Article explores potential solutions to this problem, focusing on holding AI directly criminally liable where it is acting autonomously and irreducibly. Conventional wisdom holds that punishing AI is incongruous with basic criminal law principles such as the capacity for culpability and the requirement of a guilty mind.
Drawing on analogies to corporate and strict criminal liability, as well as familiar imputation principles, we show how a coherent theoretical case can be constructed for AI punishment. AI punishment could result in general deterrence and expressive benefits, and it need not run afoul of negative limitations such as punishing in excess of culpability. Ultimately, however, punishing AI is not justified, because it might entail significant costs and it would certainly require radical legal changes. Modest changes to existing criminal laws that target persons, together with potentially expanded civil liability, are a better solution to AI crime.
TABLE OF CONTENTS
INTRODUCTION ................................................................................... 325
I.
†
*
** Alex Sarch, Reader (Associate Professor) in Legal Philosophy, University of Surrey School of Law. Thanks to Antony Duff, Sandra Marshall, Mark D’Souza, and Steve Bero for their insightful comments.
323
ARTIFICIAL INTELLIGENCE AND PUNISHMENT........................... 329 A. Introduction to Artificial Intelligence ................................. 329
Copyright © 2019 Ryan Abbott and Alex Sarch.
 Ryan Abbott, Professor of Law and Health Sciences, University of Surrey School of Law and Adjunct Assistant Professor of Medicine, David Geffen School of Medicine at University of California, Los Angeles.

542
  324
University of California, Davis [Vol. 53:323
A Framework for Understanding AI Crime ........................ 332 A Mainstream Theory of Punishment ................................. 337 1. Affirmative Reasons to Punish ................................... 338 2. Negative (Retributive) Limitations ............................ 341 3. Alternatives to Punishment........................................ 343 4. Putting the Pieces Together ....................................... 344
B. C.
II. THE AFFIRMATIVE CASE ........................................................... 344
A. Consequentialist Benefits ................................................... 344
B. Expressive Considerations ................................................. 346
III. RETRIBUTIVE AND CONCEPTUAL LIMITATIONS.......................... 349
A. The Eligibility Challenge ................................................... 349
1. Answer 1: Respondeat Superior ................................. 350
2. Answer 2: Strict Liability............................................ 352
3. Answer3:AFrameworkforDirectMensRea
Analysis for AI ............................................................ 354
B. Further Retributivist Challenges: Reducibility and
Spillover ............................................................................ 360 1. Reducibility................................................................361
2. Spillover ...................................................................... 362
C. Not Really Punishment?..................................................... 364 IV. FEASIBLE ALTERNATIVES ........................................................... 368
A. First Alternative: The Status Quo....................................... 369
1. WhattheAIcriminalgapisnot:reducibleharmful conduct by AI ............................................................. 369
2. WhattheAIcriminalgapis:irreduciblecriminal conduct by AI ............................................................. 373
B. The Costs of Punishing AI.................................................. 374
C. Second Alternative: Minimally Extending Criminal Law.... 378
D. Third Alternative: Moderate Changes to Civil Liability...... 381
E. Concluding Thoughts ......................................................... 383

543
  2019] Punishing Artificial Intelligence 325 INTRODUCTION
In 2015, an artist going by the moniker “Random Darknet Shopper” (RDS) purchased Ecstasy and a Hungarian passport for display in an art exhibit.1 This was part of a performance project where RDS was given $100 in the cryptocurrency bitcoin each week to make a purchase from an online marketplace. The items were then shipped to a Swiss art gallery and put on exhibit. After learning about the exhibit from social media, Swiss police took RDS into custody along with the purchases.2
What makes this story interesting for our purposes is that RDS was an artificial intelligence (“AI”), and hardly the first to have a run-in with law enforcement.3 If RDS had been a natural person located in the United States, it could be criminally prosecuted under U.S. law.4 For that matter, entities involved in this activity other than RDS could also be criminally prosecuted, such as those supplying the bitcoin and hosting the exhibition.5 Luckily for RDS and crew, the Swiss authorities were art fans.6
Cases like this will pose new challenges, including for criminal law doctrine.7 The RDS case may be relatively straightforward, but programs exist that are autonomous, decentralized, and “unstoppable.”8 What if
1 Arjun Kharpal, Robot with $100 Bitcoin Buys Drugs, Gets Arrested, CNBC (Apr. 22, 2015, 5:09 AM), https://www.cnbc.com/2015/04/21/robot-with-100-bitcoin-buys- drugs-gets-arrested.html.
2 See id.
3 See Matt Novak, Was This the First Robot Ever Arrested?, GIZMODO (Feb. 18, 2014,
12:00 PM), https://paleofuture.gizmodo.com/was-this-the-first-robot-ever-arrested- 1524686968 (describing police confiscation in 1982 of a robot: “The police considered citing [its owner] for failing to obtain a permit for advertising . . . but no charges were filed and the robot was ultimately returned.”). Robot encounters with law enforcement are becoming more common. See, e.g., Peter Dockrill, A Robot Was Just ‘Arrested’ by Russian Police, SCI. ALERT (Sept. 20, 2016), https://www.sciencealert.com/a-robot-was- just-arrested-by-russian-police.
4 See 21 U.S.C. § 841(a)(1) (2019) (criminalizing distribution and possession with intent to distribute a controlled substance).
5 See 18 U.S.C. § 2(a) (2019) (criminalizing aiding and abetting offenses).
6 Random Darknet Shopper was eventually returned to its creators together with
all of the purchases except the Ecstasy. See Kharpal, supra note 1 (noting the prosecutor’s comment that “the possession of Ecstasy was indeed a reasonable means for the purpose of sparking public debate about questions related to the exhibition”). Apparently, the Hungarian passport was also returned. See id.
7 See Christopher Markou, We Could Soon Face a Robot Crimewave. . .The Law Needs to Be Ready, CONVERSATION (Apr. 11, 2017, 9:36 AM), https://theconversation. com/we-could-soon-face-a-robot-crimewave-the-law-needs-to-be-ready-75276.
8 See infra Part I.A (discussing The Decentralized Autonomous Organization (“The DAO”)).
 
544
  326 University of California, Davis [Vol. 53:323
RDS had been open source software that individuals from around the world independently helped program? What if RDS was instead “Random Shopper,” designed to purchase necessities for college dorms while relying on machine learning to improve? What if it had been initially programmed to only purchase items from Amazon, but learned from user content that some necessities could be purchased at lower cost from other websites, and that a broader understanding of “necessities” exists? If Random Shopper autonomously buys Ecstasy in a manner not reasonably foreseeable to its developers, should those individuals be criminally liable? For that matter, who should count as its developers, and which ones would be liable? Should its owners be liable, and what if it has no owners? Should its users be liable, and what if it has no users? Perhaps Random Shopper itself should be held criminally liable.
The possibility of directly criminally punishing AI is receiving increased attention by the popular press and legal scholars alike.9 Perhaps the best-known defender of punishing AI is Gabriel Hallevy. He contends that “[w]hen an AI entity establishes all elements of a specific offense, both external and internal, there is no reason to prevent imposition of criminal liability upon it for that offense.”10 In his view, “[i]f all of its specific requirements are met, criminal liability may be imposed upon any entity — human, corporate or AI entity.”11 Drawing on the analogy to corporations,12 Hallevy asserts that “AI entities are taking larger and larger parts in human activities, as do corporations,” and he concludes that “there is no substantive legal difference between the idea of criminal liability imposed on corporations and on AI entities.”13 “Modern times,” he contends, “warrant modern legal
9 See, e.g., Gabriel Hallevy, The Punishibility of Artificial Intelligence Technology, in LIABILITY FOR CRIMES INVOLVING ARTIFICIAL INTELLIGENCE SYSTEMS 185-229 (2014); J.K.C. Kingston, Artificial Intelligence and Legal Liability, in RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XXXIII: INCORPORATING APPLICATIONS AND INNOVATIONS IN INTELLIGENT SYSTEMS XXIV 269 (Max Bramer & Miltos Petridis eds., 2016), https://arxiv.org/pdf/1802.07782.pdf; Christina Mulligan, Revenge Against Robots, 69 S.C. L. REV. 579, 580 (2018); Jeffrey Wale & David Yuratich, Robot Law: What Happens If Intelligent Machines Commit Crimes?, CONVERSATION (July 1, 2015, 8:06 AM), http://theconversation.com/robot-law-what-happens-if-intelligent-machines-commit- crimes-44058; infra Part I.A (discussing The DAO).
10 Gabriel Hallevy, The Criminal Liability of Artificial Intelligence Entities — From Science Fiction to Legal Social Control, 4 AKRON INTELL. PROP. J. 171, 191 (2010).
11 Id. at 199.
12 See id. at 200 (asking why AI entities should be treated “different from corporations”).
13 Id. at 200-01.
 
545
  2019] Punishing Artificial Intelligence 327
measures.”14 More recently, Ying Hu has subjected the idea of criminal liability for AI to philosophical scrutiny and made a case “for imposing criminal liability on a type of robot that is likely to emerge in the future,” insofar as they may employ morally sensitive decision-making algorithms.15 Her arguments likewise draw heavily on the analogy to corporate criminal liability.16
In contrast to AI punishment expansionists like Hallevy and Hu, skeptics might be inclined to write off the idea of punishing AI from the start as conceptual confusion — akin to hitting one’s computer when it crashes. If AI is just a machine, then surely the fundamental concepts of the criminal law like culpability — a “guilty mind” that is characterized by insufficient regard for legally protected values17 — would be misplaced. One might think the whole idea of punishing AI can be easily dispensed with as inconsistent with basic criminal law principles.
The idea of punishing AI is due for fresh consideration. This Article takes a measured look at the proposal, informed by theory and practice alike. We argue punishment of AI cannot be categorically ruled out. Harm caused by a sophisticated AI may be more than a mere accident where no wrongdoing is implicated. Some AI-generated harms may stem from difficult-to-reduce behaviors of an autonomous system,
14 Id. at 199.
15 Ying Hu, Robot Criminals, 52 MICH. J.L. REFORM 487, 531 (2019); see also id. at
490 (“[A]n argument can be made for robot criminal liability, provided that the robot satisfies three threshold conditions . . . . [T]he robot must be (1) equipped with algorithms that can make nontrivial morally relevant decisions; (2) capable of communicating its moral decisions to humans; and (3) permitted to act on its environment without immediate human supervision.”).
16 See Ying Hu, Robot Criminal Liability Revisited, in DANGEROUS IDEAS IN LAW 494, 497-98 (Jin Soo Yoon, Sang Hoon Han & Seong Jo Ahn eds., 2018) (arguing that corporations are “structurally similar” to “robots that are equipped with machine learning algorithms to determine the appropriate course of actions in specific circumstances,” and concluding that “if there is reason to treat corporations as moral agents, there is reason to treat sophisticated robots as moral agents as well”); Hu, supra note 15, at 520-21 (“One may argue that a smart robot can act intentionally in the same way that a corporation can. A robot’s moral algorithms are functionally similar to a corporation’s internal decision structure . . . . By analogy, . . . any act made pursuant to a smart robot’s moral algorithms is an act done for the robot’s own reasons and would therefore amount to an intentional action.”). Unlike Hu, we do not argue that AIs have genuine moral responsibility. We focus on the legal notion of culpability, which involves institutional design constraints that allow it to diverge from moral responsibility or blameworthiness.
17 Alexander Sarch, Who Cares What You Think? Criminal Culpability and the Irrelevance of Unmanifested Mental States, 36 L. & PHIL. 707, 709 (2017) [hereinafter Who Cares].
 
546
  328 University of California, Davis [Vol. 53:323
whose actions resemble those of other subjects of the criminal law, especially corporations. These harms may be irreducible where, for a variety of reasons, they are not directly attributable to the activity of a particular person or persons.18 Corporations similarly can directly face criminal charges when their defective procedures generate condemnable harms19 — particularly in scenarios where structural problems in corporate systems and processes are difficult to reduce to the wrongful actions of individuals.20
It is necessary to do the difficult pragmatic work of thinking through the theoretical costs and benefits of AI punishment, how it could be implemented into criminal law doctrine, and to consider the alternatives. Our primary focus is not what form AI punishment would take, which could directly target AIs through censure, deactivation, or reprogramming, or could involve negative outcomes directed at natural persons or companies involved in the use or creation of AI.21 Rather, our focus is the prior question of whether the doctrinal and theoretical commitments of the criminal law can be reconciled with criminal liability for AI.
Our inquiry focuses on the strongest case for punishing AI: scenarios where crimes are functionally committed by machines and there is no identifiable person who has acted with criminal culpability. We call these Hard AI Crimes. This can occur when no person has acted with criminal culpability, or when it is not practicably defensible to reduce an AI’s behavior to bad actors. There could be general deterrent and expressive benefits from imposing criminal liability on AI in such scenarios. Moreover, the most important negative, retributivist-style limitations that apply to persons need not prohibit AI punishment. On the other hand, there may be costs associated with AI punishment: conceptual confusion, expressive costs, spillover, and rights creep.22 In
18 See infra Part II.B.
19 See MODEL PENAL CODE § 2.07 (AM. LAW INST. 1962) (outlining conditions under
which a corporation could be convicted of an offense).
20 See William S. Laufer, Corporate Bodies and Guilty Minds, 43 EMORY L.J. 647, 664- 68 (1994) (outlining prevalent models of “genuine corporate culpability” including proactive fault, reactive fault, corporate ethos, and corporate policy); infra notes 166– 168 and accompanying text (discussing ways to defend the irreducibility of corporate culpability).
21 See Hu, supra note 15, at 529-30 (discussing the question of how a robot should be punished, and proposing “a range of measures [that] might be taken to ensure that the robot commits fewer offenses in the future”);
  “robot death penalty”). 22 See infra Part III.
Mark A. Lemley & Bryan Casey,
 Remedies for Robots 86 U. CHI. L. REV. 1311, 1316, 1389-93 (2019)
(discussing the

547
  2019] Punishing Artificial Intelligence 329
the end, our conclusion is this: While a coherent theoretical case can be made for punishing AI, it is not ultimately justified in light of the less disruptive alternatives that can provide substantially the same benefits.
This Article proceeds as follows. Part I provides a brief background of AI and “AI crime.” It then provides a framework for justifying punishment that considers affirmative benefits, negative limitations, and feasible alternatives. Part II considers potential benefits to AI punishment, and argues it could provide general deterrence and expressive benefits. Part III examines whether punishment of AI would violate any of the negative limitations on punishment that relate to desert, fairness, and the capacity for culpability. It finds that the most important constraints on punishment, such as requiring a capacity for culpability for it to be appropriately imposed, would not be violated by AI punishment.
Finally, Part IV considers feasible alternatives to AI punishment. It argues the status quo is or will be inadequate for properly addressing AI crime. While direct AI punishment is a solution, this would require problematic changes to criminal law. Alternately, AI crime could be addressed through modest changes to criminal laws applied to individuals together with potentially expanded civil liability. We argue that civil liability is generally preferable to criminal liability for AI activity as it is proportionate to the scope of the current problem and a less significant departure from existing practice with fewer costs. In this way, the Article aims to map out the possible responses to the problem of harmful AI activity and makes the case for approaching AI punishment with extreme caution.
I. ARTIFICIAL INTELLIGENCE AND PUNISHMENT A. Introduction to Artificial Intelligence
We use the term “AI” to refer to a machine that is capable of completing tasks otherwise typically requiring human cognition.23 AI only sometimes has the ability to directly act physically, as in the case of a “robot,” but it is not necessary for an AI to directly affect physical activity to cause harm (as the RDS case demonstrates).
23 AI lacks a standard definition, but its very first definition in 1955 holds up reasonably well: “[T]he artificial intelligence problem is taken to be that of making a machine behave in ways that would be called intelligent if a human were so behaving.” J. MCCARTHY ET AL., A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE (1955), http://www-formal.stanford.edu/jmc/history/dartmouth/ dartmouth.html.
 
548
  330 University of California, Davis [Vol. 53:323
AI is rapidly improving, driven by advances in software, computing power, and big data.24 Hardly a day goes by without a new report of some impressive feat achieved by AI. In 2017, Alphabet’s flagship DeepMind AI beat the world champion of the board game Go.25 This was considered an important feat in the AI community, because of the sheer complexity of the game.26 There are more possible Go board configurations than there are atoms in the universe.27 Thus, a machine designed to play Go cannot simply be preprogrammed with optimal predetermined moves, or solely rely on a brute force approach to considering a large number of future moves.28 Go was the last traditional board game in which people had been able to outperform machines.29
In some areas, AI already makes significant practical contributions. For instance, Google Translate supports more than 100 languages, including 37 by photo input, 32 by voice input, and 27 in “augmented reality mode.”30 The increasing prevalence and capability of AI will lead to widespread social benefit, but will also cause harm. Virtually all activity involves a risk of harm, and as AI comes to do more it will inevitably cause more harm.31
A few features of AI are important to highlight. First, AI has the potential to act unpredictably.32 Some leading AIs rely on machine learning or similar technologies which involve a computer program, initially created by individuals, further developing in response to data without explicit programming.33 This is one means by which AI can
24 See Ryan Abbott, Everything Is Obvious, 66 UCLA L. REV. 2, 23-28 (2019).
25 See id. at 24.
26 See id.
27 See id.
28 See id.
29 See id.
30 GOOGLE TRANSLATE, https://translate.google.com/intl/en/about/languages/ (last
visited Oct. 9, 2019).
31 See, e.g., Daisuke Wakabayashi, Self-Driving Uber Car Kills Pedestrian in Arizona,
Where Robots Roam, N.Y. TIMES (Mar. 19, 2018), https://www.nytimes.com/2018/03/19/ technology/uber-driverless-fatality.html.
32 See, e.g., Taha Yasseri, Never Mind Killer Robots — Even the Good Ones Are Scarily Unpredictable, PHYS.ORG (Aug. 25, 2017), https://phys.org/news/2017-08-mind-killer- robots-good-scarily.html; Why Did the Neural Network Cross the Road?, AI WEIRDNESS (2018), http://aiweirdness.com/post/174691534037/why-did-the-neural-network-cross- the-road (describing a programmer who made her machine learning algorithm attempt to tell jokes).
33 See, e.g., Davide Castelvecchi, Can We Open the Black Box of AI?, NATURE (Oct. 5, 2016), https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731.
 
549
  2019] Punishing Artificial Intelligence 331
engage in activities its original programmers may not have intended or foreseen.34
Second, AI has the potential to act unexplainably. It may be possible to determine what an AI has done, but not how or why it acted as it did.35 This has led to some AIs being described as “black box” systems.36 For instance, an algorithm may refuse a credit application but not be able to articulate why the application was rejected.37 That is particularly likely in the case of AIs that learn from data, and which may have been exposed to millions or billions of data points.38 Even if it is theoretically possible to explain an AI outcome, it may be impracticable given the potentially resource intensive nature of such inquiries, and the need to maintain earlier iterative versions of AI and specific data.
Third, AI may act autonomously. For our purposes, that is to say an AI may cause harm without being directly controlled by an individual. Suppose an individual creates an AI to steal financial information by mimicking a bank’s website, stealing user information, and posting that information online. While the theft may be entirely reducible to an individual who is using the AI as a tool, the AI may continue to act in harmful ways without further human involvement. It may even be the case that the individual who sets an AI in motion is not able to regain control of the AI, which could be by design.39
Fourth, while AI can already outperform people in spectacular fashion in some domains, like playing board games, in other domains AI is not even competitive with toddlers.40 That is because all AI is
34 There has been a recent focus on biased decisions by machine learning algorithms — sometimes due to a programmer’s implicit bias, sometimes due to biased training data. See, e.g., Chris DeBrusk, The Risk of Machine-Learning Bias (and How to Prevent It), MIT SLOAN MGMT. REV. (Mar. 26, 2018), https://sloanreview.mit.edu/article/the-risk-of- machine-learning-bias-and-how-to-prevent-it/.
35 See, e.g., Castelvecchi, supra note 33.
36 Id.
37 See id.
38 See id.
39 “The DAO” was the most famous attempt to create a decentralized autonomous
organization. See Samuel Falkon, The Story of the DAO